<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>概率统计随机过程 on SurprisedCat</title><link>https://surprisedcat.github.io/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/</link><description>Recent content in 概率统计随机过程 on SurprisedCat</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2020–2021, SurprisedCat; all rights reserved.</copyright><lastBuildDate>Sat, 26 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://surprisedcat.github.io/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/index.xml" rel="self" type="application/rss+xml"/><item><title>概率统计随机过程之相关分析与因果推断</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/</link><pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/</guid><description>
&lt;h2 id="概率统计随机过程之相关分析与因果推断">概率统计随机过程之相关分析与因果推断&lt;!-- omit in toc -->&lt;/h2>
&lt;p>因果关系是人类不断探寻的深刻议题，不过想要探究因果联系并不是那么容易的，因此很多学者都会退一步从更弱的关联性分析入手，尤其在大数据时代，关联性的作用也不容小觑。因此，统计学上的关系分析是非常重要的一个环节。本文主要讲解分类数据和数值数据的列联分析和方差分析等内容。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#因果分析的复杂性">因果分析的复杂性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分类数据的chi2拟合优度检验">分类数据的&lt;span class="math">\(\\chi^2\)&lt;/span>拟合优度检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#列联分析">列联分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析analysis-of-variance-anova">方差分析（Analysis of variance, ANOVA）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单因素方差分析">单因素方差分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#双因素方差分析">双因素方差分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态性检验">正态性检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#w检验">W检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差齐性检验">方差齐性检验&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="因果分析的复杂性">因果分析的复杂性&lt;/h2>
&lt;p>统计学上如果想要进行因果分析，通常会有如下图的阶段：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/统计学探索变量间关系.png" alt="统计学探索变量间关系" />&lt;p class="caption">统计学探索变量间关系&lt;/p>
&lt;/div>
&lt;p>首先，查看变量间是否具有关联性，没有关联性的就是相互独立的变量，一个变量的变化并不会对另一个变量产生影响；当发现两个变量具备关联性时，我们还得查看关联性的强弱，是强相关还是弱相关；在之后，需要检查这种相关性是不是有什么其他隐含的因素，比如二者都是同一个原因的结果，二者本身不具备因果性；最后才能进一步确认因果性。在确认因果性时，一般通过以下模型实现：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/因果关系理论抽象.drawio.svg" alt="因果关系理论抽象.drawio.svg" />&lt;p class="caption">因果关系理论抽象.drawio.svg&lt;/p>
&lt;/div>
&lt;p>在复杂的统计模型中，其中上边的每一步也需要仔细、系统的研究。当然上述只是因果分析的简要流程。&lt;/p>
&lt;p>在因果分析中，对于自变量、因变量不同的类型，也有着不同的分析方法，对于本科生水平大概如下：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/统计分析一般方法.png" alt="统计分析一般方法" />&lt;p class="caption">统计分析一般方法&lt;/p>
&lt;/div>
&lt;p>其中，数据类型一般可以分成定性的分类数据（品质数据）、顺序数据和定量的数值数据，数值数据还可分为离散数据和连续数据。这些数据的级别是由低到高的，高阶数据可以转换为低阶数据，例如连续数据可以归并成离散数据，数值数据可以按照大小排成顺序数据，顺序数据也可以分成几类形成分类数据。但是低阶数据无法转换成高阶数据。&lt;/p>
&lt;p>对于硕士研究生可能需要掌握到下面的成程度：&lt;/p>
&lt;p>分类数据统计分析方法：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/分类数据统计分析方法.png" alt="分类数据统计分析方法" />&lt;p class="caption">分类数据统计分析方法&lt;/p>
&lt;/div>
&lt;p>数值数据统计分析方法：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/数值数据统计分析方法.png" alt="数值数据统计分析方法" />&lt;p class="caption">数值数据统计分析方法&lt;/p>
&lt;/div>
&lt;p>对于博士生大概是这样的：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/组间差异检验.png" alt="组间差异检验.png" />&lt;p class="caption">组间差异检验.png&lt;/p>
&lt;/div>
&lt;p>接下来我将有选择的挑几个阐明。首先，先从本科阶段的内容说起吧。&amp;lt;( ￣^￣)&amp;gt;&lt;/p>
&lt;h2 id="分类数据的chi2拟合优度检验">分类数据的&lt;span class="math">\(\chi^2\)&lt;/span>拟合优度检验&lt;/h2>
&lt;p>对于离散分布，比如对于二项分布&lt;span class="math">\(B(n,p)\)&lt;/span>，我们希望验证其服从二项分布，进行了&lt;span class="math">\(M\)&lt;/span>次&lt;span class="math">\(n\)&lt;/span>重伯努利实验得到&lt;span class="math">\(M\)&lt;/span>个值，其中实验成功次数为&lt;span class="math">\(0,1,2,3,\dotsb,n\)&lt;/span>的频数分别为&lt;span class="math">\(m_0,m_1,m_2,\dotsb,m_n,M=\sum\limits_{i=0}^n m_i\)&lt;/span>。那么如果想要验证该n重伯努利分布得到的随机变量&lt;span class="math">\(X\)&lt;/span>是服从二项分布&lt;span class="math">\(B(n,p)\)&lt;/span>的，需要怎么做呢？一个简单可用的方法是&lt;span class="math">\(\chi^2\)&lt;/span>拟合优度检验。&lt;/p>
&lt;p>分布的拟合检验是在随机变量&lt;span class="math">\(X\)&lt;/span>分布未知时的检验（因为我们要验证的即是其分布类型），因此不同于参数的假设检验问题，属于非参数检验。一般而言分类数据的结果是频数，&lt;span class="math">\(\chi^2\)&lt;/span>检验是对分类数据的频数进行分析的统计方法。&lt;/p>
&lt;p>之所以叫&lt;span class="math">\(\chi^2\)&lt;/span>拟合优度检验，是因为在1900年，统计学四大天王之一卡尔-皮尔逊证明提出对于实验统计出来的频数&lt;span class="math">\(f_i,i\in \mathrel{\Theta}\)&lt;/span>，它和理论期望的频数&lt;span class="math">\(e_i=M×p_i\)&lt;/span>，（&lt;span class="math">\(M\)&lt;/span>为总数，&lt;span class="math">\(p_i\)&lt;/span>为对应概率）存在以下关系： &lt;span class="math">\[
X^2=\sum_{i\in\mathrel{\Theta}} \frac{(f_i-e_i)^2}{e_i}\sim \chi^2(|\mathrel{\Theta}|-1)\tag{1}
\]&lt;/span> 即构造的统计量&lt;span class="math">\(X^2=\sum\limits_{i\in\mathrel{\Theta}} \frac{(f_i-e_i)^2}{e_i}\)&lt;/span>应该服从自由度为&lt;span class="math">\(|\mathrel{\Theta}|-1\)&lt;/span>的卡方分布，&lt;span class="math">\(|\mathrel{\Theta}|\)&lt;/span>为实验可能出现结果的样数。并且期望频数越大，该分布与卡方分布越接近。当期望频数大于5时，与卡方分布符合比较好。此外，卡方分布只适用于观测数均不小于5的大样本场合。&lt;/p>
&lt;p>我们从数学角度单纯地看式（1）,&lt;span class="math">\((f_i-e_i)^2\)&lt;/span>实际是指实验做出的实际结果与理论分布差值的平方，体现的是实际与理论的差异，这个值越大，说明二者越不相符，分母的&lt;span class="math">\(e_i\)&lt;/span>更像是正则系数，降低绝对差值的比例影响。所以，从式（1）也可以看出随机变量&lt;span class="math">\(X^2\)&lt;/span>越大，我们越倾向于不认同实验分布服从理论分布，从数学角度讲是拒绝域在右侧的&lt;span class="math">\(\chi^2\)&lt;/span>检验。这种方式最早由卡尔-皮尔逊提出，通常也称之为皮尔逊&lt;span class="math">\(\chi^2\)&lt;/span>拟合优度检验。&lt;/p>
&lt;p>下面我们用泰坦尼克号的存活率与性别是否相关举一个简单的例子：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/卡方优度检验.png" alt="卡方优度检验" />&lt;p class="caption">卡方优度检验&lt;/p>
&lt;/div>
&lt;p>拟合优度检验只针对一个分类变量进行检验，如果需要对两个或多个分类变量进行出里就需要列联分析。&lt;/p>
&lt;h2 id="列联分析">列联分析&lt;/h2>
&lt;p>如果我们希望&lt;strong>分析两个或多个分类变量之间的是否独立，可以使用列联表&lt;/strong>。列联分析是一种&lt;strong>独立性检验&lt;/strong>，通常列联表常用于分类数据的两两分析，多维数据的多维列联表不太直观，一般用的较少。&lt;/p>
&lt;p>原理也是使用卡方统计量。&lt;/p>
&lt;p>列联表中，若两个分类变量&lt;span class="math">\(A,B\)&lt;/span>，其中&lt;span class="math">\(A\)&lt;/span>有&lt;span class="math">\(r\)&lt;/span>个可取值，记为&lt;span class="math">\(A_1,\dotsb,A_r\)&lt;/span>，&lt;span class="math">\(B\)&lt;/span>有&lt;span class="math">\(c\)&lt;/span>个可取值，记为&lt;span class="math">\(B_1,\dotsb,B_c\)&lt;/span>，从总体中抽取样本容量为&lt;span class="math">\(n\)&lt;/span>的样本，设其中有&lt;span class="math">\(n_{ij}\)&lt;/span>个个体，其属性为&lt;span class="math">\(A_i,B_j\)&lt;/span>，称其为频数，我们根据上述信息可制作频数列联表：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;span class="math">\(A\setminus B\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_1\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_j\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_c\)&lt;/span>&lt;/th>
&lt;th align="center">行和&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{11}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{1j}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{1c}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{1\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_i\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{i1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{ij}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{ic}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{i\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_r\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{r1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{rj}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{rc}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{r\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">列和&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{\cdot 1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{\cdot j}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{\cdot c}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>以上列联表是会根据实验或数据集给出的数据制作而成，都是已知的数据。我们将上述表中数据都除以总数&lt;span class="math">\(n\)&lt;/span>得到频率/概率表：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;span class="math">\(A\setminus B\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_1\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_j\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_c\)&lt;/span>&lt;/th>
&lt;th align="center">行和&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{11}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{1j}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{1c}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{1\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_i\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{i1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{ij}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{ic}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{i\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_r\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{r1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{rj}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{rc}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{r\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">列和&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{\cdot 1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{\cdot j}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{\cdot c}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(1\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>根据频率/概率关系有&lt;span class="math">\(\sum_i\sum_j p_{ij}=1, \sum_j p_{\cdot j}=1, \sum_i p_{i\cdot}=1\)&lt;/span>。这个表格的目的就是计算&lt;span class="math">\(p_{i\cdot}\)&lt;/span>和&lt;span class="math">\(p_{\cdot j}\)&lt;/span>。如果变量&lt;span class="math">\(A,B\)&lt;/span>是独立的，那么会有&lt;span class="math">\(p_{ij}=p_{i\cdot}p_{\cdot j}\)&lt;/span>，但是实际统计频率&lt;span class="math">\(p_{ij}\)&lt;/span>必然和理论值有所偏差，我们用计算得到的&lt;span class="math">\(p_{i\cdot}\)&lt;/span>和&lt;span class="math">\(p_{\cdot j}\)&lt;/span>相乘，得到独立假设下的理论概率&lt;span class="math">\(\hat{p}_{ij}=p_{i\cdot}×p_{\cdot j}\)&lt;/span>，再乘以总数&lt;span class="math">\(n\)&lt;/span>得到期望频数&lt;span class="math">\(n\hat{p}_{ij}\)&lt;/span>，那么这就可以看成有&lt;span class="math">\(r×c\)&lt;/span>个可选值的卡方拟合优度检验，其自由度为&lt;span class="math">\((r-1)×(c-1)\)&lt;/span>。据此，检验统计量为： &lt;span class="math">\[
X^2=\sum_{i=1}^r\sum_{j=1}^c \frac{(n_{ij}-n\hat{p}_{ij})^2}{n\hat{p}_{ij}}\tag{2}
\]&lt;/span> 其中，&lt;span class="math">\(\hat{p}_{ij}=p_{i\cdot}×p_{\cdot j}=\frac{n_{i\cdot}}{n}×\frac{n_{\cdot j}}{n}\)&lt;/span>，同样的&lt;span class="math">\(n_{ij}\)&lt;/span>和&lt;span class="math">\(n\hat{p}_{ij}\)&lt;/span>差别越大，统计量值越大，概率分布服从性也越差。又因为理论概率为独立假设下的概率分布，概率服从性差意味着两个分类变量独立性也越差。&lt;/p>
&lt;p>下面举一个例子：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/列联表分析.png" alt="列联表分析.png" />&lt;p class="caption">列联表分析.png&lt;/p>
&lt;/div>
&lt;p>此外，对于数值型数据，我们也可以通过将其分割归类成几段，降维成分类数据，从而使用列联表分析。不过，我们还有一种对其有更佳的处理方式，即方差分析。&lt;/p>
&lt;h2 id="方差分析analysis-of-variance-anova">方差分析（Analysis of variance, ANOVA）&lt;/h2>
&lt;p>&lt;strong>当自变量是分类变量，因变量是数值变量时的相关性分析，可以使用方差分析&lt;/strong>。比如探究不同教学方式是否对成绩有影响、不同专业毕业之后的薪资是否有区别等等。区别于列联分析，方差分析的因变量都是数值。&lt;/p>
&lt;p>如果只是想知道是否数值型因变量是否受到分类型自变量影响，那么使用假设检验也是可以的。但是需要研究的目标变多时，例如设4个总体的均值分别为&lt;span class="math">\(\mu_1,\mu_2,\mu_3,\mu_4\)&lt;/span>，如果用一般假设检验方法，如t检验，一次只能研究两个样本，要检验4个均值是否相等，就需要检验6次：&lt;/p>
&lt;ul>
&lt;li>检验1：&lt;span class="math">\(H_0:\mu_1=\mu_2\)&lt;/span>；&lt;/li>
&lt;li>检验2：&lt;span class="math">\(H_0:\mu_1=\mu_3\)&lt;/span>；&lt;/li>
&lt;li>检验3：&lt;span class="math">\(H_0:\mu_1=\mu_4\)&lt;/span>；&lt;/li>
&lt;li>检验4：&lt;span class="math">\(H_0:\mu_2=\mu_3\)&lt;/span>；&lt;/li>
&lt;li>检验5：&lt;span class="math">\(H_0:\mu_2=\mu_4\)&lt;/span>；&lt;/li>
&lt;li>检验6：&lt;span class="math">\(H_0:\mu_3=\mu_4\)&lt;/span>；&lt;/li>
&lt;/ul>
&lt;p>很显然，这样做十分的繁琐，并且多次检验会导致出错概率增加，如果设拒绝域&lt;span class="math">\(\alpha=0.05\)&lt;/span>，即每次检验犯第一类错误的概率为0.05，做6次检验会使犯第一类（至少一次）的概率变成&lt;span class="math">\(1-(1-\alpha)^6\approx 0.265\)&lt;/span>，相应置信水平会降低到0.735。因此使用方差分析一是可以提升检验的效率，二是可以增加分析的可靠性，避免多次检验造成的误差累积。&lt;/p>
&lt;blockquote>
&lt;p>方差分析：在共同的显著性水平&lt;span class="math">\(\alpha\)&lt;/span>下，同时考虑多个平均值的差异。通常以F分布来进行检验，称为方差分析。&lt;/p>
&lt;/blockquote>
&lt;p>方差分析由统计学四天王之一的Fisher于1923年提出。我们在进行方差分析之前还要注意其需要满足以下三个条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>正太总体。每个组的总体应服从正态分布，对于因素的每一个水平，其观测值是来自正太分布总体的简单随机样本。&lt;/li>
&lt;li>方差齐性。各个总体的方差&lt;span class="math">\(\sigma^2\)&lt;/span>必须相同。&lt;/li>
&lt;li>独立性。每个观测值必须是独立的。&lt;/li>
&lt;/ol>
&lt;p>在上述假设成立的前提下，&lt;strong>要分析自变量对因变量是否有影响，在形式上也就转化成为检验自变量的各个水平（总体）的均值是否相等&lt;/strong>。而以上三种假设都有对应的检验方法。如正态性检验、方差齐性检验以及独立性检验。&lt;/p>
&lt;p>方差分析的原理是&lt;strong>对数据误差的来源进行分类分析&lt;/strong>。对于同一因素的不同处理水平，产生的结果可能有不同。根据误差来源，结果的不同可能确实是不同处理水平导致的，也有可能仅仅是因为随机误差。但是这种不同到底是不是不同因素水平导致的呢？方差分析就是通过将&lt;strong>误差分解成组间误差和组内误差，用二者的比值的偏离程度&lt;/strong>来进行分析。而数据中，误差的体现可以由方差透露，因此对误差的分析就能够变成对方差的分析。&lt;/p>
&lt;ul>
&lt;li>组内误差：在因素&lt;strong>同一&lt;/strong>水平处理下，数据的差异，这种差异只可能是随机性导致的。&lt;/li>
&lt;li>组间误差：在因素&lt;strong>不同&lt;/strong>水平处理下，数据的差异，这种差异虽然也包含随机性，但也可能是不同处理水平造成的系统性差异。&lt;/li>
&lt;/ul>
&lt;h3 id="单因素方差分析">单因素方差分析&lt;/h3>
&lt;p>如果只考虑一个因素（自变量）对结果（因变量）的影响，那么只需要单因素方差分析。我们将数据集用&lt;span class="math">\(X\)&lt;/span>表示，每一个样品&lt;span class="math">\(x_{ij}\)&lt;/span>表示在自变量第&lt;span class="math">\(i,i\in\{1,2,\dotsb,r\}\)&lt;/span>个处理水平下，获取到的第&lt;span class="math">\(j,j\in\{1,2,\dotsb,m\}\)&lt;/span>个结果。那么可以用&lt;span class="math">\(\overline{\overline{X}}\)&lt;/span>表示数据集整体的均值，&lt;span class="math">\(\overline{X}_i\)&lt;/span>表示第&lt;span class="math">\(i\)&lt;/span>个因素水平的组内均值。&lt;span class="math">\(x_{ij}\)&lt;/span>还可进行如下分解： &lt;span class="math">\[
\begin{aligned}
x_{ij}-\overline{\overline{X}}&amp;amp;=(x_{ij}-\overline{X}_i)+(\overline{X}_i-\overline{\overline{X}})\\
x_{ij}&amp;amp;=\overline{\overline{X}}+\underbrace{(x_{ij}-\overline{X}_i)}_{\text{组内误差}}+\underbrace{(\overline{X}_i-\overline{\overline{X}})}_{组间误差}
\end{aligned}\tag{3}
\]&lt;/span> 为了进一步分析误差来源，我们假设&lt;span class="math">\(x_{ij}\)&lt;/span>所在的处理组其服从分布为&lt;span class="math">\(N(\mu_i,\sigma^2)\)&lt;/span>的正态分布（正态总体假设，且由于方差齐性，各个处理组方差都是&lt;span class="math">\(\sigma^2\)&lt;/span>），那么&lt;span class="math">\(x_{ij}\)&lt;/span>也可以表示成&lt;span class="math">\(x_{ij}=\mu_i+\varepsilon_{ij}\)&lt;/span>，其中&lt;span class="math">\(\varepsilon_{ij}\)&lt;/span>是数据&lt;span class="math">\(x_{ij}\)&lt;/span>与组内真实均值&lt;span class="math">\(\mu_i\)&lt;/span>的离差，该离差的来源就是随机性。&lt;/p>
&lt;p>而根据中心极限定理，组内数据平均值&lt;span class="math">\(\overline{X}_i\)&lt;/span>应服从&lt;span class="math">\(N(\mu_i,\frac{\sigma^2}{m})\)&lt;/span>的正态分布，&lt;span class="math">\(m\)&lt;/span>为组内数据数量，那么&lt;span class="math">\(\overline{X}_i\)&lt;/span>可以表示成&lt;span class="math">\(\overline{X}_i=\mu_i+\varepsilon_i\)&lt;/span>，其中&lt;span class="math">\(\varepsilon_i\)&lt;/span>是数据的组内平均值&lt;span class="math">\(\overline{X}_i\)&lt;/span>与组内真实均值&lt;span class="math">\(\mu_i\)&lt;/span>的离差，不难证明&lt;span class="math">\(\varepsilon_i=\frac{1}{m}\sum_{j=1}^m \varepsilon_{ij}\)&lt;/span>，即组内均值的离差等于组内数据离差的均值。式（3）中的组内误差等价于： &lt;span class="math">\[
x_{ij}-\overline{X}_i=(\mu_i+\varepsilon_{ij})-(\mu_i+\varepsilon_i)=\varepsilon_{ij}-\varepsilon_i\tag{4}
\]&lt;/span> 所以&lt;strong>组内误差来源是随机性&lt;/strong>。&lt;/p>
&lt;p>同理，若我们将总体平均表示成&lt;span class="math">\(\overline{\overline{X}}=\mu+\varepsilon\)&lt;/span>的形式，其中&lt;span class="math">\(\mu\)&lt;/span>是所有分布真实的均值（各组都服从正态分布，和也服从正态分布），&lt;span class="math">\(\varepsilon\)&lt;/span>是数据均值与真正均值的离差，式(3)中的组间误差等价于 &lt;span class="math">\[
\overline{X}_i-\overline{\overline{X}}=(\mu_i+\varepsilon_i)-(\mu+\varepsilon)\tag{5}
\]&lt;/span> 如果组间没有系统性差异，那么组内真实均值&lt;span class="math">\(\mu_i\)&lt;/span>应该和总体均值&lt;span class="math">\(\mu\)&lt;/span>相同&lt;span class="math">\(\mu_i=\mu\)&lt;/span>，此时造成差异的只有离差&lt;span class="math">\(\varepsilon_i-\varepsilon\)&lt;/span>，就只有随机性造成的偏差。但当不同处理水平确实有影响时，那么某些组的均值就不会等同于整体均值&lt;span class="math">\(\mu_i\neq \mu\)&lt;/span>，此时式（5）中就会存在&lt;strong>系统误差&lt;/strong>项&lt;span class="math">\(\mu_i-\mu\)&lt;/span>和&lt;strong>随机误差&lt;/strong>项&lt;span class="math">\(\varepsilon_i-\varepsilon\)&lt;/span>。系统误差越大，组间误差就会越大。&lt;/p>
&lt;p>为了去除误差正负号的影响以及方便计算，我们将总误差进行平方求和&lt;span class="math">\(S_T=\sum_{i=1}^r\sum_{j=1}^m(x_{ij}-\overline{\overline{X}})^2\)&lt;/span>，结合式（3）有： &lt;span class="math">\[
\begin{aligned}
S_T&amp;amp;=\sum_{i=1}^r\sum_{j=1}^m(x_{ij}-\overline{\overline{X}})^2=\sum_{i=1}^r\sum_{j=1}^m[(x_{ij}-\overline{X}_i)+(\overline{X}_i-\overline{\overline{X}})]^2\\
S_T&amp;amp;=\sum_{i=1}^r\sum_{j=1}^m[(x_{ij}-\overline{X}_i)^2+(\overline{X}_i-\overline{\overline{X}})^2]+\underbrace{\sum_{i=1}^r\sum_{j=1}^m 2(x_{ij}-\overline{X}_i)(\overline{X}_i-\overline{\overline{X}})}_{=0}\\
S_T&amp;amp;=\underbrace{\sum_{i=1}^r\sum_{j=1}^m (x_{ij}-\overline{X}_i)^2}_{\text{组内误差平方和}}+\underbrace{\sum_{i=1}^r\sum_{j=1}^m (\overline{X}_i-\overline{\overline{X}})^2}_{组间误差平方和}
\end{aligned}
\]&lt;/span> 我们令组内误差平方和&lt;span class="math">\(S_e=\sum_{i=1}^r\sum_{j=1}^m (x_{ij}-\overline{X}_i)^2\)&lt;/span>，组间误差平方和&lt;span class="math">\(S_A=\sum_{i=1}^r\sum_{j=1}^m (\overline{X}_i-\overline{\overline{X}})^2=\sum_{i=1}^r m (\overline{X}_i-\overline{\overline{X}})^2\)&lt;/span>，所以有 &lt;span class="math">\[
S_T=S_E+S_A\tag{6}
\]&lt;/span> 这就是误差的平方和分解。下面我们不加证明地给出： &lt;span class="math">\[
\frac{S_e}{\sigma^2}\sim \chi^2(n-r)\tag{7}
\]&lt;/span> 当组间误差没有系统误差，即&lt;span class="math">\(\mu_i=\mu\)&lt;/span>时： &lt;span class="math">\[
\frac{S_A}{\sigma^2}\sim \chi^2(r-1)\tag{8}
\]&lt;/span> 且&lt;span class="math">\(S_e, S_A\)&lt;/span>二者独立。&lt;/p>
&lt;p>因此，当不同处理组没有区别，即不存在系统误差时，式(7)(8)都服从卡方分布，那么他们的商（还需除以自由度）就应该服从F分布： &lt;span class="math">\[
\frac{\frac{S_A}{\sigma^2×(r-1)}}{\frac{S_e}{\sigma^2×(n-r)}}=\frac{S_A/(r-1)}{S_e/(n-r)}\sim F(r-1,n-r)\tag{9}
\]&lt;/span> 考虑到系统误差&lt;span class="math">\(\mu_i-\mu\)&lt;/span>越大，&lt;span class="math">\(S_A\)&lt;/span>也就越大，那么式（9）的F统计量就越大。因此，该检验的拒绝域应为： &lt;span class="math">\[
W=\{F≥F_{1-\alpha}(r-1,n-r)\}\tag{10}
\]&lt;/span> 通常我们会将上述计算过程的结果汇总成&lt;strong>单因子方差分析表&lt;/strong>：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">来源&lt;/th>
&lt;th align="center">平方和&lt;/th>
&lt;th align="center">自由度&lt;/th>
&lt;th align="center">均方&lt;/th>
&lt;th align="center">F比&lt;/th>
&lt;th align="center">p值&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">因子&lt;/td>
&lt;td align="center">&lt;span class="math">\(S_A\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(f_A=r-1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(MS_A=S_A/f_A\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(F=MS_A/MS_e\)&lt;/span>&lt;/td>
&lt;td align="center">p&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">误差&lt;/td>
&lt;td align="center">&lt;span class="math">\(S_e\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(f_e=n-r\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(MS_e=S_E/f_e\)&lt;/span>&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">总和&lt;/td>
&lt;td align="center">&lt;span class="math">\(S_T\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(f_t=n-1\)&lt;/span>&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对于给定的&lt;span class="math">\(\alpha\)&lt;/span>，若&lt;span class="math">\(F≥F_{1-\alpha}(f_A,f_e)\)&lt;/span>，则认为因子显著，自变量的不同水平会对因变量有影响。&lt;/p>
&lt;p>如果认为因子的影响是显著的，想要进一步检验到底是哪个水平影响比较显著，还可继续进行多重检验（也叫事后检验），常用的一种多重检验是利用了t分布的LSD-t检验，其他还有SNK-q检验、Turkey、Duncan、Scheffe检验等。&lt;/p>
&lt;p>此外，还可以用组间方差占总误差的比例来衡量关系的强度，记为&lt;span class="math">\(R^2=SSA/SST\Rightarrow R=\sqrt{SSA/SST}\)&lt;/span>，当&lt;span class="math">\(R&amp;gt;0.5\)&lt;/span>，可认为是中等相关，若&lt;span class="math">\(R&amp;gt;0.8\)&lt;/span>可认为是强相关。&lt;/p>
&lt;h3 id="双因素方差分析">双因素方差分析&lt;/h3>
&lt;p>根据名字我们就知道，双因素方差分析是分析两个分类变量（常称为行因素和列因素）对试验结果的影响。根据两个因素对试验结果的影响是否独立，还可以分为&lt;strong>无交互作用的（无重复）双因素方差分析和有交互作用的（可重复）双因素方差分析&lt;/strong>。&lt;/p>
&lt;p>双因素方差分析也需要满足方差分析的三个假设：正态性、方差齐性、独立性。&lt;/p>
&lt;p>双因素方差分析的基本方法和单因素类似，区别是：&lt;/p>
&lt;p>无交互作用时：分为行因素误差平方和、列因素误差平方和、随机误差平方和三类。可以将行因素、列因素当成两个单因素误差分析来看。&lt;/p>
&lt;p>有交互作用时：还要添加第四类交互效应误差平方和。有交互作用时数据量会比较大。&lt;/p>
&lt;h2 id="正态性检验">正态性检验&lt;/h2>
&lt;p>正态分布时统计学中最重要的分布之一，判断一组数据是否来自正态总体是很多分析步骤的前置要求，甚至还有国标GB/T4882-2001专门设计了正态概率图来辅助我们判断数据是否服从正态分布。&lt;/p>
&lt;blockquote>
&lt;p>正态概率图是一种特殊绘制的坐标图，我们将所给数据绘制在坐标图上，如果这些数据大概处于一条直线上，那么可以认为服从正态分布。&lt;/p>
&lt;/blockquote>
&lt;p>当然，数理统计学还有更技术化的方法，比例最常用的W检验和EP检验。&lt;/p>
&lt;h3 id="w检验">W检验&lt;/h3>
&lt;p>W检验全称sharpiro-wilk检验，是由二人于1965年提出的正态检验方法。其适用范围为样本容量&lt;span class="math">\(8\leq n \leq 2000\)&lt;/span>，非常适合小样本的正态性检验，但是当样本容量小于7时，对偏离正态分布的检验不太有效，同时当数据量大于5000时也不适用。&lt;/p>
&lt;p>正态性检验方法总结图：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/正态性检验.png" alt="正态性检验.png" />&lt;p class="caption">正态性检验.png&lt;/p>
&lt;/div>
&lt;p>贝叶斯检验主要利用了KL散度，这是衡量两种分布偏差程度的一种度量，也叫相对熵。&lt;/p>
&lt;p>正态性检验的计算往往十分复杂且常常需要查表，最好使用计算机程序辅助计算。&lt;/p>
&lt;h2 id="方差齐性检验">方差齐性检验&lt;/h2>
&lt;p>总结下这几种方法的利弊及适用条件：方差比、Hartley检验、Bartlett检验都需要原始数据是正态分布，Levene检验和BF法对正态分布不是很依赖。比较常用的是Levene检验，适用于多组方差的比较，且对正态性没要求。 &lt;a href="https://zhuanlan.zhihu.com/p/313397172">https://zhuanlan.zhihu.com/p/313397172&lt;/a>&lt;/p></description></item><item><title>概率统计随机过程之极简贝叶斯统计总结</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9E%81%E7%AE%80%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</link><pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9E%81%E7%AE%80%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</guid><description>
&lt;h2 id="概率统计随机过程之极简贝叶斯统计总结">概率统计随机过程之极简贝叶斯统计总结&lt;!-- omit in toc -->&lt;/h2>
&lt;p>本篇笔记是小岛宽之的书《统计学关我什么事》的阅读笔记总结。本书大体分为两个部分，第一部分是对贝叶斯统计学本质的形象介绍，深入浅出，对我理解贝叶斯统计推断的帮助很大。第二部分，作者用较为简略的数学语言介绍贝叶斯统计中常用的数学工具如先验共轭分布Beta分布、正态分布、条件概率等，对有一定统计学基础的人而言意义不大。本篇笔记主要总结第一部分。&lt;/p>
&lt;h3 id="一">一&lt;/h3>
&lt;p>统计学分为两大分支，一个是以内曼-皮尔逊范式为主导的频率学派，关键人物有内曼、皮尔逊父子以及费希尔；另一个则是以贝叶斯范式为主的贝叶斯学派，关键人物有贝叶斯、拉普拉斯、林德利以及萨维奇。我们过去学习的统计学以频率学派的概率为主，比如点估计、区间估计、假设检验等，而贝叶斯统计则在近段时间随着机器学习，计算机领域的快速的发展得到越来越多的重视。&lt;/p>
&lt;p>贝叶斯统计的主观性和思想性才是贝叶斯统计的本质和便利性所在。&lt;/p>
&lt;p>信息增加导致概率（分布）变化是贝叶斯推理的基本方法。其步骤如下：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>通过经验设定先验概率。信息指附加的状况，无任何信息时的概率分布就是先验概率&lt;/li>
&lt;li>构建在某种情况下的条件概率，这需要额外的信息或数据来支撑构建。&lt;/li>
&lt;li>通过观察出现的行为或现象（即信息），排除“不可能的情况”&lt;/li>
&lt;li>归一化剩下的情况的概率，即逆概率——后验概率。&lt;/li>
&lt;/ol>
&lt;p>总结：先验概率+条件概率+信息=后验概率&lt;/p>
&lt;p>贝叶斯统计推断中的“逆”是指由结果&lt;span class="math">\(\rightarrow\)&lt;/span>原因。比如事件有一定出现概率，但是我们不确定到底是什么事件，我们能通过事件所引发的现象（果）来推测事件到底是什么（因）。虽然多个事件可能引起相同的现象，但不同事件引起同一现象的概率不同，因此引起的现象可以作为信息，来帮助我们推测。贝叶斯统计就是通过信息增加，不断改进推测的方法，也是其被称为执果索因的来历。&lt;/p>
&lt;h3 id="二">二&lt;/h3>
&lt;p>贝叶斯推理有时候反直觉，主要是因为先验概率和条件概率二者容易被混淆。典型的例子就是一个很少见的病，如果被医学检测出来是阳性，检测的正确性就算是99%，实际上患病的概率也不高（至少不会是99%这么高）。这是因为条件概率虽然很大，但是先验概率很小。不过这种检测并不是无用，即使真正患病的概率依旧不高，但是相比原来先验概率还是有很大变化，他们的变化体现在概率上，即先验概率和后验概率的变化。&lt;/p>
&lt;h3 id="三">三&lt;/h3>
&lt;p>贝叶斯统计也可以根据主观数字进行推理。主观概率：描述心理感受或信念程度的方式，区别于客观概率，主观概率多是心理的描述。&lt;/p>
&lt;blockquote>
&lt;p>理由不充分原理：没有任何先验信息时，先验概率假定为&lt;strong>均匀分布&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>概率的概率：概率模型参数的概率，就是说概率模型的参数也是一个概率，二者组合的一起会得到一些神奇的概率分布，比如beta分布等。&lt;/p>
&lt;h3 id="四">四&lt;/h3>
&lt;p>再回到频率学派的内曼-皮尔逊范式统计推理，其核心工具假设检验，有零假设、备择假设，显著性水平。零假设只能用来拒绝，不能用来肯定。我们只能说不能拒绝零假设，而不能说我们能证明零假设是对的。而当我们拒绝零假设时，只能接受互斥的备择假设，反而能证明备择假设是真的。其核心时小概率事件很少发生这一事实，小概率体现在显著性水平这一概念上，有设置为1%，5%的，其实这也是一种&lt;strong>主观设置&lt;/strong>，没有严肃的数学推理，这也暗示着具有主观性的先验概率和它并没有完全分离的界限。&lt;/p>
&lt;h3 id="五">五&lt;/h3>
&lt;p>如果从贝叶斯角度看假设检验，那么就是先验概率&lt;span class="math">\(\rightarrow\)&lt;/span>后验概率的变化，在选择时，贝叶斯推断并不给出最终结果，而是给出结果的概率，因此无论在何种条件下，贝叶斯推断都能得出一个暂时的结果。内曼-皮尔逊范式和贝叶斯范式区别在于前者风险（或错误率）体现在显著性水平，而且这个正确率效果时基于大量结果的统计均值，并不针对当下情况，是一概而论的感觉。而后者风险（错误率）则体现在后验概率中，因为每一个选择都是一定的概率（分布），这种概率（分布）会随着信息的累加而变化，并不是固定的数值。&lt;/p>
&lt;h3 id="六">六&lt;/h3>
&lt;p>贝叶斯范式和内曼-皮尔逊范式的衔接点是：&lt;strong>最大似然估计（又叫极大似然估计）&lt;/strong>。其思路选择的是使结果概率最大的模型（即原因）本质就是贝叶斯式的思维方式。而从频率学派角度来看，则是使似然函数最大的选值。&lt;/p>
&lt;h3 id="七">七&lt;/h3>
&lt;p>贝叶斯的悖论：猜汽车/蒙提霍尔问题。信息影响的是信息影响下事件的概率，而非所有的概率。这点很难理解，也是贝叶斯思维方式的难点，推荐看书籍原文。令人吃惊的，如果理解这个理论，你会发现这是从贝叶斯角度给“多听长辈话”这一古老谚语的严谨数学解释。&lt;/p>
&lt;h3 id="八">八&lt;/h3>
&lt;p>掌握多条信息是的推理，需要一个前提是独立实验，应用概率的乘法公式。比如垃圾邮件分类器。同时我们会发现多个独立的信息可以逐步进行贝叶斯推断结果和直接使用所有信息的效果是一样的，这就是贝叶斯推断的“序贯性”。这样我们就可以不用大量存储所有数据，而是小步迭代，逐步精确。能这么做的核心原因是：&lt;strong>之前的信息在先验概率更新为后验概率的步骤中，保存到了后验概率中&lt;/strong>。因此后验概率是先验概率和信息的集中体现。这在计算机理论尤其是机器学习中有着方便、广泛的应用。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/贝叶斯概率更新.jpg" alt="贝叶斯概率更新" />&lt;p class="caption">贝叶斯概率更新&lt;/p>
&lt;/div>
&lt;h3 id="九">九&lt;/h3>
&lt;p>Beta分布基本形式 &lt;span class="math">\[
Beta(x)=C\times x^{\alpha-1}\times(1-x)^{\beta-1},x\in [0,1],\alpha\geq 1,\beta\geq 1
\]&lt;/span> 其中，&lt;span class="math">\(C\)&lt;/span>是标准化（归一化）系数，&lt;span class="math">\(\alpha,\beta\)&lt;/span>分别是两种事件发生次数的相关量，其期望是： &lt;span class="math">\[
E(x)=\frac{\alpha}{\alpha+\beta}
\]&lt;/span>&lt;/p>
&lt;p>由于归一化时，对&lt;span class="math">\(x^{\alpha-1}\times(1-x)^{\beta-1}\)&lt;/span>的积分用到了Gamma函数来计算，因此常数&lt;span class="math">\(C\)&lt;/span>中会有Gamma函数。Beta分布是伯努利分布/二项分布的共轭先验分布，此外正态分布是正态分布的共轭先验分布。&lt;/p>
&lt;blockquote>
&lt;p>共轭分布：在贝叶斯统计中，如果后验分布与先验分布属于同类（分布形式相同），则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。&lt;/p>
&lt;/blockquote>
&lt;p>共轭先验的好处主要在于代数上的方便性，可以直接给出后验分布的封闭形式，否则的话只能数值计算。共轭先验也有助于获得关于似然函数如何更新先验分布的直观印象。一个比较好的结论是所有指数家族的分布都有共轭先验。&lt;/p></description></item><item><title>概率统计随机过程之统计实验设计</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/</link><pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/</guid><description>
&lt;h2 id="概率统计随机过程之统计实验设计">概率统计随机过程之统计实验设计&lt;!-- omit in toc -->&lt;/h2>
&lt;p>实验是现代科学中不可缺少的一环，随着科学的发展，科学实验的成本也越来越高，例如现在大型强子对撞机的建设成本需要几十亿，甚至上百亿美元。此外，合理的实验也能让我们减少实验误差，提高数据精度。因此每次我们进行实验前都需要合理的设计实验（Design of Experiments）。目前，设计实验是数理统计学的一个分支，科学探究的一部分，涉及“用何方法可更好的设计一个实验”，属于方法论的范畴。由于任何实验都会受到外来环境影响，如何设计实验，使外来环境的变化能够对实验造成最小的影响，就是实验规划的目的。实验设计法广泛用于自然科学、社会科学、医学等各学科的实验设计里。&lt;/p>
&lt;p>经常使用的实验设计方法有完全随机设计、随机区组设计、交叉设计、析因设计、拉丁方设计、正交设计、嵌套设计、重复测量设计、裂区设计以及均匀设计等。不同的实验设计方法适用不同的情况。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#实验设计总述">实验设计总述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单因素实验设计">单因素实验设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#完全随机设计">完全随机设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#配对实验设计">配对实验设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#随机区组设计">随机区组设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#双多因素实验设计">双/多因素实验设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#交叉设计">交叉设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉丁方设计">拉丁方设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#析因设计">析因设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正交设计">正交设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均匀设计">均匀设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特殊实验设计方法">特殊实验设计方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#嵌套设计">嵌套设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量设计">重复测量设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#裂区设计">裂区设计&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="实验设计总述">实验设计总述&lt;/h2>
&lt;p>实验设计原则：随机、对照、可重复、区域化（即尽量保证其他影响因素一致）。&lt;/p>
&lt;p>因素：实验中会影响最终结果的变量。&lt;/p>
&lt;p>水平：实验中一个因素所能取的值。不一定是数字，也可包含符号、文本性描述。因为实验中我们不能无限地取值，只能选取有代表性的值，因此水平必须是有限个。&lt;/p>
&lt;p>处理组：接受因素不同水平处理的实验组。比如在医学实验中，处理组A使用新的药剂，处理组B接受安慰剂。因素为用药，该因素水平为：用新药和用安慰剂两种。&lt;/p>
&lt;p>根据考虑变化因素对实验结果的影响，可以分为单因素实验设计、多因素实验设计。&lt;/p>
&lt;h2 id="单因素实验设计">单因素实验设计&lt;/h2>
&lt;p>当我们只考虑单个因素的影响时，可采用的设计方式。&lt;/p>
&lt;h3 id="完全随机设计">完全随机设计&lt;/h3>
&lt;p>完全随机设计只涉及一个处理因素的两个或多个水平，所以也称单因素设计。它的核心就是将样本中的&lt;strong>样品随机地分配&lt;/strong>到各处理组，分别接受不同的处理，然后得出实验结论。&lt;/p>
&lt;p>按照各处理组样本容量是否大体相等（个别处理组多或少一个样品，问题也不大）可分为&lt;strong>平衡设计&lt;/strong>：各组样本含量相等 （样本含量相等时检验效率较高）和&lt;strong>非平衡设计&lt;/strong>：各组样本含量不相等。&lt;/p>
&lt;p>由于分的处理组数和因素的水平个数相同，当水平数为2时为&lt;strong>二组比较的完全随机设计&lt;/strong>，当水平数有三个及以上时为&lt;strong>多组比较的完全随机设计&lt;/strong>。通常多组比较的实验比两组比较的实验要复杂一些，数据使用的检验方法也不一样。&lt;/p>
&lt;p>完全随时实验是实验设计最基础的方式，也体现了实验设计的几大原则。不过其缺点也不少例如样品个体客观上是存在个体差异的，这会导致组间样品的不平衡，尤其是在小样本场景下；其次，这种完全随机设计效率不算高；并且只能分析单因素。&lt;/p>
&lt;h3 id="配对实验设计">配对实验设计&lt;/h3>
&lt;p>配对实验设计的核心思想是&lt;strong>控制变量&lt;/strong>。&lt;/p>
&lt;p>配对设计是将受试对象按配对条件配成对子，&lt;strong>每对中的个体接受不同的处理&lt;/strong>。&lt;strong>配对设计的最主要动因是排除非考察因素的干扰&lt;/strong>，因此我们需要将非常接近的两个样品配成一对，然后每一对的两个个体分别接受不同的处理。比如动物实验中，常将同性别、同窝别、体重相近的两个动物配成一对，然后将每一对的样品分别进行两种因素水平的处理。&lt;/p>
&lt;p>配对实验的优点就是组间均衡性好，由于人为的控制了非处理因素的干扰，组间误差小需要的例数少，提高了检验效率。但是欠妥的配对方案会导致实验出现难以察觉的错误。&lt;/p>
&lt;h3 id="随机区组设计">随机区组设计&lt;/h3>
&lt;p>随机区组设计是配对实验设计的推广，因为配对实验设计只能处理因素的两个水平，而区组设计是针对因素多个水平而提出的。&lt;/p>
&lt;p>其处理手法和配对实验设计是共通的，首先是将样品按照性质分成N组，组内的样品性质要求接近，组件样品是有性质差异的，例如如病人的性别、年龄、体重和病情等非实验因素差异分成N个区组。然后将组内样品随机分配到同一因素不同水平的处理组。&lt;/p>
&lt;p>其优点和配对实验一样：每个区组内的受试对象有较好的同质性，排除了非实验因素对分析结果的影响，提高了分析效率。但是，这样也要求每个处理组至少分到一个受试对象，实验结果中若有数据缺失，统计分析较麻烦。&lt;/p>
&lt;h2 id="双多因素实验设计">双/多因素实验设计&lt;/h2>
&lt;p>当我们考虑的因素增多时，不仅要考虑每个因素对实验结果的影响，还要考虑存在因素之间的交互作用。&lt;/p>
&lt;p>对于无相互影响的多因素实验，有交叉设计和拉丁方设计；适用于有交互作用的实验设计方法有析因设计、正交设计和均匀设计。&lt;/p>
&lt;h3 id="交叉设计">交叉设计&lt;/h3>
&lt;p>交叉设计是一种特殊的自身对照设计，常用在临床试验中，在同一病人身上观察两种或多种处理水平的效应，消除不同病人之间的差异，减少误差。我们以两个阶段、两种处理水平为例说明操作步骤。首先将条件相近的观察对象进行配对，随机分配到两个实验组中。第一组先用处理方法A处理，然后再用处理方法B处理，处理顺序是AB；另一组则相反，先用处理方法B处理，再用处理方法A处理，处理顺序是BA。两种处理水平在全部实验过程中“交叉”进行。&lt;/p>
&lt;p>使用注意：交叉设计的&lt;strong>两个实验阶段之间需要留出足够的“衰减时间”&lt;/strong>。为了减除前一实验阶段对后一实验阶段的影响，需要设置“衰减时间”。如果是新药实验，那么衰减时间可以根据药物半衰期确定。&lt;/p>
&lt;p>交叉设计实际上就是自身对照实验设计，通过“交叉”的方式将时间因素的影响分解出来，避免了时间因素对研究结果的干扰。因此该设计的最大优点是可控制时间因素及个体差异对处理方式的影响，故节约样本含量，效率较高。但是交叉设计一般只用于两个因素之间的比较，如果因素多的话，就会因时间太长产生更多不可控的变量。&lt;/p>
&lt;h3 id="拉丁方设计">拉丁方设计&lt;/h3>
&lt;p>拉丁方设计用于&lt;strong>研究三个因素，各因素间无交互作用且每个因素的水平数相同的情况&lt;/strong>。其中有一个最重要的因素称之为处理因素，另外两个是需要加以控制的因素。此外，拉丁方设计还要求处理具有方差齐性。&lt;/p>
&lt;p>因素之间没有交互作用的情况毕竟是少数，更多的情况是存在因素之间的交互作用。下面几种实验设计方法适用于有交互作用的情况。&lt;/p>
&lt;h3 id="析因设计">析因设计&lt;/h3>
&lt;p>析因设计是单因素完全随机实验在多因素场景下的推广，其需要将两个或两个以上因素及其各种水平进行排列组合、交叉分组的试验设计。它可以研究单个因素多个水平的效应，也可以研究因素之间是否有交互作用，同时找到最佳组合。&lt;/p>
&lt;p>例如，现在有两个处理因素，一个因素有2个水平，另一个因素有三个水平，那么就进行&lt;span class="math">\(2\times 3=6\)&lt;/span>次实验；如果有三个处理因素，每个因素都有5个处理水平，那么就进行&lt;span class="math">\(5×5×5=125\)&lt;/span>次实验。析因分析的原理就是对每个因素的每个水平都进行实验，这样能够照顾到所有的因素和水平。&lt;/p>
&lt;p>显然析因设计是非常消耗资源的设计，需要进行大量实验，当因素或水平增加时，实验次数需要几何倍数增长，这在很多实验中是难以实施的。&lt;/p>
&lt;h3 id="正交设计">正交设计&lt;/h3>
&lt;p>为了降低析因设计的试验次数，后人提出了正交设计。&lt;strong>正交设计是析因设计的高效化&lt;/strong>。当析因设计要求的实验次数太多时，一个非常自然的想法就是从析因设计的水平组合中，选择一部分有代表性水平组合进行试验，而正交设计就能满足这个要求。&lt;/p>
&lt;p>正交试验设计一般包括以下几步：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>确定研究因素和指标水平；&lt;/li>
&lt;li>制作成正交试验表格；&lt;/li>
&lt;li>实施试验；&lt;/li>
&lt;li>试验结果分析&lt;/li>
&lt;/ol>
&lt;p>其中制作正交试验表格是关键步骤，也是正交设计的核心所在，不过我看网上大多数教程都是用软件或者直接查的正交表。&lt;/p>
&lt;h3 id="均匀设计">均匀设计&lt;/h3>
&lt;p>均匀设计是一种多因素多水平的试验设计，它放弃了正交表的整齐可比性，是在正交设计的基础上进一步发展而成的。均匀设计进一步提高了试验点的“均匀分散性”。均匀设计的最大优点是可以使因素的水平数很大，而试验次数又最节省。与正交设计一样，可以通过均匀设计表设计实验。&lt;/p>
&lt;p>正交设计和均匀设计都是设计实验需要系统学习的方法，这里我们不再赘述。&lt;/p>
&lt;h2 id="特殊实验设计方法">特殊实验设计方法&lt;/h2>
&lt;h3 id="嵌套设计">嵌套设计&lt;/h3>
&lt;p>如果处理因素之间存在层次性结构，或处理因素之间有主次之分，这时就需要用到嵌套设计。例如，研究催化剂和温度两个处理因素对化学反应速度的影响就是典型的例子。&lt;/p>
&lt;h3 id="重复测量设计">重复测量设计&lt;/h3>
&lt;p>重复测量设计广泛应用于各种科学研究中，它的显著特点就是在&lt;strong>不同的实验条件下，从同一个受试对象身上采集到多个数据，也就是同一个受试者在不同实验条件下进行数次实验&lt;/strong>，以获得更多信息。这里的数次实验需要考虑的就是“时间因素”。最常见的重复测量设计是在药物的临床试验中，例如，比较两种不同药物的疗效，将病人随机分成两组，分别给予不同的药物，然后在不同时间作病人的动态观察。&lt;/p>
&lt;h3 id="裂区设计">裂区设计&lt;/h3>
&lt;p>裂区试验设计（split-plot experiment design）：又称为分割试验设计，把一个或多个完全随机设计、随机区组设计或拉丁方设计结合起来的试验方法。其原理为先将受试对象作一级实验单位，再分为二级实验单位，分别施以不同的处理。实验单位分级是指当实验单位具有隶属关系时，高级实验单位包含低级实验单位。如小鼠接种不同的瘤株后，观察不同浓度的某注射液的抑瘤效果，这时接种瘤株的小鼠为一级单位，相应因素为一级处理，注射浓度为二级单位，相应因素为二级处理。当试验单位不存在明显的隶属关系时，实验单位分级可按因素的主次确定。在裂区试验中一级处理与一级单位混杂，而二级处理与二级单位不混杂。因此，设计时将最感兴趣或最主要的因素，差异较小、要求精度较高、试验条件较少、工序较易改变的因素作为二级因素。&lt;/p></description></item><item><title>概率统计随机过程之贝叶斯统计推断</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD/</link><pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD/</guid><description>
&lt;h2 id="概率统计随机过程之贝叶斯统计推断">概率统计随机过程之贝叶斯统计推断&lt;!-- omit in toc -->&lt;/h2></description></item><item><title>概率统计随机过程之Fisher信息</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8Bfisher%E4%BF%A1%E6%81%AF/</link><pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8Bfisher%E4%BF%A1%E6%81%AF/</guid><description>
&lt;ul>
&lt;li>&lt;a href="#理解fisher信息">理解Fisher信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="#故事的开始从最大似然估计开始">故事的开始：从最大似然估计开始&lt;/a>&lt;/li>
&lt;li>&lt;a href="#纠结方差与fisher信息">纠结：方差与Fisher信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mle的一个准严格推导">MLE的一个准严格推导&lt;/a>&lt;/li>
&lt;li>&lt;a href="#fisher信息的应用">Fisher信息的应用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#频率学派角度应用fisher信息">频率学派角度应用Fisher信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯学派角度应用fisher信息">贝叶斯学派角度应用Fisher信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小描述长度角度应用fisher信息">最小描述长度角度应用Fisher信息&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概率统计随机过程之fisher信息">概率统计随机过程之Fisher信息&lt;!-- omit in toc -->&lt;/h2>
&lt;p>Fisher信息是参数估计中的一个重要概念，它揭示了样本能够提供多少信息来给我们估计参数，并决定参数估计的精度，Fisher信息大小和样本容量、总体的概率分布、采用的估计方式都有关系。我们通过Fisher信息和Cramer-Rao界也可以确定参数估计的理论极限。Fisher信息是由著名的&lt;strong>频率学派大佬，Ronald Fisher&lt;/strong>提出并推广，Fisher是一个统计学历史上非常重要的学者，几乎以一己之力构建了现代统计学框架，一些常见的统计学名词如统计显著性，P值，线性判别分析（LDA），最大似然估计，F分布，充分统计量，Fisher信息，方差分析（ANOVA）都是由Fisher发明或推广，同时也对贝叶斯学派的观点提出诸多批判。本篇笔记旨在系统的整理有关Fisher信息的相关内容，并结合文章《A Tutorial on Fisher Information》，总结Fisher信息在频率统计学派，贝叶斯学派以及最小描述长度模型中的应用。&lt;/p>
&lt;p>注：本文前面部分默认Fisher信息的参数估计都是一维参数估计。对于多维参数估计，我们专门设置了Fisher信息矩阵的章节。&lt;/p>
&lt;h2 id="理解fisher信息">理解Fisher信息&lt;/h2>
&lt;p>为了深刻理解Fisher信息的意义，我们设计了&lt;strong>一个多次反转的叙述结构来初步介绍Fisher信息&lt;/strong>。&lt;/p>
&lt;p>在参数估计中，我们要做的&lt;strong>核心任务是已知一个含有未知参数&lt;span class="math">\(\theta\)&lt;/span>的模型，我们通过实际观测到数据来推断这个未知的参数&lt;span class="math">\(\theta\)&lt;/span>&lt;/strong>。对于观测到数据集&lt;span class="math">\(X\)&lt;/span>，我们既可以直接使用整个观测集来做参数的估计，也可以从数据集中提取一些关键信息来进行参数估计。这些提取出来的信息，被称为统计量&lt;span class="math">\(T(X)\)&lt;/span>，关于统计量的具体介绍，可以参考笔记&lt;a href="概率统计随机过程之数理统计常用概念.md">概率统计随机过程之数理统计常用概念&lt;/a>中充分统计量章节的内容。&lt;/p>
&lt;p>简单地说，统计量是由观测数据构造出来的函数&lt;span class="math">\(T=f(X)\)&lt;/span>，其&lt;strong>包含并提纯&lt;/strong>了有关未知参数&lt;span class="math">\(\theta\)&lt;/span>的信息。注意该函数&lt;span class="math">\(f(X)\)&lt;/span>中并不能有其他未知的参数，也就是说只要给定观测集合&lt;span class="math">\(X\)&lt;/span>，那么统计量的值&lt;span class="math">\(T\)&lt;/span>就是一个定值。如果该统计量&lt;span class="math">\(T\)&lt;/span>完全包含了观测数据集&lt;span class="math">\(X\)&lt;/span>中所有有关未知参数&lt;span class="math">\(\theta\)&lt;/span>的信息，则称为完全统计量（Sufficient Statistics），即&lt;span class="math">\(I(T;\theta)=I(X;\theta)\)&lt;/span>。&lt;/p>
&lt;p>无论是原始的数据集&lt;span class="math">\(X\)&lt;/span>，还是有数据集构造的统计量&lt;span class="math">\(T(X)\)&lt;/span>，其进行参数估计的理论根源是一样的：抽样的数据中含有关于&lt;strong>未知参数&lt;span class="math">\(\theta\)&lt;/span>的信息&lt;/strong>，这是所有参数估计方法的基石。但是，原始数据集&lt;span class="math">\(X\)&lt;/span>和带估计参数&lt;span class="math">\(\theta\)&lt;/span>之间的关系是模糊的，需要我们逐渐求证的，因此能够建立数据集和带估计参数之间联系的&lt;strong>参数估计方法&lt;/strong>就是非常关键的环节。本质上，参数估计方法也是通过数据集合&lt;span class="math">\(X\)&lt;/span>来构建统计量&lt;span class="math">\(T\)&lt;/span>，只不过这个统计量需要与带估计参数&lt;span class="math">\(\theta\)&lt;/span>有更加直观的联系。目前使用的最广泛的估计方法——&lt;strong>极大似然估计&lt;/strong>当然也不例外。实际上，在极大似然估计的处理流程，如构造似然函数、取&lt;span class="math">\(ln\)&lt;/span>、求偏导的过程，就是构建最大似然统计量&lt;span class="math">\(T=f(X)\)&lt;/span>的过程。&lt;/p>
&lt;p>那么，我们应该如何确定数据集&lt;span class="math">\(X\)&lt;/span>或构造的统计量&lt;span class="math">\(T=f(X)\)&lt;/span>中含有多少关于带估计参数&lt;span class="math">\(\theta\)&lt;/span>的信息呢？这就是Fisher信息的由来的内生动力。&lt;/p>
&lt;h3 id="故事的开始从最大似然估计开始">故事的开始：从最大似然估计开始&lt;/h3>
&lt;p>在参数估计中，使用的最广泛的方式就是最大似然估计（maximum likelihood estimation, MLE），而Fisher正是MLE最有力的推动者，没有之一。在Fisher所在的年代，大家都想为模型的参数估计提出一套牢靠可行的方法，Fisher从1912年到1922年发表了多篇文章系统性的阐述并推广了最大似然估计方法，并对最大似然估计的性能进行分析，而&lt;strong>Fisher信息正可用来衡量最大似然估计&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>TIPS：有趣的是，Fisher一直尝试严谨地证明最大似然估计理论，但是一直没有成功，直到1938年统计学家 Samuel S. Wilks最终给出了最大似然估计的证明结果，被称为Wilks定理，大概内容是多个独立观测值的估计的对数似然比的误差是渐近卡方分布的。其中，唯一比较难的证明步骤取决于Fisher信息矩阵的期望值，但是这个值正好已经被Fisher的另一个定理证明了，Fisher就这样可惜地错过了严谨地证明最大似然估计理论。&lt;/p>
&lt;/blockquote>
&lt;p>最大似然估计的严谨证明费了一般波折，但是其核心思想非常的直观，&lt;strong>就是如果一组事件发生了，那么我们就找出最容易使这些事件发生的概率模型&lt;/strong>。比如，我们抛了一个未知的硬币（可能是不均匀的），如果抛了10次，7次正面，3次反面，那我们就直观地认为该硬币得到正面的概率是0.7，所以说极大似然估计可能是最符合直觉的估计方法之一(虽然不一定是正确的，但是在抽样次数足够大时，能基本收敛到正确结果)。具体关于极大似然估计的内容可以参考笔记&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>，其精简步骤总结如下：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>写出总体的概率质量/密度函数（PMF/PDF）&lt;span class="math">\(p(x;\theta)\)&lt;/span>。&lt;/li>
&lt;li>根据采样的数据&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>写出似然估计函数&lt;span class="math">\(L(\theta)=\prod_{i=1}^n p(x_i;\theta)\)&lt;/span>，其中&lt;span class="math">\(\theta\)&lt;/span>是待估计参数。&lt;/li>
&lt;li>两边取自然对数&lt;span class="math">\(\ln\)&lt;/span>，即为&lt;span class="math">\(l(\theta)=\ln(L(\theta))=\sum_{i=1}^n \ln p(x_i;\theta)\)&lt;/span>。(注意，对数似然函数是小写&lt;span class="math">\(l\)&lt;/span>)&lt;/li>
&lt;li>对&lt;span class="math">\(l(\theta)\)&lt;/span>的&lt;span class="math">\(\theta\)&lt;/span>求导（多参数估计就是偏导），&lt;span class="math">\(\frac{\partial l(\theta)}{\partial\theta}=\sum_{i=1}^n \frac{\partial \ln(p(x_i;\theta))}{\partial \theta}\)&lt;/span>，令其导数为0。&lt;/li>
&lt;li>求出使导数为0的&lt;span class="math">\(\theta\)&lt;/span>，即为最大似然估计参数&lt;span class="math">\(\hat\theta_{MLE}=\arg\limits_{\theta}\{\sum_{i=1}^n \frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=0\}\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>TIPS：对于有些分布的极大似然估计没法直接求，比如均匀分布。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>似然函数取对数的原因&lt;/strong>:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>减少计算量。乘法变成加法，从而减少了计算量；同时，如果概率中含有指数项，如高斯分布，能把指数项也化为求和形式，进一步减少计算量；另外，在对联合概率求导时，和的形式会比积的形式更方便。&lt;/li>
&lt;li>计算时更准确。为概率值都在[0,1]之间，因此，概率的连乘将会变成一个很小的值，可能会引起浮点数下溢，尤其是当数据集很大的时候，联合概率会趋向于0，非常不利于之后的计算。&lt;/li>
&lt;li>取对数后，可以是一个上凸函数，更有利于求取最大值。&lt;/li>
&lt;/ol>
&lt;p>那么最大似然估计的估计结果有多准确呢？我们可以借助“估计值&lt;span class="math">\(\plusmn\)&lt;/span>一个范围”来描述，比如，&lt;span class="math">\(\hat\theta_{MLE}\plusmn \sigma\)&lt;/span>，如果我们要求一个比较高的概率能用这个范围覆盖住真实值&lt;span class="math">\(\theta_0\)&lt;/span>，同时&lt;span class="math">\(\sigma\)&lt;/span>的比较小，我们就认为估计是比较准确的。&lt;strong>如果估计的准确，我们也有理由认为估计时候信息比较充分，信息量比较多&lt;/strong>。不难发现，这里其实使用了区间估计的思想。&lt;/p>
&lt;p>至此，我们确定一个目标：&lt;strong>将最大似然估计的准确性和信息的定义联系起来，信息含量越大，估计的准确性越高。&lt;/strong>&lt;/p>
&lt;p>但是，我们现在并不知道&lt;span class="math">\(\hat{\theta}_{MLE}\)&lt;/span>的具体分布是什么，不过我们通过采样的样本，我们可以用样本的均值和方差近似 总体的样本和方差。知道了样本、方差再通过某些概率不等式，我们能够计算出一些区间估计的界（bound），比如最常见的切比雪夫不等式，即我们可以借助切比雪夫不等式，只通过期望和方差就能确定一个大致的估计准确性的范围。&lt;/p>
&lt;blockquote>
&lt;p>切比雪夫不等式：对&lt;strong>任何分布形状的数据都适用&lt;/strong>。可表示为：对于任意随机变量&lt;span class="math">\(T\)&lt;/span>，当给定某一&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>，有： &lt;span class="math">\[P(|T-E(T)|\geqslant \varepsilon)\leq {\frac {Var(T)}{\varepsilon^{2}}}\tag{1}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>注意，这里的随机变量&lt;span class="math">\(T\)&lt;/span>可以是由样本集&lt;span class="math">\(X\)&lt;/span>构造出来的任意随机变量，即&lt;span class="math">\(T\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>的某种统计量。&lt;/p>
&lt;p>使用切比雪夫不等式的最大优势就是前置要求少，并且几乎对任何概率分布都适用。从切比雪夫不等式我们可以发现，如果MLE是无偏估计的话（&lt;span class="math">\(E(T)=\theta_0\)&lt;/span>），我们可以用方差将估计误差&lt;span class="math">\(\varepsilon\)&lt;/span>限定在&lt;span class="math">\(\frac {Var(T)}{\varepsilon^{2}}\)&lt;/span>范围内。特别地，我们如果人为地取&lt;span class="math">\(\varepsilon=k\sqrt{Var(T)}=k\sigma\)&lt;/span>，那么有 &lt;span class="math">\[
P(|T-E(T)|\geqslant k\sigma)\leq {\frac {1}{k^2}}\tag{2}
\]&lt;/span>&lt;/p>
&lt;h3 id="纠结方差与fisher信息">纠结：方差与Fisher信息&lt;/h3>
&lt;p>从上文可知，&lt;strong>直观地想，信息量越大，估计的越准确，这是大多数人的直觉，而且我可以告诉，这个直觉是对的&lt;/strong>。我们回看式(1)(2)，在无偏估计下，样本期望就是真实值，即&lt;span class="math">\(E(T)=\theta_0\)&lt;/span>，同时&lt;span class="math">\(k，\varepsilon\)&lt;/span>是与样本集无关我们可以人为选定的量，那么影响估计准确性的不可控因素就只剩下方差&lt;span class="math">\(Var(T)=\sigma^2\)&lt;/span>。这显然意味着，&lt;strong>统计量的方差和样本中含有的未知参数的&lt;span class="math">\(\theta\)&lt;/span>的信息量密切相关&lt;/strong>！&lt;/p>
&lt;p>由于Fisher当时非常推崇MLE作为参数估计方法，且实际应用中效果也很好，因此Fisher在此发现的基础上，最开始提出了一个很intuitive想法：使用最大似然估计作为构造统计量的方法，然后计算方差： &lt;span class="math">\[
T(X)=\theta_{MLE}=\arg\limits_{\theta} \{\sum_{i=1}^n \frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=0\}\tag{3}
\]&lt;/span> 但是，其中的&lt;span class="math">\(\arg\limits_\theta\)&lt;/span>这个运算符很让他犯难，这个操作符的代表的运算很抽象，不同的式子，实际的求0过程可能迥然不同，比如 &lt;span class="math">\[
\arg\limits_{x} \{ax-b=0\}\Rightarrow x=\frac{b}{a},(a\neq 0)\\
\arg\limits_{x} \{ax^2+bx+c=0\}\Rightarrow x=\frac{-b\plusmn\sqrt{b^2-4ac}}{2a},(a\neq 0)
\]&lt;/span> 同时，当时Fisher也并没有建立起MLE的严谨理论，因此退而求其次，Fisher构造重新了一个&lt;strong>不包含&lt;span class="math">\(\arg\)&lt;/span>运算符且最接近MLE&lt;/strong>的统计量，即： &lt;span class="math">\[
T(X)=\sum_{i=1}^n\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=\frac{\partial l(X;\theta)}{\partial \theta}\tag{4}
\]&lt;/span> 在构造这个统计量时，仅仅是不包含&lt;span class="math">\(\arg\)&lt;/span>运算符，其他MLE结构都保留了下来。&lt;/p>
&lt;p>当式(4)是以样本随机变量&lt;span class="math">\(X\)&lt;/span>为自变量的时候，我们叫他统计量&lt;span class="math">\(T(X)\)&lt;/span>，但是如果我们换个角度，当式(4)以参数&lt;span class="math">\(\theta\)&lt;/span>为自变量时，我们将式(4)重新起一个名字，叫做分数函数&lt;span class="math">\(S(\theta)\)&lt;/span>，即； &lt;span class="math">\[
S(\theta)=\sum_{i=1}^n\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=\frac{\partial l(X;\theta)}{\partial \theta}\tag{5}
\]&lt;/span> 为了方便，我们可以将&lt;span class="math">\(\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}\)&lt;/span>记为&lt;span class="math">\(S(\theta;x_i)\)&lt;/span>，则式(5)可写为： &lt;span class="math">\[
S(\theta)=\sum_{i=1}^n S(\theta;x_i)\tag{5.1}
\]&lt;/span>&lt;/p>
&lt;p>从实质内容来看，式(4)(5)没有差别，只不过一个以&lt;span class="math">\(X\)&lt;/span>为自变量，把&lt;span class="math">\(\theta\)&lt;/span>当成已知的；一个以&lt;span class="math">\(\theta\)&lt;/span>作为自变量，把&lt;span class="math">\(X\)&lt;/span>当成已知的。我们之所以能这么做的原因，是因为式子中本质上存在两个未知变量：随机变量&lt;span class="math">\(X\)&lt;/span>和未知参数&lt;span class="math">\(\theta\)&lt;/span>，式(4)(5)都只是其中一个角度来看待。&lt;/p>
&lt;p>既然，已经选择好了一个好用的统计量&lt;span class="math">\(T(X)\)&lt;/span>或者另一个角度叫分数函数&lt;span class="math">\(S(\theta)\)&lt;/span>，我们来计算下它的方差&lt;span class="math">\(Var(S)=E[(S-E(S))^2]=E(S^2)-(E(S))^2\)&lt;/span>吧。显然，求期望的操作消除了&lt;span class="math">\(S\)&lt;/span>中的随机变量&lt;span class="math">\(X\)&lt;/span>，使之结果只可能和&lt;span class="math">\(\theta\)&lt;/span>有关（甚至可以证明期望和&lt;span class="math">\(\theta\)&lt;/span>都无关）。首先，我们来求其中的&lt;span class="math">\(E(S)\)&lt;/span>。 &lt;span class="math">\[
\begin{aligned}
E(S(X;\theta))&amp;amp;=E\big [ \sum_{i=1}^n\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}\big]=\sum_{i=1}^n E\big [\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}\big]\\
&amp;amp;=\sum_{i=1}^n\int_{-\infty}^{+\infty}\frac{\partial\ln p(x_i;\theta)}{\partial\theta}p(x_i;\theta)\mathrm{d}x_i\\
&amp;amp;=\sum_{i=1}^n\int_{-\infty}^{+\infty} \frac{1}{p(x_i;\theta)}\frac{\partial p(x_i;\theta)}{\partial \theta} p(x_i;\theta)\mathrm{d}x_i\\
&amp;amp;=\sum_{i=1}^n\int_{-\infty}^{+\infty} \frac{\partial p(x_i;\theta)}{\partial \theta}\mathrm{d}x_i(\text{交换积分和偏导})\\
&amp;amp;=\sum_{i=1}^n \frac{\partial }{\partial\theta}\int_{-\infty}^{+\infty} p(x_i;\theta)\mathrm{d}x_i=\sum_{i=1}^n \frac{\partial }{\partial\theta} 1=0
\end{aligned}\tag{6}
\]&lt;/span> 式(6)给出了一个很好的结论：分数函数的期望&lt;span class="math">\(E(S)\)&lt;/span>必然为0，和&lt;span class="math">\(\theta\)&lt;/span>也没有关系。那么，方差就可以简化为：&lt;span class="math">\(Var(S)=E(S^2)\)&lt;/span>。接下来，为了方便计算方差，我们先对其进行简化。由于对数似然函数中的每一个&lt;span class="math">\(x_i\)&lt;/span>都是独立同分布的，那么随机变量&lt;span class="math">\(x_i\)&lt;/span>的函数&lt;span class="math">\(\frac{\partial \ln (p(x_i;\theta))}{\partial \theta}\)&lt;/span>也是独立同分布的，根据方差的性质，独立同分布随机变量的和（差）的方差，等于独立同分布随机变量方差的和，即 &lt;span class="math">\[
\begin{aligned}
Var(S(\theta;X))&amp;amp;=Var[\sum_{i=1}^n\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}]=n\times Var[\frac{\partial \ln(p(x;\theta))}{\partial \theta}]\\
&amp;amp;=nE[(\frac{\partial \ln(p(x;\theta))}{\partial \theta})^2]=E[(\frac{\partial l(X;\theta)}{\partial \theta})^2]
\end{aligned}\tag{7}
\]&lt;/span> 我们注意到，式(7)存在小写的随机变量&lt;span class="math">\(x\)&lt;/span>和大写的随机变量&lt;span class="math">\(X\)&lt;/span>，大写的&lt;span class="math">\(X\)&lt;/span>是样本容量为&lt;span class="math">\(n\)&lt;/span>的样本，小写的&lt;span class="math">\(x\)&lt;/span>是每一次抽样的样品，当使用简单随机抽样时，由于每一个&lt;span class="math">\(x_i\)&lt;/span>都是独立同分布的，即每一个样品从概率论的角度来讲都是等价的，其概率特征如期望、方差都是一样的，在此情况下使用样本&lt;span class="math">\(X\)&lt;/span>和样品&lt;span class="math">\(x\)&lt;/span>的期望和方差都是&lt;span class="math">\(n\)&lt;/span>倍的关系，即&lt;span class="math">\(Var(S(\theta;X))=nVar(S(\theta;x))\)&lt;/span>。&lt;/p>
&lt;p>那么使用哪一个方差更合理呢？注意我们之前说过，我们默认使用的采样方式都是简单随机抽样，但是实际中采样方式可能不同，因此，&lt;span class="math">\(X\)&lt;/span>中每一个样品&lt;span class="math">\(\{x_1,x_2,\dotsb,x_n\}\)&lt;/span>可能并不是等价的。我们追求更精细化的表达，希望方差针对每一个样品进行定义，从这个角度，更倾向于使用单个样品的方差，即&lt;span class="math">\(Var(S(\theta;x))\)&lt;/span>。此时，整体样品本的方差就是所有单个样品方差的和。&lt;/p>
&lt;p>最终千呼万唤始出来，历史上Fisher将每一个抽样样品随机变量&lt;span class="math">\(x\)&lt;/span>的方差定义成了&lt;strong>Fisher信息&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>Fisher信息： &lt;span class="math">\[I(\theta)=Var(S(\theta;x))=E[(\frac{\partial l(x;\theta)}{\partial \theta})^2]\tag{7.1}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>注意，式(7.1)中使用的是小写&lt;span class="math">\(x\)&lt;/span>，在简单随机抽样时，样本&lt;span class="math">\(X\)&lt;/span>中的各个样品&lt;span class="math">\(x_i\)&lt;/span>独立同分布，即都为等价的随机变量。&lt;/p>
&lt;p>如果我们再对式（6）中&lt;span class="math">\(\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x\)&lt;/span>再求&lt;span class="math">\(\theta\)&lt;/span>的偏导（二阶偏导）有（由于独立同分布，省略下标&lt;span class="math">\(i\)&lt;/span>）： &lt;span class="math">\[
\begin{aligned}
0&amp;amp;=\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x(交互积分微分顺序)\\
&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)+\frac{\partial\ln p(x;\theta)}{\partial\theta}\frac{\partial p(x;\theta)}{\partial\theta}]\mathrm{d}x\\
&amp;amp;\because \frac{\partial p(x;\theta)}{\partial\theta}=\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\\
0&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)+(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2p(x;\theta)]\mathrm{d}x\\
0&amp;amp;=\int_{-\infty}^{+\infty}\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x+\underbrace{\int_{-\infty}^{+\infty}(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2p(x;\theta)\mathrm{d}x}_{I(\theta)}\\
\end{aligned}
\]&lt;/span> 从而有： &lt;span class="math">\[
\begin{aligned}
0&amp;amp;=\int_{-\infty}^{+\infty}\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x+I(\theta)\\
I(\theta)&amp;amp;=-\int_{-\infty}^{+\infty}\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x\\
&amp;amp;=-E[\frac{\partial^2 l(x;\theta)}{\partial \theta^2}]
\end{aligned}\tag{7.2}
\]&lt;/span> 综上所述，我们得到了Fisher信息的三种等价表达形式： &lt;span class="math">\[I(\theta)=Var(S(\theta;x))\tag{8.1}\]&lt;/span> &lt;span class="math">\[I(\theta)=E[(\frac{\partial l(x;\theta)}{\partial \theta})^2]=E[S^2(\theta;x)]\tag{8.2}\]&lt;/span> &lt;span class="math">\[I(\theta)=-E[\frac{\partial^2 l(x;\theta)}{\partial \theta^2}]=-E[S&amp;#39;(\theta;x)]\tag{8.3}\]&lt;/span> 为了简便，下文中&lt;span class="math">\(S(\theta;x)\)&lt;/span>默认写为&lt;span class="math">\(S(\theta)\)&lt;/span>。&lt;/p>
&lt;p>那么，现在我们明确知道了，Fisher信息是分数函数的方差（式(8.1)），式(8.2,)(8.2)是化简计算的结果。那么通常情况下，我们总觉得&lt;strong>估计的方差应该是越小越好吧&lt;/strong>？！极端情况，当估计的方差为0时，我们就可以精确地得到结果。但是，从信息的直观意思来看，通常大家是认为信息量越大越有利于估计。这两种直觉貌似是针锋相对的。&lt;/p>
&lt;p>实际上，这涉及到最根本的一个问题：&lt;strong>分数函数的方差和待估计参数的方差是一回事吗&lt;/strong>？我们之前提到，Fisher在考虑利用方差设计Fisher信息时，认为第四步&lt;span class="math">\(\arg\limits_{\theta}\)&lt;/span>操作难以处理，而采取了近似操作，正是因为这步近似，使得分数函数的方差和待估计参数&lt;span class="math">\(\theta_{MLE}\)&lt;/span>的估计方差产生了区别！二者确实紧密相关，但是并不一样。硬要说的话，二者是整体与局部的关系，分数函数的方差是描述在整个参数空间内的整体的方差大小，而真正MLE估计的方差的是描述在&lt;span class="math">\(\theta_{MLE}\)&lt;/span>附近的性质，如果是无偏估计就是描述估计真值&lt;span class="math">\(\theta_0\)&lt;/span>附近的性质！那么，如何将描述整个参数空间性质的Fisher信息和真正MLE估计的能力联系起来呢？&lt;/p>
&lt;h3 id="mle的一个准严格推导">MLE的一个准严格推导&lt;/h3>
&lt;p>MLE的估计精确估计结果需要较为复杂的推导，这里我们利用&lt;strong>中心极限定理&lt;/strong>，给出一种在抽样规模较大的情形下的准严格推导。上一小节中，我们知道Fisher信息来自于分数函数的方差，算是对整体性质的描述，而实际上我们更需要知道，在MLE估计结果在真值点&lt;span class="math">\(\theta=\theta_0\)&lt;/span>邻域的效果如何。&lt;/p>
&lt;p>数学中，如果想看一个函数在某点附近的特性，有一个非常常用的工具，即&lt;strong>级数展开&lt;/strong>，比如泰勒级数，洛朗级数（泰勒级数在复变领域的扩展），傅里叶级数，狄利克雷级数等等。&lt;strong>在这里，我们使用泰勒级数，在MLE估计的结果&lt;span class="math">\(\theta=\theta_0\)&lt;/span>进行展开，用以探索其邻域的性质&lt;/strong>。&lt;/p>
&lt;p>首先，根据MLE的计算，我们可以得到最大似然估计的结果&lt;span class="math">\(\theta_{MLE}\)&lt;/span>，此时根据MLE的步骤5，显然有： &lt;span class="math">\[
\theta_{MLE}=\arg\limits_{\theta}\{\sum_{i=1}^n \frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=0\}\\
\Rightarrow S(\theta_{MLE};X)=\sum_{i=1}^n S(\theta_{MLE};x_i)=0\tag{9}
\]&lt;/span> 注意，&lt;span class="math">\(\theta_{MLE}\)&lt;/span>是让整体样本&lt;span class="math">\(X\)&lt;/span>的分数函数&lt;span class="math">\(S(\theta_{MLE};X)\)&lt;/span>为0，并不保证每一个样品&lt;span class="math">\(x_i\)&lt;/span>的分数函数&lt;span class="math">\(S(\theta_{MLE};x_i)\)&lt;/span>为0。其中： &lt;span class="math">\[
S(\theta_{MLE};x_i)=[\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}|\theta=\theta_{MLE}]
\]&lt;/span> 其次，我们将分数函数&lt;span class="math">\(S(\theta;X)\)&lt;/span>在真值点&lt;span class="math">\(\theta_0\)&lt;/span>处泰勒展开有： &lt;span class="math">\[
\begin{aligned}
S(\theta;X)
&amp;amp;=\sum_{i=1}^n[S(\theta_0;x_i)+\frac{1}{1!}S&amp;#39;(\theta_0;x_i)(\theta-\theta_0)+o(\theta-\theta_0)]\\
&amp;amp;=\sum_{i=1}^n S(\theta_0;x_i)+ S&amp;#39;(\theta_0;x_i)(\theta-\theta_0)+\sum_{i=1}^n o(\theta-\theta_0)
\end{aligned}
\]&lt;/span> 将式(9)结果代入上市可得： &lt;span class="math">\[
0=\sum_{i=1}^n S(\theta_0;x_i)+\sum_{i=1}^n S&amp;#39;(\theta_0;x_i)(\theta_{MLE}-\theta_0)+\sum_{i=1}^n o(\theta_{MLE}-\theta_0)\\
\text{两边同时除以}\frac{1}{n}\\
0=\frac{1}{n}\sum_{i=1}^n S(\theta_0;x_i)+\frac{1}{n}\sum_{i=1}^n S&amp;#39;(\theta_0;x_i)(\theta_{MLE}-\theta_0)+\frac{1}{n}\sum_{i=1}^n o(\theta_{MLE}-\theta_0)\tag{10}
\]&lt;/span> 看似两边同时除以&lt;span class="math">\(\frac{1}{n}\)&lt;/span>是一个很普通的操作，但是，我们可以从概率论角度分别赋予等式右边各项实际含义。&lt;/p>
&lt;p>首先，我们看等式右边第一项&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n S(\theta_0;x_i)\)&lt;/span>，我们之前提到过分数函数和统计量是一体两面，实际上分数函数可以换成统计量的写法，即&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n T(x_i;\theta)\)&lt;/span>，其中&lt;span class="math">\(x_i\)&lt;/span>都是由随机变量&lt;span class="math">\(x\)&lt;/span>抽样出来的样品，所以等式左右可以看成是&lt;strong>随机变量抽取&lt;span class="math">\(n\)&lt;/span>次后求平均数&lt;/strong>，这恰好和&lt;strong>中心极限定理&lt;/strong>的应用场景匹配！&lt;/p>
&lt;blockquote>
&lt;p>林德伯格－列维中心极限定理：&lt;strong>独立同分布&lt;/strong>(iid)、且&lt;strong>数学期望和方差有限&lt;/strong>的随机变量序列均值的标准化和以标准正态分布为极限。用数学语言描述为：&lt;/p>
&lt;p>设随机变量&lt;span class="math">\(X_{1},X_{2},\cdots ,X_{n}\)&lt;/span>独立同分布，且具有&lt;strong>有限的数学期望和方差&lt;/strong>&lt;span class="math">\(E(X_{i})=\mu\)&lt;/span>，&lt;span class="math">\(D(X_{i})=\sigma ^{2}\neq 0(i=1,2,\cdots ,n)\)&lt;/span>。记 &lt;span class="math">\[{\bar{X}}={\frac {1}{n}}\sum_{i=1}^{n}X_{i}，\zeta_{n}={\frac {{\bar {X}}-\mu }{\sigma /{\sqrt {n}}}}，\]&lt;/span> 则 &lt;span class="math">\[\lim_{n\rightarrow \infty }P\left(\zeta_{n}\leq z\right)=\Phi \left(z\right)\]&lt;/span> 其中&lt;span class="math">\(\Phi (z)\)&lt;/span>是标准正态分布的分布函数。&lt;/p>
&lt;/blockquote>
&lt;p>因此当&lt;span class="math">\(n\)&lt;/span>较大时，式(10)的右侧第一项将会逐渐收敛于正态分布，且其期望、方差我们之前已经求出来：&lt;span class="math">\(E[S(\theta_0;x)]=0,Var[S(\theta_0;x)]=I(\theta_0)\)&lt;/span>，即 &lt;span class="math">\[
\frac{1}{n}\sum_{i=1}^n S(\theta_0;x_i)\sim N(0,\frac{1}{n}I(\theta_0))\tag{11}
\]&lt;/span> 式(11)是一个非常良好的结论，利用了中心极限定理得到了我们所期望的正态分布，也是后续步骤的根本。&lt;/p>
&lt;p>式(10)的第二项中&lt;span class="math">\(\theta_{MLE}-\theta_0\)&lt;/span>与求和项&lt;span class="math">\(i\)&lt;/span>无关，可以当成系数，而&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n S&amp;#39;(\theta_0;x_i)\)&lt;/span>求均值的操作，在当&lt;span class="math">\(n\)&lt;/span>较大时，根据大数定律可以看成是&lt;span class="math">\(E[S&amp;#39;(\theta_0;x)]\)&lt;/span>。而根据式(8.3)可知，&lt;span class="math">\(-E[S&amp;#39;(\theta_0;x)]=I(\theta_0)\)&lt;/span>，即 &lt;span class="math">\[
\frac{1}{n}\sum_{i=1}^n S&amp;#39;(\theta_0;x_i)\approx -I(\theta_0)\tag{12}
\]&lt;/span> 当&lt;span class="math">\(n\)&lt;/span>较大时成立。&lt;/p>
&lt;p>而式(10)的第三项，&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n o(\theta_{MLE}-\theta_0)\)&lt;/span>是&lt;span class="math">\((\theta_{MLE}-\theta_0)\)&lt;/span>的一序列高阶项的均值，其结果依然是&lt;span class="math">\((\theta_{MLE}-\theta_0)\)&lt;/span>的高阶项。当&lt;span class="math">\(n\)&lt;/span>较大时，&lt;span class="math">\(\theta_{MLE}\)&lt;/span>趋近于&lt;span class="math">\(\theta_0\)&lt;/span>，此时&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n o(\theta_{MLE}-\theta_0)\)&lt;/span>为高阶无穷小，可以近似忽略，那么，综合式(11,12)，式(10)的最终结果可以简化为： &lt;span class="math">\[
I(\theta_0)(\theta_{MLE}-\theta_0)\sim N(0,\frac{1}{n}I(\theta_0))\\
\Rightarrow \theta_{MLE}\sim N(\theta_0,\frac{1}{nI(\theta_0)})\tag{13}
\]&lt;/span> 即最大似然估计结果&lt;span class="math">\(\theta_{MLE}\)&lt;/span>渐进服从于以真值&lt;span class="math">\(\theta_0\)&lt;/span>为均值，&lt;span class="math">\(\frac{1}{nI(\theta_0)})\)&lt;/span>为方差的正态分布。&lt;/p>
&lt;p>式(13)给出的结论就很显然了，为了让&lt;span class="math">\(\theta_{MLE}\)&lt;/span>更贴近&lt;span class="math">\(\theta_0\)&lt;/span>，方差显然是越小越好。而且式(13)也给出了缩小方差的两个途径：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>增加&lt;span class="math">\(n\)&lt;/span>，即增加样本容量。这个很好理解，统计学一般都是抽样越多，效果越好。而且，在独立同分布场景下，样本总的Fisher信息量&lt;span class="math">\(I(\theta_0;X)\)&lt;/span>和样本容量&lt;span class="math">\(n\)&lt;/span>是线性关系，是单位样品Fisheries信息量&lt;span class="math">\(I(\theta_0;x)\)&lt;/span>的&lt;span class="math">\(n\)&lt;/span>倍。&lt;/li>
&lt;li>增加Fisher信息量&lt;span class="math">\(I(\theta)\)&lt;/span>。这里面门道就很多了，最重要的就是&lt;strong>设计合理的估计方法&lt;/strong>。同时也告诉我们，当估计方法确定时，估计的精确性是有上限的！这个理论上限和Fisher信息量直接相关，学术界把它命名为&lt;strong>Cramer-Rao界&lt;/strong>，和香农信息量规定的香农界一样，是统计学理论中最重要的理论边界之一。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>TIPS：需要指出的是，Fisher信息和Cramer-Rao界和香农定理一样，只是告诉了我们这个理论上界，但是并没有告诉我们如何达到这个理论界限的方法。因此，如何设计达到或接近理论界限是学术研究中乐此不疲的领域。&lt;/p>
&lt;/blockquote>
&lt;h2 id="fisher信息的应用">Fisher信息的应用&lt;/h2>
&lt;p>Fisher信息应用从三个方面来看（主要参考文章&lt;a href="https://arxiv.org/abs/1705.01064">A Tutorial on Fisher Information&lt;/a>），分别是频率学派角度，贝叶斯学派角度和最小描述长度角度。&lt;/p>
&lt;h3 id="频率学派角度应用fisher信息">频率学派角度应用Fisher信息&lt;/h3>
&lt;p>由于Fisher信息本身就是由频率学派大佬Fisher提出来的，因此Fisher信息在频率学派统计方法中也应用最早。除了为了最原始的功能，即提供参数估计的性能边界，之外还可以用于试验精度要求设计、假设检验和置信区间构造与估计。其利用Fisher信息的核心都是MLE估计结果的&lt;strong>概率分布或近似场景下的渐近正态性&lt;/strong>。&lt;/p>
&lt;p>感觉对我而言比较容易理解，同时用的不太多，需要用到是可以参考&lt;a href="https://arxiv.org/abs/1705.01064">A Tutorial on Fisher Information&lt;/a>中第二节内容，&lt;/p>
&lt;h3 id="贝叶斯学派角度应用fisher信息">贝叶斯学派角度应用Fisher信息&lt;/h3>
&lt;h3 id="最小描述长度角度应用fisher信息">最小描述长度角度应用Fisher信息&lt;/h3>
&lt;p>最小描述长度（Minimum Description Length， MDL）原则是将奥卡姆剃刀形式化后的一种结果。其想法是，在给予假说的集合的情况下，能产生最多资料压缩效果的那个假说是最好的，即该原则寻求最简单、最不复杂的模型。它是在1978年由Jorma Rissanen所引入的。其原理是对于一组给定的实例数据&lt;span class="math">\(D\)&lt;/span>，如果要对其进行保存，为了节省存储空间，&lt;strong>一般采用某种模型&lt;span class="math">\(H\)&lt;/span>对其进行编码压缩，然后再保存压缩后的数据&lt;/strong>。同时，为了以后正确恢复这些实例数据，将所用的模型&lt;span class="math">\(H\)&lt;/span>也保存起来。所以需要保存的总数据长度(比如比特数) 等于这些实例数据&lt;span class="math">\(D\)&lt;/span>进行编码压缩后的长度加上保存模型&lt;span class="math">\(H\)&lt;/span>所需的长度，将该数据长度称为总描述长度，即 &lt;span class="math">\[
L_{total}=L(H)+L(D|H)
\]&lt;/span> MDL原则就是就是要求&lt;strong>选择对此数据集&lt;span class="math">\(D\)&lt;/span>总描述长度最小的模型&lt;span class="math">\(H\)&lt;/span>&lt;/strong>。MDL计算与BIC（Bayesian Information Criterion, 贝叶斯信息准则）非常相似，在某些情况下可以证明是等效的。&lt;/p></description></item><item><title>读书笔记之协和八公众号文章总结笔记</title><link>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%8F%E5%92%8C%E5%85%AB%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%8F%E5%92%8C%E5%85%AB%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/</guid><description>
&lt;h2 id="概率统计随机过程之协和八公众号文章总结笔记">概率统计随机过程之协和八公众号文章总结笔记&lt;!-- omit in toc -->&lt;/h2>
&lt;p>在公众号【协和八】读到了一个将统计学的专栏，虽然是医学系研究者写的，但是他山之石可以攻玉，其中讲解了很多很有用的统计学知识和难解的点。本篇笔记是该专栏的阅读笔记。&lt;/p>
&lt;p>专栏链接：&lt;a href="https://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&amp;amp;mid=2652555217&amp;amp;idx=1&amp;amp;sn=da5332753856efb4a86aa695839523e6&amp;amp;chksm=80bbd60cb7cc5f1a3571f2f6389b3e0331fdf2612db62ae95de172c0d49b172efd8e4d236772&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0826Jegh5eGekDeo3gagkNmi&amp;amp;sharer_sharetime=1661503954453&amp;amp;sharer_shareid=2ac9f69d9b7a82eb74ce119fa6cee32f&amp;amp;key=38beb1eea6d155d34b2c714171a9bbdb1023b5929c6a8d4959ab5030989d46cf92a10bac409bfa0cd4d56ee20f279321f8723c97faa6694ae7d7835c23e88d84b080cf6e011938d7a86e0e3f88c01a06158aa31a07f803d47f5ec841f6d9e05a6f6c34694e85cfe4a434adb30c7f79de0d3f1f539bcca43bc11dbadcd1cb40ce&amp;amp;ascene=1&amp;amp;uin=MTcxMzg4NjU4MQ%3D%3D&amp;amp;devicetype=Windows+10+x64&amp;amp;version=62090538&amp;amp;lang=zh_CN&amp;amp;exportkey=AVUF2MlVFJXP3eTrXT%2FKrac%3D&amp;amp;acctmode=0&amp;amp;pass_ticket=BjR6ClCbka%2Fh7sRrwT4qhuK4uUG4PH9gs7DDKMgyVwMixFT6vexfMRG4H8Sf%2Fq8J&amp;amp;wx_header=0">说人话的统计学——终点·起点 | 协和八&lt;/a>&lt;/p>
&lt;h2 id="第1章高屋建瓴看统计">第1章：高屋建瓴看统计&lt;/h2>
&lt;h3 id="你真的懂p值吗">你真的懂p值吗？&lt;/h3>
&lt;h3 id="做统计多少数据才算够上">做统计，多少数据才算够？（上）&lt;/h3>
&lt;h3 id="做统计多少数据才算够下">做统计，多少数据才算够？（下）&lt;/h3>
&lt;h3 id="提升统计功效让评审心服口服">提升统计功效，让评审心服口服&lt;/h3>
&lt;h3 id="你的科研成果都是真的吗">你的科研成果都是真的吗？&lt;/h3>
&lt;h3 id="见识数据分析的独孤九剑">见识数据分析的「独孤九剑」&lt;/h3>
&lt;h3 id="贝叶斯vs频率派武功到底哪家强">贝叶斯vs频率派：武功到底哪家强？&lt;/h3>
&lt;h2 id="第2章算术平均数与正态分布">第2章：算术平均数与正态分布&lt;/h2>
&lt;h3 id="数据到手了第一件事先干啥">数据到手了，第一件事先干啥？&lt;/h3>
&lt;h3 id="算术平均数简单背后有乾坤">算术平均数：简单背后有乾坤&lt;/h3>
&lt;h3 id="正态分布到底是怎么来的">正态分布到底是怎么来的？&lt;/h3>
&lt;h2 id="第3章t检验两组平均数的比较">第3章：t检验：两组平均数的比较&lt;/h2>
&lt;h3 id="想玩转t检验你得从这一篇看起">想玩转t检验？你得从这一篇看起&lt;/h3>
&lt;h3 id="就是要实用t检验的七十二变">就是要实用！t检验的七十二变&lt;/h3>
&lt;h3 id="不是正态分布t检验还能用吗">不是正态分布，t检验还能用吗？&lt;/h3>
&lt;h3 id="只有-15-个标本也能指望t检验吗">只有 15 个标本，也能指望t检验吗？&lt;/h3>
&lt;h3 id="样本分布不正态数据变换来救场">样本分布不正态？数据变换来救场&lt;/h3>
&lt;h3 id="数据变换的万能钥匙box-cox变换">数据变换的万能钥匙：Box-Cox变换&lt;/h3>
&lt;h3 id="t检验用不了别慌还有神奇的非参数检验">t检验用不了？别慌，还有神奇的非参数检验&lt;/h3>
&lt;h3 id="只讲p值不讲效应大小都是耍流氓">只讲p值，不讲效应大小，都是耍流氓&lt;/h3>
&lt;h3 id="找出t检验的效应大小对耍流氓-say-no">找出t检验的效应大小，对耍流氓 say no&lt;/h3>
&lt;h3 id="用置信区间就是这么不自信">用置信区间，就是这么（不）自信&lt;/h3>
&lt;h3 id="如何确定t检验的置信区间">如何确定t检验的置信区间&lt;/h3>
&lt;h3 id="优雅秀出你的t检验提升paper逼格">优雅秀出你的t检验，提升Paper逼格&lt;/h3>
&lt;h3 id="要做t检验这两口毒奶可喝不得">要做t检验，这两口毒奶可喝不得&lt;/h3>
&lt;h2 id="第4章方差分析anova多组平均数的比较">第4章：方差分析（ANOVA）：多组平均数的比较&lt;/h2>
&lt;h3 id="要比较三组数据t检验还能用吗">要比较三组数据，t检验还能用吗？&lt;/h3>
&lt;h3 id="anova在手多组比较不犯愁">ANOVA在手，多组比较不犯愁&lt;/h3>
&lt;h3 id="anova的基本招式你掌握了吗">ANOVA的基本招式你掌握了吗？&lt;/h3>
&lt;h3 id="anova做出了显著性事儿还没完呢">ANOVA做出了显著性？事儿还没完呢&lt;/h3>
&lt;h3 id="听说成对t检验还有-anova进阶版">听说，成对t检验还有 ANOVA进阶版？&lt;/h3>
&lt;h3 id="重复测量-anova你要知道的事儿都在这里啦">重复测量 ANOVA：你要知道的事儿都在这里啦&lt;/h3>
&lt;h3 id="没听说过多因素-anova那你就可就-out了">没听说过多因素 ANOVA？那你就可就 OUT了&lt;/h3>
&lt;h3 id="多因素-anova好几个单因素-anova可没这么简单">多因素 ANOVA＝好几个单因素 ANOVA？可没这么简单&lt;/h3>
&lt;h3 id="两个因素相互影响anova结果该如何判读">两个因素相互影响，ANOVA结果该如何判读？&lt;/h3>
&lt;h3 id="anova还能搞三四五因素等等我头有点儿晕">ANOVA还能搞三四五因素？等等，我头有点儿晕&lt;/h3>
&lt;h3 id="要做-anova样本量多大才够用">要做 ANOVA，样本量多大才够用&lt;/h3>
&lt;h2 id="第5章线性回归统计建模初步">第5章：线性回归：统计建模初步&lt;/h2>
&lt;h3 id="车模航模你玩过统计学模型你会玩吗">车模航模你玩过，统计学模型你会玩吗？&lt;/h3>
&lt;h3 id="如果只能学习一种统计方法我选择线性回归">如果只能学习一种统计方法，我选择线性回归&lt;/h3>
&lt;h3 id="回归线三千我只取这一条">回归线三千，我只取这一条&lt;/h3>
&lt;h3 id="三千回归线里选中了你你靠谱吗">三千回归线里选中了你，你靠谱吗？&lt;/h3>
&lt;h3 id="自变量不止一个线性回归该怎么做">自变量不止一个，线性回归该怎么做？&lt;/h3>
&lt;h3 id="找出交互效应让线性模型更万能">找出「交互效应」，让线性模型更万能&lt;/h3>
&lt;h3 id="天啦噜没考虑到混杂因素后果会这么严重">天啦噜！没考虑到混杂因素，后果会这么严重？&lt;/h3>
&lt;h3 id="回归系数不显著也许是打开方式不对">回归系数不显著？也许是打开方式不对&lt;/h3>
&lt;h3 id="评价线性模型r平方是个好裁判吗">评价线性模型，R平方是个好裁判吗？&lt;/h3>
&lt;h3 id="如果r平方是砒霜本文教你三种解药">如果R平方是砒霜，本文教你三种解药&lt;/h3>
&lt;h3 id="线性模型生病了你懂得怎样诊断吗">线性模型生病了，你懂得怎样诊断吗？&lt;/h3>
&lt;h3 id="脱离群众的数据点是春风化雨还是秋风扫落叶">「脱离群众」的数据点，是「春风化雨」还是「秋风扫落叶」&lt;/h3>
&lt;h2 id="第6章广义线性模型统计建模进阶">第6章：广义线性模型：统计建模进阶&lt;/h2>
&lt;h3 id="你在-或者不在-需要逻辑回归来算">你在 或者不在 需要逻辑回归来算&lt;/h3>
&lt;h3 id="逻辑回归的袅娜曲线你是否会过目难忘">逻辑回归的袅娜曲线，你是否会过目难忘？&lt;/h3>
&lt;h3 id="逻辑回归的统计检验原来招数辣么多">逻辑回归的统计检验，原来招数辣么多？&lt;/h3>
&lt;h3 id="线性回归能玩多变量逻辑回归当然也能">线性回归能玩多变量，逻辑回归当然也能&lt;/h3>
&lt;h3 id="喂你的逻辑回归模型该做个体检啦">喂，你的逻辑回归模型该做个体检啦&lt;/h3>
&lt;h3 id="逻辑回归能摆平二分类因变量那不止二分类呢">逻辑回归能摆平二分类因变量，那……不止二分类呢？&lt;/h3>
&lt;h3 id="让人眼花缭乱的多项逻辑回归原来是这么用的">让人眼花缭乱的多项逻辑回归，原来是这么用的&lt;/h3>
&lt;h3 id="只问方向无问远近定序回归的执念你懂吗">只问方向，无问远近，定序回归的执念你懂吗？&lt;/h3>
&lt;h3 id="包教包会定序回归实战">包教包会：定序回归实战&lt;/h3>
&lt;h3 id="数风流人物还靠泊松回归">「数」风流人物，还靠泊松回归&lt;/h3>
&lt;h3 id="广义线性模型到底是个什么鬼">广义线性模型到底是个什么鬼？&lt;/h3>
&lt;h2 id="自检">自检&lt;/h2>
&lt;h3 id="妈妈说答对的童鞋才能中奖">妈妈说答对的童鞋才能中奖&lt;/h3>
&lt;h3 id="统计学的十个误区你答对了吗">统计学的十个误区，你答对了吗？&lt;/h3></description></item><item><title>概率统计随机过程之时间序列分析</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/</link><pubDate>Sat, 11 Jun 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/</guid><description>
&lt;h2 id="概率统计随机过程之时间序列分析">概率统计随机过程之时间序列分析&lt;!-- omit in toc -->&lt;/h2>
&lt;p>时间序列分析（Time-Series Analysis）是统计与随机过程中常用的场景，在机器学习、强化学习的预测中也有重要地位，本笔记介绍时间序列分析的基本概念和指标，记录对时序分析的基本理解。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#时间序列的基本概念">时间序列的基本概念&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时间序列的类型">时间序列的类型&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时间序列的因素分解">时间序列的因素分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时间序列分析基础">时间序列分析基础&lt;/a>&lt;/li>
&lt;li>&lt;a href="#发展水平与平均发展水平">发展水平与平均发展水平&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#绝对数时间序列的序时平均数">绝对数时间序列的序时平均数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相对数时间序列的序时平均数">相对数时间序列的序时平均数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#增长量">增长量&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#时间序列的速度分析">时间序列的速度分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#发展速度">发展速度&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#平均发展速度">平均发展速度&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#增长速度">增长速度&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#平均增长速度">平均增长速度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#年化增长速度">年化增长速度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#增长1绝对值">增长1%绝对值&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#时间序列的预测">时间序列的预测&lt;/a>&lt;/li>
&lt;li>&lt;a href="#平稳序列预测">平稳序列预测&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#简单平均法">简单平均法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#移动平均法">移动平均法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数平滑法">指数平滑法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#趋势序列的预测">趋势序列的预测&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#线性趋势预测">线性趋势预测&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非线性趋势预测">非线性趋势预测&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#季节因素考虑">季节因素考虑&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="时间序列的基本概念">时间序列的基本概念&lt;/h2>
&lt;blockquote>
&lt;p>时间序列：又叫动态序列，是指将同一对象不同时间的观测数据按期发生的先后顺序排列而成的序列。&lt;/p>
&lt;/blockquote>
&lt;p>这就是我们常说的时域表示，自变量（横轴）是时间，因变量（纵轴）是观察值。常见的时间序列如股市K线图，各年经济统计图表，Gartner技术成熟曲线等。&lt;/p>
&lt;p>时间序列是一种动态的数列分析。时间序列中&lt;strong>每一时期的数值都是由许多不同的因素同时作用的综合结果&lt;/strong>。&lt;/p>
&lt;p>时间序列的基本要素：&lt;/p>
&lt;ul>
&lt;li>所属的时间范围：可以是年份，季度，月份或者其他任何形式的时间&lt;/li>
&lt;li>不同时间上的观察值&lt;/li>
&lt;/ul>
&lt;h3 id="时间序列的类型">时间序列的类型&lt;/h3>
&lt;p>时间序列的类型从平稳性来看分为平稳序列和非平稳序列。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>平稳序列。基本不存在趋势的序列，各观察值基本在某个固定的水平上波动，或虽存在波动程度不同，但并不存在某种规律。我觉得和随机过程中的平稳概念类似，各态历经性，每一个时刻的值域是一样的，值域不会随着时间的变化而变化。&lt;/li>
&lt;li>非平稳序列。&lt;/li>
&lt;li>趋势型，随着时间增长，序列观察值存在与时间轴线性或非线性相关的趋势。&lt;/li>
&lt;li>复合型，趋势、季节性、周期性的复合序列。&lt;/li>
&lt;/ol>
&lt;div class="figure">
&lt;img src="../../images/时间序列稳态与非稳态.png" alt="时间序列稳态与非稳态.png" />&lt;p class="caption">时间序列稳态与非稳态.png&lt;/p>
&lt;/div>
&lt;h3 id="时间序列的因素分解">时间序列的因素分解&lt;/h3>
&lt;p>时间序列分析的任务就是要正确地确定时间序列的性质，对影响时间序列的各种因素加以分解和测定，以便对未来的状况作出判断和预测。这些因素按照性质可以划分为：长期趋势、季节变动、循环变动、不规则变动。&lt;/p>
&lt;p>时间序列因素四部分组成：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>趋势（T，trend），线性趋势、非线性趋势&lt;/li>
&lt;li>季节性（S，seasonal fluctuation）&lt;/li>
&lt;li>周期性（C，cyclical fluctuation）&lt;/li>
&lt;li>随机性（I，irregular variations）&lt;/li>
&lt;/ol>
&lt;p>根据因素的关系，时间序列可以通过以下两种方式分解：&lt;/p>
&lt;p>&lt;strong>加法分解&lt;/strong>。 &lt;span class="math">\[Y_i=T_i+S_i+C_i+I_i\]&lt;/span> 加法分解中，各影响因素是相互独立的，且均与&lt;span class="math">\(Y\)&lt;/span>有着相同的计量单位。季节性变动、周期性变动在各自周期内总会应为0，随机波动长期来看，期望也是0。在加法模型中，各因素的分解是通过减法实现，如 &lt;span class="math">\[
Y-T=S+C+I
\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>乘法分解&lt;/strong>。 &lt;span class="math">\[Y_i=T_i\times S_i\times C_i\times I_i\]&lt;/span> 乘法分解中，只有趋势与与&lt;span class="math">\(Y\)&lt;/span>有着相同的计量单位，其他因素都是以&lt;strong>比率&lt;/strong>的形式出现，季节性变动、周期性变动在各自周期内几何平均会应为1（100%），随机波动长期来看，几何平均也是1（100%）。在乘法模型中，各因素的分解是通过除法实现，如 &lt;span class="math">\[
Y\div T=S×C×I
\]&lt;/span>&lt;/p>
&lt;h2 id="时间序列分析基础">时间序列分析基础&lt;/h2>
&lt;p>时间序列分析的基本目的是描述动态变化（分析过去），揭示变化规律（认识规律）以至获悉未来数量的趋势（预测未来）。最直观的时序分析方法是图像描述，在难道初始数据后，先画出一幅图，能有助于我们更快地了解大体情形。&lt;/p>
&lt;h3 id="发展水平与平均发展水平">发展水平与平均发展水平&lt;/h3>
&lt;blockquote>
&lt;p>发展水平：是指现象在不同的时间上的观察值，说明现象在某一时间上所达到的水平。表示为&lt;span class="math">\(Y_0,Y_1,\dotsb,Y_n\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>说白了，发展水平就是观察值的好听点说法。那么所谓&lt;strong>平均发展水平&lt;/strong>，就是一段时间内观察值的均值。&lt;/p>
&lt;blockquote>
&lt;p>平均发展水平:现象在不同时间上取值的平均数，又称序时平均数或动态平均数，说明现象在一段时期内所达到的一般水平。&lt;/p>
&lt;/blockquote>
&lt;h4 id="绝对数时间序列的序时平均数">绝对数时间序列的序时平均数&lt;/h4>
&lt;p>绝对数平均数就是我们常说的平均值，对于等分的&lt;strong>时期序列&lt;/strong>，其计算公式是平凡的： &lt;span class="math">\[
\bar{Y}=\frac{Y_1+Y_2+\dotsb+Y_n}{n}=\frac{\sum_{i=1}^n Y_i}{n}
\]&lt;/span> &lt;img src="../../images/时期序列时点序列.png" alt="时期序列时点序列.png" />&lt;/p>
&lt;p>所谓时期序列，就是指观察值代表了某一时段的值。对应的是时点序列，含义是指观察值只代表当前时间点的值。对于时点序列，我们可以用两侧端点的平均数作为某个时段的代表值，即&lt;span class="math">\(\bar{Y_i}=\frac{1}{2}(Y_i+Y_{i+1})\)&lt;/span>，因此时点序列的平均发展水平就是： &lt;span class="math">\[
\bar{Y}=\frac{{(Y_1+Y_2)\over 2}+{(Y_2+Y_3)\over 2}+\dotsb+{(Y_{n-1}+Y_n)\over 2}}{n-1}=\frac{{Y_1\over 2}+Y_2+\dotsb+Y_{n-1}+{Y_n\over 2}}{n-1}
\]&lt;/span> 注意，只有&lt;span class="math">\(n-1\)&lt;/span>个时段，而非像时期序列有&lt;span class="math">\(n\)&lt;/span>个时段。&lt;/p>
&lt;p>有时，时间序列并不是均等的，如下图所示： &lt;img src="../../images/间隔不相等的时间序列.png" alt="间隔不相等的时间序列" />&lt;/p>
&lt;p>我们可以用两侧端点的平均数作为某个时段的代表值，然后再乘以时间间隔的长度，最后除以总的时间，这里使用了加权平均的思想。 &lt;span class="math">\[
\bar{Y}=\frac{({Y_1+Y_2\over 2})T_1+({Y_2+Y_3\over 2})T_2+\dotsb+({Y_{n-1}+Y_n\over 2})T_{n-1}+}{\sum_{i=1}^{n-1} T_i}
\]&lt;/span>&lt;/p>
&lt;h4 id="相对数时间序列的序时平均数">相对数时间序列的序时平均数&lt;/h4>
&lt;p>首先，分别求出构成相对数的分子指标&lt;span class="math">\(a_i\)&lt;/span>和分母指标&lt;span class="math">\(b_i\)&lt;/span>的序时平均数；其次，在进行对比，即得相对数时间序列的序时平均数： &lt;span class="math">\[
\bar{Y}=\frac{\bar{a}}{\bar{b}}
\]&lt;/span>&lt;/p>
&lt;h4 id="增长量">增长量&lt;/h4>
&lt;ul>
&lt;li>逐期增长量：报告期水平与前一期水平之差，&lt;span class="math">\(\Delta_i=Y_i-Y_{i-1}\)&lt;/span>&lt;/li>
&lt;li>累计增长量：报告期水平与某一固定时期水平之差，&lt;span class="math">\(\Delta_i=Y_i-Y_{0}\)&lt;/span>。显然，累计增长量等于逐期增长量之和。&lt;/li>
&lt;li>年距增长量，为了消除季节变动影响，本期发展水平-去年同期发展水平&lt;/li>
&lt;li>平均增长量 &lt;span class="math">\[
平均增长量=\frac{累计增长量}{间隔期数}=\frac{累计增长量}{观察值个数-1}
\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;h2 id="时间序列的速度分析">时间序列的速度分析&lt;/h2>
&lt;p>时间序列的速度主要分发展速度和增长速度两个方面，发展速度是“连本带利”的速度，而增长速度主要看“利息”的增长，二者都反应了时间序列变化的快慢。&lt;/p>
&lt;h3 id="发展速度">发展速度&lt;/h3>
&lt;blockquote>
&lt;p>发展速度：报告期水平和基期水平之比，公式表达为： &lt;span class="math">\[发展速度=\frac{报告期水平}{基期水平}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>常用的发展速度有环比发展速度、同比发展速度（年距发展速度）、定基发展速度。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>环比发展速度：报告期水平与前一期水平之比&lt;span class="math">\(R_i=\frac{Y_i}{Y_{i-1}}\)&lt;/span>;&lt;/li>
&lt;li>同比发展速度：本期发展水平与去年同期发展水平之比；&lt;/li>
&lt;li>定基发展速度：报告期水平与某一固定时期水平之比&lt;span class="math">\(R_i=\frac{Y_i}{Y_0}\)&lt;/span>。如果是计算长时间内总的发展速度，称为总速度。&lt;/li>
&lt;/ol>
&lt;p>显然，观察期内各环比发展速度的乘积等于最末期的定基发展速度，即 &lt;span class="math">\[
\prod_{i=1}^n \frac{Y_i}{Y_{i-1}}=\frac{Y_n}{Y_0}
\]&lt;/span>&lt;/p>
&lt;h4 id="平均发展速度">平均发展速度&lt;/h4>
&lt;p>平均发展速度研究的是多个时期的发展速度均值，这里的均值计算方式需要仔细考虑下，并不能采用简单的算术平均，因为&lt;span class="math">\(n\)&lt;/span>个时期发展水平累积起来是乘积关系，所以求其平均数我们自然会想到将其开&lt;span class="math">\(n\)&lt;/span>次根号，即使用&lt;strong>几何平均数&lt;/strong>。 &lt;span class="math">\[
\bar{R}=\sqrt[n]{\frac{Y_1}{Y_0}\times\frac{Y_2}{Y_1}\times\dotsb\times\frac{Y_n}{Y_{n-1}}}=\sqrt[n]{\prod_{i=1}^n \frac{Y_i}{Y_{i-1}}}=\sqrt[n]{\frac{Y_n}{Y_{0}}}\\
反过来：Y_n=Y_0\bar{R}^n
\]&lt;/span>&lt;/p>
&lt;h3 id="增长速度">增长速度&lt;/h3>
&lt;blockquote>
&lt;p>增长速度：又称增长率，是增长量与基期发展水平之比，或报告期发展水平和基期发展水平之比&lt;strong>减1&lt;/strong>。 &lt;span class="math">\[\begin{aligned}
增长速度(增长率)&amp;amp;=\frac{增长量}{基期水平}=\frac{报告期水平-基期水平}{基期水平}\\&amp;amp;=发展速度-1
\end{aligned}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>增长速度表明了相对增加的程度，注意增长速度&lt;strong>可正可负可0&lt;/strong>，但是在负增长时有些概念需要谨慎使用。同样，增长速度也可分为环比增长速度、同比增长速度（年距增长速度）、定基增长速度。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>环比增长速度（率）：报告期水平与前一期水平之比减1，&lt;span class="math">\(G_i=\frac{Y_i-Y_{i-1}}{Y_{i-1}}=\frac{Y_i}{Y_{i-1}}-1\)&lt;/span>&lt;/li>
&lt;li>同比增长速度（率）:总增长率与去年同期发展水平之比，或者本期发展水平与去年同期发展水平之比减1；&lt;/li>
&lt;li>定基增长速度（率）：报告期水平与某一固定时期水平之比减1，&lt;span class="math">\(G_i=\frac{Y_i-Y_0}{Y_0}=\frac{Y_i}{Y_0}-1\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;h4 id="平均增长速度">平均增长速度&lt;/h4>
&lt;p>平均增长速度，又称平均增长率，是用来描述现象在整体观察期内平均每期增长变化的程度。它与平均发展速度有着密切关系，两者仅相差一个基数，即 &lt;span class="math">\[
平均增长速度=平均发展速度-1\\
\bar{G}=\sqrt[n]{\frac{Y_n}{Y_{0}}}-1=\bar{R}-1
\]&lt;/span>&lt;/p>
&lt;h4 id="年化增长速度">年化增长速度&lt;/h4>
&lt;p>如果我们计算不等长时期的增长率，比如要比较一个月的增长率和一个季度的增长率，由于其基准时期不同，其结果往往不客观。为了统一基准，我们可以采用年化增长率（又叫年率）来表示。我们借用平均增长速度的计算方式，先计算出观察时期的平均发展速度，然后再通过多次幂的方式求出年化发展速度。计算公式为： &lt;span class="math">\[
G_A=(\frac{Y_i}{Y_0})^{\frac{m}{n}}-1
\]&lt;/span> 其中，&lt;span class="math">\(m\)&lt;/span>为一年中的时期个数，比如按月算&lt;span class="math">\(m=12\)&lt;/span>，按季度算&lt;span class="math">\(m=4\)&lt;/span>;&lt;span class="math">\(n\)&lt;/span>表示当期和某一固定时期所跨的时期总数。&lt;/p>
&lt;h4 id="增长1绝对值">增长1%绝对值&lt;/h4>
&lt;p>在有些情况下，增长率会起到误导的作用，比如小基数场景下，不多的绝对值增长就会导致增长率的大幅上升。因此，不能单纯就增长率论增长率，需要与绝对值水平结合。为此，提出了增长1%绝对值的概念，即增长率每增长一个百分点而增加的绝对量。 &lt;span class="math">\[
增长1\%绝对值=\frac{前期水平}{100}
\]&lt;/span> 用于弥补增长率分析中的局限性。&lt;/p>
&lt;h2 id="时间序列的预测">时间序列的预测&lt;/h2>
&lt;p>我们对时间序列的研究往往是需要通过历史数据来预测未来的数据，其分析步骤如下：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>确定时间序列所包含的成分，即趋势、季节、周期、随机性的类型判断&lt;/li>
&lt;li>选择适合此类时间序列的预测方法&lt;/li>
&lt;li>对可能的预测方法进行评估，以确定最佳方案&lt;/li>
&lt;li>利用最佳方案进行预测&lt;/li>
&lt;/ol>
&lt;div class="figure">
&lt;img src="../../images/时间序列预测方法.png" alt="时间序列预测方法" />&lt;p class="caption">时间序列预测方法&lt;/p>
&lt;/div>
&lt;div class="figure">
&lt;img src="../../images/时间序列分析评估.png" alt="时间序列分析评估" />&lt;p class="caption">时间序列分析评估&lt;/p>
&lt;/div>
&lt;h3 id="平稳序列预测">平稳序列预测&lt;/h3>
&lt;p>对于既没有趋势性，也没有季节性周期性的纯随机时间序列，我们可以采用平稳序列预测方法。主要包括简单平均法、移动平均法、指数平均法三种。&lt;/p>
&lt;h4 id="简单平均法">简单平均法&lt;/h4>
&lt;p>顾名思义，就是取过去所有值的算术平均数，表达为： &lt;span class="math">\[
F_{t+1}=\frac{1}{t}(Y_1+Y_2+\dotsb+Y_t)=\frac{1}{t}\sum_{i=1}^t Y_i
\]&lt;/span> 简单平均法将所有观察值看作同等重要，不区分远期数据和近期数据的影响，适合对较为稳定的时间序列进行预测。其误差为： &lt;span class="math">\[
e_{t+1}=Y_{t+1}-F_{t+1}
\]&lt;/span>&lt;/p>
&lt;h4 id="移动平均法">移动平均法&lt;/h4>
&lt;p>一种对简单平均法做出改进的方法是移动平均法，通过对时间序列的&lt;strong>逐期递移&lt;/strong>，球的一系列平均数作为预测值。移动平均法本质上是构建了一个&lt;strong>滑动窗口&lt;/strong>，只计算窗口内的观察值的平均数。根据数据是否平等可分为简单移动平均和加权移动平均两种。&lt;/p>
&lt;p>简单移动平均：假设滑动窗口的大小为&lt;span class="math">\(k\)&lt;/span>，即计算最近的&lt;span class="math">\(k\)&lt;/span>个观察值的平均数，其预测值为： &lt;span class="math">\[
F_{t+1}=\bar{Y}_t=\frac{Y_{t-k+1}+Y_{t-k+2}+\dotsb+Y_{t-1}+Y_t}{k}
\]&lt;/span> 窗口大小&lt;span class="math">\(k\)&lt;/span>是一个超参数，需要根据实验或经验获得。&lt;span class="math">\(k\)&lt;/span>越小，越能跟上变化，&lt;span class="math">\(k\)&lt;/span>越大，预测曲线越稳定。&lt;/p>
&lt;p>对于&lt;span class="math">\(n\)&lt;/span>个预测值，我们使用均方误差（MSE）衡量其误差： &lt;span class="math">\[
MSE=\frac{\sum_{i=1}^n (Y_i-F_i)^2}{n}
\]&lt;/span>&lt;/p>
&lt;p>加权移动平均：假设滑动窗口的大小为&lt;span class="math">\(k\)&lt;/span>，这些观察值并不平等，有些观察值（比如近期的观察值）比较重要，需要加上一个权重&lt;span class="math">\(\alpha\)&lt;/span>，体现不同观察值的重要性，所有的权重值之和为1。 &lt;span class="math">\[
F_{t+1}=\bar{Y}_t=\frac{\alpha_1Y_{t-k+1}+\alpha_2Y_{t-k+2}+\dotsb+\alpha_{k-1}Y_{t-1}+\alpha_kY_t}{k}\\
\alpha_1+\alpha_2+\dotsb+\alpha_k=1
\]&lt;/span> 加权移动平均可以更加灵活的反映不同时期数据对均值的影响，但是除了超参数&lt;span class="math">\(k\)&lt;/span>，还引入了另一组超参数&lt;span class="math">\(\alpha_i,i=\{1,2,\dotsb,k\}\)&lt;/span>。我们可以通过合理地调整这些超参数，来最小化MSE。&lt;/p>
&lt;h4 id="指数平滑法">指数平滑法&lt;/h4>
&lt;p>指数平均法算是加权平均的一种特例（没有窗口限制，且权重呈等比数列）。对过去的观察值加权平均进行预测的一种方法。观察值时间越远，其权重也跟着呈现指数的下降，这也是指数平滑法的名称由来。&lt;/p>
&lt;p>指数平滑法会将时间序列进行修匀，消除随机波动，找出序列变化趋势。其计算公式为： &lt;span class="math">\[
F_{t}=\alpha Y_{t-1}+(1-\alpha)F_{t-1}
\]&lt;/span> 其中，&lt;span class="math">\(Y_{t-1}\)&lt;/span>为&lt;span class="math">\(t-1\)&lt;/span>期实际观察值，&lt;span class="math">\(F_{t-1}\)&lt;/span>为&lt;span class="math">\(t-1\)&lt;/span>期预测值，&lt;span class="math">\(\alpha\)&lt;/span>为平滑系数（&lt;span class="math">\(0&amp;lt;\alpha&amp;lt;1\)&lt;/span>），对于起点我们设&lt;span class="math">\(F_1=Y_1\)&lt;/span>。如果我们将式子展开，最终预测值会是前面各期的加权平均。显然，&lt;span class="math">\(\alpha\)&lt;/span>越大，新的观察值&lt;span class="math">\(Y_{t-1}\)&lt;/span>比例越大，预测对变动越敏感，适用于随机波动比较大的序列；反则反之。&lt;/p>
&lt;h3 id="趋势序列的预测">趋势序列的预测&lt;/h3>
&lt;p>所谓趋势，就是持续向上或持续向下的状态或规律，可分为线性趋势和非线性趋势。常用的趋势预测方法有线性趋势预测、非线性趋势预测和自回归模型预测。&lt;/p>
&lt;h4 id="线性趋势预测">线性趋势预测&lt;/h4>
&lt;p>本质上是线性回归，用观察值做线性回归。&lt;/p>
&lt;h4 id="非线性趋势预测">非线性趋势预测&lt;/h4>
&lt;p>指数曲线，使用指数曲线取回归数据。本上算是广义线性回归。对于指数曲线，只要取&lt;span class="math">\(\ln\)&lt;/span>，就可以变成线性回归。 &lt;span class="math">\[
\hat{Y}_t=b_0b_1^t
\]&lt;/span>&lt;/p>
&lt;p>修正指数曲线：加了一个常数偏置&lt;span class="math">\(K\)&lt;/span>。 &lt;span class="math">\[
\hat{Y}_t=K+b_0b_1^t
\]&lt;/span>&lt;/p>
&lt;p>Gompertz曲线： &lt;span class="math">\[
\hat{Y}_t=Ka^{b^{t}}
\]&lt;/span>&lt;/p>
&lt;p>Logistics曲线 &lt;span class="math">\[
\hat{Y}_t=\frac{1}{K+ab^t}
\]&lt;/span>&lt;/p>
&lt;p>多阶曲线：高阶多项式拟合。 &lt;span class="math">\[
\hat{Y}_t=b_0+b_1t++b_2t^2+\dotsb++b_kt^k
\]&lt;/span>&lt;/p>
&lt;h3 id="季节因素考虑">季节因素考虑&lt;/h3>
&lt;blockquote>
&lt;p>季节指数:刻画序列在一个年度个月或各季度的典型季节特征。&lt;/p>
&lt;/blockquote></description></item><item><title>概率统计随机过程之C-R不等式</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8Bc-r%E4%B8%8D%E7%AD%89%E5%BC%8F/</link><pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8Bc-r%E4%B8%8D%E7%AD%89%E5%BC%8F/</guid><description>
&lt;h2 id="概率统计随机过程之c-r不等式">概率统计随机过程之C-R不等式&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#前置条件">前置条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单参数c-r正则分布族">单参数C-R正则分布族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#费舍尔信息量">费舍尔信息量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#c-r不等式">C-R不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单参数c-r不等式">单参数C-R不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单参数c-r不等式等号成立条件">单参数C-R不等式等号成立条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多参数c-r不等式">多参数C-R不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#c-r不等式应用">C-R不等式应用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#求umvue">求UMVUE&lt;/a>&lt;/li>
&lt;li>&lt;a href="#估计的效率和有效性">估计的效率和有效性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#在渐进正态性中的应用">在渐进正态性中的应用&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Cramer-Rao不等式是另一个判别无偏估计是否为UMVUE的方法，但是Cramer-Rao不等式有更深层的含义。&lt;/p>
&lt;p>我们知道估计量始终会是一个随机变量，有自己的概率分布，而不是一个准确的值。Cramer-Rao除了给出了Cramer-Rao正则分布族这种费舍尔信息的存在条件，还有另一个更重要的贡献：&lt;strong>C-R不等式&lt;/strong>，可以说给了统计学理论上的绝望。&lt;/p>
&lt;p>C-R不等式，其实就是在说：统计，对真实的概率分布参数估计能力是有限的。举个不太恰当的类比，有点像量子理论中的测不准原理 （二者证明有相似之处哦）。C-R不等式告诉我们，无论我们如何抽样充足，无论我们统计方法如何科学，我们对参数的估计值，永远不可能无限逼近是逻辑上的真实值！&lt;/p>
&lt;p>回到C-R不等式和UMVUE的关系上来，其思想如下：设&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一切无偏估计构成的集合，所有的这些&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>中的无偏估计的方差必有一个下界（一定非负），这个下界称为C-R下界。如果&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>中某一个估计量&lt;span class="math">\(\hat g\)&lt;/span>的方差达到了这个下界，则&lt;span class="math">\(\hat{g}\)&lt;/span>就一定是参数的UMVUE，当然会对样本分布族和&lt;span class="math">\(\hat{g}\)&lt;/span>有一些正则条件。当时，使用这种下界的方法，都一个缺点，即&lt;strong>C-R不等式给出的下界经常比实际的下界更小一些&lt;/strong>。这一情况下，C-R不等式就无法判断UMVUE的存在性。此外，C-R不等式还有其他一些用处，比如计算估计的效率、有效估计等等。&lt;/p>
&lt;h2 id="前置条件">前置条件&lt;/h2>
&lt;p>C-R不等式成立需要样本分布族满足一些正则条件，适合这些条件的分布族称为&lt;strong>C-R正则分布族&lt;/strong>。&lt;/p>
&lt;h3 id="单参数c-r正则分布族">单参数C-R正则分布族&lt;/h3>
&lt;blockquote>
&lt;p>定义1：单参数Cramer-Rao正则分布族：若单参数概率分布族&lt;span class="math">\(p(x;\theta)\)&lt;/span>，&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>属于Cramer-Rao正则分布族，则需要满足以下五个条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>参数空间&lt;span class="math">\(\varTheta\)&lt;/span>是直线上的开区间；&lt;/li>
&lt;li>&lt;span class="math">\(\frac{\partial p(x;\theta)}{\partial\theta}\)&lt;/span>对所有&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>都存在；&lt;/li>
&lt;li>分布的支撑&lt;span class="math">\(\{x:p(x;\theta)&amp;gt;0\}\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关，即分布族具有共同的支撑；&lt;/li>
&lt;li>&lt;span class="math">\(p(x;\theta)\)&lt;/span>的微分与积分运算可交换；&lt;/li>
&lt;li>对所有&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，期望 &lt;span class="math">\[0&amp;lt;I(\theta)=E[(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2]&amp;lt;+\infty\tag{1}\]&lt;/span> 其中，&lt;span class="math">\(I(\theta)\)&lt;/span>为分布&lt;span class="math">\(p(x;\theta)\)&lt;/span>中含有&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>费舍尔信息量&lt;/strong>，简称信息量。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h3 id="费舍尔信息量">费舍尔信息量&lt;/h3>
&lt;p>上面，我们用式（1）定义了费舍尔信息量。其具体解释可以理解为样本中关于&lt;span class="math">\(\theta\)&lt;/span>有多少信息。&lt;span class="math">\(I(\theta)\)&lt;/span>越大，意味着样本中含有位置参数&lt;span class="math">\(\theta\)&lt;/span>的信息越多，该参数越容易估计。&lt;span class="math">\(I(\theta)\)&lt;/span>也可解释成&lt;strong>单个样品&lt;/strong>提供的信息量，由于简单抽样中，各个样品是i.i.d的，故每个样品提供的信息量&lt;span class="math">\(I(\theta)\)&lt;/span>也是一样多的，即整个样本&lt;span class="math">\((X_1,\dotsb,X_n)\)&lt;/span>所含信息量为&lt;span class="math">\(nI(\theta)\)&lt;/span>。&lt;/p>
&lt;h2 id="c-r不等式">C-R不等式&lt;/h2>
&lt;h3 id="单参数c-r不等式">单参数C-R不等式&lt;/h3>
&lt;blockquote>
&lt;p>定理1：设&lt;span class="math">\(\mathcal{F}=\{f(x;\theta),\theta\in\varTheta\}\)&lt;/span>是C-R正则分布族，&lt;span class="math">\(g(\theta)\)&lt;/span>是定义在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上的可微函数，设&lt;span class="math">\(X=(X_1,X_2,\dotsb,X_n)\)&lt;/span>是由总体&lt;span class="math">\(f(x;\theta)\in\mathcal{F}\)&lt;/span>中抽取的简单随机样本，&lt;span class="math">\(\hat g(X)\)&lt;/span>是&lt;span class="math">\(g(\theta)\)&lt;/span>的任一无偏估计，且满足下列条件： &lt;span class="math">\[\int\dotsb\int \hat{g}(\bm{x})f(\bm{x},\theta)\mathrm{d}\bm{x}\]&lt;/span> 可在积分号下对&lt;span class="math">\(\theta\)&lt;/span>求导数，此出&lt;span class="math">\(\mathrm{d}\bm{x}=\mathrm{d}x_1\dotsb\mathrm{d}x_n\)&lt;/span>，则有： &lt;span class="math">\[D[\hat{g}(X)]\geq \frac{(\hat g&amp;#39;(\theta))^2}{nI(\theta)},\forall \theta\in \varTheta\tag{2}\]&lt;/span> 其中,&lt;span class="math">\(I(\theta)\)&lt;/span>为Fisher信息量。&lt;/p>
&lt;/blockquote>
&lt;p>特别地，当&lt;span class="math">\(\hat g(\theta)=\theta\)&lt;/span>时，式(2)变成 &amp;gt;&lt;span class="math">\[D[\hat{g}(X)]\geq \frac{1}{nI(\theta)},\forall \theta\in \varTheta\tag{3}\]&lt;/span> 当&lt;span class="math">\(f(x;\theta)\)&lt;/span>为离散概率分布列时，式（2）变成 &amp;gt;&lt;span class="math">\[D[\hat{g}(X)]\geq \frac{(\hat g&amp;#39;(\theta))^2}{n\sum\limits_i\left\{[\frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]^2f(x_i;\theta)\right\}},\forall \theta\in \varTheta\tag{4}\]&lt;/span>&lt;/p>
&lt;p>证明：C-R不等式的证明本质上是柯西-施瓦茨不等式的应用。&lt;/p>
&lt;blockquote>
&lt;p>首先，在概率论中，柯西-施瓦茨不等式形式为： &lt;span class="math">\[\mathrm{Var}(X)\cdot\mathrm{Var}(Y)\geq[\mathrm{Cov}(X,Y)]^2\]&lt;/span> 我们再来看看C-R不等式，式（2）： &lt;span class="math">\[D[\hat{g}(X)]\geq \frac{(\hat g&amp;#39;(\theta))^2}{nI(\theta)},\forall \theta\in \varTheta\]&lt;/span> 和柯西施瓦茨不等式对比下，&lt;span class="math">\(D[\hat{g}(X)]\)&lt;/span>是方差。根据&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>笔记中的内容，可知&lt;span class="math">\(nI(\theta)\)&lt;/span>其实是&lt;strong>分数函数的方差&lt;/strong>。前面多了一个&lt;span class="math">\(n\)&lt;/span>是因为，此时有&lt;span class="math">\(n\)&lt;/span>个i.i.d简单抽样出来的随机变量，是多维随机变量场景。&lt;/p>
&lt;p>为了阐述清晰，在这里我们在写一遍关于分数函数&lt;span class="math">\(S(\bm{x})\)&lt;/span>的相关证明。由于随机样本中每一样品都是i.i.d的，所以有&lt;span class="math">\(f(\bm{x};\theta)=\prod\limits_{i=1}^n f(x_i;\theta)\)&lt;/span>，那么分数函数可记 &lt;span class="math">\[S(\bm{x};\theta)=\frac{\partial\log{f(\bm{x};\theta)}}{\partial\theta}=\sum_{i=1}^n \frac{\partial\log{f(x_i;\theta)}}{\partial\theta}\tag{5}\]&lt;/span> 使用和&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>中一样的方法，可知： &lt;span class="math">\[\begin{aligned}
E[S(\bm{x};\theta)]&amp;amp;=\sum_{i=1}^n E[\frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]=\sum_{i=1}^n \int\frac{1}{f(x_i;\theta)}\frac{\partial f(x_i;\theta)}{\partial\theta}\cdot f(x_i;\theta)\mathrm{d}x_i\\
&amp;amp;=\sum_{i=1}^n \int\frac{\partial f(x_i;\theta)}{\partial\theta}\mathrm{d}x_i=\sum_{i=1}^n \frac{\partial }{\partial\theta}\int f(x_i;\theta)\mathrm{d}x_i=\sum_{i=1}^n \frac{\partial 1}{\partial\theta}=0
\end{aligned}\]&lt;/span> 注意，C-R正则族的条件（2）保证了导数的存在，条件（4）保证了积分、微分顺序可交换。由于分数函数的期望为0，因此分数函数的方差为 &lt;span class="math">\[\begin{aligned}
D[S(\bm{x};\theta)]&amp;amp;=D[\sum_{i=1}^n \frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]=\sum_{i=1}^n D[\frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]\\
&amp;amp;=\sum_{i=1}^n \{E[(\frac{\partial\log{f(x_i;\theta)}}{\partial\theta})^2]-(\underbrace{E[\frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]}_{=0})^2\}\\
(x_i都是i.i.d)&amp;amp;=n\cdot E[(\frac{\partial\log{f(x_i;\theta)}}{\partial\theta})^2]=nI(\theta)
\end{aligned}\]&lt;/span> 由上可知，&lt;span class="math">\(nI(\theta)\)&lt;/span>是分数函数的方差。那么式（2）可转换为要证： &lt;span class="math">\[D[\hat{g}(X)]\cdot D[S(X;\theta)]\geq (\hat g&amp;#39;(\theta))^2,\forall \theta\in \varTheta\]&lt;/span> 再将上式和柯西-施瓦茨不等式对比下，发现区别就是将&lt;span class="math">\(\hat g&amp;#39;(\theta)\)&lt;/span>换成&lt;span class="math">\(\hat g(X)\)&lt;/span>与&lt;span class="math">\(S(X;\theta)\)&lt;/span>的相关系数。注意，&lt;span class="math">\(\hat g&amp;#39;(\theta)\)&lt;/span>是关于&lt;span class="math">\(\theta\)&lt;/span>的函数，而&lt;span class="math">\(\hat g(X)\)&lt;/span>与&lt;span class="math">\(S(X;\theta)\)&lt;/span>的相关系数会将&lt;span class="math">\(X\)&lt;/span>消掉，只剩下&lt;span class="math">\(\theta\)&lt;/span>。下面我们就来验证这一点： &lt;span class="math">\[\begin{aligned}
\mathrm{Cov}(\hat{g}(X),S(X;\theta))&amp;amp;=E[\hat{g}(X)\cdot S(X;\theta)]-E[\hat{g}(X)]\underbrace{E[S(X;\theta)}_{=0}]\\
&amp;amp;=E[\hat{g}(X)\cdot S(X;\theta)]\\
&amp;amp;=\int\dotsb\int \hat{g}(\bm{x})(\frac{\partial\log{f(x_i;\theta)}}{\partial\theta})f(\bm{x};\theta)\mathrm{d}\bm{x}\\
&amp;amp;=\int\dotsb\int \hat{g}(\bm{x})\frac{\partial f(\bm{x};\theta)}{\partial \theta}\mathrm{d}\bm{x}\\
&amp;amp;=\frac{\partial}{\partial \theta}\underbrace{\int\dotsb\int \hat{g}(\bm{x})f(\bm{x};\theta)\mathrm{d}\bm{x}}_{E[\hat{g}(X)]}\\
&amp;amp;\because \hat g(X)是g(\theta)的任一无偏估计\\
&amp;amp;\therefore E[\hat g(X)]=g(\theta)\\
&amp;amp;=\frac{\partial{g(\theta)}}{\partial{\theta}}=g&amp;#39;(\theta)
\end{aligned}\]&lt;/span> 即，&lt;span class="math">\(\mathrm{Cov}(\hat{g}(X),S(X;\theta))=g&amp;#39;(\theta)\)&lt;/span>。这样C-R不等式就完全转变成了柯西-施瓦茨不等式的形式。C-R不等式得证。&lt;/p>
&lt;/blockquote>
&lt;p>C-R不等式表明，&lt;strong>在给定分布族、样本后，我们的估计能力是有限的&lt;/strong>，无论用什么估计方式，其方差最小也是C-R不等式给出的结果。如果希望方差无限小，唯一的途径就是样本数量无限大。&lt;/p>
&lt;h3 id="单参数c-r不等式等号成立条件">单参数C-R不等式等号成立条件&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>若样本分布族&lt;strong>非指数族&lt;/strong>，任何&lt;span class="math">\(g(\theta)\)&lt;/span>的任何无偏估计，其方差不能处处达到C-R不等式下界。这意味着，非指数族就没法用C-R不等式来求证UMVUE。&lt;/li>
&lt;li>即使样本的总体是指数族，&lt;span class="math">\(f(\bm{x};\theta)=C(\theta)\exp{Q(\theta)T(\bm x)}h(\bm x)\)&lt;/span>，也不是让和&lt;span class="math">\(g(\theta)\)&lt;/span>都能找到无偏估计&lt;span class="math">\(\hat{g}(X)\)&lt;/span>，使其方差处处达到C-R下界。唯有&lt;span class="math">\(g(\theta)=E[aT(X)+b]\)&lt;/span>时才有，即&lt;span class="math">\(\hat{g}(X)=aT(X)+b\)&lt;/span>（线性函数）的情形才有，此处&lt;span class="math">\(a\neq 0,b\)&lt;/span>与&lt;span class="math">\(X\)&lt;/span>无关，但可以是&lt;span class="math">\(\theta\)&lt;/span>的函数。&lt;/li>
&lt;/ol>
&lt;p>从上面两个条件，我们不能发现：&lt;strong>用C-R不等于求UMVUE是很受限的&lt;/strong>。&lt;/p>
&lt;h3 id="多参数c-r不等式">多参数C-R不等式&lt;/h3>
&lt;p>TODO用到时再说。&lt;/p>
&lt;h2 id="c-r不等式应用">C-R不等式应用&lt;/h2>
&lt;h3 id="求umvue">求UMVUE&lt;/h3>
&lt;p>当分布族满足正则分布族条件时，我们可以计算&lt;span class="math">\(\frac{g&amp;#39;(\theta)}{nI(\theta)}\)&lt;/span>。然后再计算估计量的方差&lt;span class="math">\(D[\hat g(X)]\)&lt;/span>。如果二者相等，且估计量是无偏估计，那么此估计量就是UMVUE。&lt;/p>
&lt;p>这个方法对于指数族都是很好用的，因为指数族都是C-R正则分布族，而且可以求出费舍尔信息。但是，其缺点也很明显。一是因为很多分布族不满足C-R正则条件；二是一些UMVUE的实际方差确实比C-R不等式给出的更大，因此即使一个估计量方差大于C-R下界，那它也可能是UMVUE。即C-R不等式是必要条件，不是充分条件。&lt;/p>
&lt;h3 id="估计的效率和有效性">估计的效率和有效性&lt;/h3>
&lt;p>无偏估计的效率定义很简单，就是C-R界与估计方差的比值：&lt;/p>
&lt;blockquote>
&lt;p>定义2:&lt;strong>无偏估计的效率&lt;/strong>。设&lt;span class="math">\(\hat{g}(X)\)&lt;/span>为&lt;span class="math">\(g(\theta)\)&lt;/span>的无偏估计，比值 &lt;span class="math">\[e_{\hat{g}}(\theta)=\frac{[g&amp;#39;(\theta)]^2/nI(\theta)}{D[\hat{g}(X)]}\]&lt;/span> 称为无偏估计&lt;span class="math">\(\hat{g}(X)\)&lt;/span>的效率。&lt;/p>
&lt;/blockquote>
&lt;p>显然，根据C-R不等式必有&lt;span class="math">\(0&amp;lt;e_{\hat{g}}(\theta)\leq 1\)&lt;/span>。&lt;/p>
&lt;ul>
&lt;li>当&lt;span class="math">\(e_{\hat{g}}(\theta)=1\)&lt;/span>，则称&lt;span class="math">\(\hat{g}(X)\)&lt;/span>是&lt;span class="math">\(g(\theta)\)&lt;/span>的&lt;strong>有效估计&lt;/strong>（UMVUE）（有效估计是UMVUE，但是UMVUE不一定是有效估计）；&lt;/li>
&lt;li>若&lt;span class="math">\(\hat{g}(X)\)&lt;/span>不是&lt;span class="math">\(g(\theta)\)&lt;/span>的有效估计，但是&lt;span class="math">\(\lim\limits_{n\rightarrow \infty}e_{\hat{g}}(\theta)=1\)&lt;/span>，则称&lt;span class="math">\(\hat{g}(X)\)&lt;/span>是&lt;span class="math">\(g(\theta)\)&lt;/span>的&lt;strong>渐进有效估计&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>虽然有效估计是无偏估计中最好的，但是从常用分布来看有效估计并不多，渐进有效估计不少。&lt;/p>
&lt;h3 id="在渐进正态性中的应用">在渐进正态性中的应用&lt;/h3>
&lt;p>在一定条件下，最大似然估计具有渐进正态性。我们将通过如下定理阐释。需要指出的是，定理是以连续分布的形式给出，但是对于离散场景也是适用的。&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(p(x;\theta)\)&lt;/span>是某密度函数，其参数空间&lt;span class="math">\(\varTheta=\{\theta\}\)&lt;/span>是直线上的非退化区间（即不是一个点），假如：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，&lt;span class="math">\(p(x;\theta)\)&lt;/span>对&lt;span class="math">\(\theta\)&lt;/span>如下偏导都存在：&lt;span class="math">\(\frac{\partial\ln p}{\partial\theta},\frac{\partial^2\ln p}{\partial\theta^2},\frac{\partial^3\ln p}{\partial\theta^3}\)&lt;/span>&lt;/li>
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，有&lt;span class="math">\(|\frac{\partial\ln p}{\partial\theta}|&amp;lt;F_1(x),|\frac{\partial^2\ln p}{\partial\theta^2}|&amp;lt;F_2(x),\frac{\partial^3\ln p}{\partial\theta^3}&amp;lt;H(x)\)&lt;/span>成立，其中&lt;span class="math">\(F_1(x)\)&lt;/span>与&lt;span class="math">\(F_2(x)\)&lt;/span>在实数轴上可积，而&lt;span class="math">\(H(x)\)&lt;/span>满足：&lt;span class="math">\(\int_{-\infty}^\infty H(x)p(x;\theta)&amp;lt;M\)&lt;/span>，这里&lt;span class="math">\(M\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关。&lt;/li>
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，有&lt;span class="math">\(0&amp;lt;I(\theta)=E[(\frac{\partial\ln p}{\partial \theta})^2]&amp;lt;+\infty\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>则在参数真值&lt;span class="math">\(\theta\)&lt;/span>为参数空间&lt;span class="math">\(\varTheta\)&lt;/span>内点的情况下，其似然方程有一个解存在，且此解&lt;span class="math">\(\hat\theta_n=\theta(x_1,x_2,\dotsb,x_n)\)&lt;/span>依概率收敛于&lt;span class="math">\(\theta\)&lt;/span>，且： &lt;span class="math">\[
\hat\theta_n\sim AN(\theta,[nI(\theta)]^{-1})
\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这个定理的意义在于给定了最大似然分布有渐进正态性的条件，其中渐进方差（体现大样本效率）完全由&lt;strong>样本数量&lt;span class="math">\(n\)&lt;/span>和分布的费舍尔信息量&lt;span class="math">\(I(\theta)\)&lt;/span>决定&lt;/strong>，且费舍尔信息量越大（分布中含有&lt;span class="math">\(\theta\)&lt;/span>）的信息越多，渐进方差在同等样本数量下越小，从而最大似然估计效果越好。&lt;/p></description></item><item><title>概率统计随机过程之条件期望与重期望公式</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9D%A1%E4%BB%B6%E6%9C%9F%E6%9C%9B%E4%B8%8E%E9%87%8D%E6%9C%9F%E6%9C%9B%E5%85%AC%E5%BC%8F/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9D%A1%E4%BB%B6%E6%9C%9F%E6%9C%9B%E4%B8%8E%E9%87%8D%E6%9C%9F%E6%9C%9B%E5%85%AC%E5%BC%8F/</guid><description>
&lt;h2 id="概率统计随机过程之条件期望与重期望公式">概率统计随机过程之条件期望与重期望公式&lt;!-- omit in toc -->&lt;/h2>
&lt;p>之前对条件期望的理解有一些偏差，现在重新看了下条件期望的内容与重期望公式。注意（X|Y）的条件期望实际上是关于Y的函数，而重期望公式则与分区加权求和有着本质联系，提供了求X期望的另一种方式。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#条件数学期望">条件数学期望&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重期望公式">重期望公式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#随机个随机变量和的数学期望">随机个随机变量和的数学期望&lt;/a>&lt;/li>
&lt;li>&lt;a href="#条件期望的其他推论">条件期望的其他推论&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="条件数学期望">条件数学期望&lt;/h2>
&lt;p>如果我们对条件分布求期望，则称为&lt;strong>条件数学期望&lt;/strong>。在离散分布列和连续密度函数的定义方式如下，以二维举例：&lt;/p>
&lt;blockquote>
&lt;p>&lt;span class="math">\(X\)&lt;/span>关于&lt;span class="math">\(Y=y\)&lt;/span>的条件期望： &lt;span class="math">\[E(X|Y=y)=\begin{cases}\sum\limits_i x_iP(X=x_i|Y=y),\qquad(X,Y)为二维离散随机变量\\
\int_{-\infty}^{\infty}xp(x|y)\mathrm{d}x,\qquad(X,Y)为二维连续随机变量\end{cases}\tag{1}\]&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(Y\)&lt;/span>关于&lt;span class="math">\(X=x\)&lt;/span>的条件期望： &lt;span class="math">\[E(Y|X=x)=\begin{cases}\sum\limits_i y_iP(Y=y_i|X=x),\qquad(X,Y)为二维离散随机变量\\
\int_{-\infty}^{\infty}yp(y|x)\mathrm{d}y,\qquad(X,Y)为二维连续随机变量\end{cases}\tag{2}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>注意，&lt;span class="math">\(E(X|Y=y)\)&lt;/span>是在&lt;span class="math">\(y\)&lt;/span>为特定值时，对&lt;span class="math">\(x\)&lt;/span>求和/积分，抹去了&lt;span class="math">\(x\)&lt;/span>的随机性，得到一个关于&lt;span class="math">\(y\)&lt;/span>的函数。同理，&lt;span class="math">\(E(Y|X=x)\)&lt;/span>抹去的是&lt;span class="math">\(y\)&lt;/span>的随机性，得到一个关于&lt;span class="math">\(x\)&lt;/span>的函数。&lt;/p>
&lt;p>条件期望&lt;span class="math">\(E(X|Y=y)\)&lt;/span>和无条件期望&lt;span class="math">\(E(X)\)&lt;/span>的一大区别是，&lt;span class="math">\(E(X)\)&lt;/span>是一个数，而条件期望&lt;span class="math">\(E(X|Y)\)&lt;/span>是一个函数&lt;span class="math">\(g(y)\)&lt;/span>。&lt;/p>
&lt;p>举个例子，如用&lt;span class="math">\(X\)&lt;/span>表示中国成年人的身高，则&lt;span class="math">\(E(X)=170\)&lt;/span>表示中国成年人的平均身高为170 cm，是一个具体的数字。若用&lt;span class="math">\(Y\)&lt;/span>表示中国成年人的足长，则&lt;span class="math">\(E(X|Y=y)\)&lt;/span>表示足长为&lt;span class="math">\(y\)&lt;/span>的中国成年人的平均身高，根据研究可知 &lt;span class="math">\[
E(X|Y=y)=6.876y
\]&lt;/span> 这显然是一个与&lt;span class="math">\(y\)&lt;/span>相关的函数，对&lt;span class="math">\(y\)&lt;/span>的不同取值，条件期望的取值也在变化。可以记： &lt;span class="math">\[
g(y)=E(X|Y=y)
\]&lt;/span> 进一步，还可以将条件期望看成是随机变量&lt;span class="math">\(Y\)&lt;/span>的函数，即&lt;span class="math">\(E(X|Y)=g(Y)\)&lt;/span>，而将&lt;span class="math">\(E(X|Y=y)\)&lt;/span>看成是&lt;span class="math">\(Y=y\)&lt;/span>时&lt;span class="math">\(E(X|Y)\)&lt;/span>的一个取值。从这个角度来看，&lt;strong>&lt;span class="math">\(E(X|Y)\)&lt;/span>也是一个随机变量&lt;/strong>。&lt;/p>
&lt;p>如果条件期望也是一个随机数，那么条件期望的期望是什么呢？下面就用重期望公式做进一步说明。&lt;/p>
&lt;h2 id="重期望公式">重期望公式&lt;/h2>
&lt;p>前面提到，&lt;span class="math">\(g(Y)=E(X|Y)\)&lt;/span>也是一个随机变量，如果我们对其求期望，以连续函数为例,注意随机变量是&lt;span class="math">\(Y\)&lt;/span>： &lt;span class="math">\[
E[g(Y)]=\int_{-\infty}^\infty E(X|Y=y) p_Y(y)\mathrm{d}y
\]&lt;/span> 我们将条件期望的定义（1）式代入可得： &lt;span class="math">\[
\begin{aligned}
E[g(Y)]&amp;amp;=\int_{-\infty}^\infty[\int_{-\infty}^\infty xp(x|Y=y)\mathrm{d}x]\;p_{_Y}(y)\mathrm{d}y\\
(全概率公式)&amp;amp;=\int_{-\infty}^\infty\int_{-\infty}^\infty xp(x,y)\mathrm{d}x\mathrm{d}y\\
（提出x）&amp;amp;=\int_{-\infty}^\infty x\{\int_{-\infty}^\infty p(x,y)\mathrm{d}y\}\mathrm{d}x\\
(求x的边际pdf)&amp;amp;=\int_{-\infty}^\infty xp_{_X}(x)\mathrm{d}x\\
&amp;amp;=E(X)
\end{aligned}\tag{3}
\]&lt;/span> 我们“惊讶”的发现，条件期望的期望竟然是&lt;span class="math">\(X\)&lt;/span>的无条件期望！由此，我们给出重期望公式：&lt;/p>
&lt;blockquote>
&lt;p>定理：（重期望公式）设&lt;span class="math">\((X,Y)\)&lt;/span>是二维随机变量，且&lt;span class="math">\(E(X)\)&lt;/span>存在，则&lt;/p>
&lt;p>&lt;span class="math">\[E(X)=E[E(X|Y)]\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>重期望公式是概率论中比较深刻的一个结论。我们也可以换个角度理解：我们找到一个与&lt;span class="math">\(X\)&lt;/span>相关的量&lt;span class="math">\(Y\)&lt;/span>，用&lt;span class="math">\(Y\)&lt;/span>的不同取值（要互斥）把&lt;span class="math">\(X\)&lt;/span>划分成若干小区域（场景），现在小区域上求&lt;span class="math">\(X\)&lt;/span>的期望或均值，然后再根据&lt;span class="math">\(Y\)&lt;/span>的出现概率对各个小区域的期望&lt;span class="math">\(E(X_{y_i})\)&lt;/span>求加权平均，即可求出整体&lt;span class="math">\(X\)&lt;/span>的期望。&lt;/p>
&lt;p>具体一些，重期望公式也可以写成如下形式： &lt;span class="math">\[
E(X)=\begin{cases}\sum\limits_i E(X|Y=y_i)P(Y=y_i),\qquad 离散场景\\
\int_{-\infty}^\infty E(X|Y=y)P_{_Y}(y)\mathrm{d}y,\qquad 连续场景\end{cases}
\]&lt;/span>&lt;/p>
&lt;h3 id="随机个随机变量和的数学期望">随机个随机变量和的数学期望&lt;/h3>
&lt;p>设&lt;span class="math">\(X_1,X_2,\dotsb\)&lt;/span>为一系列独立同分布的随机变量，随机变量&lt;span class="math">\(N\)&lt;/span>只取正整数值，且&lt;span class="math">\(N\)&lt;/span>与&lt;span class="math">\(\{X_n\}\)&lt;/span>独立，证明： &lt;span class="math">\[
E(\sum_{i=1}^N X_i)=E(X_1)E(N)
\]&lt;/span>&lt;/p>
&lt;p>证明：由重期望公式可知： &lt;span class="math">\[
\begin{aligned}
E(\sum_{i=1}^N X_i)&amp;amp;=E[E(\sum_{i=1}^N X_i | N)]\\
&amp;amp;=\sum_{i=1}^\infty E(\sum_{i=1}^N X_i | N=n)P(N=n)\\
（\{X_n\}与N独立）&amp;amp;=\sum_{i=1}^\infty E(\sum_{i=1}^n X_i)P(N=n)\\
（\{X_n\}i.i.d）&amp;amp;=\sum_{i=1}^\infty nE(X_1)P(N=n)\\
&amp;amp;=E(X_1)\sum_{i=1}^\infty nP(N=n)\\
&amp;amp;=E(X_1)E(N)
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;h2 id="条件期望的其他推论">条件期望的其他推论&lt;/h2>
&lt;ul>
&lt;li>&lt;span class="math">\(\mathrm{Var}(X)=E[\mathrm{Var}(X|Y)]+\mathrm{Var}[E(X|Y)]\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>证明： &lt;span class="math">\[
\left .
\begin{aligned}
&amp;amp;E[\mathrm{Var}(X|Y)]=E\{E(X^2|Y)-[E(X|Y)]^2\}=E(X^2)-E[E^2(X|Y)]\\
\\
&amp;amp;\mathrm{Var}[E(X|Y)]=E[E^2(X|Y)]-[\underbrace{E\cdot E(X|Y)}_{E(X)}]^2=E[E^2(X|Y)]-[E(X)]^2
\end{aligned}
\right\}\Rightarrow\\
E[\mathrm{Var}(X|Y)]+\mathrm{Var}[E(X|Y)]=E(X^2)-E[E^2(X|Y)]+E[E^2(X|Y)]-[E(X)]^2\\
=E(X^2)-E^2(X)=\mathrm{Var}(X)
\]&lt;/span>&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(E[f(Y)|Y]=f(Y)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>证明： 当随机变量&lt;span class="math">\(Y\)&lt;/span>取到固定值&lt;span class="math">\(y\)&lt;/span>时（&lt;span class="math">\(Y=y\)&lt;/span>），就不存在随机性了。所以对于&lt;span class="math">\(\forall Y=y\)&lt;/span>，有 &lt;span class="math">\[
E[f(Y)|Y=y]=E[f(Y=y)|Y=y]=E[f(y)]=f(y)
\]&lt;/span> 所以，有&lt;span class="math">\(E[f(Y)|Y]=f(Y)\)&lt;/span>。&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(E[g(X)\cdot Y|X]=g(X)E[Y|X]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(E(XY)=E[X\cdot E(Y|X)]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathrm{Cov}[X,E(Y|X)]=\mathrm{Cov}(X,Y)\)&lt;/span>&lt;/li>
&lt;/ul></description></item><item><title>概率统计随机过程之抽样的分布</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%8A%BD%E6%A0%B7%E7%9A%84%E5%88%86%E5%B8%83/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%8A%BD%E6%A0%B7%E7%9A%84%E5%88%86%E5%B8%83/</guid><description>
&lt;h2 id="概率统计随机过程之抽样的分布统计量的分布">概率统计随机过程之抽样的分布（统计量的分布）&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#样本均值的抽样分布正态分布">样本均值的抽样分布——正态分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本方差的抽样分布">样本方差的抽样分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#卡方分布">卡方分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本方差的抽样分布服从chi2n分布">样本方差的抽样分布服从&lt;span class="math">\(\\chi^2(n)\)&lt;/span>分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本均值与样本方差比值的分布">样本均值与样本方差比值的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#t分布">t分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#tips名字来源">TIPS名字来源&lt;/a>&lt;/li>
&lt;li>&lt;a href="#t分布介绍">t分布介绍&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#样本均值与样本方差比值服从t分布">样本均值与样本方差比值服从t分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个独立正态样本方差比的分布">两个独立正态样本方差比的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#f分布">F分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个独立正态样本方差比的服从f分布">两个独立正态样本方差比的服从F分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态总体下的抽样分布总结">正态总体下的抽样分布总结&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录">附录&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录1-gamma分布可加性证明">附录1-Gamma分布可加性证明&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="样本均值的抽样分布正态分布">样本均值的抽样分布——正态分布&lt;/h2>
&lt;p>样本均值的分布问题是被最早研究的问题，关于它的研究为中心极限定理的出现提供了巨大帮助。&lt;/p>
&lt;blockquote>
&lt;p>定理1：设&lt;span class="math">\(\{x_1,x_2,\dotsb,x_n\}\)&lt;/span>是来自某个总体的样本,&lt;span class="math">\(\bar{x}\)&lt;/span>为其样本均值：&lt;/p>
&lt;p>(1)若总体分布为&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>，则&lt;span class="math">\(\bar{x}\)&lt;/span>的精确（抽样）分布为&lt;span class="math">\(N(\mu,\sigma^2/n)\)&lt;/span>;&lt;/p>
&lt;p>(2)若总体的分布未知或不是正态分布，但&lt;span class="math">\(E(x)=\mu,Var(x)=\sigma^2\)&lt;/span>存在，则&lt;span class="math">\(n\)&lt;/span>较大时&lt;span class="math">\(\bar{x}\)&lt;/span>的渐进分布为&lt;span class="math">\(N(\mu,\sigma^2/n)\)&lt;/span>（通常&lt;span class="math">\(n&amp;gt;30\)&lt;/span>就接近于正态分布），常记为&lt;span class="math">\(\bar{x}\dot{\sim} N(\mu,\sigma^2/n)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>（1）样本中的每个样品都是独立同分布的随机变量且服从&lt;span class="math">\(x_i\sim N(\mu,\sigma^2)\)&lt;/span>，则根据正态分布的线性性质，n个i.i.d的正态随机变量和为&lt;span class="math">\(N(n\mu,n\sigma^2)\)&lt;/span>。同样根据正态分布的线性性质，和再除以&lt;span class="math">\(1/n\)&lt;/span>，有&lt;span class="math">\(\bar{x}\sim N(\mu,\sigma^2/n)\)&lt;/span>。&lt;/p>
&lt;p>（2）就是独立同分布的中心极限定理（林德伯格-莱维中心极限定理）的结果。证明的话是用随机变量分布列的特征函数收敛到正态分布的特征函数的思路。&lt;/p>
&lt;p>需要指出，此处的抽样分布一般都是放回抽样，对于无放回抽样，样本均值的标准误差需要添加一个修正系数： &lt;span class="math">\[
\sigma^2_{\bar{x}}=\sigma^2/n\times \frac{N-n}{N-1}&amp;lt;\sigma^2/n
\]&lt;/span> 显然不放回抽样的样本均值的标准误差更小。&lt;/p>
&lt;h2 id="样本方差的抽样分布">样本方差的抽样分布&lt;/h2>
&lt;h3 id="卡方分布">卡方分布&lt;/h3>
&lt;blockquote>
&lt;p>定义1：卡方分布&lt;span class="math">\(\chi^2(n)\)&lt;/span>的概率密度是 &lt;span class="math">\[f(x)=\begin{cases}
\frac{1}{2^{n/2}\Gamma(n/2)}x^{{n\over2} -1}e^{-x\over 2},&amp;amp;x&amp;gt;0\\
0,&amp;amp;\text{其他}
\end{cases}\]&lt;/span> 其中参数&lt;span class="math">\(n\)&lt;/span>称为自由度，&lt;span class="math">\(\Gamma(x)=\int_0^{\infty}t^{x-1}e^{-t} \mathrm{d}t\)&lt;/span>为伽马函数。&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../../images/卡方分布.png" alt="卡方分布.png" />&lt;p class="caption">卡方分布.png&lt;/p>
&lt;/div>
&lt;p>一些没用的观察：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>自由度为2时，卡方分布就是一个的指数分布。&lt;/li>
&lt;li>&lt;span class="math">\(n&amp;gt;2\)&lt;/span>，单峰曲线，且在&lt;span class="math">\(x=n-2\)&lt;/span>时取最大值。&lt;/li>
&lt;li>卡方分布不对称，但是&lt;span class="math">\(n\)&lt;/span>越大越对称，且趋向于正态分布。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>定理2（服从卡方分布）：如果随机变量&lt;span class="math">\(x_1,x_2,\dotsb,x_n\)&lt;/span>独立，且服从&lt;span class="math">\(N(0,1)\)&lt;/span>分布，那么 &lt;span class="math">\[
\sum_{i=1}^n x_i^2 \sim \chi^2(n)
\]&lt;/span> 即标准正态分布的&lt;strong>平方和&lt;/strong>是卡方分布，自由度是&lt;span class="math">\(n\)&lt;/span>表示加数的个数。&lt;/p>
&lt;/blockquote>
&lt;p>证明：令&lt;span class="math">\(y=x_i^2 ≥ 0\)&lt;/span>，其分布函数为&lt;span class="math">\(F_{y}(y)\)&lt;/span>，所以当&lt;span class="math">\(y≤0\)&lt;/span>时有&lt;span class="math">\(F_{y}(y)=0\)&lt;/span>，当&lt;span class="math">\(y&amp;gt;0\)&lt;/span>时有 &lt;span class="math">\[\begin{aligned}
F_{y}(y)&amp;amp;=P(x_i^2≤y)=P(-\sqrt{y}≤x≤\sqrt{y})\\
&amp;amp;=\int_{-\sqrt{y}}^0 p_x(x) dx + \int_{0}^{\sqrt{y}} p_x(x) dx\\
&amp;amp;=F_x(\sqrt{y}) - F_x(-\sqrt{y})
\end{aligned}\]&lt;/span> 我们把上式对&lt;span class="math">\(y\)&lt;/span>求导，有： &lt;span class="math">\[\begin{aligned}
p_y(y)&amp;amp;=F_y&amp;#39;(y)=F&amp;#39;_x(\sqrt{y}) - F&amp;#39;_x(-\sqrt{y})\\
&amp;amp;=p_x(\sqrt{y})\cdot (\sqrt{y})&amp;#39;-p_x(-\sqrt{y})\cdot (-\sqrt{y})&amp;#39;\\
&amp;amp;=[p_x(\sqrt{y})+p_x(-\sqrt{y})]/(2\sqrt{y})\\
&amp;amp;=\frac{1}{\sqrt{2\pi}}y^{-1/2}e^{-y/2},y&amp;gt;0\\
&amp;amp;=\frac{(1/2)^{1/2}}{\Gamma(1/2)}y^{\frac{1}{2}-1}e^{-\frac{1}{2}y}=Ga(\frac{1}{2},\frac{1}{2})\end{aligned}\]&lt;/span> 即单个标准正态分布的平方服从&lt;span class="math">\(Ga(\frac{1}{2},\frac{1}{2})\)&lt;/span>。而Gamma分布是有可加性的（证明见&lt;a href="#附录1-gamma分布可加性证明">附录1-Gamma分布可加性证明&lt;/a>）。因此，n个标准正态分布的平方的和服从&lt;span class="math">\(Ga(\frac{n}{2},\frac{1}{2})\)&lt;/span>。将&lt;span class="math">\(Ga(\frac{n}{2},\frac{1}{2})\)&lt;/span>写出可发现就等于&lt;span class="math">\(\chi^2(n)\)&lt;/span>。得证。&lt;/p>
&lt;p>卡方分布性质:若&lt;span class="math">\(X\sim \chi^2(n)\)&lt;/span>，则有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(E(X)=n,D(X)=2n\)&lt;/span>。分部积分可得，或者直接用特征函数求。&lt;/li>
&lt;li>由中心极限定理可知，&lt;span class="math">\(X\sim \chi^2(n)\)&lt;/span>，&lt;span class="math">\(n\)&lt;/span>充分大，&lt;span class="math">\(\frac{X-n}{\sqrt{2n}}\overset{近似}{\sim} N(0,1)\)&lt;/span>。参见中心极限定理的林德伯格－列维形式。&lt;/li>
&lt;li>卡方分布可加性：&lt;span class="math">\(X\sim \chi^2(n),Y\sim \chi^2(m),X,Y\)&lt;/span>独立，则&lt;span class="math">\(X+Y\sim \chi^2(m+n)\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>推论：&lt;span class="math">\(X_i\sim \chi^2(m_i)\)&lt;/span>，各个&lt;span class="math">\(X_i\)&lt;/span>独立，则&lt;span class="math">\(\sum_{i=1}^n X_i\sim \chi^2(\sum_{i=1}^n m_i)\)&lt;/span>&lt;/li>
&lt;li>简单论证：&lt;span class="math">\(\chi^2\)&lt;/span>分布是特殊的Gamma分布（即&lt;span class="math">\(Ga(\frac{n}{2},\frac{1}{2})\)&lt;/span>），而Gamma分布有可加性，所以&lt;span class="math">\(\chi^2\)&lt;/span>分布也有可加性。也可以从&lt;span class="math">\(\chi^2\)&lt;/span>分布的构成来看，它是由多个随机变量&lt;span class="math">\(X_i\sim N(0,1)\)&lt;/span>加出来，因此卡方分布的相加无非就是多几个标准正态分布相加的事，因此不改变分布类型，只改变分布参数。&lt;/li>
&lt;/ul>
&lt;h3 id="样本方差的抽样分布服从chi2n分布">样本方差的抽样分布服从&lt;span class="math">\(\chi^2(n)\)&lt;/span>分布&lt;/h3>
&lt;blockquote>
&lt;p>引理1： N维随机变量线性变换的分布。设在两个n维随机变量&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)&amp;#39;\)&lt;/span>与&lt;span class="math">\(Y=(_1,y_2,\dotsb,y_n)&amp;#39;\)&lt;/span>间存在一个线性变换关系&lt;span class="math">\(Y=AX\)&lt;/span>，其中&lt;span class="math">\(A=(a_{ij})\)&lt;/span>为一个&lt;span class="math">\(n\times n\)&lt;/span>的n阶方阵，则它们的期望向量和方差（协方差）矩阵之间有如下关系： &lt;span class="math">\[E(Y)=AE(X)\\Var(Y)=AVar(X)A&amp;#39;\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>（1）首先矩阵变换是一种线性变换，求期望（无论是积分还是求和）也是线性变换，有&lt;span class="math">\(E(Y)=E(AX)\)&lt;/span>，&lt;span class="math">\(A\)&lt;/span>都是常数，所以可以把它当成线性变换的系数提出来，就有了&lt;span class="math">\(E(Y)=AE(X)\)&lt;/span>。&lt;/p>
&lt;p>（2）由于方差（协方差）运算中有变量之间的乘法，所以不是线性运算。我们老实根据定义求解，根据协方差矩阵的定义有： &lt;span class="math">\[\begin{aligned}
Var(Y)&amp;amp;=E[(Y-E(Y))(Y-E(Y))&amp;#39;]\\
&amp;amp;\overset{Y=AX}{=}E[AX-E(AX)(AX-E(AX))&amp;#39;]\\
&amp;amp;=E[AX-AE(X)(AX-AE(X))&amp;#39;]\\
&amp;amp;\overset{\text{结合律}}{=}E[A(X-E(X))(A(X-E(X)))&amp;#39;]\\
&amp;amp;\overset{(AB)&amp;#39;=B&amp;#39;A&amp;#39;}{=}E[A(X-E(X))(X-E(X))&amp;#39;A&amp;#39;]\\
&amp;amp;=AE[(X-E(X))(X-E(X))&amp;#39;]A&amp;#39;\\
&amp;amp;=AVar(X)A&amp;#39;
\end{aligned}\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理3：设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>的样本，其样本均值和样本方差分别为&lt;span class="math">\(\bar{x}\)&lt;/span>和&lt;span class="math">\(s^2\)&lt;/span>，则有&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\frac{(n-1)s^2}{\sigma^2}\sim \chi^2(n-1)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\bar{x},s^2\)&lt;/span>相互独立。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>首先对于n维随机变量&lt;span class="math">\(X\)&lt;/span>，期望&lt;span class="math">\(E(X)=\underbrace{(\mu,\mu,\dotsb,\mu)&amp;#39;}_{n个}\)&lt;/span>；n维随机变量的自协方差矩阵为&lt;span class="math">\(Var(X)=\sigma^2I\)&lt;/span>，因为各维度之间是独立的（&lt;span class="math">\(\Rightarrow\)&lt;/span>不相关），所以只有对角线上的元素&lt;span class="math">\(conv(x_i,x_i)=\sigma^2\)&lt;/span>，其他&lt;span class="math">\(conv(x_i,x_j)=0,i\neq j\)&lt;/span>。（conv表示协方差）。&lt;/p>
&lt;p>我们看 &lt;span class="math">\[(n-1)s^2=\sum_{i=1}^n (x-\bar{x})^2=\sum_{i=1}^n x_i^2 - 2\underbrace{\sum_{i=1}^n x_i}_{=n\bar{x}}\bar{x}+n\bar{x}^2=(\sum_{i=1}^n x_i^2)- n\bar{x}^2\]&lt;/span>&lt;/p>
&lt;p>我们在处理样本方差的时候，最需要注意的一点就是&lt;strong>样本样品和样本均值不独立&lt;/strong>，此外，&lt;strong>一般分布都是看可加性，而上式是两个随机变量相减&lt;/strong>。所以我们需要通过&lt;code>引理1&lt;/code>构造一组不相关的随机变量相加，再通过正态分布不相关=独立的性质进行计算。证明&lt;code>定理3&lt;/code>最精巧的一步在于构造的统计量为&lt;span class="math">\(Y=AX\)&lt;/span>，&lt;span class="math">\(A\)&lt;/span>是个正交矩阵(&lt;span class="math">\(AA&amp;#39;=I\)&lt;/span>)，因为根据&lt;code>引理1&lt;/code>的方差公式，正交矩阵不改变&lt;span class="math">\(Var(X)\)&lt;/span>，既 &lt;span class="math">\[Var(Y)=AVar(X)A&amp;#39;=A\sigma^2I A&amp;#39;=\sigma^2I\]&lt;/span> 由于&lt;span class="math">\(Y\)&lt;/span>各元素是独立的正态分布随机变量&lt;span class="math">\(\{x_i\}\)&lt;/span>的线性组合，即&lt;span class="math">\(Y\)&lt;/span>各维度也服从正态分布;而&lt;span class="math">\(Y\)&lt;/span>的协方差矩阵为&lt;span class="math">\(I\)&lt;/span>，说明各维度之间不相关，又因为正态分布的不相关和独立等价，所以&lt;span class="math">\(Y=(y_1,y_2,\dotsb,y_n)&amp;#39;\)&lt;/span>的各个分量相互独立，且其方差都是&lt;span class="math">\(\sigma^2\)&lt;/span>。&lt;/p>
&lt;p>如果我们想把&lt;span class="math">\((n-1)s^2\)&lt;/span>往&lt;span class="math">\(\chi^2(n-1)\)&lt;/span>分布上靠，首先就要把其变换成&lt;strong>独立的正态分布平方的和&lt;/strong>。同时正交矩阵&lt;span class="math">\(A\)&lt;/span>不改变原n维向量的模（的平方），即&lt;span class="math">\(\sum_{i=1}^n y_i^2=Y&amp;#39;Y=(AX)&amp;#39;AX=X&amp;#39;(A&amp;#39;A)X=X&amp;#39;X=\sum_{i=1}^n x_i^2\)&lt;/span>。这正是&lt;span class="math">\((n-1)s^2\)&lt;/span>的前半部分。下一步关键是如果改造&lt;span class="math">\(\bar{x}\)&lt;/span>，由于&lt;span class="math">\(\sum_{i=1}^n y_i^2=\sum_{i=1}^n x_i^2\)&lt;/span>都很像自由度为n的&lt;span class="math">\(\chi^2(n)\)&lt;/span>分布（只是“像”）。我们希望&lt;span class="math">\(n\bar{x}^2\)&lt;/span>变成某个&lt;span class="math">\(y_k^2\)&lt;/span>，这样就可以正好减掉一个自由度，变成&lt;span class="math">\(\chi^2(n-1)\)&lt;/span>，这就需要更精细的构造矩阵&lt;span class="math">\(A\)&lt;/span>。先来看看&lt;span class="math">\(n\bar{x}^2\)&lt;/span>的构成： &lt;span class="math">\[n\bar{x}^2=n\times (\frac{1}{n}\sum_{i=1}^n x)^2=(\sum_{i=1}^n\frac{1}{\sqrt{n}}x_i)^2\]&lt;/span> 如果我们令&lt;span class="math">\(A\)&lt;/span>的第&lt;span class="math">\(k\)&lt;/span>行为&lt;span class="math">\((\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}},\dotsb,\frac{1}{\sqrt{n}})\)&lt;/span>且&lt;span class="math">\(A_k(A_k)&amp;#39;=1\)&lt;/span>，那么&lt;span class="math">\(y_k=A_{k}X=\sqrt{n}\bar{x}\)&lt;/span>，即&lt;span class="math">\(n\bar{x}^2 = y_k^2\)&lt;/span>。这样，就使得 &lt;span class="math">\[(n-1)s^2=(\sum_{i=1}^n x_i^2)- n\bar{x}^2=(\sum_{i=1}^n y_i^2)-y_k^2,k\in\{1,2,\dotsb,n\}\]&lt;/span> 不失一般性，我们不妨让&lt;span class="math">\(k=1\)&lt;/span>，则 &lt;span class="math">\[(n-1)s^2=\sum_{i=2}^n y_i^2\]&lt;/span> 我们发现，&lt;span class="math">\((n-1)s^2\)&lt;/span>已经变成了&lt;span class="math">\(n-1\)&lt;/span>个正态分布随机变量的和，但是还不是标准正态随机变量。而根据要证的&lt;span class="math">\(\frac{(n-1)s^2}{\sigma^2}\)&lt;/span>和已知的&lt;span class="math">\(Y\)&lt;/span>各维度元素方差都是&lt;span class="math">\(\sigma^2\)&lt;/span>，因此有 &lt;span class="math">\[\frac{(n-1)s^2}{\sigma^2}=\sum_{i=2}^n (\frac{y_i}{\sigma})^2\]&lt;/span> 其中，&lt;span class="math">\(\frac{y_i}{\sigma}\sim N(\frac{\mu_i}{\sigma},1),i=2,3,\dotsb,n,\mu_i\)&lt;/span>为&lt;span class="math">\(y_i\)&lt;/span>的均值。我们希望&lt;span class="math">\(\sum_{i=2}^n (\frac{y_i}{\sigma})^2\)&lt;/span>则需要&lt;span class="math">\(\frac{y_i}{\sigma}\sim N(0,1)\)&lt;/span>，现在还差&lt;span class="math">\(\mu_i=0\)&lt;/span>，还需要进一步构造&lt;span class="math">\(A\)&lt;/span>的第&lt;span class="math">\(2\sim n\)&lt;/span>行,由于&lt;span class="math">\(x_i\)&lt;/span>是i.i.d的，即&lt;span class="math">\(x_i\)&lt;/span>的均值都是&lt;span class="math">\(\mu\)&lt;/span>，而&lt;span class="math">\(\mu_k=E(y_k)=\mu\sum_{i=1}^n a_{ki}\)&lt;/span>。要让&lt;span class="math">\(\mu_k=0\)&lt;/span>，则需要让&lt;span class="math">\(\sum_{i=1}^n a_{ki}=0\)&lt;/span>，也就是说&lt;span class="math">\(A\)&lt;/span>的第&lt;span class="math">\(2\sim n\)&lt;/span>行的和都为0，又需要&lt;span class="math">\(A\)&lt;/span>是正交矩阵，我们因此构造如下矩阵： &lt;span class="math">\[
A=\begin{bmatrix}
\frac{1}{\sqrt{n}}&amp;amp;\frac{1}{\sqrt{n}}&amp;amp;\frac{1}{\sqrt{n}}&amp;amp;\dotsb&amp;amp;\frac{1}{\sqrt{n}}\\
\frac{1}{\sqrt{2\cdot 1}}&amp;amp;-\frac{1}{\sqrt{2\cdot 1}}&amp;amp;0&amp;amp;\dotsb&amp;amp;0\\
\frac{1}{\sqrt{3\cdot 2}}&amp;amp;\frac{1}{\sqrt{3\cdot 2}}&amp;amp;-\frac{2}{\sqrt{3\cdot 2}}&amp;amp;\dotsb&amp;amp;0\\
\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots&amp;amp;\\
\frac{1}{\sqrt{n\cdot (n-1))}}&amp;amp;\frac{1}{\sqrt{n\cdot (n-1))}}&amp;amp;\frac{1}{\sqrt{n\cdot (n-1))}}&amp;amp;\dotsb&amp;amp;-\frac{n-1}{\sqrt{n\cdot (n-1))}}\\
\end{bmatrix}
\]&lt;/span> 这时有&lt;span class="math">\(\frac{y_i}{\sigma}\sim N(0,1),i=2,3,\dotsb,n;y_1=\sqrt{n}\bar{x}\)&lt;/span>，且 &lt;span class="math">\[\frac{(n-1)s^2}{\sigma^2}=\sum_{i=2}^n (\frac{y_i}{\sigma})^2\sim \chi^2(n-1)\]&lt;/span> 样本方差的抽样分布服从&lt;span class="math">\(\chi^2(n-1)\)&lt;/span>分布得证。&lt;/p>
&lt;p>通过构造的矩阵&lt;span class="math">\(A\)&lt;/span>，我们还发现&lt;span class="math">\(\bar{x}\)&lt;/span>只和&lt;span class="math">\(y_1\)&lt;/span>有关，而&lt;span class="math">\(s^2\)&lt;/span>只和&lt;span class="math">\(y_2,\dotsb,y_n\)&lt;/span>有关，同时&lt;span class="math">\(y_i\)&lt;/span>之间是相互独立的，因此&lt;span class="math">\(\bar{x},s^2\)&lt;/span>也是相互独立的。&lt;/p>
&lt;h2 id="样本均值与样本方差比值的分布">样本均值与样本方差比值的分布&lt;/h2>
&lt;h3 id="t分布">t分布&lt;/h3>
&lt;h4 id="tips名字来源">TIPS名字来源&lt;/h4>
&lt;p>t，为Student简写，则是William Sealy Gosset（戈塞特）的笔名。他当年在爱尔兰都柏林的一家酒厂工作，设计了一种后来被称为t检验的方法来评价酒的质量。因为行业机密，酒厂不允许他的工作内容外泄，所以当他后来将其发表到至今仍十分著名的一本杂志《Biometrika》时，就署了student的笔名。所以现在很多人知道student，知道t，却不知道Gosset。&lt;/p>
&lt;h4 id="t分布介绍">t分布介绍&lt;/h4>
&lt;p>表达式：t分布的概率密度为 &lt;span class="math">\[
p(x)=\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{x^2}{n} \right)^{-\frac{n+1}{2}}
\]&lt;/span> 其中参数&lt;span class="math">\(n\)&lt;/span>称为自由度，&lt;span class="math">\(\Gamma(x)=\int_0^{\infty}t^{x-1}e^{-t} \mathrm{d}t\)&lt;/span>为伽马函数。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/TStudent分布.png" alt="TStudent分布.png" />&lt;p class="caption">TStudent分布.png&lt;/p>
&lt;/div>
&lt;p>一些观察：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>t分布是比正态分布“宽容”分布，像正态分布但是集中度没有正态分布强。在描述重尾分布时更好。&lt;/li>
&lt;li>当&lt;span class="math">\(n=1\)&lt;/span>时，t分布为柯西分布，它的期望、方差都不存在。&lt;/li>
&lt;li>t分布是关于y轴对称的，所以&lt;span class="math">\(X\sim t(n),E(X)=0(n&amp;gt;1)\)&lt;/span>。当&lt;span class="math">\(n&amp;gt;2\)&lt;/span>时，t分布的方差存在，为&lt;span class="math">\(n/(n-2)\)&lt;/span>。&lt;/li>
&lt;li>当&lt;span class="math">\(n\geq 30\)&lt;/span>时，t分布于正态分布差别很小。&lt;/li>
&lt;li>t分布被广泛应用于&lt;strong>小样本假设检验&lt;/strong>。虽然是很小的样本，但是，却强大到可以轻松的排除异常值的干扰，准确把握住数据的特征（集中趋势和离散趋势）&lt;/li>
&lt;/ol>
&lt;h3 id="样本均值与样本方差比值服从t分布">样本均值与样本方差比值服从t分布&lt;/h3>
&lt;blockquote>
&lt;p>定理4（相互独立的标准正态分布与卡方分布之比服从t分布）：&lt;span class="math">\(X\sim N(0,1),Y\sim \chi^2(n),X,Y独立\)&lt;/span>，则&lt;span class="math">\(\frac{X}{\sqrt{Y/n}}\sim t(n)\)&lt;/span>，其中&lt;span class="math">\(t(n)\)&lt;/span>是自由度为n的分布。&lt;/p>
&lt;/blockquote>
&lt;p>证明：思路先求&lt;span class="math">\(\sqrt{Y/n}\)&lt;/span>的分布，然后再通过&lt;strong>独立&lt;/strong>随机变量商的分布求&lt;span class="math">\(\frac{X}{\sqrt{Y/n}}\)&lt;/span>。&lt;/p>
&lt;p>令&lt;span class="math">\(z=g(y)=\sqrt{y/n}(y\geq 0)\)&lt;/span>，则其反函数为&lt;span class="math">\(y=h(z)=nz^2(z\geq 0)\)&lt;/span>。卡方分布在&lt;span class="math">\(y&amp;lt;0\)&lt;/span>时都等于0，因此有&lt;span class="math">\(F_{z}(z)=0,z&amp;lt;0\)&lt;/span>。当&lt;span class="math">\(y,z&amp;gt;0\)&lt;/span>时，根据随机变量的单调函数分布定理有： &lt;span class="math">\[
p_{_Z}(z)=p_{_Y}(h(z))h&amp;#39;(z)=p_{_Y}(nz^2)(2nz)\\
=\frac{1}{2^{\frac{n}{2}-1}\Gamma(\frac{n}{2})}n^{\frac{n}{2}}z^{n-1}e^{-\frac{nz^2}{2}}
\]&lt;/span> 由于&lt;span class="math">\(X,Y\)&lt;/span>相互独立，所以&lt;span class="math">\(X,Z\)&lt;/span>也是独立的。联合概率密度&lt;span class="math">\(p(x,z)\)&lt;/span>就是&lt;span class="math">\(X,Z\)&lt;/span>两个概率密度的乘积，因此我们可以通过随机变量商的密度函数公式（参见笔记：&lt;a href="概率统计随机过程之随机变量函数的分布.md">概率统计随机过程之随机变量函数的分布.md&lt;/a>）可得&lt;span class="math">\(T=X/Z\)&lt;/span>的密度函数为： &lt;span class="math">\[
\begin{aligned}
p_{_T}(t;n)&amp;amp;=\int_{-\infty}^\infty p_{_Z}(z)p_{_X}(zt) |z| \mathrm{d}z (z&amp;gt;0)\\
&amp;amp;=\int_{0}^\infty \frac{1}{2^{\frac{n}{2}-1}\Gamma(\frac{n}{2})}n^{\frac{n}{2}}z^{n-1}e^{-\frac{nz^2}{2}}\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{(zt)^2}{2}}z \mathrm{d}z\\
\overset{\text{提出非积分项}}{=}&amp;amp;\frac{n^{\frac{n}{2}}}{\sqrt{\pi}2^{\frac{n-1}{2}}\Gamma(\frac{n}{2})}\int_{0}^\infty z^{n}e^{-\frac{z^2}{2}(n+t^2)}\mathrm{d}z\\
\overset{u=\frac{z^2}{2}(n+t^2)}{=}&amp;amp;\frac{1}{\sqrt{n\pi}\Gamma(\frac{n}{2})(1+\frac{t^2}{n})^{\frac{n+1}{2}}}\int_{0}^\infty u^{\frac{n+1}{2}-1}e^{-u}\mathrm{d}u\\
&amp;amp;=\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{t^2}{n} \right)^{-\frac{n+1}{2}}
\end{aligned}
\]&lt;/span> 得证。&lt;/p>
&lt;blockquote>
&lt;p>定理5：样本均值与样本方差比值服从t分布。设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>的样本，其样本均值和样本方差分别为&lt;span class="math">\(\bar{x}\)&lt;/span>和&lt;span class="math">\(s^2\)&lt;/span>，则有 &lt;span class="math">\[t=\frac{\sqrt{n}(\bar{x}-\mu)}{s}\sim t(n-1)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：首先根据&lt;code>定理3&lt;/code>，&lt;span class="math">\(\bar{x},s^2\)&lt;/span>是独立的，所以&lt;span class="math">\(\bar{x},s\)&lt;/span>也是独立的。根据&lt;code>定理1&lt;/code>有&lt;span class="math">\(\bar{x}\sim N(\mu,\sigma^2/n)\)&lt;/span>，则&lt;span class="math">\(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\sim N(0,1)\)&lt;/span>，而根据&lt;code>定理3&lt;/code>有&lt;span class="math">\(\frac{(n-1)s^2}{\sigma^2}\sim \chi^2(n-1)\)&lt;/span>。仿照&lt;code>定理5&lt;/code>的结构，我们可以构造： &lt;span class="math">\[\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\bigg / \sqrt{\frac{(n-1)s^2}{\sigma^2}\big /(n-1)}\sim t(n-1)\]&lt;/span> 化简后即可得： &lt;span class="math">\[t=\frac{\sqrt{n}(\bar{x}-\mu)}{s}\sim t(n-1)\]&lt;/span>&lt;/p>
&lt;h2 id="两个独立正态样本方差比的分布">两个独立正态样本方差比的分布&lt;/h2>
&lt;h3 id="f分布">F分布&lt;/h3>
&lt;p>F分布是1924年英国统计学家Ronald.A.Fisher爵士提出，并以其姓氏的第一个字母命名的。&lt;/p>
&lt;p>F分布的PDF表达式： &lt;span class="math">\[
p(x,n_1,n_2)=\begin{cases}
\frac{(n1/n2)^{n_1 \over 2}}{B(n_1/2,n_2/2)}x^{{n_1\over 2}-1}(1+{n_1\over n_2}x)^{-{n_1+n_2 \over 2}},x&amp;gt;0\\
0,x\leq 0
\end{cases}
\]&lt;/span> 其中，&lt;span class="math">\(n_1,n_2\)&lt;/span>都是自由度，&lt;span class="math">\(B(n_1/2,n_2/2)\)&lt;/span>是BETA函数，&lt;span class="math">\(B(m,n)=\frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}\)&lt;/span>。&lt;/p>
&lt;p>自由度为&lt;span class="math">\(m, n\)&lt;/span>的F 分布的密度函数如下图： &lt;img src="../../images/F分布.png" alt="F分布.png" />&lt;/p>
&lt;p>一些观察：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(F\sim F(n_1,n_2)\)&lt;/span>，则&lt;span class="math">\(\frac{1}{F}\sim F(n_2,n_1)\)&lt;/span>;&lt;/li>
&lt;li>当&lt;span class="math">\(n_2&amp;gt;2\)&lt;/span>时，F分布存在数学期望&lt;span class="math">\(n_2/(n_2-2)\)&lt;/span>;&lt;/li>
&lt;li>当&lt;span class="math">\(n_2&amp;gt;4\)&lt;/span>时，F分布存在方差&lt;span class="math">\(\frac{2n_2^2(n_1+n_2-2)}{n_1(n_2-2)^2(n_2-4)}\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(t\sim t(n)\)&lt;/span>，则&lt;span class="math">\(t^2\sim F(1,n)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;h3 id="两个独立正态样本方差比的服从f分布">两个独立正态样本方差比的服从F分布&lt;/h3>
&lt;blockquote>
&lt;p>定理6（两个独立的卡方分布之比服从F分布）：&lt;span class="math">\(X\sim \chi^2(n_1),Y\sim \chi^2(n_2),X,Y\)&lt;/span>独立，则&lt;span class="math">\(\frac{X/n_1}{Y/n_2}\sim F(n_1.n_2)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：首先通过两独立随机变量的商的分布得到&lt;span class="math">\(\frac{X}{Y}\)&lt;/span>的分布，然后再通过随机变量的单调函数的分布得到&lt;span class="math">\(\frac{n_2}{n_1}\frac{X}{Y}\)&lt;/span>的分布。证明不难，但是比较繁琐，我直接贴图片了。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/F分布形式证明.png" alt="F分布形式证明" />&lt;p class="caption">F分布形式证明&lt;/p>
&lt;/div>
&lt;blockquote>
&lt;p>定理7：两个独立正态样本方差比的服从F分布。设&lt;span class="math">\(X,Y\)&lt;/span>是分别来自正态分布&lt;span class="math">\(N(\mu_1,\sigma_1^2),N(\mu_2,\sigma_2^2)\)&lt;/span>的容量为&lt;span class="math">\(n_1,n_2\)&lt;/span>样本,，样本方差分别为&lt;span class="math">\(s_1^2,s_2^2\)&lt;/span>，则有 &lt;span class="math">\[\frac{X_1/(n_1-1)}{X_2/(n_2-1)}=\frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}\sim F(n_1-1,n_2-1)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：由&lt;code>定理3&lt;/code>可知样本方差&lt;span class="math">\(s_1^2,s_2^2\)&lt;/span>的线性变换&lt;span class="math">\(X_1=(n_1-1)s_1^2/\sigma_1^2,X_2=(n_2-1)s_2^2/\sigma_2^2\)&lt;/span>的分布分别服从&lt;span class="math">\(\chi^2(n_1-1),\chi^2(n_2-1)\)&lt;/span>，且相互独立。在将分子分母的自由度分别代入&lt;code>定理6&lt;/code>，可得&lt;span class="math">\(\frac{X_1/(n_1-1)}{X_2/(n_2-1)}\sim F(n_1-1,n_2-1)\)&lt;/span>，&lt;code>定理7&lt;/code>得证。&lt;/p>
&lt;h2 id="正态总体下的抽样分布总结">正态总体下的抽样分布总结&lt;/h2>
&lt;div class="figure">
&lt;img src="../../images/正态总体下的抽样分布.jpg" alt="正态总体下的抽样分布.jpg" />&lt;p class="caption">正态总体下的抽样分布.jpg&lt;/p>
&lt;/div>
&lt;h2 id="附录">附录&lt;/h2>
&lt;h3 id="附录1-gamma分布可加性证明">附录1-Gamma分布可加性证明&lt;/h3>
&lt;p>可以利用Gamma分布的特征函数快速证明。记Gamma分布为&lt;span class="math">\(Ga(\alpha_i,\lambda)\)&lt;/span>，则其特征函数为 &lt;span class="math">\[
\varphi_i(t)=(1-\frac{it}{\lambda})^{-\alpha_i}
\]&lt;/span> 当&lt;span class="math">\(\lambda\)&lt;/span>相同时，有 &lt;span class="math">\[Ga(\alpha_i,\lambda)+Ga(\alpha_j,\lambda)=\varphi_i(t)*\varphi_j(t)\\
=(1-\frac{it}{\lambda})^{-(\alpha_i+\alpha_j)}=Ga(\alpha_i+\alpha_j,\lambda)\]&lt;/span> 得证。&lt;/p></description></item><item><title>概率统计随机过程之指数型分布族应用</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%8C%87%E6%95%B0%E5%9E%8B%E5%88%86%E5%B8%83%E6%97%8F%E5%BA%94%E7%94%A8/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%8C%87%E6%95%B0%E5%9E%8B%E5%88%86%E5%B8%83%E6%97%8F%E5%BA%94%E7%94%A8/</guid><description>
&lt;h2 id="概率统计随机过程之指数型分布族应用">概率统计随机过程之指数型分布族应用&lt;!-- omit in toc -->&lt;/h2>
&lt;p>在学习广义线性模型的时候，其各种模型都可以通过指数型分布族的形式来表示，而指数型分布族可以给出求原始分布均值和方差的统一形式，这在机器学习、数理统计中有重要作用。此外，本文还介绍了指数型分布族使用最大似然估计来估计参数的方法。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#指数型分布族指数族">指数型分布族（指数族）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族的转换例子">指数型分布族的转换例子&lt;/a>&lt;/li>
&lt;li>&lt;a href="#伯努利分布的指数族形式">伯努利分布的指数族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多类别分布的指数族形式">多类别分布的指数族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均值未知方差已知的高斯分布的指数族形式">均值未知方差已知的高斯分布的指数族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均值方差皆未知的高斯分布的指数族形式">均值方差皆未知的高斯分布的指数族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泊松分布的指数型分布族形式">泊松分布的指数型分布族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数族的期望与方差的统一形式">指数族的期望与方差的统一形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数族的期望">指数族的期望&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数族的方差">指数族的方差&lt;/a>&lt;/li>
&lt;li>&lt;a href="#theta与eta的一一对应缘由">&lt;span class="math">\(\\theta\)&lt;/span>与&lt;span class="math">\(\\eta\)&lt;/span>的一一对应缘由&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族的最大似然估计">指数型分布族的最大似然估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#自然指数族">自然指数族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数分散族">指数分散族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分散参数讨论">分散参数讨论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#配分函数讨论">配分函数讨论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数分散族的方差">指数分散族的方差&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="指数型分布族指数族">指数型分布族（指数族）&lt;/h2>
&lt;p>指数型分布族是指数分布族的推广，囊括了正态分布族、二项分布族、伽马分布族、多项分布族常见分布等等。具体定义形式如下：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>指数型分布族&lt;/strong>：一个概率分布族&lt;span class="math">\(\mathfrak{p}=\{p_{\theta}(x);\theta∈\varTheta\}\)&lt;/span>可称为&lt;strong>指数型分布族&lt;/strong>，假如&lt;span class="math">\(\mathfrak{p}\)&lt;/span>中的分布（分布列或密度函数）都可表示为如下形式： &lt;span class="math">\[p_\theta(x)=h(x)c(\theta)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\}\tag{1}\]&lt;/span> 其中，k为自然数；&lt;span class="math">\(\theta\)&lt;/span>可以是数字，也可以是向量。分布的支撑&lt;span class="math">\(\{x:p(x)&amp;gt;0\}\)&lt;/span>与参数&lt;span class="math">\(\theta\)&lt;/span>无关；诸&lt;span class="math">\(c(\theta),c_1(\theta),\dotsb,c_k(\theta)\)&lt;/span>是定义在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上的函数；诸&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>是&lt;span class="math">\(x\)&lt;/span>的函数，称为充分统计向量，但&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>线性无关。&lt;span class="math">\(h(x)\)&lt;/span>也只是&lt;span class="math">\(x\)&lt;/span>的函数，且&lt;span class="math">\(h(x)&amp;gt;0\)&lt;/span>，通常是一个常数。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;span class="math">\(c(\theta)\)&lt;/span>是作为归一化参数存在的，称为叫做配分函数(partition function)。 &lt;span class="math">\[c(\theta)^{-1} = \int h(x) \exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\} dx\]&lt;/span> 此外，指数族还有另一种表述方式，就是将外面的&lt;span class="math">\(c(\theta)\)&lt;/span>放到指数符号中： &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)-A(\theta)\right\}\tag{2}\]&lt;/span> 由于通常&lt;span class="math">\(A(\theta)\)&lt;/span>含有&lt;span class="math">\(\log\)&lt;/span>符号，该部分也称为“Log Partition Function”，易知&lt;span class="math">\(A(\theta)=\ln c(\theta)\)&lt;/span>。 如果我们使用向量值函数来表达指数型分布族可写为: &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\mathbf{C}^T(\theta)\mathbf{T}(x)-A(\theta)\right\}\tag{3}\]&lt;/span>&lt;/p>
&lt;p>从上述定义可知，一个分布族是不是指数型分布族的&lt;strong>关键在于其概率分布能否改写为定义中方式&lt;/strong>。&lt;/p>
&lt;h3 id="指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/h3>
&lt;p>下面我们使用&lt;strong>向量值函数&lt;/strong>将式(3)进行进一步改造。&lt;/p>
&lt;blockquote>
&lt;p>向量值函数：有时也称为向量函数，是一个单变量或多变量的、&lt;strong>值域是多维向量或者无穷维向量的集合的函数&lt;/strong>。向量值函数的输入可以是一个标量或者一个向量，输出是向量，定义域的维度和值域的维度是不相关的。&lt;/p>
&lt;/blockquote>
&lt;p>对于&lt;span class="math">\(\theta\)&lt;/span>的一系列函数&lt;span class="math">\(c_1(\theta),c_2(\theta),\dotsb\)&lt;/span>和充分统计量向量&lt;span class="math">\(T_1(x),T_2(x),\dotsb\)&lt;/span>，我们写出列向量形式： &lt;span class="math">\[
\mathbf{C}(\theta)=\begin{bmatrix}c_1(\theta)\\c_2(\theta)\\\vdots\\c_k(\theta)\end{bmatrix}
\mathbf{T}(x)=\begin{bmatrix}T_1(x)\\T_2(x)\\\vdots\\T_k(x)\end{bmatrix}
\]&lt;/span> 那么式（3）可写成 &lt;span class="math">\[
p(x;\theta)=h(x)\exp\left\{\mathbf{C}^T(\theta)\mathbf{T}(x)-A(\theta)\right\}\tag{4}
\]&lt;/span> 其中，&lt;span class="math">\(\mathbf{C}(\theta),\mathbf{T}(x)\)&lt;/span>都是向量值函数，&lt;span class="math">\(h(x),A(\theta)\)&lt;/span>都是普通函数。通常文章会把&lt;span class="math">\(A(\theta)\)&lt;/span>写成&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的形式，这两种本质上是等价的，但是&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的参数形式更加统一，为主流用法。由于&lt;span class="math">\(\mathbf{C}(\theta)\)&lt;/span>的计算结果本质上就是一个向量，我们可令向量值函数&lt;span class="math">\(\mathbf{C(\theta)}=\eta\)&lt;/span>，那么式（4）可表示为： &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{T}(x)-A(\eta)\right\}\tag{5}
\]&lt;/span> 这就是其他资料中的常见形式。其中&lt;span class="math">\(\eta=\mathbf{C}(\theta)\)&lt;/span>，参数&lt;span class="math">\(η\)&lt;/span>通常叫做自然参数(natural parameter)或者标准参数(canonical parameter)。这里注明：&lt;span class="math">\(A(\theta)\)&lt;/span>与&lt;span class="math">\(A(\eta)\)&lt;/span>实际上是两个不同的函数，但是可以通过&lt;span class="math">\(\eta=\mathbf{C}(\theta),\theta=\mathbf{C}^{-1}(\eta)\)&lt;/span>进行互换，因此在后文对他们不做区分。此外，在&lt;a href="#指数族的期望与方差的统一形式">指数族的期望与方差的统一形式&lt;/a>一节中，我们还会证明为什么&lt;span class="math">\(\eta,\theta\)&lt;/span>是一一对应的，这里先写出这个引理。&lt;/p>
&lt;blockquote>
&lt;p>引理1：在指数族中函数&lt;span class="math">\(C(\cdot)\)&lt;/span>总是&lt;strong>单调连续的(存在逆函数)&lt;/strong>，所以自然参数&lt;span class="math">\(η\)&lt;/span>和原始参数&lt;span class="math">\(θ\)&lt;/span>是&lt;strong>存在一一映射关系的&lt;/strong>。 &lt;span class="math">\[
\eta=\mathbf{C}(\theta)\\
\theta=\mathbf{C}^{-1}(\eta)
\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>在指数型分布族中，使用标准参数&lt;span class="math">\(η\)&lt;/span>表示的公式形式称为&lt;strong>指数族分布的标准形式(canonical form)&lt;/strong>，在标准形式下，分布的参数是&lt;span class="math">\(η\)&lt;/span>。&lt;strong>实际上，从原始分布向指数型分布转换的过程就是将&lt;span class="math">\(\theta\)&lt;/span>转换为&lt;span class="math">\(\eta\)&lt;/span>的过程&lt;/strong>，在下节中我们会用例子说明。&lt;/p>
&lt;p>指数分布族的意义在于给了我们常见分布一个统一的形式，我们通过此形式得出的结论，可以推广到所有符合该形式的概率分布。指数族有以下特殊之处，可能有些内容暂时不会涉及。&lt;/p>
&lt;ul>
&lt;li>指数族分布是&lt;strong>唯一有共轭先验的分布族&lt;/strong>,这就简化了后验的计算&lt;/li>
&lt;li>在特定的规范化条件下(regularity conditions),指数族分布是&lt;strong>唯一有限规模充分统计量&lt;/strong>(finite-sized sufficient statistics)的分布族,这意味着可以将数据压缩称固定规模的浓缩概括而不损失信息&lt;/li>
&lt;li>指数族分布是&lt;strong>广义线性模型&lt;/strong>(generalized linear models)的核心&lt;/li>
&lt;li>指数族分布也是&lt;strong>变分推理&lt;/strong>(variational inference)的核心&lt;/li>
&lt;/ul>
&lt;h2 id="指数型分布族的转换例子">指数型分布族的转换例子&lt;/h2>
&lt;h3 id="伯努利分布的指数族形式">伯努利分布的指数族形式&lt;/h3>
&lt;p>伯努利分布又叫两点分布或者0-1分布，是最简单的概率分布形式之一。常见伯努利分布写成： &lt;span class="math">\[
p(x;p)=p^x(1-p)^{1-x},x\in\{0,1\}
\]&lt;/span> 转写为指数型分布族形式为： &lt;span class="math">\[
\begin{aligned}
p(x;p)&amp;amp;=\exp\{x\ln{p}+(1-x)\ln{(1-p)}\}\\
&amp;amp;=\exp\{x\ln(\frac{p}{1-p})+\ln{(1-p)}\}
\end{aligned}
\]&lt;/span> 对照指数族的形式，有&lt;span class="math">\(h(x)=1;c(p)=\ln(\frac{p}{1-p});T(x)=x;A(p)=-\ln (1-p)\)&lt;/span>。写成标准形式为： &lt;span class="math">\[
p(x;\eta)=\exp\{\eta x-\ln(1+e^\eta)\}
\]&lt;/span> 标准参数&lt;span class="math">\(\eta\)&lt;/span>和原始参数&lt;span class="math">\(p\)&lt;/span>的关系为： &lt;span class="math">\[
\eta=\ln(\frac{p}{1-p})\\
p=\frac{1}{1+e^{-\eta}}\tag{6}
\]&lt;/span> 其中&lt;span class="math">\(\ln(\frac{p}{1-p})\)&lt;/span>成为logit函数，其反函数&lt;span class="math">\(\frac{1}{1+e^{-\eta}}\)&lt;/span>称为sigmoid函数。如果我们得到了&lt;span class="math">\(\eta\)&lt;/span>就可以用sigmoid函数反推得到&lt;span class="math">\(p\)&lt;/span>。&lt;/p>
&lt;h3 id="多类别分布的指数族形式">多类别分布的指数族形式&lt;/h3>
&lt;p>伯努利分布中是只有两个取值的离散随机变量的概率分布，当随机变量的取值扩展到多个(大于2个并且有限集)的时候，就是称为多类别分布。比如掷一个均匀的骰子，就是6个等概率结果的多类别分布。假设多类别分布中共有&lt;span class="math">\(m\)&lt;/span>个类别，其概率分别为&lt;span class="math">\(\theta_1,\theta_2,\dotsb,\theta_m\)&lt;/span>，那么其概率质量函数为： &lt;span class="math">\[
p(x;\mathbf{\theta})=\prod_{i=1}^m \theta_i^{x_i}\\
\sum_{i=1}^m \theta_i=1\\
\sum_{i=1}^m x_i=1
\]&lt;/span> 在一次实验的&lt;span class="math">\(m\)&lt;/span>个&lt;span class="math">\(x_i\)&lt;/span>中，只有被取到的那个&lt;span class="math">\(x_i\)&lt;/span>为1，其余的&lt;span class="math">\(x_j\)&lt;/span>都是0。我们将其改写成指数型分布族的形式： &lt;span class="math">\[
p(x;\mathbf{\theta})=\exp\{\sum_{i=1}^m x_i\ln{\theta_i}\}
\]&lt;/span> 我们注意到，由于条件&lt;span class="math">\(\sum_{i=1}^m \theta_i=1,\sum_{i=1}^m x_i=1,x_i=\{0,1\}\)&lt;/span>的限制，实际上&lt;span class="math">\(m\)&lt;/span>个&lt;span class="math">\(\theta_i,x_i\)&lt;/span>都只有&lt;span class="math">\(m-1\)&lt;/span>个自由变量，最后的&lt;span class="math">\(\theta_m,x_m\)&lt;/span>可以用&lt;span class="math">\(\theta_m=1-\sum_{i=1}^{m-1}\theta_i,x_m=1-\sum_{i=1}^{m-1}x_i\)&lt;/span>表示。因此上面的等式可以改写为： &lt;span class="math">\[
\begin{aligned}
p(x;\mathbf{\theta})&amp;amp;=\exp\{\sum_{i=1}^{m-1} x_i\ln{\theta_i}+(1-\sum_{i=1}^{m-1}x_i)\ln{(1-\sum_{i=1}^{m-1}\theta_i)}\}\\
&amp;amp;=\exp\left\{\sum_{i=1}^{m-1} x_i\ln{\frac{\theta_i}{1-\sum_{j=1}^{m-1}\theta_j}}+\ln{(1-\sum_{i=1}^{m-1}\theta_i)}\right\}\\
其中，&amp;amp;\theta_m=1-\sum_{j=1}^{m-1}\theta_j\\
&amp;amp;=\exp\left\{\sum_{i=1}^{m-1} x_i\ln{\frac{\theta_i}{\theta_m}}+\ln{\theta_m}\right\}
\end{aligned}
\]&lt;/span> 对照向量化形式的指数型分布族形式，有&lt;span class="math">\(h(x)=1;\mathbf{C}(\theta)=\begin{bmatrix}\ln(\theta_1/\theta_m)\\\ln(\theta_2/\theta_m)\\\vdots\\\ln(\theta_{m-1}/\theta_m)\end{bmatrix},\mathbf{T}(x)=\begin{bmatrix}x_1\\x_2\\\vdots\\x_{m-1}\end{bmatrix};A(\theta)=-\ln(1-\sum_{i=1}^{m-1}\theta_i)=-\ln(\theta_m)\)&lt;/span>。&lt;/p>
&lt;p>将其写成标准形式为： &lt;span class="math">\[
p(x;\eta)=\exp\{\eta^T T(x)-\ln(\sum_{i=1}^me^{\eta_i})\}
\]&lt;/span> 其中，&lt;span class="math">\(\eta=\mathbf{C}(\theta),A(\eta)=\ln(\sum_{i=1}^me^{\eta_i})\)&lt;/span>&lt;/p>
&lt;p>在&lt;span class="math">\(\theta\)&lt;/span>与&lt;span class="math">\(\eta\)&lt;/span>的换算中需要一个技巧，我们在&lt;span class="math">\(\mathbf{C}(\theta)\)&lt;/span>最后添加一项&lt;span class="math">\(c_m(\theta)=\ln(\theta_m/\theta_m)\equiv 0\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>PS:若我们将添加&lt;span class="math">\(c_m(\theta)\)&lt;/span>的&lt;span class="math">\(\mathbf{C}(\theta)\)&lt;/span>记为&lt;span class="math">\(\mathbf{\hat{C}}(\theta)\)&lt;/span>，&lt;span class="math">\(\mathbf{T}(x)\)&lt;/span>最后也添加一项&lt;span class="math">\(x_m\)&lt;/span>，记为&lt;span class="math">\(\mathbf{\hat{T}}(x)\)&lt;/span>，由于&lt;span class="math">\(c_m(\theta)=0\)&lt;/span>，因此&lt;span class="math">\(\mathbf{\hat{C}}^T(\theta)\mathbf{\hat{T}}(x)=\sum_{i=1}^{m} x_i\ln{\theta_i}=\sum_{i=1}^{m-1} x_i\ln{\theta_i}=\mathbf{C}^T(\theta)\mathbf{T}(x)\)&lt;/span>。因此，在此定义下，可以用后者替代前者得到更加工整的表达形式。&lt;/p>
&lt;/blockquote>
&lt;p>那么有： &lt;span class="math">\[
\eta_i=c_i(\theta)=\ln(\frac{\theta_i}{\theta_m})\Rightarrow \theta_i=\theta_m e^{\eta_i}\\
\Rightarrow \sum_{i=1}^m \theta_i=\theta_m\sum_{i=1}^m e^{\eta_i}=1\\
\Rightarrow \theta_m=\frac{1}{\sum_{i=1}^m e^{\eta_i}}\\
\Rightarrow\theta_i=\theta_m e^{\eta_i}=\frac{e^{\eta_i}}{\sum_{i=1}^m e^{\eta_i}}\tag{7}
\]&lt;/span> 我们将上式称为softmax函数，普遍用于多分类问题。&lt;/p>
&lt;h3 id="均值未知方差已知的高斯分布的指数族形式">均值未知方差已知的高斯分布的指数族形式&lt;/h3>
&lt;p>典型的高斯分布写成（方差&lt;span class="math">\(\sigma^2\)&lt;/span>已知）： &lt;span class="math">\[
p(x;\mu)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}
\]&lt;/span> 转写为指数族形式为： &lt;span class="math">\[
p(x;\mu)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{1}{2\sigma^2}x^2\}\cdot\exp\{\frac{1}{2\sigma^2}(2\mu x-\mu^2)\}
\]&lt;/span> 对照指数族的形式，有&lt;span class="math">\(h(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{1}{2\sigma^2}x^2\};c(\mu)=\frac{\mu}{\sigma^2};T(x)=x;A(\mu)=\frac{\mu^2}{2\sigma^2}\)&lt;/span>。写成标准形式为： &lt;span class="math">\[
p(x;\eta)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{1}{2\sigma^2}x^2\}\cdot\exp\{\eta x-\frac{\eta^2\sigma^2}{2}\}
\]&lt;/span> 当服从标准正态分布时，&lt;span class="math">\(\sigma=1\Rightarrow\eta=\mu\)&lt;/span>。&lt;/p>
&lt;h3 id="均值方差皆未知的高斯分布的指数族形式">均值方差皆未知的高斯分布的指数族形式&lt;/h3>
&lt;p>如果高斯分布的均值和方差都是未知的，那么就需要使用多个线性不相关的充分统计量来表示指数型分布族： &lt;span class="math">\[
\begin{aligned}
p(x;\mu,\sigma^2)&amp;amp;=\frac{1}{(2\pi\sigma^2)^{1/2}} \exp[ -\frac{1}{2\sigma^2}(x-\mu)^2]\\
&amp;amp;=\frac{1}{(2\pi\sigma^2)^{1/2}} \exp[-\frac{1}{2\sigma^2} x^2 +\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2]\\
&amp;amp;=\frac{1}{(2\pi)^{1/2}} \exp[-\frac{1}{2\sigma^2} x^2 +\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2-\ln{\sigma}]
\end{aligned}
\]&lt;/span> 对照指数族的形式，有&lt;span class="math">\(h(x)=\frac{1}{(2\pi)^{1/2}}, \mathbf{C}=\begin{bmatrix}\mu/\sigma^2 \\-1/(2\sigma^2)\end{bmatrix},\mathbf{T}=\begin{bmatrix}x\\x^2\end{bmatrix},A(\mu,\sigma)=\frac{1}{2\sigma^2}\mu^2+\ln{\sigma}\)&lt;/span>。&lt;/p>
&lt;p>需要指出的是，一般情况下，我们在&lt;span class="math">\(A(\mu,\sigma)\)&lt;/span>中最好不要在使用原有的&lt;span class="math">\(\mu,\sigma\)&lt;/span>，而是使用&lt;span class="math">\(\mathbf{C}\)&lt;/span>中的分量&lt;span class="math">\(c_1=\mu/\sigma^2,c_2=-1/(2\sigma^2)\)&lt;/span>表示，即 &lt;span class="math">\[
A(\mathbf{C})=\frac{-c_1^2}{4c_2}-\frac{1}{2}\log(-2c_2)\tag{8.1}
\]&lt;/span> 所以，其指数型分布族表示形式为 &lt;span class="math">\[
p(x;\mathbf{C})=\frac{1}{(2\pi)^{1/2}} \exp[\mathbf{C}^T(\mu,\sigma)\mathbf{T}(x)-A(\mathbf{C})]
\]&lt;/span> 在其他文献中，也有令&lt;span class="math">\(h(x)=1\)&lt;/span>，然后把&lt;span class="math">\(\frac{1}{(2\pi)^{1/2}}\)&lt;/span>放到&lt;span class="math">\(A(\mathbf{C})\)&lt;/span>中的，即&lt;span class="math">\(A(\mathbf{C})=\frac{-c_1^2}{4c_2}-\frac{1}{2}\log(-2c_2)+\frac{1}{2}\log(2\pi)=\frac{-c_1^2}{4c_2}+\frac{1}{2}\log(-\frac{\pi}{c_2})\)&lt;/span>，这样是等价的。&lt;/p>
&lt;h3 id="泊松分布的指数型分布族形式">泊松分布的指数型分布族形式&lt;/h3>
&lt;p>泊松分布的概率质量函数如下： &lt;span class="math">\[
p(x;\theta)=\frac{\theta^x e^{-\theta}}{x!}
\]&lt;/span> 其中，&lt;span class="math">\(x\)&lt;/span>为正整数。将其改写为指数型分布族： &lt;span class="math">\[
p(x;\theta)=\frac{1}{x!}\exp\{x\ln(\theta)-\theta\}
\]&lt;/span> 对照指数型分布族形式易知：&lt;span class="math">\(h(x)=\frac{1}{x!},c(\theta)=\ln(\theta),T(x)=x,A(\theta)=\theta\)&lt;/span>。根据&lt;span class="math">\(\eta=c(\theta)\)&lt;/span>显然有： &lt;span class="math">\[
\eta=\ln(\theta)\\
\theta=e^{\eta}
\]&lt;/span> 此时，&lt;span class="math">\(A(\eta)=e^{\eta}\)&lt;/span>。&lt;/p>
&lt;p>其他常见指数型分布族可参见&lt;a href="https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions">维基百科词条&lt;/a>。&lt;/p>
&lt;p>由此可见，不少常见的分布如泊松分布、Beta分布、Gamma分布都是指数分布族中的一员。我们对指数族形式的推导都可以应用到这些概率分布上。&lt;/p>
&lt;h2 id="指数族的期望与方差的统一形式">指数族的期望与方差的统一形式&lt;/h2>
&lt;h3 id="指数族的期望">指数族的期望&lt;/h3>
&lt;p>我们在定义指数型分布族时提过，&lt;span class="math">\(A(\eta)\)&lt;/span>作为Log配分函数(log partition function)，实现了概率分布的归一化，即: &lt;span class="math">\[
c(\theta)^{-1} = \int h(x) \exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\} dx=e^{A(\theta)}\\
\Rightarrow A(\theta)=\ln\left\{\int h(x) \exp\left [\sum_{j=1}^k c_j(\theta)T_j(x)\right ] dx\right\}
\]&lt;/span> 由于&lt;span class="math">\(\eta\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>存在一一关系，我们用&lt;span class="math">\(\eta\)&lt;/span>替代&lt;span class="math">\(\theta\)&lt;/span>得到 &lt;span class="math">\[
A(\eta)=\ln\left\{\int h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx\right\}
\]&lt;/span> 指数族有一个特点，就是我们可以通过对&lt;span class="math">\(A(η)\)&lt;/span>求（偏）导来得到关于&lt;span class="math">\(T(x)\)&lt;/span>的矩，而一阶矩和二阶矩能够推导出概率分布的期望和方差。当&lt;span class="math">\(T(x)=x\)&lt;/span>或存在&lt;span class="math">\(T_i(x)=x\)&lt;/span>的分量时，我们就可以用求导或者求偏导得到关于&lt;span class="math">\(x\)&lt;/span>的期望和方差。具体做法如下，先&lt;span class="math">\(A(\eta)\)&lt;/span>求一阶导： &lt;span class="math">\[
\begin{aligned}
\frac{dA(\eta)}{d \eta}&amp;amp;=\frac{d}{d\eta}\ln\left\{\int h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx\right\}\\
&amp;amp;=\frac{\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx}{\int h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx}\\
&amp;amp;\because \int h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx=e^{A(\eta)}\\
&amp;amp;=\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)] dx\\
&amp;amp;=E[\mathbf{T}(x)]
\end{aligned}\tag{9}
\]&lt;/span> 可见，Log配分函数的一阶导就是&lt;span class="math">\(T(x)\)&lt;/span>的概率期望，显然当&lt;span class="math">\(T(x)=x\)&lt;/span>时，有&lt;span class="math">\(E(T(x))=E(x)=\mu\)&lt;/span>。对于伯努利分布、多项分布、泊松分布、高斯分布等这些&lt;span class="math">\(T(x)=x\)&lt;/span>的分布来说，&lt;span class="math">\(A(\eta)\)&lt;/span>的一阶导就是分布的均值&lt;span class="math">\(\mu\)&lt;/span>。&lt;/p>
&lt;p>下面举两个例子。第一个多类别分布，其&lt;span class="math">\(A(\eta)=\ln(\sum_{i=1}^me^{\eta_i})\)&lt;/span>，期望应该为&lt;span class="math">\(\mathbf{\theta}=\begin{bmatrix}\theta_1\\\theta_2\\\vdots\\\theta_m\end{bmatrix}\)&lt;/span>（很特殊，其期望是一个向量）。我们求&lt;span class="math">\(A(\eta)\)&lt;/span>的一阶导(&lt;span class="math">\(\eta\)&lt;/span>是向量，因此结果是向量导数)有： &lt;span class="math">\[
\frac{d A(\eta)}{d\eta}=\frac{d \ln(\sum_{i=1}^me^{\eta_i})}{d\eta}=\frac{d\sum_{i=1}^me^{\eta_i}/d\eta}{\sum_{i=1}^me^{\eta_i}}\\
=\begin{bmatrix}
\frac{e^{\eta_1}}{\sum_{i=1}^me^{\eta_i}}\\
\frac{e^{\eta_2}}{\sum_{i=1}^me^{\eta_i}}\\
\vdots\\
\frac{e^{\eta_m}}{\sum_{i=1}^me^{\eta_i}}
\end{bmatrix}=\begin{bmatrix}\theta_1\\\theta_2\\\vdots\\\theta_m\end{bmatrix}
\]&lt;/span> 即&lt;span class="math">\(\frac{d A(\eta)}{d\eta}=E[x]\)&lt;/span>。&lt;/p>
&lt;p>第二个例子，我们来看均值方差皆未知的高斯分布，有&lt;span class="math">\(\eta=\mathbf{C}(\theta)\)&lt;/span>中的分量&lt;span class="math">\(\eta_1=c_1(\mu,\sigma)=\mu/\sigma^2,\eta_2=c_2(\mu,\sigma)=-1/2\sigma^2\)&lt;/span>表示，即 &lt;span class="math">\[
A(\mathbf{\eta})=\frac{-\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2)\tag{8.2}
\]&lt;/span> 其&lt;span class="math">\(T(x)=[x\quad x^2]^T\)&lt;/span>，如果我们仅对&lt;span class="math">\(x\)&lt;/span>那一项对应的&lt;span class="math">\(\eta_1\)&lt;/span>求偏导，就可以得到概率分布的均值： &lt;span class="math">\[
\frac{\partial A(\eta)}{\partial \eta_1}=\frac{-2\eta_1}{4\eta_2}=\frac{-\mu/\sigma^2}{2\times (-{1\over 2\sigma^2})}=\mu
\]&lt;/span> 即&lt;span class="math">\(\frac{\partial A(\eta)}{\partial \eta_1}=\mu\)&lt;/span>。&lt;/p>
&lt;h3 id="指数族的方差">指数族的方差&lt;/h3>
&lt;p>在一阶导数的基础上，我们可以求出&lt;span class="math">\(A(\eta)\)&lt;/span>的二阶导，由式（9）继续求（偏）导： &lt;span class="math">\[
\begin{aligned}
\frac{d^2A(\eta)}{d\eta^2}&amp;amp;=\frac{d}{d\eta}\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)] dx\\
&amp;amp;=\int \mathbf{T}(x) h(x) \frac{d}{d\eta}\{\exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)]\} dx\\
&amp;amp;=\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)](\mathbf{T}(x)-\underbrace{\frac{d}{d\eta}A(\eta))}_{E[T(x)]} dx\\
&amp;amp;=\int \mathbf{T}^2(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)] dx\\
&amp;amp;-E[\mathbf{T}(x)]\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)] dx\\
&amp;amp;=E[\mathbf{T}^2(x)]-E[\mathbf{T}(x)]^2\\
&amp;amp;=\mathrm{Var}[\mathbf{T}(x)]
\end{aligned}\tag{10}
\]&lt;/span> 从上式可知，&lt;span class="math">\(A(η)\)&lt;/span>的二阶导数正好是&lt;span class="math">\(T(x)\)&lt;/span>的方差，对于&lt;span class="math">\(T(x)=x\)&lt;/span>的分布，就是概率分布的方差。&lt;/p>
&lt;p>我们来看均值方差皆未知的高斯分布，对&lt;span class="math">\(\eta_1\)&lt;/span>求两次偏导： &lt;span class="math">\[
\frac{\partial^2 A(\eta)}{\partial \eta_1^2}=\frac{-2}{4\eta_2}=\frac{-2}{4\times (-{1\over 2\sigma^2})}=\sigma^2
\]&lt;/span> 显然就是高斯分布的方差。&lt;/p>
&lt;p>我们再举一个泊松分布的例子，其&lt;span class="math">\(A(\eta)=e^\eta\)&lt;/span>。显然，其对&lt;span class="math">\(\eta\)&lt;/span>求任意阶导数，都是&lt;span class="math">\(e^\eta=\theta\)&lt;/span>，而泊松分布的均值和方差也都是&lt;span class="math">\(\eta\)&lt;/span>。&lt;/p>
&lt;h3 id="theta与eta的一一对应缘由">&lt;span class="math">\(\theta\)&lt;/span>与&lt;span class="math">\(\eta\)&lt;/span>的一一对应缘由&lt;/h3>
&lt;p>我们发现现函数&lt;span class="math">\(A(η)\)&lt;/span>的二阶导数是&lt;span class="math">\(T(x)\)&lt;/span>的方差，我们都知道方差肯定是大于等于0的，一个函数的二阶导数大于等于0，证明这个函数是一个&lt;strong>凸函数&lt;/strong>(convex，碗状的)， 对于凸函数，其一阶导数单调递增。而其一阶导数正好又是&lt;span class="math">\(\mu=E[T(x)]\)&lt;/span>，因此一阶导数结果&lt;span class="math">\(\mu\)&lt;/span>和函数自变量&lt;span class="math">\(\eta\)&lt;/span>是一一对应的，&lt;span class="math">\(\eta\)&lt;/span>可以用&lt;span class="math">\(\mu\)&lt;/span>的反函数表示。此外在指数型分布族中，&lt;span class="math">\(\mu\)&lt;/span>和原分布中的参数&lt;span class="math">\(\theta\)&lt;/span>有着很简单的关系，且是一一对应的。因此，&lt;span class="math">\(\eta\)&lt;/span>与参数&lt;span class="math">\(\theta\)&lt;/span>也是一一对应关系。总结来时就是： &lt;span class="math">\[
\eta\stackrel{一一对应}{\longleftrightarrow}\mu\stackrel{一一对应}{\longleftrightarrow} \theta\tag{11}
\]&lt;/span>&lt;/p>
&lt;h2 id="指数型分布族的最大似然估计">指数型分布族的最大似然估计&lt;/h2>
&lt;p>现在我们讨论下指数族的最大似然估计，我们知道指数族的自然参数&lt;span class="math">\(η\)&lt;/span>和特定分布的原始参数&lt;span class="math">\(θ\)&lt;/span>是一一对应的，二者是存在可逆关系的，所有只要我们能估计出自然参数&lt;span class="math">\(η\)&lt;/span>，就一定能通过逆函数&lt;span class="math">\(c(⋅)^{−1}\)&lt;/span>得到分布的真实参数&lt;span class="math">\(θ\)&lt;/span>的估计值，也就是说对于指数族，我们只需要推导自然参数的估计量&lt;span class="math">\(\eta\)&lt;/span>就能求出原始参数&lt;span class="math">\(\theta\)&lt;/span>。&lt;/p>
&lt;p>我们在指数型分布族中同样可以使用&lt;strong>最大似然估计&lt;/strong>来估计自然参数（标准参数）&lt;span class="math">\(\eta\)&lt;/span>。按照最大似然估计的套路（详细可见&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>），我们需要假设这些样本都是独立同分布（i.i.d）的，用符号&lt;span class="math">\(\mathcal{D}\)&lt;/span>表示随机变量的一个观测样本集，样本容量是&lt;span class="math">\(N\)&lt;/span>。根据式（5）的指数型分布族表达式 &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{T}(x)-A(\eta)\right\}\tag{5}
\]&lt;/span> 可得样本的联合概率密度为： &lt;span class="math">\[
\begin{aligned}
L(\eta;\mathcal{D})&amp;amp;=\prod_{k=1}^N \left\{h(x_k)\exp[\eta^T\mathbf{T}(x_k)-A(\eta)]\right\}\\
&amp;amp;=\prod_{k=1}^N h(x_k)\times\exp\left\{\eta^T\sum_{k=1}^N\mathbf{T}(x_k)-NA(\eta)\right\}
\end{aligned}\tag{12}
\]&lt;/span> 对比一下，我们发现指数族分布的联合概率仍然是指数族，只是每一个部分有了变化： &lt;span class="math">\[
h_{ML}(\mathbf{x})=\prod_{k=1}^N h(x_k)\\
\eta_{ML}=\eta\\
T_{ML}(\mathbf{x})=\sum_{k=1}^N\mathbf{T}(x_k)\\
A_{ML}(\eta)=NA(\eta)
\]&lt;/span> 除了标准参数&lt;span class="math">\(\eta\)&lt;/span>没有改变，其他部分都发生了变化，但总体还是属于指数型分布族。&lt;/p>
&lt;p>按照最大似然估计的步骤，我们对式（12）取对数&lt;span class="math">\(\ln(\log)\)&lt;/span>有： &lt;span class="math">\[
\begin{aligned}
l(\eta;\mathcal{D})&amp;amp;=\ln(L(\eta;\mathcal{D}))\\
&amp;amp;=\sum_{k=1}^n\ln(h(x_k))+\eta^T \sum_{k=1}^N\mathbf{T}(x_k)-NA(\eta)
\end{aligned}\tag{13}
\]&lt;/span> 接下来，为了求&lt;span class="math">\(\eta\)&lt;/span>的极值，我们对式（13）求&lt;span class="math">\(\eta\)&lt;/span>的导数并令其等于0： &lt;span class="math">\[
\nabla_\eta l(\eta;\mathcal{D})=\sum_{k=1}^N\mathbf{T}(x_k)-N\nabla_\eta A(\eta)=0\\
\Rightarrow \nabla_\eta A(\eta)=\frac{1}{N}\sum_{k=1}^N\mathbf{T}(x_k)\tag{14}
\]&lt;/span> 由于&lt;span class="math">\(\eta\)&lt;/span>可能是向量，对于向量的导数，我们使用梯度&lt;span class="math">\(\nabla_\eta\)&lt;/span>表示。又由式（9）可知&lt;span class="math">\(A(η)\)&lt;/span>的一阶导数等于&lt;span class="math">\(T(x)\)&lt;/span>的期望&lt;span class="math">\(E[T(x)]\)&lt;/span>，即&lt;span class="math">\(E[T(x)]=\frac{d A(\eta)}{d \eta}=\nabla_\eta A(\eta)\)&lt;/span>，令其结果为&lt;span class="math">\(\mu_{ML}\)&lt;/span>，结合公式（14）有： &lt;span class="math">\[
\mu_{ML}=E[T(x)]=\frac{1}{N}\sum_{k=1}^N\mathbf{T}(x_k)\tag{15}
\]&lt;/span> 从式(15)可以看出，指数族分布&lt;span class="math">\(T(x)\)&lt;/span>期望值（均值参数&lt;span class="math">\(\mu\)&lt;/span>）的最大似然估计等于样本的平均值。且均值参数的最大似然估计值，只和样本的统计量&lt;span class="math">\(\sum_{k=1}^N=T(x_k)\)&lt;/span>有关，而不再依赖样本的其它信息，所以&lt;span class="math">\(\sum_{k=1}^N=T(x_k)\)&lt;/span>(或者说&lt;span class="math">\(T(x)\)&lt;/span>)是指数族的充分统计量。对于满足&lt;span class="math">\(T(x)=x\)&lt;/span>的分布，比如伯努利分布、多项式分布、泊松分布等等，样本的均值就是&lt;span class="math">\(T(x)\)&lt;/span>的均值，样本的均值就是均值参数的最大似然估计值。同理，对于单变量的高斯分布，样本的方差就是方差参数的最大似然估计值。&lt;/p>
&lt;p>最后，我们结合式（11）知道，&lt;span class="math">\(\eta,\mu,\theta\)&lt;/span>是有一一对应关系的，可以通过函数和反函数相互计算。最大似然估计给出了&lt;span class="math">\(μ_{ML}\)&lt;/span>的估计值，我们就是可以换算出&lt;span class="math">\(η_{ML},\theta_{ML}\)&lt;/span>。&lt;/p>
&lt;h2 id="自然指数族">自然指数族&lt;/h2>
&lt;p>我们在式（5）中给出了指数型分布族的一般形式 &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{T}(x)-A(\eta)\right\}\tag{5}
\]&lt;/span> 但是对于广义线性模型的应用场景而言，还是复杂了一些，因此有一种简化的&lt;strong>自然指数族&lt;/strong>。在自然指数族中，&lt;span class="math">\(\mathbf{T}(\mathbf{x})=\mathbf{x}\)&lt;/span>，不存在类似于&lt;span class="math">\(x^2,x^3,\log(x),\frac{1}{x}\)&lt;/span>这种带有函数关系的充分统计量，其可以简化写成： &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{x}-A(\eta)\right\}\tag{16}
\]&lt;/span> 二项分布，负二项分布，伯努利分布，泊松分布，参数&lt;span class="math">\(\alpha\)&lt;/span>已知的Gamma分布，已知方差的高斯分布，参数&lt;span class="math">\(\lambda\)&lt;/span>已知的逆高斯分布（又称Wald分布）等都可以写成自然指数族形式，其他分布如卡方分布、Beta分布、帕累托分布，对数正态分布，一般正态分布，一般Gamma分布则无法写成自然指数族的形式。他们是否是自然指数族的核心就在于是不是充分统计量&lt;span class="math">\(T(x)=x\)&lt;/span>。&lt;/p>
&lt;h2 id="指数分散族">指数分散族&lt;/h2>
&lt;p>在自然指数族的基础上，研究者们为了方便探究分布的期望和方差，对自然指数族做了少些变形得到指数分散族。其处理方法是将自然指数族的规范形式(式(16))的规范（自然）参数&lt;span class="math">\(\eta\)&lt;/span>拆分成与位置（期望）相关的位置函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>以及和方差相关的分散函数&lt;span class="math">\(a(\phi)\)&lt;/span>。其形式如下： &lt;span class="math">\[
p(x;\vartheta)=\exp\{\frac{\vartheta^T x-b(\vartheta)}{a(\phi)}+c(x,\phi)\}\tag{17}
\]&lt;/span> 这种形式的指数族通常被称为指数分散族(exponential dispersion family,EDF)，&lt;span class="math">\(a(ϕ)\)&lt;/span>称为分散函数(dispersion function)，是已知的。&lt;span class="math">\(ϕ\)&lt;/span>称为分散参数(dispersion parameter)。&lt;span class="math">\(\vartheta\)&lt;/span>仍然叫自然参数(natural parameter)或者规范参数(canonical parameter)，它和自然指数族中参数差了个系数，因为两种模式中&lt;span class="math">\(\vartheta^T x,\eta^Tx\)&lt;/span>的模式都是&lt;strong>参数&lt;span class="math">\(\times\)&lt;/span>充分统计量&lt;/strong>，所以不难发现，实际上我们对自然参数做一个&lt;span class="math">\(\frac{1}{a(\phi)}\)&lt;/span>倍的缩放。&lt;/p>
&lt;p>&lt;strong>指数分散族形式本质上是对自然指数族的参数&lt;span class="math">\(\eta\)&lt;/span>进行了拆分，把期望参数和方差参数拆分开&lt;/strong>。使得自然参数&lt;span class="math">\(\vartheta\)&lt;/span>仅和期望&lt;span class="math">\(μ\)&lt;/span>相关，分散参数&lt;span class="math">\(ϕ\)&lt;/span>和分布的方差参数相关。分拆后，规范参数&lt;span class="math">\(\vartheta\)&lt;/span>仅和分布的期望参数&lt;span class="math">\(μ\)&lt;/span>相关，并且和&lt;span class="math">\(μ\)&lt;/span>之间存在一一映射的函数关系，换句话说，&lt;span class="math">\(\vartheta\)&lt;/span>和&lt;span class="math">\(μ\)&lt;/span>可以互相转化。 &lt;span class="math">\[
\vartheta=f(\mu)\\
\mu=f^{−1}(\vartheta)\tag{18}
\]&lt;/span> 后面在配分函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>的讨论中可以证明这一点。&lt;/p>
&lt;h3 id="分散参数讨论">分散参数讨论&lt;/h3>
&lt;p>&lt;span class="math">\(a(ϕ)\)&lt;/span>的函数形式并没有严格的要求，其函数形式并不重要。在大多数文献中，&lt;span class="math">\(a(\phi)\)&lt;/span>被定义为： &lt;span class="math">\[
a_i(\phi)=\frac{\phi}{w_i}
\]&lt;/span> 其中&lt;span class="math">\(w_i\)&lt;/span>是观测样本的权重，一般是已知的。不同的样本可以拥有不同的权重值，比如进行参数估计时，对于某些样本设置成&lt;span class="math">\(w_i=0\)&lt;/span>，这就相当于抛弃了这些样本。如果不需要对样本进行加权（大多数场景），那么直接令 &lt;span class="math">\[
a(\phi)=\phi
\]&lt;/span> 即可。分散参数和分布的方差相关，它影响着方差的大小。此外，由于随机变量&lt;span class="math">\(x\)&lt;/span>不变，&lt;strong>在指数分布族和自然分布族中，其自然参数之间差&lt;span class="math">\(\frac{1}{a(\phi)}\)&lt;/span>倍&lt;/strong>。 &lt;span class="math">\[
\eta=\frac{\vartheta}{a(\phi)}
\]&lt;/span>&lt;/p>
&lt;h3 id="配分函数讨论">配分函数讨论&lt;/h3>
&lt;p>在指数分散族中，我们将&lt;span class="math">\(b(\vartheta)\)&lt;/span>也称为配分函数，和一般形态的配分函数&lt;span class="math">\(A(\eta)\)&lt;/span>显然有如下关系： &lt;span class="math">\[
A(\eta)=\frac{b(\vartheta)}{a(\phi)}
\]&lt;/span> 在指数型分布族中，我们可以用&lt;span class="math">\(A(\eta)\)&lt;/span>的导数求出分布的矩，一阶导数是分布的期望，二阶导数是分布的方差。&lt;span class="math">\(b(\vartheta)\)&lt;/span>也有类似的作用。由于&lt;span class="math">\(\eta=\frac{\vartheta}{a(\phi)},A(\eta)=\frac{b(\vartheta)}{a(\phi)}=\frac{b(\eta\cdot a(\phi))}{a(\phi)}\)&lt;/span>，所以概率分布的期望为： &lt;span class="math">\[
E[X]=\frac{dA(\eta)}{d\eta}=\frac{d\frac{b(\vartheta)}{a(\phi)}}{d\eta}=\frac{d\frac{b(\vartheta)}{a(\phi)}}{d\frac{\vartheta}{a(\phi)}}\\
\Rightarrow E[X]=\frac{d b(\vartheta)}{d\vartheta}=b&amp;#39;(\vartheta)=\mu\tag{19}
\]&lt;/span> 同样的，我们可以推导出概率分布的方差： &lt;span class="math">\[
\mathrm{Var}[X]=A&amp;#39;&amp;#39;(\eta)=\frac{d^2\frac{b(\vartheta)}{a(\phi)}}{d[\frac{\vartheta}{a(\phi)}]^2}\\
又\because A&amp;#39;(\eta)=b&amp;#39;(\vartheta)\\
\Rightarrow \mathrm{Var}[X]=\frac{d{b&amp;#39;(\vartheta)}}{d\frac{\vartheta}{a(\phi)}}=\frac{1}{a(\phi)}\frac{db&amp;#39;(\vartheta)}{d\vartheta}\\
\Rightarrow \mathrm{Var}[X]=a(\phi)b&amp;#39;&amp;#39;(\vartheta)\tag{20}
\]&lt;/span> 由于&lt;span class="math">\(b(\vartheta)\)&lt;/span>是在&lt;span class="math">\(A(η)\)&lt;/span>的基础上拆分出去&lt;span class="math">\(a(ϕ)\)&lt;/span>，所以&lt;span class="math">\(b(\vartheta)\)&lt;/span>的二阶导数不再等于分布的方差，需要再乘上&lt;span class="math">\(a(ϕ)\)&lt;/span>才能得到分布的方差。&lt;/p>
&lt;p>从期望和方差的关系，我们能发现&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>也是一一对应关系。根据式（19）可知，&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>有函数关系，且由于&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>的导数&lt;span class="math">\(b&amp;#39;&amp;#39;(\vartheta)\)&lt;/span>是方差（恒大于0）乘以一个已知数&lt;span class="math">\(a(\phi)\)&lt;/span>（式（20）结论），因此&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>的导数必然恒为正数或负数（取决于已知数&lt;span class="math">\(a(\phi)\)&lt;/span>），即&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>必为单调函数，而单调函数必存在反函数，推得必存在&lt;span class="math">\(b&amp;#39;^{-1}\)&lt;/span>，使得&lt;span class="math">\(\vartheta=b&amp;#39;^{-1}(\mu)\)&lt;/span>。因此&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>是一一对应的。&lt;/p>
&lt;h3 id="指数分散族的方差">指数分散族的方差&lt;/h3>
&lt;p>在指数分散族中，分布的方差可以表示成两部分的乘积，一部分是分散函数&lt;span class="math">\(a(ϕ)\)&lt;/span>，另一部分是配分函数的二阶导数&lt;span class="math">\(b&amp;#39;&amp;#39;(\vartheta)\)&lt;/span>。其中，函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>是一个关于&lt;span class="math">\(\vartheta\)&lt;/span>的函数， 其二阶导数要么是一个常数，要么是一个关于自然参数&lt;span class="math">\(\vartheta\)&lt;/span>的函数。 而自然参数&lt;span class="math">\(\vartheta\)&lt;/span>和均值参数&lt;span class="math">\(μ\)&lt;/span>存在一一对应关系，所以一定可以把&lt;span class="math">\(\vartheta\)&lt;/span>替换成&lt;span class="math">\(μ\)&lt;/span>。&lt;/p>
&lt;p>我们定义配分函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>的二阶导数为&lt;strong>方差函数&lt;/strong>(variance function)，方差函数是一个关于期望&lt;span class="math">\(μ\)&lt;/span>的函数，即 &lt;span class="math">\[
b&amp;#39;&amp;#39;(\vartheta)=\nu(μ)\tag{21}
\]&lt;/span> 方差函数&lt;span class="math">\(ν(μ)\)&lt;/span>存在两种情况：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>方差函数是一个常量值，&lt;span class="math">\(ν(μ)=b&amp;#39;&amp;#39;(\vartheta)=C\)&lt;/span>，此时分布的方差与均值无关。典型的分布就是正态分布。&lt;/li>
&lt;li>方差函数是一个关于均值&lt;span class="math">\(μ\)&lt;/span>的函数，&lt;span class="math">\(ν(μ)=b&amp;#39;&amp;#39;(\vartheta)\)&lt;/span>，此时分布的方差与均值有关。&lt;/li>
&lt;/ol>
&lt;p>方差函数，是一个平滑函数，它把分布的均值参数&lt;span class="math">\(μ\)&lt;/span>和分布的方差关联在一起。如果其值一个常数值，说明均值和方差是独立无关的；反之，如果是&lt;span class="math">\(μ\)&lt;/span>的函数，说明均值和方差是相关联的。&lt;/p>
&lt;p>例如，在高斯分布中，&lt;span class="math">\(b&amp;#39;&amp;#39;(\vartheta)=1\)&lt;/span>，所以方差和均值是相互独立的，对于其他分布，这是不成立的，高斯分布是特例。&lt;/p>
&lt;p>影响方差的，除了方差函数&lt;span class="math">\(ν(μ)\)&lt;/span>以外，还有分散参数&lt;span class="math">\(a(ϕ)=ϕ\)&lt;/span>，它起到一个缩放的作用。 参数&lt;span class="math">\(\vartheta\)&lt;/span>和&lt;span class="math">\(ϕ\)&lt;/span>&lt;strong>本质上是位置和尺度参数，位置参数反映数据的均值，尺度参数反映数据方差&lt;/strong>。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/常见分布的方差函数表.png" alt="常见分布的方差函数表" />&lt;p class="caption">常见分布的方差函数表&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之数理统计常用概念</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%B8%B8%E7%94%A8%E6%A6%82%E5%BF%B5/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%B8%B8%E7%94%A8%E6%A6%82%E5%BF%B5/</guid><description>
&lt;h2 id="概率统计随机过程之数理统计常用概念">概率统计随机过程之数理统计常用概念&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#总体与样本">总体与样本&lt;/a>&lt;/li>
&lt;li>&lt;a href="#统计量与估计量">统计量与估计量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#估计量的评价指标">估计量的评价指标&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本的经验分布函数和样本矩">样本的经验分布函数和样本矩&lt;/a>&lt;/li>
&lt;li>&lt;a href="#抽样分布">抽样分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常见统计量的抽样分布">常见统计量的抽样分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#概率分布族">概率分布族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族">指数型分布族&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#次序统计量">次序统计量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#充分统计量">充分统计量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#因子分解定理">因子分解定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小充分统计量">最小充分统计量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#完全完备统计量">完全（完备）统计量&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#指数族中统计量的完全性">指数族中统计量的完全性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小充分统计量与完备性关系">最小充分统计量与完备性关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有界完全统计量及其性质">有界完全统计量及其性质&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#常用概率分布与特征数表">常用概率分布与特征数表&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="总体与样本">总体与样本&lt;/h2>
&lt;p>总体是个体的集合；样本是样品的集合。一个样本里样品个数叫样本容量&lt;span class="math">\(n\)&lt;/span>，每个样品都是随机抽取的，所以&lt;strong>样品是随机变量&lt;/strong>。每次抽取的实际值/物品，是观察/观测值。&lt;span class="math">\(n\)&lt;/span>个样品（随机变量）组成&lt;span class="math">\(n\)&lt;/span>维样本空间，因此一个样本容量为&lt;span class="math">\(n\)&lt;/span>个样本是一个&lt;span class="math">\(n\)&lt;/span>维随机变量。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">总体&lt;/th>
&lt;th align="center">&lt;span class="math">\(\overset{N\rightarrow \infty}{\leftarrow}\)&lt;/span>&lt;/th>
&lt;th align="center">个体&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;td align="center">抽样&lt;/td>
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">样本（&lt;span class="math">\(n\)&lt;/span>维）&lt;/td>
&lt;td align="center">&lt;span class="math">\(\overset{n个}{\leftarrow}\)&lt;/span>&lt;/td>
&lt;td align="center">样品&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;td align="center">等效&lt;/td>
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(n\)&lt;/span>维随机变量&lt;/td>
&lt;td align="center">&lt;span class="math">\(\overset{n个}{\leftarrow}\)&lt;/span>&lt;/td>
&lt;td align="center">随机变量&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;td align="center">观测&lt;/td>
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(n\)&lt;/span>维向量&lt;/td>
&lt;td align="center">&lt;span class="math">\(\overset{n个}{\leftarrow}\)&lt;/span>&lt;/td>
&lt;td align="center">观测值&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>对总体的要求：远比样本容量大，最好是无限总体。&lt;/li>
&lt;li>对样本的要求：抽样要独立、随机&lt;span class="math">\(\Rightarrow\)&lt;/span>n个独立同分布样品(i.i.d)&lt;span class="math">\(\Rightarrow\)&lt;/span>n维随机变量&lt;span class="math">\(\leftrightarrow\)&lt;/span>n个i.i.d的一维随机变量&lt;/li>
&lt;li>抽样方法：简单随机抽样、分层抽样、系统抽样、按比例抽样……&lt;/li>
&lt;/ul>
&lt;h2 id="统计量与估计量">统计量与估计量&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>统计量&lt;/strong>：通过&lt;strong>样本构造&lt;/strong>出的&lt;strong>不含任何未知量&lt;/strong>的函数（样本函数），称此函数为统计量。&lt;/p>
&lt;/blockquote>
&lt;p>统计量通过构造函数把分散在样品中的反应总体的信息按人们的要求提取出来。需要强调的是由于样本是N维随机变量，因此其构造的函数（统计量）也是随机变量（大多数是一维）。我们通过观测，得到样本观察值后，立即可算得统计量的值。&lt;/p>
&lt;p>关于统计量有以下几点说明：&lt;/p>
&lt;ul>
&lt;li>统计量&lt;strong>既是函数也是随机变量&lt;/strong>。函数是从统计量的构造方式角度来说的，它是样本空间到参数空间的一个映射；而由于其自变量是一个N维随机变量（样本），同时一个随机变量的函数也是随机变量，随机变量是从值域的角度来考虑。&lt;/li>
&lt;li>构造统计量的目的是统计推断。&lt;strong>统计推断包括：抽样分布（精确，渐进，近似）、参数估计（点，区间）、假设检验（参数，非参数）&lt;/strong>。&lt;/li>
&lt;li>统计量随机性来源于自变量的随机性，当有一组样本的观测值被取出，那么统计量的随机性就没了，值也就固定了（就是单纯函数映射关系）。&lt;/li>
&lt;li>统计量可以简单理解为&lt;strong>随机变量的函数&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>估计量&lt;/strong>：在参数估计大类的点估计中，那么用于估计未知参数的&lt;strong>统计量&lt;/strong>称为&lt;strong>点估计(量)&lt;/strong>，简称为&lt;strong>估计(量)&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>从上面的定义中，我们注意以下几点：&lt;/p>
&lt;ul>
&lt;li>估计量是一种统计量，也是由样本构造的函数。二者区别在于目的：估计量要估计一个&lt;strong>未知参数&lt;/strong>；&lt;/li>
&lt;li>要估计的未知参数设为&lt;span class="math">\(\theta\)&lt;/span>，估计量用&lt;span class="math">\(\hat{\theta}=\hat{\theta}(x_1,x_2,\dotsb,x_n)\)&lt;/span>表示，&lt;span class="math">\(\hat{\theta}\)&lt;/span>的取值范围称为&lt;strong>参数空间&lt;/strong>&lt;span class="math">\(\varTheta=\{\theta\}\)&lt;/span>，那么估计量就是一个从&lt;strong>样本空间到参数空间的映射&lt;/strong>，一个具体的观测值所得一个估计值；&lt;/li>
&lt;li>参数&lt;span class="math">\(\theta\)&lt;/span>可以是（1）分布中的未知参数（2）分布中的特征数（期望、方差、偏度、分位数……）（3）某事件的概率；&lt;/li>
&lt;li>由于构造出来以估计参数的函数不止一种，因此一个参数的估计量也不止一个。&lt;/li>
&lt;/ul>
&lt;h3 id="估计量的评价指标">估计量的评价指标&lt;/h3>
&lt;p>（1）无偏性（小样本性质）/渐进无偏性（大样本性质）：表示的是无系统偏差，虽然每次的估计值与参数会有随机偏差，但这些随机偏差期望为0；&lt;/p>
&lt;p>无偏估计&lt;strong>不具有变换不变性&lt;/strong>，典型的例子就是无偏样本方差&lt;span class="math">\(s^2=\frac{\sum (x_i-\bar{x})^2}{n-1}\)&lt;/span>的平方根&lt;span class="math">\(s\)&lt;/span>（样本标准差）是总体标准差的有偏估计。但是如果变换是&lt;strong>线性变换&lt;/strong>，那么无偏估计还是无偏的。&lt;/p>
&lt;p>（2）有效性：之前指出估计量也是随机变量，如果这个估计的期望等于位置参数则为无偏；在&lt;strong>无偏估计的基础上&lt;/strong>，我们希望每次观测得出估计值尽量差距小，这就用&lt;strong>估计量的方差&lt;/strong>来表示。方差越小（即偏离无偏估计参数的概率越小），有效性越高。&lt;/p>
&lt;p>通常，我们希望在&lt;strong>无偏估计的基础上，估计量的方差尽量小&lt;/strong>，这就是&lt;strong>一致最小方差无偏估计&lt;/strong>，简称为&lt;strong>UMVUE&lt;/strong>。&lt;/p>
&lt;p>（3）相合性（大样本性质）：当样本容量增加时，随机变量&lt;span class="math">\(\hat{\theta}\)&lt;/span>收敛于&lt;span class="math">\(\theta\)&lt;/span>，直观的讲就是随着样本容量的增大，一个估计量的值能够稳定（以很大概率）在待估参数真值的附近，这就是估计量的相合性的要求。&lt;/p>
&lt;p>根据随机变量收敛性的强弱，又可分为弱相合（&lt;span class="math">\(\hat{\theta}\overset{P}{\rightarrow}\theta\)&lt;/span>），强相合（&lt;span class="math">\(\hat{\theta}\overset{a.s.}{\rightarrow}\theta\)&lt;/span>）,r阶矩相合（&lt;span class="math">\(\hat{\theta}\overset{r}{\rightarrow}\theta\)&lt;/span>）。理论基础时&lt;strong>大数定理&lt;/strong>。相合性也可等效于无偏（或渐进无偏）+ 方差（弱、强、r阶矩）收敛到0。&lt;/p>
&lt;p>（4）渐进正态性（大样本性质）：估计量的渐进正态性来源于&lt;strong>中心极限定理&lt;/strong>，若统计量在样本容量&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>时，也渐近于正态分布，称为渐进正态性。具体可定义为：如果存在一序列&lt;span class="math">\(\{\sigma_n^2\}\)&lt;/span>，满足&lt;span class="math">\((\hat\theta_n-\theta)/\sigma_n(\theta)\overset{L}{\rightarrow}N(0,1)\)&lt;/span>，则称&lt;span class="math">\(\hat\theta_n\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的渐进正态估计，&lt;span class="math">\(\sigma_n^2\)&lt;/span>称为&lt;span class="math">\(\hat\theta_n\)&lt;/span>的渐进方差。从&lt;span class="math">\((\hat\theta_n-\theta)/\sigma_n(\theta)\)&lt;/span>来看，分子项依概率收敛于&lt;span class="math">\(\theta\)&lt;/span>的速度与分母项&lt;span class="math">\(\sigma_n(\theta)\)&lt;/span>趋近于0的速度相同时，其比值才会稳定与正态分布。因此，&lt;span class="math">\(\hat\theta_n\)&lt;/span>收敛速度与渐近方差直接相关，渐近方差越小，收敛越快。&lt;/p>
&lt;p>渐进正态性和相合性的关系类似于中心极限定理和大数定律。相合性是对估计的一种较低要求，它只要求估计序列&lt;span class="math">\(\{\hat\theta_n\}\)&lt;/span>在样本数量&lt;span class="math">\(n\)&lt;/span>增加的时候也趋近于&lt;span class="math">\(\theta\)&lt;/span>，但是并没有指出趋近的速度（例如是&lt;span class="math">\(1/n,1/\sqrt{n}\)&lt;/span>或&lt;span class="math">\(1/\ln n\)&lt;/span>）。而渐进正态性补充了这一点，收敛速度与渐进方差相关。&lt;strong>经验来看，大多数渐进正态估计都是以&lt;span class="math">\(1/\sqrt{n}\)&lt;/span>的速度收敛于被估参数的&lt;/strong>。&lt;/p>
&lt;h3 id="样本的经验分布函数和样本矩">样本的经验分布函数和样本矩&lt;/h3>
&lt;p>经验分布函数部分可参考笔记《概率统计随机过程之经验函数分布》。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>样本矩&lt;/strong>：样本的经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>的各阶矩统称为样本矩，又称矩统计量。&lt;/p>
&lt;/blockquote>
&lt;p>这个说法太形式化了，直白的说，经验分布函数的概率质量函数都是&lt;span class="math">\(\frac{1}{n}\)&lt;/span>，样本矩就是以&lt;span class="math">\(p=\frac{1}{n}\)&lt;/span>为概率的各种期望。矩统计量我们心里默默替换成用样本构造的矩相关的函数来理解就行了。大体就这下面两种：&lt;/p>
&lt;p>&lt;span class="math">\[\begin{aligned}
A_k&amp;amp;=\frac{1}{n}\sum_{i=1}^n x_i^k(k=1,2,\dotsb)&amp;amp;\text{样本k阶（原点）矩}\\
B_k&amp;amp;=\frac{1}{n}\sum_{i=1}^n (x_i-\bar x)^k(k=1,2,\dotsb) &amp;amp;\text{样本k阶中心矩}
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>我们可以容易证明：一般样本k阶原点矩是无偏的，样本k阶中心矩是有偏的。&lt;/p>
&lt;p>所谓矩估计，就是用这些样本矩构造（凑）出特定的参数，例如偏度系数&lt;span class="math">\(\hat{\beta}=B_3/B_2^{\frac{3}{2}}\)&lt;/span>。如果需要估计的未知参数没法用样本矩构造出来，则认为此参数的矩估计不存在；相对的，如果可以用多种方法构造出来，那么就有多种矩估计，可以通过有效性，无偏性再进行进一步筛选。&lt;/p>
&lt;h2 id="抽样分布">抽样分布&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>抽样分布&lt;/strong>：统计量作为一个随机变量，其概率分布称为抽样分布，也称统计量分布、随机变量函数分布。&lt;/p>
&lt;/blockquote>
&lt;p>抽样分布就是寻求特定样本的函数（统计函数）的分布，大体分为以下三类：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>精确（抽样）分布：在总体&lt;span class="math">\(X\)&lt;/span>的分布已知，如果对任意&lt;span class="math">\(n\)&lt;/span>都能导出统计量&lt;span class="math">\(T(x_1,\dotsb,x_n)\)&lt;/span>的分布的解析表达式，则为精确分布。在&lt;strong>小样本问题&lt;/strong>中很有用。一般比较难求，大多以正态总体为研究对象，例如统计三大分布&lt;span class="math">\(\chi^2,t,F\)&lt;/span>分布等。&lt;/li>
&lt;li>渐进（抽样）分布：精确分布大多数求不出来，或者很复杂。因此，退一步，在样本容量n很大或者趋近无穷时作为近似的分布，称为渐进分布。&lt;strong>大样本&lt;/strong>时可以使用，比如中心极限定理下的正态分布、卡方分布等。&lt;/li>
&lt;li>近似（抽样）分布：没啥特定规律的分布，可以在&lt;em>一定条件&lt;/em>下用近似分布，常见的有假定正态分布并用样本前两阶矩替代总体前两阶矩；还有很多随机模拟方法，MCMC，Gibs采样等等。&lt;/li>
&lt;/ol>
&lt;h3 id="常见统计量的抽样分布">常见统计量的抽样分布&lt;/h3>
&lt;p>统计抽样三大分布：卡方分布、t分布、F分布。&lt;/p>
&lt;p>见笔记《概率统计随机过程之抽样的分布》&lt;/p>
&lt;h2 id="概率分布族">概率分布族&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>分布族&lt;/strong>：&lt;strong>分布类型&lt;/strong>和&lt;strong>参数空间&lt;/strong>组成一个（概率）参数分布族。此外，分布/概率和特定条件组成，且不能由该特定条件确定具体分布的分布族成为非参数分布族。&lt;/p>
&lt;/blockquote>
&lt;p>分布族是一个看似很高端，其实就是某一类概率分布的统称，比如常见的参数分布族可写成：&lt;/p>
&lt;ul>
&lt;li>二项分布族：&lt;span class="math">\(\{b(n,p);0&amp;lt;1&amp;lt;p,n\in Z^+\}\)&lt;/span>&lt;/li>
&lt;li>泊松分布族：&lt;span class="math">\(\{p(\lambda);\lambda&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;li>正态分布族：&lt;span class="math">\(\{N(\mu,\sigma^2);-∞&amp;lt;\mu&amp;lt;∞,\sigma&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;li>均匀分布族：&lt;span class="math">\(\{U(a,b);-∞&amp;lt;a&amp;lt;b&amp;lt;∞\}\)&lt;/span>&lt;/li>
&lt;li>指数分布族：&lt;span class="math">\(\{\exp(\lambda);\lambda&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;li>伽马分布族：&lt;span class="math">\(\{Ga(\alpha,\lambda);\alpha&amp;gt;0,\lambda&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;li>贝塔分布族：&lt;span class="math">\(\{Beta(a,b);a&amp;gt;0,b&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>还有一些非参数分布族的例子：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\mathfrak{p}_1=\{p(x);p(x)\text{是连续分布}\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathfrak{p}_2=\{F(x);F(x)\text{的一二阶矩存在}\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathfrak{p}_3=\{p(x);p(x)\text{是对称连续分布}\}\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="指数型分布族">指数型分布族&lt;/h3>
&lt;p>区别于指数分布族，指数型分布族是指数分布族的推广，更是囊括了正态分布族、二项分布族、伽马分布族、多项分布族等等。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>指数型分布族&lt;/strong>：一个概率分布族&lt;span class="math">\(\mathfrak{p}=\{p_{\theta}(x);\theta∈\varTheta\}\)&lt;/span>可称为&lt;strong>指数型分布族&lt;/strong>，假如&lt;span class="math">\(\mathfrak{p}\)&lt;/span>中的分布（分布列或密度函数）都可表示为如下形式： &lt;span class="math">\[p_\theta(x)=h(x)c(\theta)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\}\tag{1}\]&lt;/span> 其中，k为自然数；&lt;span class="math">\(\theta\)&lt;/span>可以是数字，也可以是向量。分布的支撑&lt;span class="math">\(\{x:p(x)&amp;gt;0\}\)&lt;/span>与参数&lt;span class="math">\(\theta\)&lt;/span>无关；诸&lt;span class="math">\(c(\theta),c_1(\theta),\dotsb,c_k(\theta)\)&lt;/span>是定义在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上的函数；诸&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>是&lt;span class="math">\(x\)&lt;/span>的函数，称为充分统计向量，但&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>线性无关。&lt;span class="math">\(h(x)\)&lt;/span>也只是&lt;span class="math">\(x\)&lt;/span>的函数，且&lt;span class="math">\(h(x)&amp;gt;0\)&lt;/span>，通常是一个常数。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;span class="math">\(c(\theta)\)&lt;/span>是作为归一化参数存在的，称为叫做配分函数(partition function)。 &lt;span class="math">\[c(\theta)^{-1} = \int h(x) \exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\} dx\]&lt;/span> 此外，指数族还有另一种表述方式，就是将外面的&lt;span class="math">\(c(\theta)\)&lt;/span>放到指数符号中： &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)-A(\theta)\right\}\tag{2}\]&lt;/span> 由于通常&lt;span class="math">\(A(\theta)\)&lt;/span>含有&lt;span class="math">\(\log\)&lt;/span>符号，该部分也称为“Log Partition Function”，易知&lt;span class="math">\(A(\theta)=\ln c(\theta)\)&lt;/span>。 如果我们使用向量值函数来表达指数型分布族可写为: &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)-A(\theta)\right\}\tag{3}\]&lt;/span>&lt;/p>
&lt;p>从上述定义可知，一个分布族是不是指数型分布族的&lt;strong>关键在于其概率分布能否改写为定义中方式&lt;/strong>，其中主要有两条：一条是“分布的支撑与&lt;span class="math">\(\theta\)&lt;/span>无关”；另一条是“&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>线性无关”，若其间线性相关，如&lt;span class="math">\(T_1(x)=2T_2(x)+3T_3(x)\)&lt;/span>，则把&lt;span class="math">\(T_1(x)\)&lt;/span>归拢到&lt;span class="math">\(T_2(x),T_3(x)\)&lt;/span>中即可。&lt;/p>
&lt;p>下面这张截图就是将3个常见分布族改写成指数型分布族的例子。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/指数型分布族例子.png" alt="指数型分布族例子" />&lt;p class="caption">指数型分布族例子&lt;/p>
&lt;/div>
&lt;h4 id="指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/h4>
&lt;p>下面我们使用&lt;strong>向量值函数&lt;/strong>将式（3）进行进一步改造。&lt;/p>
&lt;blockquote>
&lt;p>向量值函数：有时也称为向量函数，是一个单变量或多变量的、&lt;strong>值域是多维向量或者无穷维向量的集合的函数&lt;/strong>。向量值函数的输入可以是一个标量或者一个向量，输出是向量，定义域的维度和值域的维度是不相关的。&lt;/p>
&lt;/blockquote>
&lt;p>对于&lt;span class="math">\(\theta\)&lt;/span>的一系列函数&lt;span class="math">\(c_1(\theta),c_2(\theta),\dotsb\)&lt;/span>和充分统计量向量&lt;span class="math">\(T_1(x),T_2(x),\dotsb\)&lt;/span>，我们写出列向量形式： &lt;span class="math">\[
\mathbf{C}(\theta)=\begin{bmatrix}c_1(\theta)\\c_2(\theta)\\\vdots\\c_k(\theta)\end{bmatrix}
\mathbf{T}(x)=\begin{bmatrix}T_1(x)\\T_2(x)\\\vdots\\T_k(x)\end{bmatrix}
\]&lt;/span> 那么式（3）可写成 &lt;span class="math">\[
p(x;\theta)=h(x)\exp\left\{\mathbf{C}^T(\theta)\mathbf{T}(x)-A(\theta)\right\}\tag{4}
\]&lt;/span> 其中，&lt;span class="math">\(\mathbf{C}(\theta),\mathbf{T}(x)\)&lt;/span>都是向量值函数，&lt;span class="math">\(h(x),A(\theta)\)&lt;/span>都是普通函数，通常文章会把&lt;span class="math">\(A(\theta)\)&lt;/span>写成&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的形式，这两种本质上是等价的，但是&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的参数形式更加统一，为主流用法。&lt;/p>
&lt;p>&lt;strong>均值方差皆未知的高斯分布的指数族形式&lt;/strong>：如果高斯分布的均值和方差都是未知的，那么就需要使用多个线性不相关的充分统计量来表示指数型分布族： &lt;span class="math">\[
\begin{aligned}
p(x|\mu,\sigma^2)&amp;amp;=\frac{1}{(2\pi\sigma^2)^{1/2}} \exp[ -\frac{1}{2\sigma^2}(x-\mu)^2]\\
&amp;amp;=\frac{1}{(2\pi\sigma^2)^{1/2}} \exp[-\frac{1}{2\sigma^2} x^2 +\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2]\\
&amp;amp;=\frac{1}{(2\pi)^{1/2}} \exp[-\frac{1}{2\sigma^2} x^2 +\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2-\ln{\sigma}]
\end{aligned}
\]&lt;/span> 对照指数族的形式，有&lt;span class="math">\(h(x)=\frac{1}{(2\pi)^{1/2}}, \mathbf{C}=\begin{bmatrix}\mu/\sigma^2 \\-1/2\sigma^2\end{bmatrix},\mathbf{T}=\begin{bmatrix}x\\x^2\end{bmatrix},A(\mu,\sigma)=\frac{1}{2\sigma^2}\mu^2+\ln{\sigma}\)&lt;/span>。&lt;/p>
&lt;p>需要指出的是，一般情况下，我们在&lt;span class="math">\(A(\mu,\sigma)\)&lt;/span>中最好不要在使用原有的&lt;span class="math">\(\mu,\sigma\)&lt;/span>，而是使用&lt;span class="math">\(\mathbf{C}\)&lt;/span>中的分量&lt;span class="math">\(c_1=\mu/\sigma^2,c_2=-1/2\sigma^2\)&lt;/span>表示，即 &lt;span class="math">\[
A(\mathbf{C})=\frac{-c_1^2}{4c_2}-\frac{1}{2}\log(-2c_2)
\]&lt;/span> 在其他文献中，也有令&lt;span class="math">\(h(x)=1\)&lt;/span>，然后把&lt;span class="math">\(\frac{1}{(2\pi)^{1/2}}\)&lt;/span>放到&lt;span class="math">\(A(\mathbf{C})\)&lt;/span>中的，即&lt;span class="math">\(A(\mathbf{C})=\frac{-c_1^2}{4c_2}-\frac{1}{2}\log(-2c_2)-\frac{1}{2}\log(2\pi)\)&lt;/span>，这样是等价的。&lt;/p>
&lt;p>指数型分布族的重要性体现在以下多个方面：&lt;/p>
&lt;ul>
&lt;li>指数族分布是&lt;strong>唯一有共轭先验的分布族&lt;/strong>,这就简化了后验的计算&lt;/li>
&lt;li>在特定的规范化条件下(regularity conditions),指数族分布是&lt;strong>唯一有限规模充分统计量&lt;/strong>(finite-sized sufficient statistics)的分布族,这意味着可以将数据压缩称固定规模的浓缩概括而不损失信息&lt;/li>
&lt;li>指数族分布是&lt;strong>广义线性模型&lt;/strong>(generalized linear models)的核心&lt;/li>
&lt;li>指数族分布也是&lt;strong>变分推理&lt;/strong>(variational inference)的核心&lt;/li>
&lt;/ul>
&lt;h2 id="次序统计量">次序统计量&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>次序统计量&lt;/strong>：设&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>是取自总体&lt;span class="math">\(X\)&lt;/span>的一个样本，&lt;span class="math">\(X_{(k)}\)&lt;/span>称为该样本的的第k个次序统计量，假如每当获得样本观测值后将其&lt;strong>从小到大排序可得如下有序样本&lt;/strong>： &lt;span class="math">\[x_{(1)}≤x_{(2)}≤x_{(3)}≤\dotsb≤x_{(k)}≤\dotsb≤x_{(n)}\]&lt;/span> 其中，第k个观测值&lt;span class="math">\(x_{(k)}\)&lt;/span>就是&lt;span class="math">\(X_{(k)}\)&lt;/span>的取值，并称&lt;span class="math">\(X_{(1)},X_{(2)},\dotsb,X_{(n)}\)&lt;/span>为该&lt;strong>样本的次序统计量&lt;/strong>，特别的，&lt;span class="math">\(X_{(1)}=\min(X_1,X_2,\dotsb,X_n)\)&lt;/span>称为该样本的&lt;strong>最小次序统计量&lt;/strong>，&lt;span class="math">\(X_{(n)}=\max(X_1,X_2,\dotsb,X_n)\)&lt;/span>称为该样本的&lt;strong>最大次序统计量&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>次序统计量引申出来的概念有最小次序统计量、最大次序统计量、极差、中位数、P分位数。次序统计量作为随机变量也是有概率分布的，用的不多，等用到的时候再补充。&lt;/p>
&lt;h2 id="充分统计量">充分统计量&lt;/h2>
&lt;p>充分统计量的概念比较难理解，但十分重要。由于我看得是茆诗松的《数理统计学》，这本书里的定义包括了分布族的充分统计量，这和其他资料上的充分统计量有一个区别，就是一般资料上充分统计量都是和分布的某个参数&lt;span class="math">\(\theta\)&lt;/span>相关的，我也更倾向这样定义（更实用主义）。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>充分统计量&lt;/strong>：假如&lt;span class="math">\(\mathcal{F}=\{F_\theta,\theta\in \varTheta\}\)&lt;/span>是参数分布族（&lt;span class="math">\(\theta\)&lt;/span>可以是向量），在给定&lt;span class="math">\(T=t\)&lt;/span>下，样本&lt;span class="math">\(x\)&lt;/span>（n维随机变量）的条件分布与&lt;span class="math">\(\theta\)&lt;/span>无关，则称&lt;span class="math">\(T\)&lt;/span>&lt;strong>为参数&lt;span class="math">\(\theta\)&lt;/span>的充分统计量&lt;/strong>。 &lt;span class="math">\[P(X=x\mid T(X)=t,\,\theta )=P(X=x\mid T(X)=t)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>从上面这个式子中，我们两边同时乘以&lt;span class="math">\(P(\theta)\)&lt;/span>，有 &lt;span class="math">\[
P(X=x\mid T(X)=t,\,\theta)P(\theta)=P(X=x,\theta\mid T(X)=t)\\
=P(X=x\mid T(X)=t)P(\theta)
\]&lt;/span> 显然，有&lt;span class="math">\(P(X=x\mid T(X)=t),P(\theta)\)&lt;/span>二者独立。当我们已知&lt;span class="math">\(T(x)=t_0\)&lt;/span>以后，样本的联合概率函数中不含有&lt;span class="math">\(\theta\)&lt;/span>项，或者说&lt;span class="math">\(T(x)\)&lt;/span>完全包含了&lt;span class="math">\(\theta\)&lt;/span>对&lt;span class="math">\(X\)&lt;/span>有影响的信息。例子如下：&lt;/p>
&lt;p>设&lt;span class="math">\(x_1,x_2,\dotsb,x_n\)&lt;/span>是来自两点分布&lt;span class="math">\(b(1,p)\)&lt;/span>的一个样本，其中&lt;span class="math">\(p\in(0,1),n&amp;gt;2\)&lt;/span>，考察以下两个统计量是否为充分统计量： &lt;span class="math">\[
T_1=\sum_{i=1}^n x_i,\qquad T_2=x_1+x_2
\]&lt;/span> 首先，该样本的联合分布是： &lt;span class="math">\[
P(X_1=x_1,X_2=x_2,\dotsb,X_n=x_n)=p^{\sum_{i=1}^n x_i}(1-p)^{n-\sum_{i=1}^n x_i}
\]&lt;/span> 其中，&lt;span class="math">\(x_i\)&lt;/span>非0即1。统计量&lt;span class="math">\(T_1=\sum_{i=1}^n x_i\)&lt;/span>的分布为二项分布&lt;span class="math">\(b(n,p)\)&lt;/span>，即 &lt;span class="math">\[
P(T_1=t)={n\choose t}p^t(1-p)^{n-t},t=0,1,\dotsb,n
\]&lt;/span> 在给定&lt;span class="math">\(T_1=t\)&lt;/span>下，样本的条件分布为： &lt;span class="math">\[
\begin{aligned}
&amp;amp;P(X_1=x_1,X_2=x_2,\dotsb,X_n=x_n|T_1=t)\\
&amp;amp;=\frac{P(X_1=x_1,X_2=x_2,\dotsb,X_n=t-\sum_{i=1}^{n-1}x_i)}{P(T_1=t)}\\
&amp;amp;=\frac{p^t(1-p)^{n-t}}{{n\choose t}p^t(1-p)^{n-t}}\\
&amp;amp;=\frac{1}{{n\choose t}}
\end{aligned}
\]&lt;/span> 计算结果表明，这个条件分布与参数&lt;span class="math">\(p\)&lt;/span>无关，即他不含参数&lt;span class="math">\(p\)&lt;/span>的信息，这意味着样本中有关&lt;span class="math">\(p\)&lt;/span>的信息都含在统计量&lt;span class="math">\(T_1\)&lt;/span>中。&lt;/p>
&lt;p>在统计量&lt;span class="math">\(T_2=x_1+x_2\)&lt;/span>的分布为&lt;span class="math">\(b(2,p)\)&lt;/span>,在&lt;span class="math">\(T_2=t\)&lt;/span>下，样本的条件分布为： &lt;span class="math">\[
\begin{aligned}
&amp;amp;P(X_1=x_1,X_2=x_2,\dotsb,X_n=x_n|T_2=t)\\
&amp;amp;=\frac{P(X_1=x_1,X_2=t-x_1,\dotsb,X_n=x_n)}{P(T_2=t)}\\
&amp;amp;=\frac{p^{t+\sum_{i=3}^n x_i}(1-p)^{n-t-\sum_{i=3}^n x_i}}{{2\choose t}p^t(1-p)^{2-t}}\\
&amp;amp;=\frac{p^{\sum_{i=3}^n x_i}(1-p)^{n-2-\sum_{i=3}^n x_i}}{{2\choose t}}
\end{aligned}
\]&lt;/span> 显然此条件分布与参数&lt;span class="math">\(p\)&lt;/span>有关，即它还有参数&lt;span class="math">\(p\)&lt;/span>的信息，而样本中关于&lt;span class="math">\(p\)&lt;/span>的信息没有完全包含在统计量&lt;span class="math">\(T_2\)&lt;/span>中。&lt;/p>
&lt;p>可以设想为： &lt;span class="math">\[
\left\{样本\mathbf{x}中所含有有关\theta的信息\right\}=\\
\left\{t统计量T中含有有关\theta的信息\right\}+\left\{当T取值为t后，样本\mathbf{x}中还含有有关\theta的信息\right\}
\]&lt;/span> 如果后一项为0，那么统计量&lt;span class="math">\(T\)&lt;/span>即为充分统计量。&lt;/p>
&lt;p>我们再从统计量定义的角度反过来看，统计量是一组样本为自变量的函数。样本中包含了未知参数&lt;span class="math">\(\theta\)&lt;/span>部分信息的，&lt;span class="math">\(\theta\)&lt;/span>的统计量是将样本中的关于&lt;span class="math">\(\theta\)&lt;/span>的信息提取出来，如果我们构造这个函数（统计量）能够把样本中所有&lt;span class="math">\(\theta\)&lt;/span>信息都提取出来，那么对于估计未知参数&lt;span class="math">\(\theta\)&lt;/span>而言，样本和该关于&lt;span class="math">\(\theta\)&lt;/span>统计量效果是一样的，此时的统计量就是充分统计量。&lt;/p>
&lt;p>我们举个例子来说明充分统计量不损失有关&lt;span class="math">\(\theta\)&lt;/span>信息，比如你已经抽样的1000个数据全都写在了一张纸上，这些数据是给你写论文用的。突然有一天你的狗把你这张写满数据的纸吃掉了，这个时候假如你的数据满足指数分布，只有参数&lt;span class="math">\(\lambda\)&lt;/span>未知，且你已经提前把这些数据的的&lt;span class="math">\(\hat\lambda\)&lt;/span>算了出来，那你的狗也没坏了什么大事——因为这个充分统计量包含了这1000个数据的所有有用信息。你可以设计一个指数分布且&lt;span class="math">\(\lambda=\hat\lambda\)&lt;/span>的随机试验重新获得样本，这个新样本和过去的样本可能不完全一样，但它和老样本有相同的分布，当样本容量趋于无穷，新老两个样本应是等效的。&lt;/p>
&lt;blockquote>
&lt;p>从信息论的角度来说，就是&lt;span class="math">\(X,T(X)\)&lt;/span>中包含关于&lt;span class="math">\(\theta\)&lt;/span>的信息相同，即&lt;strong>互信息相同&lt;/strong>： &lt;span class="math">\[I{\bigl (}\theta ;T(X){\bigr )}=I(\theta ;X)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>根据&lt;strong>数据处理不等式&lt;/strong>，当我们仅对已知数据进行加工时，不可能获得额外的信息，只可能保持信息量不减，即&lt;span class="math">\(I(X;Y)≥I(X;f(Y))\)&lt;/span>，充分统计量就是在对数据&lt;span class="math">\(X\)&lt;/span>进行加工变成&lt;span class="math">\(T(X)\)&lt;/span>时，里面关于&lt;span class="math">\(\theta\)&lt;/span>的信息没有变化。&lt;/p>
&lt;p>此外，如果&lt;strong>另一个统计量&lt;span class="math">\(S(x)\)&lt;/span>和&lt;span class="math">\(T(x)\)&lt;/span>有一一对应关系&lt;/strong>，那么&lt;span class="math">\(S(x)\)&lt;/span>也是&lt;span class="math">\(\theta\)&lt;/span>的一个充分统计量。这种一一对应关系一般并不会导致信息的损失。&lt;/p>
&lt;h3 id="因子分解定理">因子分解定理&lt;/h3>
&lt;p>虽然我们说的很好听，但是实际情况下通过条件概率方式的验证并不是特别容易。因此我们给出一个叫作因子分解定理的东西。它的证明并不需要掌握，而其用法又很简单，所以这个计算的难度就大大降低了。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>因子分解定理（费希尔分解定理）&lt;/strong>：设总体概率函数（分布列或密度函数）为&lt;span class="math">\(f(x;\theta)\)&lt;/span>,&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>为样本，那么&lt;span class="math">\(T=T(X_1,X_2,\dotsb,X_n)\)&lt;/span>为充分统计量的&lt;strong>充要条件&lt;/strong>为：存在函数&lt;span class="math">\(g(t,\theta)\)&lt;/span>与&lt;span class="math">\(h(x_1,\dotsb,x_n)\)&lt;/span>使得对任意的&lt;span class="math">\(\theta\)&lt;/span>和任意一组的观测值&lt;span class="math">\(x_1,\dotsb,x_n\)&lt;/span>，都有 &lt;span class="math">\[f(x_1,\dotsb,x_n;\theta)=g(T(x_1,\dotsb,x_n),\theta)h(x_1,\dotsb,x_n)\]&lt;/span> 其中，&lt;span class="math">\(g(t,\theta)\)&lt;/span>只通过统计量&lt;span class="math">\(T(x_1,\dotsb,x_n)\)&lt;/span>的取值二依赖于样本。&lt;/p>
&lt;/blockquote>
&lt;p>这个定理把抽样的样本发生的联合概率密度分成两个部分，一是只和&lt;span class="math">\(t,\theta\)&lt;/span>相关的&lt;span class="math">\(g(t,\theta)\)&lt;/span>，其中&lt;span class="math">\(t\)&lt;/span>可以用样本&lt;span class="math">\(x_1,\dotsb,x_n\)&lt;/span>表示；另一个是只和&lt;span class="math">\(x_1,\dotsb,x_n\)&lt;/span>相关的&lt;span class="math">\(h(x_1,\dotsb,x_n)\)&lt;/span>。因子分解定理比条件概率的方法容易了许多。有一个一般性的情况是通常&lt;span class="math">\(h(x_1,\dotsb,x_n)=c\)&lt;/span>，即它通常是一个常数。然后把样本的概率函数用统计量&lt;span class="math">\(t\)&lt;/span>和&lt;span class="math">\(\theta\)&lt;/span>表示出来。&lt;/p>
&lt;h3 id="最小充分统计量">最小充分统计量&lt;/h3>
&lt;p>充分统计量做到了“用已知刻画未知”。那么更进一步的，我们当然希望充分统计量越简单，越精细越好。所以这其实就是极小充分统计量的定义。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>最小充分统计量&lt;/strong>：若一个充分统计量是任何其他充分统计量的函数，则称其是一个最小充分统计量。即，统计量&lt;span class="math">\(S(X)\)&lt;/span>是最小充分统计量当且仅当&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(S(X)\)&lt;/span>是充分统计量，&lt;/li>
&lt;li>如果&lt;span class="math">\(T(X)\)&lt;/span>是一个充分统计量，那么存在一个函数&lt;span class="math">\(f\)&lt;/span>使得&lt;span class="math">\(S(X)= f(T(X))\)&lt;/span>。&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>如果任何一个充分统计量&lt;span class="math">\(T\)&lt;/span>都可以通过加工得到&lt;span class="math">\(S\)&lt;/span>，这就说明&lt;span class="math">\(S\)&lt;/span>一定是更精细的（或者更严谨地说，一定不会变得更粗糙）充分统计量，而“任何一个”保证了最小性。我们也可以通过数据处理不等于来理解加工只可能减少信息量这一事实。一般而言，最小充分统计量不太好证出来，而且最小充分统计量也不是唯一的，只要与最小充分统计量有一一对应关系的统计量，都是最小充分统计量。&lt;/p>
&lt;p>可以证明：&lt;/p>
&lt;blockquote>
&lt;p>一个&lt;strong>充分完全统计量&lt;/strong>必是最小充分统计量。&lt;/p>
&lt;/blockquote>
&lt;p>下面我们来解释什么是完全统计量。&lt;/p>
&lt;h3 id="完全完备统计量">完全（完备）统计量&lt;/h3>
&lt;p>完全统计量，又称完备统计量。打个不大恰当的比方，完全统计量类似充分必要条件中的必要条件。我们首先定义一个辅助统计量来帮助理解完全统计量：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>辅助(ancillary)统计量&lt;/strong>:一个统计&lt;span class="math">\(V(x)\)&lt;/span>是辅助统计量，当且仅当其概率分布不依赖与任何未知量。&lt;/p>
&lt;/blockquote>
&lt;p>一个典型但没啥用的辅助统计量就是常数，即&lt;span class="math">\(V(x)\equiv C\)&lt;/span>。&lt;strong>常数是不含有任何概率分布信息的&lt;/strong>。再举一个典型例子（位移族）：&lt;/p>
&lt;blockquote>
&lt;p>已知某一概率密度函数&lt;span class="math">\(f(x)\)&lt;/span>，对其沿&lt;span class="math">\(x\)&lt;/span>轴向右平移一段未知距离&lt;span class="math">\(\mu\)&lt;/span>，则新的pdf为&lt;span class="math">\(f(x-\mu)\)&lt;/span>，现在从平移后的pdf中抽样得到i.i.d的样本&lt;span class="math">\(x_1,x_2,\dotsb,x_n\)&lt;/span>，我们想从样本估计参数&lt;span class="math">\(\mu\)&lt;/span>。如果我们构造一个统计量&lt;span class="math">\(V(x)=x_{(k)}-x_{(l)}\)&lt;/span>，其中&lt;span class="math">\(x_{(k)},x_{(l)}\)&lt;/span>是次序统计量，那么&lt;span class="math">\(V(x)\)&lt;/span>的pdf只会和&lt;span class="math">\(f(x)\)&lt;/span>有关，与&lt;span class="math">\(\mu\)&lt;/span>无关，即为辅助统计量。&lt;/p>
&lt;/blockquote>
&lt;p>这是一个过于严格的要求，下面再给出一个放松的定义：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>一阶辅助统计量&lt;/strong>：一个统计&lt;span class="math">\(V(x)\)&lt;/span>是一阶辅助统计量，当且仅当其期望&lt;span class="math">\(E[V(x)]\)&lt;/span>不依赖与任何未知量。&lt;/p>
&lt;/blockquote>
&lt;p>显然，辅助统计量必然是一阶辅助统计量。需要指出，ancillary一词的翻译辅助，有附加、附属的意思，不一定是指辅助做了什么事。&lt;/p>
&lt;p>如果对一个统计量&lt;span class="math">\(T\)&lt;/span>能找到(存在)一个非平凡（不是常函数）的辅助统计量&lt;span class="math">\(V\)&lt;/span>，使得&lt;span class="math">\(V(T(x))\neq C\)&lt;/span>，也就是说&lt;span class="math">\(V(x&amp;#39;)\)&lt;/span>可以从统计量&lt;span class="math">\(T(x)\)&lt;/span>中再提取一些和带估计参数&lt;span class="math">\(\theta\)&lt;/span>无关的信息出来。那么说明，统计量&lt;span class="math">\(T(x)\)&lt;/span>中除了含有&lt;span class="math">\(\theta\)&lt;/span>的信息之外，还有其他冗余的信息，可以进一步压缩，&lt;strong>此时我们就不能称&lt;span class="math">\(T\)&lt;/span>是完全统计量&lt;/strong>。完全统计量是纯粹性的体现，意味着不包含其他与参数估计无关的信息。统计量可以是完全但不充分的，这意味再样本信息压缩过程中，不仅仅把和待估计参数&lt;span class="math">\(\theta\)&lt;/span>无关的信息剔除掉，甚至与&lt;span class="math">\(\theta\)&lt;/span>有关的信息也可能去掉了一些，属于“有损压缩”。由此，我们给出完全统计量定义：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>完全（完备）统计量&lt;/strong>：设&lt;span class="math">\(\mathcal{F}=\{f(x,\theta),\theta\in\varTheta\}\)&lt;/span>为一分布族，&lt;span class="math">\(\varTheta\)&lt;/span>是参数空间，设统计量&lt;span class="math">\(T=T(x)\)&lt;/span>，若对任何辅助统计量&lt;span class="math">\(\varphi(T(x))\)&lt;/span>，当 &lt;span class="math">\[E[\varphi(T(x))]=0\]&lt;/span> 都有 &lt;span class="math">\[P(\varphi(T(x))=0)=1\]&lt;/span> 则称&lt;span class="math">\(T(x)\)&lt;/span>是一完全（完备）统计量。&lt;/p>
&lt;/blockquote>
&lt;p>注意，辅助统计量是和待估计参数&lt;span class="math">\(\theta\)&lt;/span>无关的随机数，因此也可以说是对&lt;span class="math">\(\forall \theta\in\varTheta\)&lt;/span>都成立。&lt;span class="math">\(P(\varphi(T(x))=0)=1\)&lt;/span>表示以1的概率为0，其实等于其他常数也无所谓，只有将常数移到另一边就可以了，都不含分布信息，效果一样的。简单的说，如果以统计量&lt;span class="math">\(T\)&lt;/span>为自变量的辅助统计量的期望为0，那么它就是常数0。&lt;/p>
&lt;p>关于统计量的充分性、完全性还可以参考网页资料文件加中的&lt;a href="../../网页资料/概率论数理统计随机过程-关于充分完全统计量的一点儿思考.html">《概率论数理统计随机过程-关于充分完全统计量的一点儿思考.html》&lt;/a>一文。&lt;/p>
&lt;p>有函数处理的信息不增特性可知，若&lt;span class="math">\(T(x)\)&lt;/span>是完全统计量，则它任一函数&lt;span class="math">\(\delta(T)\)&lt;/span>也是完全统计量。&lt;/p>
&lt;h4 id="指数族中统计量的完全性">指数族中统计量的完全性&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>定理&lt;/strong>：设样本&lt;span class="math">\(x=(x_1,x_2,\dotsb,x_n)\)&lt;/span>的概率函数 &lt;span class="math">\[f(x,\theta)=h(x)c(\theta)\exp\left\{\sum_{j=1}^k \theta_j T_j(x)\right\},\theta=(\theta_1,\dotsb,\theta_k)\in\varTheta^\star\]&lt;/span> 为指数族的自然形式。令&lt;span class="math">\(T(x)=(T_1(x),\dotsb,T_k(x))\)&lt;/span>，且自然参数空间&lt;span class="math">\(\varTheta^\star\)&lt;/span>作为&lt;span class="math">\(R_k\)&lt;/span>的自己有内点，则&lt;span class="math">\(T(x)\)&lt;/span>是完全统计量。&lt;/p>
&lt;/blockquote>
&lt;p>首先，该定理一个充分条件，只适用于判别指数族的完全性。&lt;span class="math">\(\theta_j\)&lt;/span>是可以是其他参数的函数，其结果作为&lt;span class="math">\(\varTheta^\star\)&lt;/span>的空间一维。&lt;/p>
&lt;p>其次，有内点意味着&lt;span class="math">\(\varTheta^\star\)&lt;/span>不可以是K维空间的超平面，即&lt;span class="math">\(\theta=(\theta_1,\dotsb,\theta_k)\)&lt;/span>是满秩的。&lt;/p>
&lt;p>最后，即使没有内点，统计量也不一定是不完全的，需要用定义或其他方法进一步判别。&lt;/p>
&lt;h4 id="最小充分统计量与完备性关系">最小充分统计量与完备性关系&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>定理&lt;/strong>：如果最小充分统计量存在，那么任何充分完全统计量都是最小充分统计量。&lt;/p>
&lt;/blockquote>
&lt;p>这是一个充分条件，而非充要条件。最小充分统计量可能是不完全的，即其可能包含无法分离出去的冗余内容。&lt;/p>
&lt;h4 id="有界完全统计量及其性质">有界完全统计量及其性质&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>有界完全（完备）统计量&lt;/strong>：设&lt;span class="math">\(\mathcal{F}=\{f(x,\theta),\theta\in\varTheta\}\)&lt;/span>为一分布族，&lt;span class="math">\(\varTheta\)&lt;/span>是参数空间，设统计量&lt;span class="math">\(T=T(x)\)&lt;/span>，若对任何&lt;strong>有界或a.s.有界&lt;/strong>辅助统计量&lt;span class="math">\(\varphi(T(x))\)&lt;/span>，当 &lt;span class="math">\[E[\varphi(T(x))]=0\]&lt;/span> 都有 &lt;span class="math">\[P(\varphi(T(x))=0)=1\]&lt;/span> 则称&lt;span class="math">\(T(x)\)&lt;/span>是一有界完全（完备）统计量。&lt;/p>
&lt;/blockquote>
&lt;p>有界完全统计量是对完全统计量的放松，只是给辅助统计量的函数&lt;strong>添加了有界这一条件&lt;/strong>。因此，一个完全统计量（严要求）必为有界完全统计量（松要求）。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Basu定理&lt;/strong>：&lt;span class="math">\(x\)&lt;/span>是分布中抽取的一个样本，如果&lt;span class="math">\(T(x)\)&lt;/span>是一个&lt;strong>充分，且有界完全&lt;/strong>统计量，那么&lt;span class="math">\(T(x)\)&lt;/span>独立于所有的辅助统计量。&lt;/p>
&lt;/blockquote>
&lt;h2 id="常用概率分布与特征数表">常用概率分布与特征数表&lt;/h2>
&lt;div class="figure">
&lt;img src="../../images/常用概率分布族.png" alt="常用概率分布族" />&lt;p class="caption">常用概率分布族&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之母函数特征函数矩母函数</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%AF%8D%E5%87%BD%E6%95%B0%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0%E7%9F%A9%E6%AF%8D%E5%87%BD%E6%95%B0/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%AF%8D%E5%87%BD%E6%95%B0%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0%E7%9F%A9%E6%AF%8D%E5%87%BD%E6%95%B0/</guid><description>
&lt;h2 id="概率统计随机过程之母函数特征函数矩母函数">概率统计随机过程之母函数特征函数矩母函数&lt;!-- omit in toc -->&lt;/h2>
&lt;p>为什么要引入母函数、特征函数（矩母函数）？因为它们是处理概率论问题的有力工具。它们能把寻求独立随机变量法和的分布的卷积运算（积分运算）转换成函数的乘法运算，还能把求分布的各阶原点矩运算变成函数的微分运算，特别的，它能把寻求随机变量序列的极限分布转换成一般的函数极限问题。为概率论提供了数学分析方面的强大武器。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#母函数只适用于非负整数离散随机变量">母函数（只适用于非负整数离散随机变量）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常见非负离散分布的母函数">常见非负离散分布的母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#离散随机变量和的分布与母函数关系">离散随机变量和的分布与母函数关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#母函数与数字特征关系">母函数与数字特征关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数">特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几种常见分布的特征函数">几种常见分布的特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数性质">特征函数性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数唯一决定分布函数">特征函数唯一决定分布函数&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="母函数只适用于非负整数离散随机变量">母函数（只适用于非负整数离散随机变量）&lt;/h2>
&lt;p>为何要定义概率母函数？概率论起步的时候由于其研究不确定性的特点，难以找到系统的方法，&lt;strong>设计一个包含某一随机变量所有信息的函数使其具有解析性&lt;/strong>便成为处理概率问题的一种方法。&lt;/p>
&lt;blockquote>
&lt;p>定义：非负整值随机变量的母函数：一个离散随机变量的概率母函数是指该随机变量的概率质量函数的幂级数表达式。&lt;/p>
&lt;p>单变量情形&lt;/p>
&lt;p>如果&lt;span class="math">\(X\)&lt;/span>是在非负整数域&lt;span class="math">\(\{0,1, ...\}\)&lt;/span>上取值的离散随机变量,那么&lt;span class="math">\(X\)&lt;/span>的概率母函数定义为 &lt;span class="math">\[G(z)=\operatorname {E} (z^{X})=\sum _{x=0}^{\infty }p(x)z^{x},\]&lt;/span> 其中&lt;span class="math">\(p\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的概率质量函数。&lt;/p>
&lt;p>多变量情形&lt;/p>
&lt;p>如果&lt;span class="math">\(X = (X1,...,Xd )\)&lt;/span>是在&lt;span class="math">\(d-\)&lt;/span>非负整数格&lt;span class="math">\(\{0,1, ...\}^d\)&lt;/span>上取值的离散随机变量, 那么&lt;span class="math">\(X\)&lt;/span>的概率母函数定义为 &lt;span class="math">\[G(z)=G(z_{1},\ldots ,z_{d})=\operatorname {E} {\bigl (}z_{1}^{X_{1}}\cdots z_{d}^{X_{d}}{\bigr )}=\\
\sum _{x_{1},\ldots ,x_{d}=0}^{\infty }p(x_{1},\ldots ,x_{d})z_{1}^{x_{1}}\cdots z_{d}^{x_{d}},\]&lt;/span> 其中&lt;span class="math">\(p\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的概率质量函数。&lt;/p>
&lt;/blockquote>
&lt;p>由于&lt;span class="math">\(\forall p(x)，有0≤p(x)≤1，\sum p(x)=1\)&lt;/span>。所以概率母函数的收敛半径≥1。引进母函数的好处是它有很好的分析性质，而一旦知道了&lt;span class="math">\(X\)&lt;/span>的母函数，那么&lt;span class="math">\(X\)&lt;/span>的分布列可以通过下式获得： &lt;span class="math">\[p_k=\frac{g^{(k)}(0)}{k!}，k=0,1,2,\dotsb\]&lt;/span> 分布列和母函数的项是&lt;strong>一一对应的&lt;/strong>。&lt;/p>
&lt;h3 id="常见非负离散分布的母函数">常见非负离散分布的母函数&lt;/h3>
&lt;p>注：以下各式中&lt;span class="math">\(p+q=1，P(X=0)=q\)&lt;/span>&lt;/p>
&lt;ul>
&lt;li>伯努利分布(0-1分布)：&lt;span class="math">\(X\sim B(1,p)\)&lt;/span> &lt;span class="math">\[g(z)=q+pz\]&lt;/span>&lt;/li>
&lt;li>二项分布：&lt;span class="math">\(X\sim B(n,p)\)&lt;/span> &lt;span class="math">\[g(z)=\sum_{k=0}^∞ C_n^kp^kq^{n-k}z^k\\
=(q+pz)^n\]&lt;/span> 0-1分布和二项分布的关系也体现出独立同分布概率联合概率的关系。&lt;/li>
&lt;li>泊松分布：&lt;span class="math">\(X\sim P(\lambda)\)&lt;/span> &lt;span class="math">\[g(z)=\sum_{k=0}^∞\frac{\lambda^k}{k!}e^{-\lambda}z^k\\
=e^{\lambda(z-1)}\]&lt;/span>&lt;/li>
&lt;li>几何分布：：&lt;span class="math">\(X\sim Geo(p)\)&lt;/span> &lt;span class="math">\[g(z)=\sum_{k=1}^∞ q^{k-1}p z^k=pz\sum_{k=1}^∞ q^{k-1}z^{k-1}\\
\because -1&amp;lt;qz&amp;lt;1\\
=pz\times \frac{1}{1-qz}=\frac{pz}{1-qz}\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="离散随机变量和的分布与母函数关系">离散随机变量和的分布与母函数关系&lt;/h3>
&lt;blockquote>
&lt;p>定理：设非负整值随机变量&lt;span class="math">\(X_1,X_2,\dots,X_n\)&lt;/span>相互独立，而&lt;span class="math">\(g_1,g_2,\dots,g_n\)&lt;/span>分别是他们的母函数，那么&lt;span class="math">\(Y=\sum\limits_1^n X_k\)&lt;/span>的母函数为 &lt;span class="math">\[g(z)=g_1(z)g_2(z)\dotsb g_n(z)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h3 id="母函数与数字特征关系">母函数与数字特征关系&lt;/h3>
&lt;p>期望：&lt;span class="math">\(E(X)=g&amp;#39;(1)=\sum\limits_{k=1}^∞ kp_kz^{k-1}|_{z=1}\)&lt;/span>。 相仿的，&lt;span class="math">\(E(z^X)=\sum\limits_{k=0}^∞ z^kp_k=g(z)\)&lt;/span>。从这里，我们可以看出&lt;strong>母函数实际上是&lt;span class="math">\(z^X\)&lt;/span>的期望。&lt;/strong>&lt;/p>
&lt;p>方差：&lt;span class="math">\(Var(x)=g&amp;#39;&amp;#39;(1)+g&amp;#39;(1)-g&amp;#39;(1)^2\)&lt;/span>&lt;/p>
&lt;h2 id="特征函数">特征函数&lt;/h2>
&lt;p>母函数为我们处理概率提供了数学分析的角度与方法，极大方便了概率的处理，但是并不是所有随机变量都是有母函数的（只有离散的非负整数随机变量才有母函数），对于一般的随机变量是否具有类似的东西呢？&lt;strong>这就是特征函数&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>定义：对任一随机变量&lt;span class="math">\(X\)&lt;/span>，称 &lt;span class="math">\[\varphi_X(t)=E(e^{itX})=\int_{-∞}^∞ e^{itx}p(x) \mathrm{d}x，-∞&amp;lt;t&amp;lt;+∞\]&lt;/span> 为随机变量&lt;span class="math">\(X\)&lt;/span>的特征函数。&lt;/p>
&lt;/blockquote>
&lt;p>说明：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>规定&lt;span class="math">\(E(\xi+i\eta)=E(\xi)+iE(\eta)\Rightarrow E(e^{itX})=E(\cos tX)+iE(\sin tX)\)&lt;/span>&lt;/li>
&lt;li>由于&lt;span class="math">\(|e^{itX}|=1\)&lt;/span>，所以对任一随机变量都有特征函数。对于离散随机变量，其为求和形式；对于连续随机变量为积分形式。&lt;/li>
&lt;li>特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>都是实变复值的。&lt;/li>
&lt;li>&lt;span class="math">\(\varphi(0)=1\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;h3 id="几种常见分布的特征函数">几种常见分布的特征函数&lt;/h3>
&lt;p>注：以下各式中&lt;span class="math">\(p+q=1，P(X=0)=q\)&lt;/span>&lt;/p>
&lt;ul>
&lt;li>伯努利分布(0-1分布)：&lt;span class="math">\(X\sim B(1,p)\)&lt;/span> &lt;span class="math">\[\varphi(t)=q+pe^{it}\]&lt;/span>&lt;/li>
&lt;li>二项分布：&lt;span class="math">\(X\sim B(n,p)\)&lt;/span>，0-1分布和二项分布的关系也体现出独立同分布概率联合概率的关系。 &lt;span class="math">\[\varphi(t)=(q+pe^{it})^n\]&lt;/span>&lt;/li>
&lt;li>泊松分布：&lt;span class="math">\(X\sim P(\lambda)\)&lt;/span> &lt;span class="math">\[\varphi(t)=\sum_{k=0}^∞\frac{\lambda^k}{k!}e^{-\lambda}e^{\lambda e^{it}}\\
=e^{\lambda(e^{it}-1)}\]&lt;/span>&lt;/li>
&lt;li>几何分布：：&lt;span class="math">\(X\sim Geo(p)\)&lt;/span> &lt;span class="math">\[\varphi(t)=\frac{pe^{it}}{1-qe^{it}}\]&lt;/span>&lt;/li>
&lt;li>均匀分布：&lt;span class="math">\(X \sim U(a,b)\)&lt;/span> &lt;span class="math">\[\varphi(t)=\frac{e^{ibt}-e^{iat}}{i(b−a)t}\]&lt;/span>&lt;/li>
&lt;li>正态分布：&lt;span class="math">\(X\sim N(\mu,\sigma^2)\)&lt;/span> &lt;span class="math">\[\varphi(t)=e^{iut-\frac{\sigma^2t^2}{2}}\]&lt;/span>&lt;/li>
&lt;li>指数分布：&lt;span class="math">\(X \sim \exp(\lambda)\)&lt;/span> &lt;span class="math">\[\varphi(t)=(1-\frac{it}{\lambda})^{-1}\]&lt;/span>&lt;/li>
&lt;li>gamma分布：&lt;span class="math">\(X \sim Ga(\alpha,\lambda)\)&lt;/span> &lt;span class="math">\[\varphi(t)=(1-\frac{it}{\lambda})^{-\alpha}\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>其他分布的特征函数可见茆诗松《概率论与数理统计教程 第二版》P219&lt;/p>
&lt;h3 id="特征函数性质">特征函数性质&lt;/h3>
&lt;blockquote>
&lt;p>性质1：&lt;span class="math">\(|\varphi(t)|\leq \varphi(0)=1\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明： &lt;span class="math">\[
|\varphi(t)|=|\int_{-∞}^∞ e^{itx}p(x) \mathrm{d}x|\overset{\text{柯西不等式}}{\le}\int_{-∞}^∞ |e^{itx}|p(x) \mathrm{d}x\\
|e^{itx}|=\sqrt{\cos^2(tx)+\sin^2(tx)}=1=e^{ix\cdot 0}\\
\Rightarrow|\varphi(t)|=\int_{-∞}^∞ 1\cdot p(x) \mathrm{d}x=\varphi(0)=1
\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>性质2：&lt;span class="math">\(\varphi(-t)=\overline{\varphi(t)}\)&lt;/span>，其中&lt;span class="math">\(\overline{\varphi(t)}\)&lt;/span>表示&lt;span class="math">\(\varphi(t)\)&lt;/span>的复共轭。&lt;/p>
&lt;/blockquote>
&lt;p>证明： &lt;span class="math">\[\varphi(-t)=\int_{-∞}^∞ e^{-itx}p(x) \mathrm{d}x=\int_{-∞}^∞ \overline{e^{itx}}p(x) \mathrm{d}x\]&lt;/span> 由于&lt;span class="math">\(p(x)\)&lt;/span>是非负实数，不影响虚数，所以&lt;span class="math">\(\int_{-∞}^∞ \overline{e^{itx}}p(x) \mathrm{d}x=\overline{\int_{-∞}^∞ e^{itx}p(x)}\mathrm{d}x=\overline{\varphi(t)}\)&lt;/span>，所以&lt;span class="math">\(\varphi(-t)=\overline{\varphi(t)}\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>性质3：&lt;span class="math">\(X\)&lt;/span>的特征函数为&lt;span class="math">\(\varphi(t)\)&lt;/span>，则&lt;span class="math">\(Y=aX+b\)&lt;/span>的特征函数为&lt;span class="math">\(e^{itb}\varphi(at)\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>证明： &lt;span class="math">\[\varphi_Y(t)=E(e^{itY})=E(e^{it(aX+b)})=E(e^{itaX})\cdot e^{itb}\]&lt;/span> 如果我们将&lt;span class="math">\(ta\)&lt;/span>作为变量整体，则有&lt;span class="math">\(E(e^{itaX})=\varphi(at)\)&lt;/span>，综上所述有&lt;span class="math">\(\varphi_Y(t)=e^{itb}\varphi(at)\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>性质4：独立随机变量的和的特征函数为每个随机变量的特征函数的积，即设&lt;span class="math">\(X_1,X_2,\dotsb X_n\)&lt;/span>相互独立，则&lt;span class="math">\(Y=\sum_{k=1}^n X_k\)&lt;/span>有 &lt;span class="math">\[\varphi_{Y}(t)=\prod_{k=1}^n \varphi_{X_k}(t)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：因为&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>相互独立，所以其随机变量的函数&lt;span class="math">\(E(e^{itX_1}),E(e^{itX_2}),\dotsb,E(e^{itX_n})\)&lt;/span>也是相互独立的，从而有 &lt;span class="math">\[\begin{aligned}
\varphi_{Y}(t)&amp;amp;=E(e^{itY})=E(e^{it\sum_{k=1}^n X_i})=E(\prod_{k=1}^n e^{itX_k})\\
&amp;amp;\overset{\text{独立性}}{=}\prod_{k=1}^n E(e^{itX_k})=\prod_{k=1}^n \varphi_{X_k}(t)
\end{aligned}\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>性质5：若&lt;span class="math">\(E(X^l)\)&lt;/span>存在，则&lt;span class="math">\(X\)&lt;/span>的特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>可&lt;span class="math">\(l\)&lt;/span>次求导，且对&lt;span class="math">\(1\leq k \leq l\)&lt;/span>，有 &lt;span class="math">\[\varphi^{(k)}(0)=i^kE(X^k)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明可以从特征函数的积分式进行微分即可。从性质5我们也可以用以下方式求期望和方差： &lt;span class="math">\[
E(X)=\frac{\varphi&amp;#39;(0)}{i},\text{Var}(X)=-\varphi&amp;#39;&amp;#39;(0)+(\varphi&amp;#39;(0))^2
\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>性质6：一致连续性。随机变量&lt;span class="math">\(X\)&lt;/span>的特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>在&lt;span class="math">\((-\infty,\infty)\)&lt;/span>上一致连续。&lt;/p>
&lt;/blockquote>
&lt;p>证明略。&lt;/p>
&lt;blockquote>
&lt;p>性质7：非负定性。随机变量&lt;span class="math">\(X\)&lt;/span>的特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>是非负定的，即对任意正整数&lt;span class="math">\(n\)&lt;/span>及&lt;span class="math">\(n\)&lt;/span>个实数&lt;span class="math">\(t_1,t_2,\dotsb,t_n\)&lt;/span>和&lt;span class="math">\(n\)&lt;/span>个复数&lt;span class="math">\(z_1,z_2,\dotsb,z_n\)&lt;/span>，有 &lt;span class="math">\[\sum_{k=1}^n\sum_{j=1}^n \varphi(t_k-t_j)z_k\bar{z_j}\geq 0\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明略。&lt;/p>
&lt;h3 id="特征函数唯一决定分布函数">特征函数唯一决定分布函数&lt;/h3>
&lt;p>由特征函数的定义可知，随机变量的分布唯一地确定了它的特征函数。我们也可以同样推出特征函数完全决定了分布，也就是说，&lt;strong>两个分布函数相等当且仅当它们所对应的特征函数相等&lt;/strong>。我们不加具体证明给出如下定理：&lt;/p>
&lt;blockquote>
&lt;p>定理：随机变量的分布函数由其特征函数&lt;strong>唯一决定&lt;/strong>。且当&lt;span class="math">\(X\)&lt;/span>为连续随机变量，其密度函数为&lt;span class="math">\(p(x)\)&lt;/span>，特征函数为&lt;span class="math">\(\varphi(t)\)&lt;/span>，如果&lt;span class="math">\(\int_{-\infty}^\infty |\varphi(t)|\mathrm{d}t&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\[p(x)=\frac{1}{2\pi} \int_{-\infty}^\infty e^{itx}\varphi(t)\mathrm{d}t\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>我们可以称由分布转换为特征函数的过程为傅里叶变换，由特征函数转换成分布的过程为傅里叶逆变换。需要指出：这里定义的傅里叶正逆变换和我们通常在通信、复变函数中定义的傅里叶变换&lt;strong>在&lt;span class="math">\(e\)&lt;/span>的指数上相差了一个负号&lt;/strong>。&lt;/p>
&lt;p>由于分布函数和特征函数的一一对应关系，我们可以把随机变量序列的收敛问题和函数的收敛问题联系起来，从而有以下定理：&lt;/p>
&lt;blockquote>
&lt;p>定理：分布函数序列&lt;span class="math">\(\{F_n(x)\}\)&lt;/span>弱收敛于分布函数&lt;span class="math">\(F(x)\)&lt;/span>的充要条件是&lt;span class="math">\(\{F_n(x)\}\)&lt;/span>的特征函数序列&lt;span class="math">\(\{\varphi_n(t)\}\)&lt;/span>收敛于&lt;span class="math">\(F(x)\)&lt;/span>的特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>例题： &lt;img src="../../images/特征函数列收敛.png" alt="特征函数列收敛" />&lt;/p></description></item><item><title>概率统计随机过程核心之大数定理和中心极限定理</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E6%A0%B8%E5%BF%83%E4%B9%8B%E5%A4%A7%E6%95%B0%E5%AE%9A%E7%90%86%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E6%A0%B8%E5%BF%83%E4%B9%8B%E5%A4%A7%E6%95%B0%E5%AE%9A%E7%90%86%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</guid><description>
&lt;h2 id="概率统计随机过程核心大数定理和中心极限定理">概率统计随机过程核心大数定理和中心极限定理&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#随机变量序列的收敛性">随机变量序列的收敛性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#依分布收敛">依分布收敛&lt;/a>&lt;/li>
&lt;li>&lt;a href="#依概率收敛">依概率收敛&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几乎处处收敛依概率1收敛">几乎处处收敛（依概率1收敛）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#收敛性之间的关系">收敛性之间的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大数定理">大数定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#弱大数定律及其几种形式">弱大数定律及其几种形式&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#切比雪夫不等于相关的大数定律">切比雪夫不等于相关的大数定律&lt;/a>&lt;/li>
&lt;li>&lt;a href="#用特征函数证明的大数定律">用特征函数证明的大数定律&lt;/a>&lt;/li>
&lt;li>&lt;a href="#八个弱大数定律的比较">八个弱大数定律的比较&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#强大数定律">强大数定律&lt;/a>&lt;/li>
&lt;li>&lt;a href="#中心极限定理">中心极限定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#独立同分布条件下的中心极限定理">独立同分布条件下的中心极限定理&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#棣莫佛拉普拉斯定理">棣莫佛－拉普拉斯定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#林德伯格列维中心极限定理">林德伯格－列维中心极限定理&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#非独立同分布条件下的中心极限定理">非独立同分布条件下的中心极限定理&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#林德伯格-费勒定理">林德伯格-费勒定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#李雅普诺夫中心极限定理">李雅普诺夫中心极限定理&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#弱大数定理和强大数定理的区别">弱大数定理和强大数定理的区别&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="随机变量序列的收敛性">随机变量序列的收敛性&lt;/h2>
&lt;h3 id="依分布收敛">依分布收敛&lt;/h3>
&lt;blockquote>
&lt;p>定义1：设随机变量序列&lt;span class="math">\(\{X_n\}\)&lt;/span>的分布函数列是&lt;span class="math">\(\{F_n(X)\}\)&lt;/span>，&lt;span class="math">\(X\)&lt;/span>的分布函数是&lt;span class="math">\(F(X)\)&lt;/span>，若在&lt;span class="math">\(F(X)\)&lt;/span>的每一个连续点都成立&lt;span class="math">\(\lim_{n→\infty}F_n(X)=F(X)\)&lt;/span>，则称&lt;span class="math">\(F_n(X)\overset{W}{\longrightarrow} F(X)\)&lt;/span>，或&lt;span class="math">\(X_n\overset{L/D}{\longrightarrow} X\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>其中&lt;span class="math">\(W,\text{Weak}\)&lt;/span>表示弱收敛，用于分布函数列；&lt;span class="math">\(L,\text{convergence in law}; D,\text{Distribution}\)&lt;/span>表示依分布收敛，用于随机变量序列。依分布收敛的本质是函数列的收敛，而非随机变量的收敛，因此我们要重点关注的是分布“函数”，在函数列弱收敛的“弱”则体现在函数不是所有点的点点收敛，而是只有在连续点才收敛。在这个定义中，我们需要注意几个点：&lt;/p>
&lt;p>（1）&lt;span class="math">\(F(x)\)&lt;/span>要求的是所有连续点，并非所有点。&lt;/p>
&lt;p>例1：一个离散随机变量序列&lt;span class="math">\(X_n\)&lt;/span>分布列如下所示： |&lt;span class="math">\(X_n\)&lt;/span>|&lt;span class="math">\(\frac{1}{n}\)&lt;/span>|&lt;span class="math">\(\overset{n→\infty}{\longrightarrow}\)&lt;/span>|&lt;span class="math">\(X\)&lt;/span>| 0 | |:---:|:-----------:|:-----------------------------------:|:-:|:-:| | P | 1 |&lt;span class="math">\({\longrightarrow}\)&lt;/span>|&lt;span class="math">\(P\)&lt;/span>| 1 | &lt;span class="math">\(X_n\)&lt;/span>依分布收敛到&lt;span class="math">\(X\)&lt;/span>，但是分布函数&lt;span class="math">\(F(x)\)&lt;/span>在间断点&lt;span class="math">\(x=0\)&lt;/span>显然是不收敛的。&lt;/p>
&lt;p>（2）只有&lt;span class="math">\(F(x)\)&lt;/span>是一个分布函数的时候，而不是任意函数，才能说是依分布收敛。&lt;/p>
&lt;h3 id="依概率收敛">依概率收敛&lt;/h3>
&lt;blockquote>
&lt;p>定义2：设&lt;span class="math">\(\{X_n\}\)&lt;/span>为一随机变量序列，&lt;span class="math">\(X\)&lt;/span>为一随机变量，如果对任意的&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>，有 &lt;span class="math">\[\lim_{n→∞}P({|X_n-X|&amp;lt;\varepsilon})=1\]&lt;/span> 则称&lt;span class="math">\(\{X_n\}\)&lt;/span>依概率收敛于&lt;span class="math">\(X\)&lt;/span>，记作&lt;span class="math">\(X_n\overset{P}{\longrightarrow}X\)&lt;/span>。&lt;span class="math">\(P\)&lt;/span>表示Probability，概率。&lt;/p>
&lt;/blockquote>
&lt;p>这个定义也有几个注意事项：&lt;/p>
&lt;p>（1）与数列极限&lt;span class="math">\(\{a_n\}\)&lt;/span>的区别。数列极限的收敛比较好理解，就是逐渐逼近某个点。比如下右图中&lt;span class="math">\(y=(\frac{\sin x}{x})^2\)&lt;/span>所示，点序列含逐渐趋向于0。而依概率收敛，是指偏离收敛目标的概率趋于0，以下左图为例，Gamma分布的概率密度函数随&lt;span class="math">\(\beta\)&lt;/span>值减小，逐渐集中到0附近，也就是说取值大于&lt;span class="math">\(0+\varepsilon\)&lt;/span>的概率会越来越小并趋于0，但是仍然有取到一个远大于0的值的可能性。所以依概率收敛是从概率密度/质量函数的角度理解的。 &lt;img src="../../images/依概率收敛与数列收敛区别.png" alt="依概率收敛与数列收敛区别.png" />&lt;/p>
&lt;p>（2）&lt;span class="math">\(X\)&lt;/span>既可以是随机变量，也可以是一常数（退化分布）。（但是，我遇到的场景基本上都是常数）。&lt;/p>
&lt;p>依概率收敛的等价形式：&lt;/p>
&lt;ul>
&lt;li>若&lt;span class="math">\(\lim_{n→∞} E(X_n) = c\)&lt;/span>，且&lt;span class="math">\(\lim_{n→∞} \text{var}(X_n) = 0\)&lt;/span>，则&lt;span class="math">\(X_n\overset{P}{\longrightarrow}c\)&lt;/span>。(可用切比雪夫不等于证明)&lt;/li>
&lt;/ul>
&lt;p>依概率收敛的性质：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>四则运算。设&lt;span class="math">\({X_n},{Y_n}\)&lt;/span>是两个随机变量序列，&lt;span class="math">\(X,Y\)&lt;/span>是两个随机变量（常数也可以），如果&lt;span class="math">\(X_n\overset{P}{\longrightarrow}X,Y_n\overset{P}{\longrightarrow}Y\)&lt;/span> &lt;span class="math">\[1)~~ X_n\pm Y_n \overset{P}{\longrightarrow} X\pm Y\\
2)~~ X_n\times Y_n \overset{P}{\longrightarrow} X\times Y\\
3)~~ X_n\div Y_n \overset{P}{\longrightarrow} X\div Y\]&lt;/span>&lt;/li>
&lt;li>适用于函数。如果&lt;span class="math">\(X_n\overset{P}{\longrightarrow}X,g(x)\)&lt;/span>是直线上的连续函数，则：&lt;span class="math">\(g(X_n)\overset{P}{\longrightarrow}g(X)\)&lt;/span>。&lt;/li>
&lt;li>依概率收敛与依分布收敛的关系。依概率收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>依分布收敛；当二者收敛到同一常数时，有依概率收敛&lt;span class="math">\(\overset{P}{\longrightarrow} C\Leftrightarrow \)&lt;/span>依分布收敛&lt;span class="math">\(\overset{P}{\longrightarrow} C\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;h3 id="几乎处处收敛依概率1收敛">几乎处处收敛（依概率1收敛）&lt;/h3>
&lt;blockquote>
&lt;p>定义3：设&lt;span class="math">\(\{X_n\}\)&lt;/span>为一随机变量序列，&lt;span class="math">\(X\)&lt;/span>为一随机变量，如果有 &lt;span class="math">\[\lim_{n→∞}P({X_n=X})=1\]&lt;/span> 则称&lt;span class="math">\(\{X_n\}\)&lt;/span>几乎处处（依概率1）收敛于&lt;span class="math">\(X\)&lt;/span>，记作&lt;span class="math">\(X_n\overset{a.s/a.e}{\longrightarrow}X\)&lt;/span>。&lt;span class="math">\(a.s/a.e\)&lt;/span>表示almost surely 或 almost everywhere，几乎处处。&lt;/p>
&lt;/blockquote>
&lt;p>显然，几乎处处收敛（依概率1收敛）的收敛性比依概率收敛更强，与&lt;span class="math">\(\varepsilon\)&lt;/span>无关。&lt;/p>
&lt;h3 id="收敛性之间的关系">收敛性之间的关系&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>几乎处处收敛（依概率1收敛）&lt;span class="math">\(\Rightarrow\)&lt;/span>依概率收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>依分布收敛。&lt;/li>
&lt;li>&lt;span class="math">\(L^p\)&lt;/span>收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>依概率收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>依分布收敛。&lt;/li>
&lt;li>&lt;span class="math">\(L^p\)&lt;/span>收敛与几乎处处收敛（依概率1收敛）之间互相不可推导，即没有等价性。&lt;/li>
&lt;li>依概率收敛与一系列弱大数定律相关。&lt;/li>
&lt;li>几乎处处收敛（依概率1收敛）与强大数定律相关。&lt;/li>
&lt;li>在收敛到同一常数时，依概率收敛与依分布收敛等价。&lt;/li>
&lt;/ol>
&lt;h2 id="大数定理">大数定理&lt;/h2>
&lt;p>在数学与统计学中，大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。这个定律核心是，&lt;strong>样本数量越多，则其算术平均值就有越高的概率接近期望值&lt;/strong>。&lt;/p>
&lt;p>大数定律很重要，因为它“说明”了一些随机事件的均值的&lt;strong>长期稳定性&lt;/strong>。人们发现，在重复试验中，随着试验次数的增加，事件发生的频率趋于一个稳定值；人们同时也发现，在对物理量的测量实践中，测定值的算术平均也具有稳定性。比如，我们向上抛一枚硬币，硬币落下后哪一面朝上是偶然的，但当我们上抛硬币的次数足够多后，达到上万次甚至几十万几百万次以后，我们就会发现，硬币每一面向上的次数约占总次数的二分之一，亦即偶然之中包含着必然。&lt;/p>
&lt;p>&lt;strong>切比雪夫不等式的一个特殊情况&lt;/strong>、&lt;strong>辛钦定理&lt;/strong>和&lt;strong>伯努利大数定律&lt;/strong>等等都概括了这一现象，都可以称为大数定律。而这几种大数定律都是&lt;strong>依概率收敛&lt;/strong>的，而相对于几乎处处收敛较弱，所以又是&lt;strong>弱大数定律&lt;/strong>的几种表现形式。对应的，能够证明为&lt;strong>几乎处处收敛&lt;/strong>的大数定律称为&lt;strong>强大数定律&lt;/strong>。&lt;/p>
&lt;h3 id="弱大数定律及其几种形式">弱大数定律及其几种形式&lt;/h3>
&lt;p>弱大数定律主要描述一系列&lt;strong>依概率收敛&lt;/strong>的&lt;strong>随机变量序列&lt;/strong>，利用在序列序数趋近于无穷时，序列与收敛目标的距离小于一小正数&lt;span class="math">\(\varepsilon\)&lt;/span>这种模式定义。具体描述为，当随机变量序列&lt;span class="math">\(\{X_n\}\)&lt;/span>&lt;strong>满足一些条件时&lt;/strong>有： &lt;span class="math">\[\frac{1}{n}\sum_{i=1}^n X_i\overset{P}{\longrightarrow}E(\frac{1}{n}\sum_{i=1}^n X_i)=\frac{1}{n}\sum_{i=1}^n E(X_i)\\
即\lim_{n\rightarrow\infty} P\{|\frac{1}{n}\sum_{i=1}^n X_i-\frac{1}{n}\sum_{i=1}^n E(X_i)|&amp;lt;\varepsilon\}=1\tag{1}\]&lt;/span> 也就是说，这些随机变量的平均值趋近于其各期望的和的平均值。注意，我们在第一项中不需要取平均的期望，虽然是随机变量，但是其平均仍然具有稳定性（趋向于确定性）。&lt;/p>
&lt;p>历史上，有很多人名命名的弱大数定律，以下分两大类共八个弱大数定律来具体阐释。&lt;/p>
&lt;h4 id="切比雪夫不等于相关的大数定律">切比雪夫不等于相关的大数定律&lt;/h4>
&lt;blockquote>
&lt;p>（一）切比雪夫大数定律：设&lt;span class="math">\(\{X_n\}\)&lt;/span>为&lt;strong>两两不相关&lt;/strong>的随机变量序列，方差为：&lt;span class="math">\({\displaystyle \operatorname {Var} (X_{i})=\sigma_i^{2}\quad (i=1,\ 2,\ \dots )}\)&lt;/span>，且&lt;strong>有一致上界&lt;/strong>，即&lt;span class="math">\(\text{var}(X_i)\leq c\)&lt;/span>，对任意&lt;span class="math">\(i\)&lt;/span>成立。则有式&lt;span class="math">\((1)\)&lt;/span>成立。&lt;/p>
&lt;/blockquote>
&lt;p>证明：利用切比雪夫不等式。 &lt;span class="math">\[\begin{aligned}
P\{|\frac{1}{n}\sum_{i=1}^n X_i-E(\frac{1}{n}\sum_{i=1}^n X_i)|\geq \varepsilon\}&amp;amp;\leq \frac{\text{var}(\frac{1}{n}\sum_{i=1}^n X_i)}{\varepsilon^2}\\
\overset{两两不相关}{=}\frac{\sum_{i=1}^n\text{var}(X_i)}{n^2\varepsilon^2}\overset{\text{var}(X_i)\leq c}{\leq} \frac{nc}{n^2\varepsilon^2}&amp;amp;=\frac{c}{n\varepsilon^2}
\end{aligned}\]&lt;/span> 显然当&lt;span class="math">\(n→∞\)&lt;/span>时，有&lt;span class="math">\(\frac{c}{n\varepsilon^2}→0\)&lt;/span>。切比雪夫大数定律得证。&lt;/p>
&lt;blockquote>
&lt;p>（二）独立同分布场合的大数定律：设&lt;span class="math">\(\{X_n\}\)&lt;/span>为独立同分布的随机变量序列，且方差&lt;span class="math">\(\sigma^2\)&lt;/span>存在，则有式&lt;span class="math">\((1)\)&lt;/span>成立。&lt;/p>
&lt;/blockquote>
&lt;p>这个大数定律没有以人名命名，是一直被普遍认为比较直观的大数定律。独立同分布是比两两不相关更强的条件，且由于是i.i.d的，所以方差存在，等同于有一致上界。因此可以由&lt;span class="math">\((一)\Rightarrow (二)\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>（三）伯努利大数定律：设在&lt;span class="math">\(n\)&lt;/span>次&lt;strong>独立重复伯努利&lt;/strong>试验中，事件&lt;span class="math">\(X\)&lt;/span>发生的次数为&lt;span class="math">\(n_{x}\)&lt;/span>，事件&lt;span class="math">\(X\)&lt;/span>在每次试验中发生的总体概率为&lt;span class="math">\(p\)&lt;/span>，&lt;span class="math">\(\frac{n_{x}}{n}\)&lt;/span>代表样本发生事件&lt;span class="math">\(X\)&lt;/span>的频率。则对任意正数&lt;span class="math">\(\varepsilon &amp;gt;0\)&lt;/span>，伯努利大数定律表明： &lt;span class="math">\[\lim _{n\to \infty }{P{\left\{\left|{\frac {n_{x}}{n}}-p\right|&amp;lt;\varepsilon \right\}}}=1\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>换言之，事件发生的&lt;strong>频率依概率收敛于事件的总体概率&lt;/strong>。该定理以严格的数学形式表达了频率的稳定性，也就是说当&lt;span class="math">\(n\)&lt;/span>很大时，事件发生的频率于总体概率有较大偏差的可能性很小。&lt;/p>
&lt;p>伯努利大数定律是第一个被明确提出的大数定律。原始证明没有用切比雪夫不等于证明（当时还没有此不等式），而是用了很繁琐的方式。今天，我们可以直接从（一）或（二）证明出来。因为&lt;span class="math">\(n\)&lt;/span>次独立重复伯努利是&lt;span class="math">\(n\)&lt;/span>个i.i.d的随机变量序列，且方差为&lt;span class="math">\(p(1-p)\)&lt;/span>存在，显然（三）就是（二）的一种特殊情况。&lt;/p>
&lt;blockquote>
&lt;p>（四）马尔可夫大数定律&lt;/p>
&lt;/blockquote>
&lt;p>切比雪夫大数定理的进一步&lt;/p>
&lt;blockquote>
&lt;p>（五）泊松大数定律&lt;/p>
&lt;/blockquote>
&lt;p>伯努利大数定律大数定理的进一步&lt;/p>
&lt;blockquote>
&lt;p>（六）伯恩斯坦大数定律&lt;/p>
&lt;/blockquote>
&lt;p>有相互独立性扩展为序列渐进不相关（比独立性更弱的要求）&lt;/p>
&lt;blockquote>
&lt;p>（七）格涅坚科大数定律&lt;/p>
&lt;/blockquote>
&lt;p>矩估计的理论基础&lt;/p>
&lt;h4 id="用特征函数证明的大数定律">用特征函数证明的大数定律&lt;/h4>
&lt;blockquote>
&lt;p>（八）辛钦大数定理：陈述为：&lt;strong>独立同分布&lt;/strong>的样本均值&lt;strong>依概率收敛&lt;/strong>于期望值。 &lt;span class="math">\[
{\displaystyle {\overline {X}}_{n}\ {\xrightarrow {P}}\ \mu \quad {\textrm {as}}\quad n\to \infty }
\]&lt;/span> 也就是说对于任意正数&lt;span class="math">\(ε\)&lt;/span>, &lt;span class="math">\[\lim_{n\to \infty }P\left(\,|{\overline {X}}_{n}-\mu |&amp;gt;\varepsilon \,\right)=0 或者\\
\lim_{n\to \infty }P\left(\,|{\overline {X}}_{n}-\mu |&amp;lt;\varepsilon \,\right)=1\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h4 id="八个弱大数定律的比较">八个弱大数定律的比较&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">定律&lt;/th>
&lt;th align="center">分布情况&lt;/th>
&lt;th align="center">期望&lt;/th>
&lt;th align="center">方差&lt;/th>
&lt;th align="center">结论&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">辛钦大数定律&lt;/td>
&lt;td align="center">相互独立且同分布&lt;/td>
&lt;td align="center">存在&lt;/td>
&lt;td align="center">无要求（存在即能相等）&lt;/td>
&lt;td align="center">估算期望&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">切比雪夫大数定律&lt;/td>
&lt;td align="center">相互独立（不必同分布）&lt;/td>
&lt;td align="center">相同&lt;/td>
&lt;td align="center">相同&lt;/td>
&lt;td align="center">估算期望&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">伯努利大数定律&lt;/td>
&lt;td align="center">二项分布&lt;/td>
&lt;td align="center">相同&lt;/td>
&lt;td align="center">相同&lt;/td>
&lt;td align="center">频率=概率&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">相同点：n−&amp;gt;+∞,依概率趋近&lt;/td>
&lt;td align="center">条件逐渐变得严格&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="强大数定律">强大数定律&lt;/h3>
&lt;p>后面的数学家在弱大数定理的基础上证明出了更好的强大数定理。&lt;/p>
&lt;p>强大数定律指出，样本均值以概率1收敛于期望值。a.s. 表示almost surely. &lt;span class="math">\[
{\displaystyle {\overline {X}}_{n}\ {\xrightarrow {\text{a.s.}}}\ \mu \quad {\textrm {as}}\quad n\to \infty }\]&lt;/span> 即 &lt;span class="math">\[
{\displaystyle P\left(\lim_{n\to \infty }{\overline {X}}_{n}=\mu \right)=1}\]&lt;/span>&lt;/p>
&lt;h2 id="中心极限定理">中心极限定理&lt;/h2>
&lt;p>中心极限定理是概率论中的&lt;strong>一组定理&lt;/strong>。中心极限定理说明，在适当的条件下，&lt;strong>大量相互独立随机变量&lt;/strong>的均值经适当标准化后&lt;strong>依分布收敛于正态分布&lt;/strong>。这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量之和近似服从正态分布的条件。&lt;/p>
&lt;h3 id="独立同分布条件下的中心极限定理">独立同分布条件下的中心极限定理&lt;/h3>
&lt;h4 id="棣莫佛拉普拉斯定理">棣莫佛－拉普拉斯定理&lt;/h4>
&lt;p>棣莫佛－拉普拉斯（de Moivre - Laplace）定理是中央极限定理的最初版本，讨论了服从二项分布的随机变量序列。它指出，参数为&lt;span class="math">\(n, p\)&lt;/span>的二项分布以&lt;span class="math">\(np\)&lt;/span>为均值、&lt;span class="math">\(np(1-p)\)&lt;/span>为方差的正态分布为极限。&lt;/p>
&lt;p>若&lt;span class="math">\(X\sim B(n,p)\)&lt;/span>是&lt;span class="math">\(n\)&lt;/span>次伯努利实验中事件&lt;span class="math">\(A\)&lt;/span>出现的次数，每次试验成功的概率为&lt;span class="math">\(p\)&lt;/span>，且&lt;span class="math">\(q=1-p\)&lt;/span>，则对任意有限区间&lt;span class="math">\([a,b]\)&lt;/span>：&lt;/p>
&lt;p>令&lt;span class="math">\(x_{k}={\frac {k-np}{\sqrt {npq}}}\)&lt;/span>(标准化&lt;span class="math">\(x_k\)&lt;/span>)，当&lt;span class="math">\(n\to {\infty }\)&lt;/span>时&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\({\displaystyle P(X=k)\to {\frac {1}{\sqrt {npq}}}\cdot {\frac {1}{\sqrt {2\pi }}}e^{-{\frac {1}{2}}x_{\mu_{n}}^{2}}}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\({\displaystyle P(a\leq {\frac {X-np}{\sqrt {npq}}}\leq {b})\to \int _{a}^{b}\varphi (x)dx}\)&lt;/span>，其中&lt;span class="math">\(\varphi (x)={\frac {1}{\sqrt {2\pi }}}e^{-{\frac {x^{2}}{2}}}(-\infty &amp;lt;x&amp;lt;\infty)\)&lt;/span>.&lt;/li>
&lt;/ol>
&lt;p>棣莫弗－拉普拉斯定理指出二项分布的极限为正态分布。&lt;/p>
&lt;h4 id="林德伯格列维中心极限定理">林德伯格－列维中心极限定理&lt;/h4>
&lt;p>林德伯格－列维（Lindeberg-Levy）定理，是棣莫佛－拉普拉斯定理的扩展，讨论&lt;strong>独立同分布&lt;/strong>随机变量序列均值的中心极限定理。它表明，&lt;strong>独立同分布&lt;/strong>(iid)、且&lt;strong>数学期望和方差有限&lt;/strong>的随机变量序列均值的标准化和以标准正态分布为极限。&lt;/p>
&lt;p>设随机变量&lt;span class="math">\(X_{1},X_{2},\cdots ,X_{n}\)&lt;/span>独立同分布，且具有&lt;strong>有限的数学期望和方差&lt;/strong>&lt;span class="math">\(E(X_{i})=\mu\)&lt;/span>，&lt;span class="math">\(D(X_{i})=\sigma ^{2}\neq 0(i=1,2,\cdots ,n)\)&lt;/span>。记 &lt;span class="math">\[
{\bar{X}}={\frac {1}{n}}\sum_{i=1}^{n}X_{i}，\zeta_{n}={\frac {{\bar {X}}-\mu }{\sigma /{\sqrt {n}}}}，\]&lt;/span> 则 &lt;span class="math">\[\lim_{n\rightarrow \infty }P\left(\zeta_{n}\leq z\right)=\Phi \left(z\right)
\]&lt;/span> 其中&lt;span class="math">\(\Phi (z)\)&lt;/span>是标准正态分布的分布函数。&lt;/p>
&lt;h3 id="非独立同分布条件下的中心极限定理">非独立同分布条件下的中心极限定理&lt;/h3>
&lt;h4 id="林德伯格-费勒定理">林德伯格-费勒定理&lt;/h4>
&lt;p>TODO 林德伯格条件&lt;/p>
&lt;p>林德伯格－费勒定理，是中心极限定理的高级形式，是对林德伯格－列维定理的扩展，讨论&lt;strong>独立，但不同分布&lt;/strong>的情况下的随机变量和。它表明，&lt;strong>满足林德伯格条件时&lt;/strong>，独立，但不同分布的随机变量序列的标准化和依然以标准正态分布为极限。&lt;/p>
&lt;h4 id="李雅普诺夫中心极限定理">李雅普诺夫中心极限定理&lt;/h4>
&lt;p>TODO 李雅普诺夫条件&lt;/p>
&lt;h2 id="弱大数定理和强大数定理的区别">弱大数定理和强大数定理的区别&lt;/h2>
&lt;p>作者：runze Zheng 链接：&lt;a href="https://www.zhihu.com/question/21110761/answer/23815273">https://www.zhihu.com/question/21110761/answer/23815273&lt;/a> 来源：知乎。著作权归作者所有。&lt;/p>
&lt;p>强弱大数定律都是在说：随着样本数的增大，用样本的平均数来估计总体的平均数，是靠谱的。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>强弱大数定律的前提条件一样：要求独立同分布iid的随机序列，要求其期望存在。&lt;/li>
&lt;li>强弱大数定律的结论不同（废话）。弱大数定律比较早被证明出来，弱大数定律表示样本均值“&lt;strong>依概率收敛&lt;/strong>”于总体均值；而强大数定律是比较晚被证明出来的，它证明了样本均值可以“&lt;strong>以概率为1收敛&lt;/strong>”于总体均值。简单的来说，就是数学家先证明了弱大数定律，后来在没有改变前提的情况下把弱大数定律推进了一步，得到了更厉害的强大数定律。&lt;/li>
&lt;li>弱大数定律和强大数定律的区别在于，前者是“依概率收敛(convergence in probability)”，后者是“几乎确定收敛(almost surely convergence)或以概率为1收敛、几乎处处收敛”。后者比前者强，满足后者的必定满足前者，而满足前者的未必满足后者。&lt;/li>
&lt;/ol>
&lt;p>依概率收敛的例子：考虑下图，图中的每条线都代表一个数列，虚线表示一个非常小的区间。总的来说每个数列都越来越趋近0，且大部分时候不会超过虚线所表示的小边界，但是，偶尔会有一两条线超过虚线、然后再回到虚线之内。而且我们不能保证，有没有哪一个数列会在未来再次超出虚线的范围然后再回来——虽然概率很小。注意虚线的范围可以是任意小的实数，此图中大约是，可以把这个边界缩小到，甚至，随你喜欢，这个性质始终存在。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/弱大数定理.jpg" alt="弱大数定理" />&lt;p class="caption">弱大数定理&lt;/p>
&lt;/div>
&lt;p>几乎处处收敛的例子：图中的黑线表示一个随机数列，这个数列在大约n=200之后进入了一个我们定的小边界（用虚线表示），之后我们可以确定，它再也不会超出虚线所表示的边界（超出这个边界的概率是0）。跟上面的例子一样，虚线所表示的边界可以定得任意小，而一定会有一个n值，当这个数列超过了n值之后，超出这个边界的概率就是0了。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/强大数定理.jpg" alt="强大数定理" />&lt;p class="caption">强大数定理&lt;/p>
&lt;/div>
&lt;p>弱大数定律是较早被数学家最早证明的，即对于独立同分布的随机序列&lt;span class="math">\(X_1,X_2,\dotsb,X_n,\dotsb\)&lt;/span>，只要总体均值&lt;span class="math">\(\mu\)&lt;/span>存在，那么样本均值会随着&lt;span class="math">\(n\)&lt;/span>增大而“依概率收敛”到总体均值，就是弱大数定律。但是弱大数定律“依概率收敛”不够完美，随着增大，样本均值有没有可能（即使概率很小）偶然偏离总体均值很多呢？后来数学家们证明了强大数定律，就是告诉我们不用担心，&lt;span class="math">\(S_n=\frac{1}{n}\sum_{i=1}^n X_i\)&lt;/span>会“几乎处处收敛”到&lt;span class="math">\(\mu\)&lt;/span>。&lt;/p></description></item><item><title>概率统计随机过程之如何推导得到正态分布—正态分布的理解角度</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%A6%82%E4%BD%95%E6%8E%A8%E5%AF%BC%E5%BE%97%E5%88%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E7%90%86%E8%A7%A3%E8%A7%92%E5%BA%A6/</link><pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%A6%82%E4%BD%95%E6%8E%A8%E5%AF%BC%E5%BE%97%E5%88%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E7%90%86%E8%A7%A3%E8%A7%92%E5%BA%A6/</guid><description>
&lt;h2 id="概率统计随机过程之如何推导得到正态分布正态分布的理解角度">概率统计随机过程之如何推导得到正态分布————正态分布的理解角度&lt;!-- omit in toc -->&lt;/h2>
&lt;p>原文：Kim, Kiseon, 和Georgy Shevlyakov. 《Why Gaussianity?》 IEEE Signal Processing Magazine 25, 期 2 (2008年3月): 102–13. &lt;a href="https://doi.org/10.1109/MSP.2007.913700">https://doi.org/10.1109/MSP.2007.913700&lt;/a>.&lt;/p>
&lt;p>中文部分靳志辉正态分布章节推导完善。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#正态分布表达式形式的出现">正态分布表达式形式的出现&lt;/a>&lt;/li>
&lt;li>&lt;a href="#高斯误差与正态分布">高斯——误差与正态分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#赫歇尔和麦克斯韦正态分布的pi从哪里来">赫歇尔和麦克斯韦——正态分布的&lt;span class="math">\(\pi\)&lt;/span>从哪里来？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#为什么正态分布那么普遍稳定性兰登的推导">为什么正态分布那么普遍？——稳定性，兰登的推导&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基于最大熵的推导">基于最大熵的推导&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录">附录&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附1-似然函数取对数">附1 似然函数取对数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附2-高斯积分">附2 高斯积分&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附3-高斯在推导正态分布时系数c和方差sigma2的关系">附3 高斯在推导正态分布时系数&lt;span class="math">\(c\)&lt;/span>和方差&lt;span class="math">\(\sigma^2\)&lt;/span>的关系&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="正态分布表达式形式的出现">正态分布表达式形式的出现&lt;/h2>
&lt;p>正态分布是概率论和数理统计中的一个重要的分布。在最开始的工作中（1733年），法国数学家&lt;strong>棣莫佛&lt;/strong>为了求二项分布在数值较大时的近似计算值，推导出正态分布的表达形式。但是，棣莫佛并没有把它当作是一个概率分布，只是认为这是二项分布在&lt;span class="math">\(N→∞\)&lt;/span>时的一个近似表达式。40年后，法国另一个数学家&lt;strong>拉普拉斯&lt;/strong>在推导最初版本的中心极限定理时也得出了正态分布表达式的形式，但当时他也没有意识这是一个概率的分布，也把它当作单纯的一种数学表达式。但是，这是第一次正态密度函数被数学家刻画出来，而且是以二项分布的极限分布的形式被推导出来的。熟悉基础概率统计的人都知道这个结果其实叫&lt;strong>棣莫弗-拉普拉斯中心极限定理&lt;/strong>。当然当时还没有这个称呼，直到1920年才有数学家波利亚提出中心极限定理这个名称。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>棣莫弗-拉普拉斯中心极限定理&lt;/strong>：若&lt;span class="math">\(X\thicksim B(n,p)\)&lt;/span>是n次伯努利试验中出现事件&lt;span class="math">\(A\)&lt;/span>的次数，每次试验成功的概率为&lt;span class="math">\(p\)&lt;/span>，且&lt;span class="math">\(q=1-p\)&lt;/span>，则对任意有限区间&lt;span class="math">\([a,b]\)&lt;/span>，令&lt;span class="math">\(x_k=\frac{k-np}{\sqrt{npq}}\)&lt;/span>，当&lt;span class="math">\(n→∞\)&lt;/span>时：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(P(X=k)→\frac{1}{\sqrt{npq}}\cdot\frac{1}{\sqrt{2\pi}}e^{-{1\over2}x^2_k}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(P(a≤\frac{k-np}{\sqrt{npq}}≤b)→\int_a^b\varphi(x)\mathrm{d}x\)&lt;/span>，其中&lt;span class="math">\(\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}(-∞&amp;lt;x&amp;lt;∞)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>棣莫佛-拉普拉斯中心极限定理指出：参数&lt;span class="math">\(n,p\)&lt;/span>的二项分布以&lt;span class="math">\(np\)&lt;/span>为均值，&lt;span class="math">\(np(1-p)\)&lt;/span>为方差的正态分布为极限。当&lt;span class="math">\(n\)&lt;/span>很大时，可以用作二项分布的近似值。&lt;/p>
&lt;p>总结一句：这个阶段正态分布表达式只是作为一个函数近似的结果，并没有被认为是一种概率分布。&lt;/p>
&lt;h2 id="高斯误差与正态分布">高斯——误差与正态分布&lt;/h2>
&lt;p>高斯所在的18-19世纪是天文学迅速发展的时期，积累了大量观测数据。但是，这些数据或多或少都是存在误差的。如何处理这些数据得到最准确的结果，或者这些误差有着什么样的数学规律？在当时，这是一个亟待解决的问题。&lt;/p>
&lt;p>辛普森和拉普拉斯认为误差应该服从这样一些普遍规律：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>误差分布是对称的，即&lt;span class="math">\(f(x)=f(-x)\)&lt;/span>。&lt;/li>
&lt;li>小误差应该比大误差更常见，即&lt;span class="math">\(f(x)\)&lt;/span>是&lt;span class="math">\(x\)&lt;/span>的减函数。且误差应该随&lt;span class="math">\(|x|\)&lt;/span>增大而逐渐趋于0。&lt;/li>
&lt;li>每次测量产生的误差应该是独立的。&lt;/li>
&lt;li>误差分布函数应是连续函数。&lt;/li>
&lt;/ol>
&lt;p>为此，辛普森和拉普拉斯分别设计了三角概率分布函数和重指数分布（拉普拉斯分布）作为误差的分布函数。并用这些分布证明&lt;strong>观测值取平均数比恰当选择观测值更能接近真实值&lt;/strong>。拉普拉斯在重指数分布基础上，希望使用贝叶斯法（当时拉普拉斯称之为不充分推理原则）逐渐修正后验概率得到的误差分布。但是，拉普拉斯的工作到此基本进展就很小了，在18世纪70-80年代沿着这条路径磕磕绊绊走了十几年，依旧进展甚微。&lt;/p>
&lt;p>1809年，高斯发表了其数学和天体力学的名著《绕日天体运动的理论》。在此书末尾，他写了一节有关“数据结合”的问题，实际涉及的就是这个误差分布的问题。&lt;/p>
&lt;p>高斯采用了拉普拉斯设计的函数&lt;span class="math">\(L(\theta)\)&lt;/span>，设真值为&lt;span class="math">\(\theta\)&lt;/span>，n个独立的测量值&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>。&lt;span class="math">\(L(\theta)\)&lt;/span>为出现这一现象的联合概率密度： &lt;span class="math">\[\begin{aligned}
L(\theta)=&amp;amp;L(\theta;X_1,X_2,\dotsb,X_n)\\
\stackrel{\text{独立性}}{=}&amp;amp;f(X_1-\theta)f(X_2-\theta)\dotsb f(X_n-\theta)\tag{1}
\end{aligned}\]&lt;/span> 其中，&lt;span class="math">\(f(\cdot)\)&lt;/span>为&lt;strong>待定的误差密度函数&lt;/strong>。当时接下来的处理高斯与拉普拉斯完全不同。&lt;/p>
&lt;p>一是高斯没有采取贝叶斯的推理方式，而是径直让&lt;span class="math">\((1)\)&lt;/span>达到最大值的&lt;span class="math">\(\hat{\theta}=\hat{\theta}(X_1,\dotsb,X_n)\)&lt;/span>作为&lt;span class="math">\(\theta\)&lt;/span>的估计。即 &lt;span class="math">\[L(\hat{\theta})=\max_{\theta} L(\theta)\tag{2}\]&lt;/span> 当然现在我们把这种方法叫做&lt;strong>最大似然估计&lt;/strong>（Maximum Likelihood Estimation, MLE），&lt;span class="math">\(L(\theta)\)&lt;/span>叫做似然函数（正式方法由Fisher推广于1922年）。最大似然估计的思想其实就是很直接：最合理的事情（即真值）最有可能发生。举个简化的例子，如果在一个不透明的袋子里有10个球，黑白两种颜色，但是不知道各有几个；我们袋子中有放回地取100次，其中71次是白球，那么大家认为白球、黑球各有多少个？一般人都会猜7个白球，3个黑球吧。但如果是黑白球各有5个，会不会抽样出上述结果呢？当然有可能啊，只不过概率没有7个白球，3个黑球大。这就是我们在不知不觉中直观地使用了最大似然估计。&lt;/p>
&lt;p>二是高斯把辛普森和拉普拉斯的思路倒了过来。高斯先认可算数平均&lt;span class="math">\(\bar{X}\)&lt;/span>是应取的估计值，然后去找误差密度函数&lt;span class="math">\(f(\cdot)\)&lt;/span>以迎合这一点，即找这样的&lt;span class="math">\(f(\cdot)\)&lt;/span>，使由&lt;span class="math">\((2)\)&lt;/span>式决定的&lt;span class="math">\(\hat{\theta}\)&lt;/span>就是&lt;span class="math">\(\bar{X}\)&lt;/span>。&lt;/p>
&lt;p>高斯发现，使用最大似然估计的情况下，只有当 &lt;span class="math">\[f(x)=\frac{1}{\sqrt{2\pi}h}e^{\frac{-x^2}{2h^2}}，h&amp;gt;0\]&lt;/span> 时，才有&lt;span class="math">\(\hat{\theta}=\bar{X}\)&lt;/span>。注意，高斯在原文中，并没有让&lt;span class="math">\(h=\sigma\)&lt;/span>，即方差。下面，我们来推导这个过程。&lt;/p>
&lt;p>我们从似然函数&lt;span class="math">\((1)\)&lt;/span>式开始，要求关于&lt;span class="math">\((1)\)&lt;/span>式的最小值，而其定义域&lt;span class="math">\(x\in (-∞,∞)\)&lt;/span>，由费马引理（皮埃尔·德·费马 1601-1665，法国律师和业余数学家，业余数学家之王）可知，&lt;span class="math">\((1)\)&lt;/span>式的极值点必然是驻点，即导数为0的点。但是，连乘表达式的求导难以处理，因此高斯采用了化乘为加，且不改变驻点位置（证明见附1）的取&lt;span class="math">\(\log(ln)\)&lt;/span>法，即 &lt;span class="math">\[\frac{\mathrm{d}ln L(\theta)}{\mathrm{d}\theta}=0\]&lt;/span> 展开&lt;span class="math">\(\ln L(\theta)\)&lt;/span>求导可得： &lt;span class="math">\[\sum_{i=1}^n\frac{f&amp;#39;(x_i-\theta)}{f(x_i-\theta)}=0\]&lt;/span> 为了方便计算，我们令&lt;span class="math">\(g(x)=\frac{f&amp;#39;(x)}{f(x)}\)&lt;/span>，则有 &lt;span class="math">\[\sum_{i=1}^n g(x_i-\theta)=0\]&lt;/span> 由于高斯假设极大似然估计的解就是算术平均&lt;span class="math">\(\bar{x}\)&lt;/span>，把解直接带入上式可得 &lt;span class="math">\[\sum_{i=1}^n g(x_i-\bar{x})=0 \tag{3}\]&lt;/span> 下面就是高斯精彩的数学表演，由于&lt;span class="math">\(n,x_i\)&lt;/span>的任意性，在&lt;span class="math">\((3)\)&lt;/span>式中取&lt;span class="math">\(n=2\)&lt;/span>有： &lt;span class="math">\[g(x_1-x)+g(x_2-x)=0\]&lt;/span> 又因为&lt;span class="math">\(\bar{x}=\frac{x_1+x_2}{2}\)&lt;/span>，所以&lt;span class="math">\(x_1-x=-(x_2-x)\)&lt;/span>，即&lt;span class="math">\(g(x_1-x)+g(-(x_1-x))=0\)&lt;/span>，由于&lt;span class="math">\(x_i\)&lt;/span>的任意性可得 &lt;span class="math">\[g(-x)=-g(x),g(x)是奇函数\]&lt;/span> 在&lt;span class="math">\((3)\)&lt;/span>式中再取&lt;span class="math">\(n=m+1\)&lt;/span>，并要求&lt;span class="math">\(x_1=\dotsb=x_m=-x,x_{m+1}=mx\)&lt;/span>，则有&lt;span class="math">\(\bar{x}=0\)&lt;/span>，并且 &lt;span class="math">\[\sum_{i=1}^n g(x_i-\bar{x})=mg(-x)+g(mx)=0\]&lt;/span> 又因为&lt;span class="math">\(g(x)\)&lt;/span>是奇函数，所以有 &lt;span class="math">\[g(mx)=mg(x)\]&lt;/span> 而满足上式的&lt;strong>唯一的连续奇函数解&lt;/strong>就是&lt;span class="math">\(g(x)=cx\)&lt;/span>（可用两边求导数证明），从而进一步可以解微分方程： &lt;span class="math">\[\frac{f&amp;#39;(x)}{f(x)}=cx\Rightarrow \frac{1}{y}\frac{dy}{dx}=cx\stackrel{\text{分离变量法}}{\Rightarrow}\frac{1}{y}dy=cx dx\]&lt;/span> 两边求补丁积分，且y=f(x)作为概率值恒大于等于0，可得： &lt;span class="math">\[\ln y=\frac{1}{2}cx^2+K\Rightarrow y = e^Ke^{\frac{1}{2}cx^2}\]&lt;/span> 我们令&lt;span class="math">\(e^K=M\)&lt;/span>，且&lt;span class="math">\(f(x)\)&lt;/span>作为概率密度函数，必然有&lt;span class="math">\(\int_{-∞}^∞f(x)dx=1\)&lt;/span>，所以 &lt;span class="math">\[\int_{-∞}^∞f(x) dx= \int_{-∞}^∞ Me^{\frac{1}{2}cx^2}dx =\int_{-∞}^∞\frac{M}{\sqrt{0.5|c|}} e^{-(\sqrt{0.5|c|}x)^2} d(\sqrt{0.5|c|}x)= 1\]&lt;/span> 这里需要注意，为了保证积分收敛到1，那么常数&lt;span class="math">\(c&amp;lt;0\)&lt;/span>是必然的，在&lt;span class="math">\(e\)&lt;/span>的指数部分要加上负号，同时开方时需要加绝对值符号。此外，令&lt;span class="math">\(t=\sqrt{0.5|c|}x\)&lt;/span>，则t的积分区间为&lt;span class="math">\((-∞,∞)\)&lt;/span>，上式可转换为 &lt;span class="math">\[\frac{M}{\sqrt{0.5|c|}}\int_{-∞}^∞ e^{-t^2}dt=1\]&lt;/span> 而这个广义积分正好有&lt;span class="math">\(\int_{-∞}^∞ e^{-t^2}dt=\sqrt{\pi}\)&lt;/span>（即&lt;strong>高斯积分&lt;/strong>证明附2），所以可得 &lt;span class="math">\[M=\frac{\sqrt{|c|}}{\sqrt{2\pi}}\]&lt;/span> 将&lt;span class="math">\(M\)&lt;/span>代入&lt;span class="math">\(f(x)\)&lt;/span>可得 &lt;span class="math">\[
f(x)=\frac{\sqrt{|c|}}{\sqrt{2\pi}}e^\frac{cx^2}{2}\tag{4}
\]&lt;/span> 这就是高斯推导的正态分布的表达式，它满足以算数平均作为最大似然估计的结果。如果我们用方差定义式计算一下&lt;span class="math">\(\sigma^2\)&lt;/span>，可以得到&lt;span class="math">\(\sigma^2=-\frac{1}{c}\)&lt;/span>（证明见附3），由此可得，&lt;span class="math">\(f(x)\)&lt;/span>就是&lt;span class="math">\(N(0,\sigma^2)\)&lt;/span>的表达式。正态分布在误差分布计算上有着极大的理论优势，对后世影响极大，以至于正态分布有了高斯分布的名称。&lt;/p>
&lt;p>但是，高斯的解释在逻辑上有点循环论证的味道。由于取算术平均是合理的方式，所以用MLE导出正态分布；又因为误差服从正态分布，所以算术平均能够获得更小的误差值。但是，拉普拉斯很快得知了高斯的工作，并将正态分布与中心极限定理联系起来。在1810年，拉普拉斯提出了元误差学说：如误差可以看成是大量的，有各种原因导致的微小的量的叠加，则根据中心极限定理，误差的分布应服从正态分布。拉普拉斯的补充使误差的正态分布理论有了一个更自然、更令人信服的解释。&lt;/p>
&lt;h2 id="赫歇尔和麦克斯韦正态分布的pi从哪里来">赫歇尔和麦克斯韦——正态分布的&lt;span class="math">\(\pi\)&lt;/span>从哪里来？&lt;/h2>
&lt;p>每当公式中出现&lt;span class="math">\(\pi\)&lt;/span>的时候，我都不禁会问自己？圆在哪里？而正态分布的表达式中正好有一个&lt;span class="math">\(\frac{1}{\sqrt{2\pi}}\)&lt;/span>系数，这个怎么解释呢？我们可以从赫歇尔和麦克斯韦的推导过程中一窥究竟。&lt;/p>
&lt;p>1850年，天文学家赫歇尔在对星星位置进行测量时，需要考虑二维误差的分布，为了推导这个误差的概率密度分布&lt;span class="math">\(p(x,y)\)&lt;/span>，赫歇尔设置了两个准则：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>x轴，y轴的误差时相互独立的，即随机误差在正交方向上相互独立&lt;/li>
&lt;li>误差的概率分布在空间上具有&lt;strong>旋转对称性&lt;/strong>，即误差的概率分布和角度没有关系&lt;/li>
&lt;/ol>
&lt;p>这两个准则对于进行了大量实际测量的赫歇尔都非常合理，而空间的旋转对称性，在二维空间里，那不正是让我们想到了“圆”吗？（三维空间就是球~）&lt;/p>
&lt;p>由第一条准则，我们可以把&lt;span class="math">\(x,y\)&lt;/span>联合概率分布，分离成两部分： &lt;span class="math">\[p(x,y)=f(x)\cdot f(y)\]&lt;/span> 而面对旋转对称性，和角度相关的，我们不禁就和极坐标联系起来： &lt;span class="math">\[p(x,y)=p(r\cos\theta,r\sin\theta)=g(r,\theta)\]&lt;/span> 由第二条准则，旋转不变性可知，函数&lt;span class="math">\(g(r,\theta)\)&lt;/span>实际上与角度&lt;span class="math">\(\theta\)&lt;/span>无关，即&lt;span class="math">\(g(r,\theta)=g(r)\)&lt;/span>。实际上，正是这个旋转不变性，使得&lt;span class="math">\(\theta\)&lt;/span>的积分域是&lt;span class="math">\([0,2\pi]\)&lt;/span>，也就是一个圆，即正态分布中&lt;span class="math">\(\pi\)&lt;/span>的来历。综上所述，我们可得： &lt;span class="math">\[f(x)f(y)=g(r)=g(\sqrt{x^2+y^2})\tag{5}\]&lt;/span> 由于&lt;span class="math">\(x,y\)&lt;/span>的任意性，我们取&lt;span class="math">\(y=0\)&lt;/span>，得到&lt;span class="math">\(g(x)=f(x)f(0)\)&lt;/span>，所以上式中 &lt;span class="math">\[g(\sqrt{x^2+y^2})=f(\sqrt{x^2+y^2})f(0)\tag{6}\]&lt;/span> 联合式&lt;span class="math">\((5)(6)\)&lt;/span>并在两边同时除以&lt;span class="math">\(f^2(0)\)&lt;/span>有： &lt;span class="math">\[\frac{f(x)f(y)}{f(0)f(0)}=\frac{f(\sqrt{x^2+y^2})f(0)}{f(0)f(0)}\Rightarrow \frac{f(x)}{f(0)}\frac{f(y)}{f(0)}=\frac{f(\sqrt{x^2+y^2})}{f(0)}\]&lt;/span> 两边取&lt;span class="math">\(\log\)&lt;/span>则有 &lt;span class="math">\[\log\left[\frac{f(x)}{f(0)}\right]+\log\left[\frac{f(y)}{f(0)}\right]=\log\left[\frac{f(\sqrt{x^2+y^2})}{f(0)}\right]\]&lt;/span> 令&lt;span class="math">\(h(x)=\log[\frac{f(x)}{f(0)}]\)&lt;/span>，则上式可简化为函数方程： &lt;span class="math">\[h(x)+h(y)=h(\sqrt{x^2+y^2})\tag{7}\]&lt;/span> 函数方程与代数方程、微分方程不同，并没有通用的解法。所以这个分支也没能发展起来。但是，我们不难猜出&lt;span class="math">\(h(x)=ax^2\)&lt;/span>是符合&lt;span class="math">\((7)\)&lt;/span>的一个连续函数解（我觉得可以用此函数方程的二阶导数为常数和&lt;span class="math">\(h(0)=0\)&lt;/span>证明其解的唯一性）。同时，由于概率积分的收敛性，需要&lt;span class="math">\(a&amp;lt;0\)&lt;/span>，不妨令&lt;span class="math">\(\alpha=-a\)&lt;/span>： &lt;span class="math">\[h(x)=ax^2\Rightarrow f(x)=f(0)e^{-\alpha x^2}\]&lt;/span> 对于概率密度函数定义域积分为1，则有 &lt;span class="math">\[\int_{-\infty}^\infty f(0)e^{-\alpha x^2}dx = 1\Rightarrow \frac{f(0)}{\sqrt{\alpha}}\int_{-\infty}^\infty e^{-(\sqrt{\alpha} x)^2}d\sqrt{\alpha}x=1\]&lt;/span> 不妨令&lt;span class="math">\(\sqrt{\alpha}x = t, t\in (-\infty,\infty)\)&lt;/span>，根据附2的高斯积分&lt;span class="math">\(\int_{-∞}^∞ e^{-x^2}dx=\sqrt{\pi}\)&lt;/span>，可推得&lt;span class="math">\(f(0)=\sqrt{\frac{\alpha}{\pi}}\)&lt;/span>，综上所述有： &lt;span class="math">\[f(x)=\sqrt{\frac{\alpha}{\pi}}e^{-\alpha x^2}\]&lt;/span> 这也是正态分布的表达形式。这也是在0均值正态分布，当&lt;span class="math">\(\sigma^2=\frac{1}{2\alpha}\)&lt;/span>的结果（方差证明的方式类似附3）。 而满足准则1，2的二维表达式，就是&lt;em>两个独立同分布&lt;/em>的&lt;span class="math">\(f(x)\)&lt;/span>的乘机，即&lt;span class="math">\(p(x,y)\)&lt;/span>为二维i.i.d正态分布的密度函数： &lt;span class="math">\[p(x,y)=\frac{\alpha}{\pi}e^{-\alpha(x^2+y^2)}\]&lt;/span>&lt;/p>
&lt;p>1860年麦克斯韦在考虑气体分子运动速度分布的时候，基于类似的准则推出了著名的气体分子运动速率分布的麦克斯韦-玻尔兹曼气体速率分布定率： &lt;span class="math">\[
F(v)=(\frac{m}{2\pi kT})^{3/2}e^{-\frac{mv^2}{2kT}}
\]&lt;/span> 这拆开看其实就是三维空间的i.i.d正态分布嘛，其中&lt;span class="math">\(\sigma^2=\frac{kT}{m}\)&lt;/span> &lt;span class="math">\[\begin{aligned}
F(v)&amp;amp;=(\frac{m}{2\pi kT})^{3/2}e^{-\frac{mv^2}{2kT}}\\
&amp;amp;=(\frac{m}{2\pi kT})^{1/2}e^{-\frac{mv_x^2}{2kT}}\times(\frac{m}{2\pi kT})^{1/2}e^{-\frac{mv_y^2}{2kT}}\times(\frac{m}{2\pi kT})^{1/2}e^{-\frac{mv_z^2}{2kT}}
\end{aligned}\]&lt;/span> 其中，&lt;span class="math">\(v^2=v_x^2+v_y^2+v_z^2,v_x,v_y,v_z\)&lt;/span>是三维空间正交的三个方向向量。赫歇尔和麦克斯韦的工作神奇之处在于，他们没有利用任何概率论只是，只是基于&lt;strong>空间不变性&lt;/strong>，就推导出了正态分布。&lt;/p>
&lt;h2 id="为什么正态分布那么普遍稳定性兰登的推导">为什么正态分布那么普遍？——稳定性，兰登的推导&lt;/h2>
&lt;p>1941年电子工程师兰登通过分析经验数据他发现噪声电压的分布模式很相似，不同的是分布的层级，而这个层级可以使用方差&lt;span class="math">\(σ^2\)&lt;/span>来刻画。基于这些经验，兰登提出以下两个准则：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>随机噪声具有稳定的分布模式。&lt;/li>
&lt;li>累加一个微小的随机噪声，不改变其稳定的分布模式，只改变分布的层级。(用方差度量)&lt;/li>
&lt;li>微小的随机噪声关于0对称分布。（本质上是相位(&lt;span class="math">\([0-2\pi]\)&lt;/span>)的随机性，也是正态分布中&lt;span class="math">\(\pi\)&lt;/span>的来源）&lt;/li>
&lt;/ol>
&lt;p>用数学的语言描述: 如果 &lt;span class="math">\[X \sim p(x;\sigma^2),\epsilon\sim q(e),X&amp;#39;=X+\epsilon\]&lt;/span> 则有 &lt;span class="math">\[
X&amp;#39;\sim p(x;\sigma^2+\text{var}(\epsilon))
\]&lt;/span> 按照两个随机变量和的分布的计算方式，&lt;span class="math">\(X′\)&lt;/span>的分布密度函数将是&lt;span class="math">\(X\)&lt;/span>的分布密度函数和&lt;span class="math">\(\epsilon\)&lt;/span>的分布密度函数的卷积，即有 &lt;span class="math">\[
f(x&amp;#39;)=\int_{-\infty}^\infty p(x&amp;#39;-e;\sigma^2)q(e)de
\]&lt;/span> 通过泰勒级数展开和解二阶偏微分方程（也是著名的扩散方程），我们可以解得 &lt;span class="math">\[p(x;\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{-x^2}{2\sigma^2}}
\]&lt;/span> (这里详情可以扩展写)&lt;/p>
&lt;p>杰恩斯（概率论沉思录作者）指出这个推导这基本上就是&lt;strong>中心极限定理的增量式&lt;/strong>版本，相比于中心极限定理是一次性累加所有的因素，兰登的推导是每次在原有的分布上去累加一个微小的扰动。而在这个推导中，我们看到，正态分布具有相当好的稳定性；只要数据中正态的模式已经形成，他就容易继续保持正态分布，无论外部累加的随机噪声 q(e) 是什么分布，正态分布就像一个黑洞一样把这个累加噪声吃掉。这也是稳定性也是正态分布普遍存在的原因。&lt;/p>
&lt;h2 id="基于最大熵的推导">基于最大熵的推导&lt;/h2>
&lt;p>如果&lt;strong>给定一个分布密度函数&lt;span class="math">\(p(x)\)&lt;/span>的均值&lt;span class="math">\(µ\)&lt;/span>和方差&lt;span class="math">\(σ^2\)&lt;/span>&lt;/strong>(给定均值和方差这个条件，也可以描述为给定一阶原点矩和二阶原点矩，这两个条件是等价的)，则在所有满足这两个限制的概率分布中，熵最大的概率分布&lt;span class="math">\(p(x|µ; σ^2)\)&lt;/span>就是正态分布&lt;span class="math">\(N(µ; σ^2)\)&lt;/span>&lt;/p>
&lt;p>此部分的证明用到了古典泛函分析的变分法。&lt;/p>
&lt;h2 id="附录">附录&lt;/h2>
&lt;h3 id="附1-似然函数取对数">附1 似然函数取对数&lt;/h3>
&lt;blockquote>
&lt;p>命题：对于定义域&lt;span class="math">\([x_a,x_b]\)&lt;/span>上的正函数&lt;span class="math">\(f(x)\)&lt;/span>，取对数不影响驻点位置。&lt;/p>
&lt;/blockquote>
&lt;p>证明： 驻点要求函数导数为0，即&lt;span class="math">\(f&amp;#39;(x)=0\)&lt;/span>；而&lt;span class="math">\((\log_a f(x))&amp;#39;=\frac{f&amp;#39;(x)}{f(x)\ln a}\)&lt;/span>。由于&lt;span class="math">\(f(x)&amp;gt;0\)&lt;/span>，且&lt;span class="math">\(a \neq 1\)&lt;/span>，所以只要当&lt;span class="math">\(f&amp;#39;(x)=0\)&lt;/span>时，&lt;span class="math">\((\log_a f(x))&amp;#39;=0\)&lt;/span>，即二者驻点位置一致。&lt;/p>
&lt;h3 id="附2-高斯积分">附2 高斯积分&lt;/h3>
&lt;blockquote>
&lt;p>命题：证明高斯积分&lt;span class="math">\(I=\int_{-∞}^∞ e^{-x^2}dx=\sqrt{\pi}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：令两个独立的变量&lt;span class="math">\(x，y\)&lt;/span> &lt;span class="math">\[
I^2=\int_{-∞}^∞ e^{-x^2}dx\times \int_{-∞}^∞ e^{-y^2}dy=\int_{-∞}^∞\int_{-∞}^∞ e^{-(x^2+y^2)} dx dy
\]&lt;/span> 对于含有&lt;span class="math">\(x^2+y^2\)&lt;/span>的形式，我们一般使用极坐标变换：&lt;span class="math">\(x=r\cos\theta,y=r\sin\theta\)&lt;/span>；元面积变换：&lt;span class="math">\(dxdy=rdrd\theta\)&lt;/span> &lt;span class="math">\[
\begin{aligned}
I^2 &amp;amp;= \int_0^{2\pi}\int_0^∞ e^{-r^2}rdrd\theta=\int_0^{2\pi}d\theta\int_0^∞ e^{-r^2}rdr \\
&amp;amp;=2\pi\times (-\frac{1}{2}e^{-r^2}\bigg|_0^∞)=2\pi\times{1\over2}=\pi\\
&amp;amp;\Rightarrow I=\sqrt{\pi}
\end{aligned}
\]&lt;/span> 得证。&lt;/p>
&lt;p>补充：&lt;strong>高斯积分与Gamma函数（第二类欧拉积分）的关系&lt;/strong>。在&lt;span class="math">\(I\)&lt;/span>中，因为&lt;span class="math">\(e^{-x^2}\)&lt;/span>是一个偶函数，因此积分域可以从&lt;span class="math">\(\int_{-∞}^∞dx\)&lt;/span>变成&lt;span class="math">\(2\cdot\int_0^∞dx\)&lt;/span>，此时&lt;span class="math">\(x\in(0,∞)\)&lt;/span>。如果我们做一个积分变量替换&lt;span class="math">\(x=\sqrt{t},dx={1\over 2}t^{-{1\over 2}}dt\)&lt;/span>，则&lt;span class="math">\(t\in(0,∞)\)&lt;/span>，且&lt;span class="math">\(I\)&lt;/span>有如下变换： &lt;span class="math">\[
I= \sqrt{\pi}=\int_{-∞}^∞ e^{-x^2}dx =2\int_0^∞ e^{-t}\cdot{1\over 2}t^{-{1\over 2}}dt=\int_0^∞ e^{-t}t^{-{1\over 2}}dt=\Gamma({1\over2})
\]&lt;/span> 即 &lt;span class="math">\[I=\underbrace{\int_{-∞}^∞ e^{-x^2}dx}_{\text{高斯积分}}=\underbrace{\int_0^∞ e^{-x}x^{-{1\over 2}}dx}_{\text{第二类欧拉积分}}=\Gamma({1\over2})=\sqrt{\pi}\]&lt;/span>&lt;/p>
&lt;h3 id="附3-高斯在推导正态分布时系数c和方差sigma2的关系">附3 高斯在推导正态分布时系数&lt;span class="math">\(c\)&lt;/span>和方差&lt;span class="math">\(\sigma^2\)&lt;/span>的关系&lt;/h3>
&lt;blockquote>
&lt;p>命题：式&lt;span class="math">\((4)\)&lt;/span>中的系数&lt;span class="math">\(c=-\frac{1}{\sigma^2}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：首先，从&lt;span class="math">\((4)\)&lt;/span>中，我们不难观察出分布函数是一个偶函数，所以有&lt;span class="math">\(E[x]=\mu=0\)&lt;/span>,则方差&lt;span class="math">\(E[(x-\mu)^2]=E[x^2]\)&lt;/span>，则有: &lt;span class="math">\[
\begin{aligned}
\sigma^2=&amp;amp;\frac{\sqrt{|c|}}{\sqrt{2\pi}}\int_∞^∞ x^2e^\frac{cx^2}{2}dx\\
\stackrel{\text{分部积分}}{=}&amp;amp;\frac{\sqrt{|c|}}{\sqrt{2\pi}}\left\{x\cdot\frac{1}{c}e^\frac{cx^2}{2}\bigg |_{-\infty}^\infty-\int_∞^∞ 1\cdot \frac{1}{c}e^\frac{cx^2}{2}dx\right\}\\
\stackrel{\text{等价无穷小}}{=}&amp;amp;\frac{\sqrt{|c|}}{\sqrt{2\pi}}\left\{0-\int_∞^∞ 1\cdot \frac{1}{c}e^\frac{cx^2}{2}dx\right\}\\
\stackrel{\text{第一类积分变量替换}}{=}&amp;amp;\frac{\sqrt{|c|}}{\sqrt{2\pi}}\left\{-\int_∞^∞ \frac{1}{c}\cdot\sqrt{\frac{2}{|c|}} e^{-(\sqrt{\frac{|c|}{2}}x)^2}d(\sqrt{\frac{|c|}{2}}x)\right\}\\
\stackrel{t=\sqrt{\frac{|c|}{2}}x}{=}&amp;amp;\frac{-1}{\sqrt{\pi}c}\left\{\int_∞^∞ e^{-t^2}dt\right\}\\
\stackrel{\text{附录2高斯积分}}{=}&amp;amp;\frac{-1}{\sqrt{\pi}c}\cdot \sqrt{\pi}=\frac{-1}{c}
\end{aligned}
\]&lt;/span> 即系数&lt;span class="math">\(c=-\frac{1}{\sigma^2}\)&lt;/span>，代入&lt;span class="math">\((4)\)&lt;/span>可得正态分布&lt;span class="math">\(N(0,\sigma^2)\)&lt;/span>。&lt;/p></description></item><item><title>概率统计随机过程之随机变量函数的分布</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E5%88%86%E5%B8%83/</link><pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E5%88%86%E5%B8%83/</guid><description>
&lt;h2 id="概率统计随机过程之随机变量函数的分布">概率统计随机过程之随机变量函数的分布&lt;!-- omit in toc -->&lt;/h2>
&lt;p>概率论与数理统计－－茆诗松（第二版）随机变量函数的分布内容总结&lt;/p>
&lt;p>写在最前面：在随机变量的学习中，我们一定要明确知晓，&lt;strong>概率分布函数和随机变量的定义密切相关&lt;/strong>。概率密度函数是概率分布函数的衍生结论，因此，我们在处理问题时优先考虑概率分布函数。&lt;/p>
&lt;p>现有随机变量&lt;span class="math">\(X\)&lt;/span>定义在&lt;span class="math">\((\Omega,\mathcal{F},P)\)&lt;/span>上。设存在一个定义在&lt;span class="math">\(\Omega\)&lt;/span>上的函数&lt;span class="math">\(y=g(x)\)&lt;/span>，若使用随机变量&lt;span class="math">\(X\)&lt;/span>作为函数&lt;span class="math">\(g\)&lt;/span>的自变量，则&lt;span class="math">\(Y=g(X)\)&lt;/span>显然也是一个随机变量。那么，问题来了：已知随机变量&lt;span class="math">\(X\)&lt;/span>的分布，如何求出另一个随机变量&lt;span class="math">\(Y=g(X)\)&lt;/span>的分布呢？&lt;/p>
&lt;p>多维随机变量其实就是多个随机变量的意思，这些个随机变量之间可能存在关联性（破坏了独立性），导致多维随机变量的分布有时很不直观。这里也只是介绍了一部分多维随机变量的场景（和、商、最大值最小值等），很多多维随机变量的函数没有解析的结果。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#单个随机变量函数的分布">单个随机变量函数的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单个离散型随机变量函数的分布">单个离散型随机变量函数的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单个连续型随机变量函数的分布">单个连续型随机变量函数的分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#当gx严格单调时">当&lt;span class="math">\(g(x)\)&lt;/span>严格单调时&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单个连续型随机变量函数的几个常用命题">单个连续型随机变量函数的几个常用命题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯假设的一个悖论">贝叶斯假设的一个悖论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#当gx为其他形式时">当&lt;span class="math">\(g(x)\)&lt;/span>为其他形式时&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#多维随机变量函数的分布">多维随机变量函数的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多维离散型随机变量函数的分布">多维离散型随机变量函数的分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#离散型随机变量和的分布与卷积公式">离散型随机变量和的分布与卷积公式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#思考分布可加性的本质">思考：分布可加性的本质&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#多维随机变量的最大最小值分布">多维随机变量的最大最小值分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#最大值分布">最大值分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小值分布">最小值分布&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#多维连续型随机变量函数的分布">多维连续型随机变量函数的分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#连续型随机变量和的分布与卷积公式">连续型随机变量和的分布与卷积公式&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#变量变换法">变量变换法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#box-muller变换">Box-muller变换&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#增补变量法">增补变量法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二维随机变量积的分布">二维随机变量积的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二维随机变量商的分布">二维随机变量商的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二维随机变量积和商的分布直接推导">二维随机变量积和商的分布直接推导&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;h2 id="单个随机变量函数的分布">单个随机变量函数的分布&lt;/h2>
&lt;h3 id="单个离散型随机变量函数的分布">单个离散型随机变量函数的分布&lt;/h3>
&lt;p>离散型随机变量函数的分布时&lt;strong>比较容易&lt;/strong>的，主要是因为离散型随机变量的函数变换结果是离散固定的。其一般方法如下：&lt;/p>
&lt;p>设&lt;span class="math">\(X\)&lt;/span>是离散型随机变量，X的分布列为&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;span class="math">\(X\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(x_1\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(x_2\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(x_n\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(P\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_1)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_2)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_n)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>则&lt;span class="math">\(Y=g(X)\)&lt;/span>也是一个离散型随机变量，且此时&lt;span class="math">\(Y\)&lt;/span>的分布列相应可表示为&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;span class="math">\(Y\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(g(x_1)\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(g(x_2)\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(g(x_n)\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(P\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_1)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_2)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_n)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>注意&lt;/strong>：当函数值&lt;span class="math">\(g(x_1),g(x_2),\dotsb\)&lt;/span>有相等值时，则把那些相等的值分别合并，并把对应的概率相加。&lt;/p>
&lt;p>以下是几个例题：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/单个离散型随机变量函数的分布.png" alt="单个离散型随机变量函数的分布" />&lt;p class="caption">单个离散型随机变量函数的分布&lt;/p>
&lt;/div>
&lt;h3 id="单个连续型随机变量函数的分布">单个连续型随机变量函数的分布&lt;/h3>
&lt;p>找出离散型随机变量函数分布相对简单，只要按步骤求出对应随机变量值就可以了。而对连续性随机变量&lt;span class="math">\(X\)&lt;/span>，我们需要分两种情况讨论。&lt;/p>
&lt;h4 id="当gx严格单调时">当&lt;span class="math">\(g(x)\)&lt;/span>严格单调时&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>定理1&lt;/strong>:设&lt;span class="math">\(X\)&lt;/span>是连续性随机变量，其密度函数为&lt;span class="math">\(p_{_X}(x)\)&lt;/span>.&lt;span class="math">\(Y=g(X)\)&lt;/span>是另一个随机变量。若&lt;span class="math">\(y=g(x)\)&lt;/span>严格单调，其反函数&lt;span class="math">\(h(y)\)&lt;/span>有连续导函数，则&lt;span class="math">\(Y=g(X)\)&lt;/span>的密度函数为 &lt;span class="math">\[p_{_Y}(y)=\begin{cases}
p_{_X}(h(y))|h&amp;#39;(y)|, &amp;amp;a&amp;lt;y&amp;lt;b\\
0,&amp;amp;\text{otherwise}
\end{cases}\tag{1}\]&lt;/span> 其中，&lt;span class="math">\(a=\min\{g(-∞),g(∞)\},b=\max\{g(-∞),g(∞)\}\)&lt;/span>，即&lt;span class="math">\(a，b\)&lt;/span>为边界。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;blockquote>
&lt;p>不妨设&lt;span class="math">\(g(x)\)&lt;/span>为严格单调递增函数，这时它的反函数&lt;span class="math">\(h(y)\)&lt;/span>也是严格单调递增的,有&lt;span class="math">\(y\in (a,b)\)&lt;/span>，且&lt;span class="math">\(h&amp;#39;(y)&amp;gt;0\)&lt;/span>。 对于随机变量&lt;span class="math">\(Y\)&lt;/span>，由于其取值范围为&lt;span class="math">\((a,b)\)&lt;/span>,当&lt;/p>
&lt;ul>
&lt;li>当&lt;span class="math">\(y&amp;lt;a\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=0\)&lt;/span>&lt;/li>
&lt;li>当&lt;span class="math">\(y&amp;gt;b\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=1\)&lt;/span>&lt;/li>
&lt;li>当&lt;span class="math">\(a≤y≤b\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=P(g(X)≤y)=P(X≤h(y))=\int_{-\infty}^{h(y)}p_{_X}(x)\mathrm{d}x\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>由此可得&lt;span class="math">\(Y\)&lt;/span>的概率密度函数为 &lt;span class="math">\[p_{_Y}(y)=\begin{cases}
p_{_X}[h(y)]\times h&amp;#39;(y),&amp;amp;a&amp;lt;y&amp;lt;b\\
0,&amp;amp;\text{others}
\end{cases}\tag{2}\]&lt;/span> &lt;span class="math">\[\stackrel{\text{加绝对值符号无所谓}}{\Longrightarrow}式(1)\]&lt;/span> 当&lt;span class="math">\(g(x)\)&lt;/span>时严格单调递减函数时，这时它的反函数&lt;span class="math">\(h(y)\)&lt;/span>也是严格单调递减的,有&lt;span class="math">\(y\in (a,b)\)&lt;/span>，且&lt;span class="math">\(h&amp;#39;(y)&amp;lt;0\)&lt;/span>。&lt;/p>
&lt;p>对于随机变量&lt;span class="math">\(Y\)&lt;/span>，由于其取值范围为&lt;span class="math">\((a,b)\)&lt;/span>,当&lt;/p>
&lt;ul>
&lt;li>当&lt;span class="math">\(y&amp;lt;a\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=0\)&lt;/span>(这一项和单调递增函数一样)&lt;/li>
&lt;li>当&lt;span class="math">\(y&amp;gt;b\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=1\)&lt;/span>(这一项和单调递增函数一样)&lt;/li>
&lt;li>当&lt;span class="math">\(a≤y≤b\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=P(g(X)≤y){\color{red}=P(X≥h(y))}\)&lt;/span>。由于&lt;span class="math">\(g(x)\)&lt;/span>是减函数，所以自变量符号应从“≤”变成“≥”。而&lt;span class="math">\(P(X≥h(y))=1-P(X≤h(y))=1-\int_{-\infty}^{h(y)}p_{_X}(x)\mathrm{d}x\)&lt;/span>。&lt;/li>
&lt;/ul>
&lt;p>由于&lt;span class="math">\(h(y)\)&lt;/span>是单调递减函数，所以&lt;span class="math">\(h&amp;#39;(y)&amp;lt;0\)&lt;/span>。由此可得&lt;span class="math">\(Y\)&lt;/span>的概率密度函数为 &lt;span class="math">\[p_{_Y}(y)=\begin{cases}
-p_{_X}[h(y)]\times h&amp;#39;(y),&amp;amp;a&amp;lt;y&amp;lt;b\\
0,&amp;amp;\text{others}
\end{cases}\tag{3}\]&lt;/span> &lt;span class="math">\[\stackrel{\text{h&amp;#39;(y)≤0}}{\Longrightarrow}式(1)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>特别的，当&lt;span class="math">\(g(X)\)&lt;/span>的表达式为随机变量&lt;span class="math">\(X\)&lt;/span>的累积分布函数（CDF）&lt;span class="math">\(F_{_X}(X)\)&lt;/span>时，我们有以下命题：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>命题1-1&lt;/strong>：设存在一随机变量&lt;span class="math">\(X\)&lt;/span>，另一随机变量&lt;span class="math">\(Z=F(X)\)&lt;/span>，其中&lt;span class="math">\(F(\cdot)\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的分布函数，那么&lt;span class="math">\(Z\thicksim U(0,1)\)&lt;/span>&lt;/p>
&lt;p>&lt;strong>命题1-2&lt;/strong>：令&lt;span class="math">\(Z\thicksim U(0,1)\)&lt;/span>，&lt;span class="math">\(F^{-1}\)&lt;/span>是随机变量&lt;span class="math">\(X\)&lt;/span>分布函数&lt;span class="math">\(F\)&lt;/span>的反函数，那么&lt;span class="math">\(X=F^{-1}(Z)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这一对命题证明比较容易，借助&lt;code>定理1&lt;/code>同样的证明方法就能证明。&lt;strong>本质上这一对命题是&lt;code>定理1&lt;/code>的一个特例&lt;/strong>。但是一开始这个结论令我很不解，主要是我没有分清累积分布函数和随机变量函数的关系。这里的&lt;span class="math">\(F\)&lt;/span>只是随机变量的函数恰好等于累积分布函数，&lt;span class="math">\(X\)&lt;/span>经过&lt;span class="math">\(F\)&lt;/span>变换的得到随机变量&lt;span class="math">\(Z=F(X)\)&lt;/span>。而在累积分布函数CDF中，&lt;span class="math">\(F(x)\)&lt;/span>(注意这里写正确了，一定是&lt;span class="math">\(x\)&lt;/span>)求的是随机变量&lt;span class="math">\(X≤x\)&lt;/span>的概率值，是一个固定值。这二者看着相似，实有根本区别。&lt;/p>
&lt;p>此外，这对命题在计算机产生各种分布随机变量时有很大作用。因为我们只要能产生均匀分布的随机变量&lt;span class="math">\(Z\thicksim U(0,1)\)&lt;/span>就可以通过随机变量&lt;span class="math">\(X\)&lt;/span>的CDF的反函数&lt;span class="math">\(F^{-1}(Z)\)&lt;/span>，得到一个服从累计分布函数&lt;span class="math">\(F\)&lt;/span>的随机变量&lt;span class="math">\(X\)&lt;/span>。事实上，计算机中大多数常用分布随机变量都是通过这个方式产生的,这也是计算机进行随机模拟法（又称蒙特卡洛法）的基础。（有趣的是最常见的正态分布却不用这个方法产生，因为正态分布的CDF不好求，其CDF的反函数更不好求。正态分布随机变量可以以均匀分布随机变量用Box-Muller算法或其改进算法生成。）&lt;/p>
&lt;h4 id="单个连续型随机变量函数的几个常用命题">单个连续型随机变量函数的几个常用命题&lt;/h4>
&lt;blockquote>
&lt;p>命题2-1：设随机变量&lt;span class="math">\(X\)&lt;/span>服从正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>，当&lt;span class="math">\(a\neq 0\)&lt;/span>时，有&lt;span class="math">\(Y=aX+b\thicksim N(a\mu+b,a^2\sigma^2)\)&lt;/span>。&lt;/p>
&lt;p>命题2-2：对数正态分布，设随机变量&lt;span class="math">\(X\)&lt;/span>服从正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>，则&lt;span class="math">\(Y=e^X\)&lt;/span>的概率密度函数为： &lt;span class="math">\[p_Y(y)=\begin{cases}
\frac{1}{\sqrt{2\pi}y\sigma}e^{-\frac{(\ln y -\mu)^2}{2\sigma^2}},&amp;amp;y&amp;gt;0\\
0, &amp;amp;y \le 0
\end{cases}\]&lt;/span> 即这个分布为对数正态分布&lt;span class="math">\(LN(\mu,\sigma^2)\)&lt;/span>&lt;/p>
&lt;p>命题2-3：随机变量&lt;span class="math">\(X\)&lt;/span>服从伽马分布&lt;span class="math">\(Ga(\alpha,\lambda)\)&lt;/span>，则当&lt;span class="math">\(k&amp;gt;0\)&lt;/span>时，有&lt;span class="math">\(Y=kX\thicksim Ga(\alpha,\lambda/k)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>以上三个命题都是单调函数变换，可以用&lt;code>定理1&lt;/code>证明。&lt;/p>
&lt;h4 id="贝叶斯假设的一个悖论">贝叶斯假设的一个悖论&lt;/h4>
&lt;p>随机变量函数的分布还曾被Fisher（发明F分布、Fisher信息量等，频率学派大佬）用来举例反对贝叶斯学派。迫使贝叶斯学派重新找角度解释先验分布的合理性。&lt;/p>
&lt;p>事情大体是这样的，贝叶斯学派在求解概率问题时，&lt;strong>先要规定主观规定一个先验分布，然后获取样本，再通过样本和条件概率修正先验分布，得到后验分布&lt;/strong>。其中，后面两步都是有扎实的数学理论支撑的，唯有第一步选取先验分布是一个凭个人主观推测的事情。贝叶斯采取了这样一个假设（贝叶斯假设）：&lt;em>如果我们对某一个统计量没有任何了解，那么我们就不应该对任何值有偏好，所以在统计量的空间里，选取均匀分布作为先验分布&lt;/em>。&lt;/p>
&lt;p>这个解释初一听，还是非常有道理的。但是Fisher提出：按照贝叶斯假设某一随机变量&lt;span class="math">\(\theta\)&lt;/span>的先验分布属于均匀分布，因为我们对其一无所知。那么对于随机变量&lt;span class="math">\(\beta=\theta^2\)&lt;/span>，我们对&lt;span class="math">\(\beta\)&lt;/span>也同样一无所知，那么&lt;span class="math">\(\beta\)&lt;/span>是不是应该也是均匀分布？两者同时是均匀分布显然是不可能的。（注1证明）。&lt;strong>到底谁应该是取均匀分布&lt;/strong>?这一悖论被Fisher用来反对贝叶斯学派的可靠性。&lt;/p>
&lt;blockquote>
&lt;p>注1：如果一随机变量&lt;span class="math">\(X\thicksim U(0,1)\)&lt;/span>，那么它的平方&lt;span class="math">\(Y=X^2\)&lt;/span>的概率密度函数pdf并不是均匀分布。&lt;/p>
&lt;p>证明：&lt;span class="math">\(X\in [0,1]\Rightarrow Y\in [0,1]\)&lt;/span> 累积分布函数CDF:&lt;span class="math">\(F_X(x) = x\)&lt;/span>. &lt;span class="math">\(F_Y(Y)=P(Y\le y)=P(X^2\le y)=P(-\sqrt{y}\le X\le\sqrt{y})\)&lt;/span>。因为&lt;span class="math">\(X\in [0,1]\)&lt;/span>，所以&lt;span class="math">\(P(-\sqrt{y}\le X\le\sqrt{y})=P(X\le\sqrt{y})=F_X(\sqrt{y})=\sqrt{y}\Rightarrow F_Y(y)=\sqrt{y}\)&lt;/span>。因此 &lt;span class="math">\[f_Y(y)=\begin{cases}\frac{1}{2\sqrt{y}},y\in [0,1]\\0,\text{others}\end{cases}\]&lt;/span> 显然&lt;span class="math">\(Y\)&lt;/span>并不是均匀分布。&lt;/p>
&lt;/blockquote>
&lt;h4 id="当gx为其他形式时">当&lt;span class="math">\(g(x)\)&lt;/span>为其他形式时&lt;/h4>
&lt;p>当&lt;code>定理1&lt;/code>不适用时，我们可以从最基础的&lt;strong>分布函数&lt;/strong>（&lt;span class="math">\(F_Y(y)=P(g(X)≤y)\)&lt;/span>）入手，就像证明&lt;code>定理1&lt;/code>中使用的方法那样。具体可见下面例子&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/随机变量函数的分布例3.jpg" alt="随机变量函数的分布例3" />&lt;p class="caption">随机变量函数的分布例3&lt;/p>
&lt;/div>
&lt;p>对照&lt;span class="math">\(\chi^2\)&lt;/span>分布的密度函数，可以看出&lt;span class="math">\(Y\thicksim \chi^2(1)\)&lt;/span>。（因为&lt;span class="math">\(\chi^2(n)\)&lt;/span>正是n个服从正态分布随机变量的平方和的分布）&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/随机变量函数的分布例4.jpg" alt="随机变量函数的分布例4" />&lt;p class="caption">随机变量函数的分布例4&lt;/p>
&lt;/div>
&lt;h2 id="多维随机变量函数的分布">多维随机变量函数的分布&lt;/h2>
&lt;p>设&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>为n维随机变量，则&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>的函数&lt;span class="math">\(Y=g(X_1,X_2,\dotsb,X_n)\)&lt;/span>时一维随机变量。现在问题是如何由&lt;span class="math">\((X_1,X_2,\dotsb,X_n)\)&lt;/span>的联合分布，求出&lt;span class="math">\(Y\)&lt;/span>的分布。这是一类技巧性很强的工作，不仅对离散场合和连续场合由不同的方法，而且对不同形式的函数&lt;span class="math">\(g(X_1,X_2,\dotsb,X_n)\)&lt;/span>要采用不同的方法，甚至有些方法只对特殊形式的&lt;span class="math">\(g(\cdot)\)&lt;/span>适用。下面就几个常见的场景做介绍。&lt;/p>
&lt;blockquote>
&lt;p>补充定义1：n维随机变量（向量）：如果&lt;span class="math">\(X_1(\omega),X_2(\omega),\dotsb,X_n(\omega)\)&lt;/span>是定义在&lt;strong>同一个样本空间&lt;/strong>&lt;span class="math">\(\Omega=\{\omega\}\)&lt;/span>上的n个随机变量，则称 &lt;span class="math">\[X(\omega)=(X_1(\omega),X_2(\omega),\dotsb,X_n(\omega))\]&lt;/span> 为n维随机变量（向量）。&lt;/p>
&lt;/blockquote>
&lt;p>注意，多维随机变量的关键是定义在&lt;strong>同一个样本空间&lt;/strong>，对于不同样本空间&lt;span class="math">\(\Omega_1,\Omega_2\)&lt;/span>上的两个随机变量。我们只能在其乘机空间&lt;span class="math">\(\Omega_1\times\Omega_2=\{(\omega_1,\omega_2):\omega_1\in \Omega_1,\omega_2\in\Omega_2\}\)&lt;/span>及其事件域上讨论。以下多维随机变量默认遵从这一点。&lt;/p>
&lt;h3 id="多维离散型随机变量函数的分布">多维离散型随机变量函数的分布&lt;/h3>
&lt;p>首先，如果离散随机变量概率空间比较小，可将&lt;span class="math">\(Y\)&lt;/span>的取值一一求出再合并得到分布列表。这是最直观最基本的方法。见下例：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/离散型随机变量函数的分布例5.jpg" alt="多维离散型随机变量分布例5" />&lt;p class="caption">多维离散型随机变量分布例5&lt;/p>
&lt;/div>
&lt;h4 id="离散型随机变量和的分布与卷积公式">离散型随机变量和的分布与卷积公式&lt;/h4>
&lt;p>下面我们讨论一种比较常见的情景，即多维离散型随机变量和的分布。我们先讨论两维随机变量，然后在拓展到多维。&lt;/p>
&lt;p>设随机变量&lt;span class="math">\(Z=X+Y\)&lt;/span>，其中&lt;span class="math">\(X,Y\)&lt;/span>都是在同一样本空间&lt;span class="math">\(\Omega\)&lt;/span>的一维离散型随机变量，事件&lt;span class="math">\(\{Z=k\}\)&lt;/span>可以由如下诸互不相容事件 &lt;span class="math">\[\{X=i,Y=k-i\}, i\in \Omega\]&lt;/span> 的并集组成，再考虑到&lt;span class="math">\(X,Y\)&lt;/span>的独立性，则对&lt;span class="math">\(\forall k \in \Omega_Z\)&lt;/span>，有 &lt;span class="math">\[P(Z=k)=\sum_{i\in\Omega}P(X=i)P(Y=k-i)\tag{4}\]&lt;/span> 这个概率等式被称为离散场合下的&lt;strong>卷积公式&lt;/strong>。我们在这里让&lt;span class="math">\(P(X=i)\)&lt;/span>的&lt;span class="math">\(i\in\Omega\)&lt;/span>，而对&lt;span class="math">\(P(Y=k-i)\)&lt;/span>则要求所有超出样本空间的值的概率都为0。&lt;/p>
&lt;p>从二维到多维的变换可以看出是一个逐步的过程，两两逐渐相加即可。&lt;span class="math">\(N\)&lt;/span>维离散随机变量的和在一般需要&lt;span class="math">\(N-1\)&lt;/span>次求和（本质是对和的&lt;span class="math">\(N-1\)&lt;/span>次分解）。&lt;/p>
&lt;h4 id="思考分布可加性的本质">思考：分布可加性的本质&lt;/h4>
&lt;p>通过离散场合下的卷积公式，我们可以证明如下三个命题：&lt;/p>
&lt;blockquote>
&lt;p>命题3-1：&lt;strong>泊松分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\thicksim P(\lambda_1),X_2\thicksim P(\lambda_2)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\thicksim P(\lambda_1+\lambda_2)\)&lt;/span>&lt;/p>
&lt;p>命题3-2：&lt;strong>二项分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\thicksim P(n,p),X_2\thicksim P(m,p)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\thicksim P(m+n,p)\)&lt;/span>&lt;/p>
&lt;p>命题3-3：&lt;strong>负二项分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\thicksim Nb(n,p),X_2\thicksim Nb(m,p)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\thicksim Nb(m+n,p)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>卷积显然和直接相加差别很大。那为什么有些分布能满足可加性呢？我们除了用形式化的数学方法证明，有什么本质能够理解本质的方法吗？&lt;/p>
&lt;p>其实回想我们在研究最基础的伯努利分布时，将N个伯努利分布叠加后，形成了二项分布&lt;span class="math">\(B(N,p)\)&lt;/span>，换句话说二项分布就是独立同分布(i.i.d)的伯努利分布加出来的，因此多几个伯努力分布相加还是二项分布，只是二项分布的参数有所改变。这是二项分布可加性的来源。而泊松分布是二项分布在&lt;span class="math">\(\lambda=np\)&lt;/span>为定值时, &lt;span class="math">\(p\rightarrow 0, n\rightarrow\infty\)&lt;/span>的极限，因此本质也是一种二项分布，不难理解为什么泊松分布也有可加性了。此外，作为二项分布&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>的极限的正态分布，自然也是有可加性的，但是这已经拓展到连续分布了。（&lt;span class="math">\(e\)&lt;/span>是联系离散和连续的桥梁之一，这也是从离散的二项分布到正态分布表达式突然多出自然常数的一个暗示）。而其它具有可加性的离散分布，比如负二项分布是由N个独立同分布的几何分布加出来的，带有可加性也算是自然而然了。&lt;/p>
&lt;p>在连续分布中，也有一些分布是“加”出来的。最常见、应用最广泛的是&lt;strong>指数分布&lt;span class="math">\(X\thicksim \exp(\lambda)\)&lt;/span>的和&lt;/strong>，k个指数分布&lt;span class="math">\(X_i\thicksim \exp(\lambda),i\in\{1,2,\dotsb,k\}\)&lt;/span>相加是Erlang分布 &lt;span class="math">\[
X_{i}\sim \exp (\lambda ),\\
\sum_{i=1}^{k}{X_{i}}\sim \operatorname {Erlang} (k,\lambda )\Rightarrow p(x;k,\lambda )=\sum_{i=1}^{k}{X_{i}}\sim \operatorname {Erlang} (k,\lambda )\\
p(x;k,\lambda )={\lambda ^{k}x^{{k-1}}e^{{-\lambda x}} \over (k-1)!}\quad {\text{for }}x,\lambda \geq 0,k\geq 1
\]&lt;/span> 它和指数分布的参数&lt;span class="math">\(\lambda\)&lt;/span>是一样的，参数&lt;span class="math">\(k\)&lt;/span>是指由&lt;span class="math">\(k\)&lt;/span>个指数分布相加。其实际含义可以指代&lt;span class="math">\(k\)&lt;/span>个用户/物件到达所用的时间间隔等。显然，有指数分布加出来的Erlang分布也有可加性，无非就是多几个指数分布的和。而将&lt;span class="math">\(k\rightarrow \alpha\)&lt;/span>延拓到正实数域，就是Gamma分布&lt;span class="math">\(X\sim \Gamma(\alpha,\lambda)\)&lt;/span>。 &lt;span class="math">\[p(x;\alpha ,\lambda )={\frac {\lambda ^{\alpha }x^{\alpha -1}e^{-\lambda x}}{\Gamma (\alpha )}}\quad {\text{for }}x&amp;gt;0\quad \alpha ,\lambda &amp;gt;0\]&lt;/span> 其中,&lt;span class="math">\(\Gamma(\alpha)\)&lt;/span>是Gamma函数而卡方分布又是Gamma分布的特例，即&lt;span class="math">\(X\sim \chi^2(n)=\Gamma(\frac{n}{2},\frac{1}{2})\)&lt;/span>， &lt;span class="math">\[
p(x;n)={\frac {1}{2^{\frac {n}{2}}\Gamma(\frac{n}{2})}}x^{\frac {n}{2}-1}e^{\frac {-x}{2}}
\]&lt;/span> 因此这两种分布(Gamma分布、卡方分布)也不出意外的有可加性。还有一种满足可加性的分布，柯西分布，对它我了解不多，暂不描述。&lt;/p>
&lt;h3 id="多维随机变量的最大最小值分布">多维随机变量的最大最小值分布&lt;/h3>
&lt;p>最大值最小值的分布利用了分布函数和多个随机变量间的独立性，是利用定义就能搞定的多维分布。&lt;/p>
&lt;h4 id="最大值分布">最大值分布&lt;/h4>
&lt;blockquote>
&lt;p>命题4：设&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>是相互独立的n个随机变量，若&lt;span class="math">\(Y=\max\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>。则对于&lt;span class="math">\(Y\)&lt;/span>的分布有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X_i\sim F_i(x)，,i=1,2,\dotsb,n, Y\sim \prod\limits_{i=1}^n F_i(y)\)&lt;/span>;&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>i.i.d，即&lt;span class="math">\(X_i\sim F(x)\)&lt;/span>，则有&lt;span class="math">\(Y\sim [F(y)]^n\)&lt;/span>&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>为连续随机变量，且i.i.d，则&lt;span class="math">\(Y\)&lt;/span>的概率密度函数是&lt;span class="math">\(P_Y(y)=n[F(y)]^{n-1}p(y)\)&lt;/span>&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>都服从&lt;span class="math">\(X\sim \exp(\lambda)\)&lt;/span>，则&lt;span class="math">\(Y\)&lt;/span>的概率密度函数是 &lt;span class="math">\[p_Y(y)=\begin{cases}0,&amp;amp;y&amp;lt;0\\n(1-e^{-\lambda y})^{n-1}\lambda e^{-\lambda y},&amp;amp;y\ge 0\end{cases}\]&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>证明： (1) 有&lt;span class="math">\(Y=\max\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>，则&lt;span class="math">\(F_Y(y)=P(max\{X_1,X_2,\dotsb,X_n\}≤y)=P(X_1≤y,X_2≤y,\dotsb,X_n≤y)\stackrel{\text{独立性}}{=}P(X_1≤y)P(X_2≤y)\dotsb P(X_n≤y)=\prod\limits_{i=1}^n F_i(y)\)&lt;/span>。即&lt;span class="math">\(Y\sim \prod\limits_{i=1}^n F_i(y)\)&lt;/span>&lt;/p>
&lt;p>(2)因为诸&lt;span class="math">\(X_i\)&lt;/span>i.i.d，所以&lt;span class="math">\(\prod\limits_{i=1}^n F_i(y)=[F(y)]^n\)&lt;/span>，即&lt;span class="math">\(Y \sim [F(y)]^{n}\)&lt;/span> (3)将结果(2)的求导即可得证。 (4)指数分布符合(3)的前提，可以直接带入(3)的公式可证。&lt;/p>
&lt;h4 id="最小值分布">最小值分布&lt;/h4>
&lt;blockquote>
&lt;p>命题5：设&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>是相互独立的n个随机变量，若&lt;span class="math">\(Y=\min\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>。则对于&lt;span class="math">\(Y\)&lt;/span>的分布有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X_i\sim F_i(x)，,i=1,2,\dotsb,n, Y\sim 1-\prod\limits_{i=1}^n [1-F_i(y)]\)&lt;/span>;&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>i.i.d，即&lt;span class="math">\(X_i\sim F(x)\)&lt;/span>，则有&lt;span class="math">\(Y\sim 1-[1-F(y)]^n\)&lt;/span>&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>为连续随机变量，且i.i.d，则&lt;span class="math">\(Y\)&lt;/span>的概率密度函数是&lt;span class="math">\(P_Y(y)=n[1-F(y)]^{n-1}p(y)\)&lt;/span>&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>都服从&lt;span class="math">\(X\sim \exp(\lambda)\)&lt;/span>，则&lt;span class="math">\(Y\)&lt;/span>的概率密度函数是 &lt;span class="math">\[p_Y(y)=\begin{cases}0,&amp;amp;y&amp;lt;0\\n\lambda e^{-n\lambda y},&amp;amp;y\ge 0\end{cases}\]&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>证明： (1)由于&lt;span class="math">\(Y=\min\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>，因此&lt;span class="math">\(F_Y(y)=P(\min\{X_1,X_2,\dotsb X_n\}≤y)=1-P(\min\{X_1,X_2,\dotsb X_n\}&amp;gt;y)=1-P(X_1&amp;gt;y,X_2&amp;gt;y,\dotsb,X_n&amp;gt;y)\stackrel{\text{独立性了}}{=}1-P(X_1&amp;gt;y)P(X_2&amp;gt;y)\dotsb P(X_n&amp;gt;y)=1-\prod\limits_{i=1}^n[1-F_i(y)]\)&lt;/span> (2)因为诸&lt;span class="math">\(X_i\)&lt;/span>i.i.d，即&lt;span class="math">\(X_i\sim F(x)\)&lt;/span>，代入(1)的结果则有&lt;span class="math">\(Y\sim 1-[1-F(y)]^n\)&lt;/span> (3)将结果(2)的求导即可得证。 (4)指数分布符合(3)的前提，可以直接带入(3)的公式可证。&lt;/p>
&lt;h3 id="多维连续型随机变量函数的分布">多维连续型随机变量函数的分布&lt;/h3>
&lt;h4 id="连续型随机变量和的分布与卷积公式">连续型随机变量和的分布与卷积公式&lt;/h4>
&lt;p>我们之前已经研究了离散型多维随机变量的卷积公式，对于连续型多维随机变量，我们采用类似的方式推导，只是把求和换成积分。&lt;/p>
&lt;blockquote>
&lt;p>连续型随机变量的卷积公式：设&lt;span class="math">\(X,Y\)&lt;/span>是两个连续且独立的随机变量，其密度函数分别是&lt;span class="math">\(p_{_X}(x),p_{_Y}(y)\)&lt;/span>，则其和&lt;span class="math">\(Z=X+Y\)&lt;/span>的概率密度函数为： &lt;span class="math">\[\begin{aligned}p_{_Z}(z)&amp;amp;=\int_{-\infty}^\infty p_{_X}(x)p_{_Y}(z-x)dx\\
&amp;amp;=\int_{-\infty}^\infty p_{_X}(z-y)p_{_Y}(y)dy
\end{aligned}\tag{5}\]&lt;/span> 上式被称为连续随机变量的卷积公式。&lt;/p>
&lt;/blockquote>
&lt;p>证明： &lt;span class="math">\(Z=X+Y\)&lt;/span>的分布函数按照定义为 &lt;span class="math">\[\begin{aligned}
F_Z(z)&amp;amp;=P(X+Y≤Z)=\iint_{x+y≤z}p_{_X}(x)p_{_Y}(y)dxdy\\
&amp;amp;=\int_{-\infty}^\infty\{\int_{-\infty}^{z-y}p_{_X}(x)dx\}p_{_Y}(y)dy\\
&amp;amp;\xlongequal{变量替换}\int_{-\infty}^\infty\int_{-\infty}^z p_{_X}(t-y)p_{_Y}(y)dtdy\\
\Rightarrow F_{Z}(z)&amp;amp;=\int_{-\infty}^z(\int_{-\infty}^∞ p_{_X}(t-y)p_{_Y}(y)dy)dt
\end{aligned}
\]&lt;/span> 对&lt;span class="math">\(F_{Z}(z)\)&lt;/span>以&lt;span class="math">\(z\)&lt;/span>求导，可得 &lt;span class="math">\[
p_{_Z}(z)=\int_{-\infty}^∞ p_{_X}(z-y)p_{_Y}(y)dy
\]&lt;/span> 令上式积分中&lt;span class="math">\(y=z-x\)&lt;/span>则可得： &lt;span class="math">\[
p_{_Z}(z)=\int_{-\infty}^∞ p_{_X}(x)p_{_Y}(z-x)dx
\]&lt;/span> 得证。&lt;/p>
&lt;p>在之前&lt;a href="#思考分布可加性的本质">思考：分布可加性的本质&lt;/a>章节中我们已经讨论了为什么有些分布具有可加性，也涉及了部分连续性随机分布，这里我们给出详细命题：&lt;/p>
&lt;blockquote>
&lt;p>命题6-1：&lt;strong>正态分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\sim N(\mu_1,\sigma_1^2),Y\sim N(\mu_2,\sigma_2^2)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)\)&lt;/span>&lt;/p>
&lt;p>命题6-2：&lt;strong>伽马分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\sim Ga(\alpha_1,\lambda),Y\sim Ga(\alpha_2,\lambda)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\sim Ga(\alpha_1+\alpha_2,\lambda)\)&lt;/span>&lt;/p>
&lt;p>命题6-3：&lt;strong>卡方分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\sim \chi^2(m),Y\sim \chi^2(n)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\sim \chi^2(m+n)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明： (1):正态分布的可加性。首先&lt;span class="math">\(Z=X+Y\)&lt;/span>的定义域依然是&lt;span class="math">\((-∞,∞)\)&lt;/span>,利用连续随机变量的卷积公式&lt;span class="math">\((5)\)&lt;/span>可得： &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\int_{-∞}^∞\frac{1}{2\pi\sigma_1\sigma_2}\exp\left\{ -\frac{1}{2}\left[\frac{(z-y-\mu_1)^2}{\sigma_1^2}+\frac{(y-\mu_2)^2}{\sigma_2^2} \right]\right\}dy\\
&amp;amp;\xlongequal[u=y-\mu_2]{v=z-(\mu_1+\mu_2)}\frac{1}{2\pi\sigma_1\sigma_2}\int_{-∞}^∞\exp\left\{\frac{1}{2}\left[\frac{(v-u)^2}{\sigma_1^2}+\frac{u^2}{\sigma_2^2} \right]\right\}dy\\
&amp;amp;=\frac{1}{2\pi\sigma_1\sigma_2}\int_{-∞}^∞\exp\left\{\frac{1}{2}\left[\frac{(v-u)^2}{\sigma_1^2}+\frac{u^2}{\sigma_2^2} \right]\right\}du\\
\end{aligned}
\]&lt;/span> 由于变量&lt;span class="math">\(v\)&lt;/span>与概率密度函数自变量&lt;span class="math">\(z\)&lt;/span>有关，而与积分变量&lt;span class="math">\(y\)&lt;/span>无关，因此，我们将与&lt;span class="math">\(u\)&lt;/span>无关的&lt;span class="math">\(v\)&lt;/span>从积分符号中提取出来，再把&lt;span class="math">\(u\)&lt;/span>凑成平方项，方可化腐朽为神奇 。同时，&lt;span class="math">\(u=y-\mu_1\in(-∞,∞)\)&lt;/span>，积分区间未变。 &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\frac{1}{2\pi\sigma_1\sigma_2}\int_{-∞}^∞\exp\left\{-\frac{1}{2}\left[\frac{\sigma_1^2+\sigma_2^2}{\sigma_1^2\sigma_2^2}u^2-\frac{2}{\sigma_1^2}uv+\frac{v^2}{\sigma_1^2} \right]\right\}du\\
\overset{u凑平方}{=}&amp;amp;\frac{1}{2\pi\sigma_1\sigma_2}\int_{-∞}^∞\exp\left\{-\frac{1}{2}\left[\frac{\sigma_1^2+\sigma_2^2}{\sigma_1^2\sigma_2^2}u^2-\frac{2}{\sigma_1^2}uv+\frac{\sigma_2^2}{\sigma_1^2(\sigma_1^2+\sigma_2^2)}v^2\right.\right. \\
&amp;amp;\left.\left.-\frac{\sigma_2^2}{\sigma_1^2(\sigma_1^2+\sigma_2^2)}v^2+\frac{v^2}{\sigma_1^2} \right]\right\}du\\
&amp;amp;=\frac{1}{2\pi\sigma_1\sigma_2}\exp\left\{-\frac{1}{2}\frac{v^2}{\sigma_1^2+\sigma_2^2}\right\}\\
&amp;amp;\cdot\int_{-∞}^∞\exp\left\{-\frac{1}{2}\left(\frac{\sqrt{\sigma_1^2+\sigma_2^2}}{\sigma_1\sigma_2}u-\frac{\sigma_2}{\sigma_1\sqrt{\sigma_1^2+\sigma_2^2}}v\right)^2\right\}du
\end{aligned}
\]&lt;/span> 令&lt;span class="math">\(t=\frac{\sqrt{\sigma_1^2+\sigma_2^2}}{\sigma_1\sigma_2}u-\frac{\sigma_2}{\sigma_1\sqrt{\sigma_1^2+\sigma_2^2}}v\)&lt;/span>，显然&lt;span class="math">\(v\)&lt;/span>无论取什么值，在&lt;span class="math">\(u\in(-\infty,\infty)\)&lt;/span>时，都有&lt;span class="math">\(t\in (-\infty,\infty)\)&lt;/span>。同时，&lt;span class="math">\(du=\frac{\sigma_1\sigma_2}{\sqrt{\sigma_1^2+\sigma_2^2}}dt\)&lt;/span>。因此： &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\frac{1}{2\pi\sigma_1\sigma_2}\exp\left\{-\frac{1}{2}\frac{v^2}{\sigma_1^2+\sigma_2^2}\right\}\int_{-\infty}^\infty\exp \{-\frac{1}{2}t^2\}\frac{\sigma_1\sigma_2}{\sqrt{\sigma_1^2+\sigma_2^2}}dt\\
&amp;amp;=\frac{1}{2\pi\sqrt{\sigma_1^2+\sigma_2^2}}\exp\left\{-\frac{1}{2}\frac{v^2}{\sigma_1^2+\sigma_2^2}\right\}\int_{-\infty}^\infty\exp \{-\frac{1}{2}t^2\}dt
\end{aligned}
\]&lt;/span> 根据高斯积分有&lt;span class="math">\(\int_{-\infty}^\infty\exp \{-\frac{1}{2}t^2\}dt=\sqrt{2\pi}\)&lt;/span>，代入上式，并恢复&lt;span class="math">\(v=z-(\mu_1+\mu_2)\)&lt;/span>，则有 &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\frac{1}{2\pi\sqrt{\sigma_1^2+\sigma_2^2}}\exp\left\{-\frac{1}{2}\frac{(z-\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}\right\}\cdot \sqrt{2\pi}\\
&amp;amp;=\frac{1}{2\pi\sqrt{\sigma_1^2+\sigma_2^2}}\exp\left\{-\frac{(z-\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}\right\}\\
&amp;amp;\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)
\end{aligned}
\]&lt;/span> 得证。此命题还可扩展成线性可加性：&lt;/p>
&lt;blockquote>
&lt;p>命题6-4：任意n个相互独立的正态随机变量的线性组合仍是正态随机变量。即若&lt;span class="math">\(X_i\sim N(\mu_i,\sigma_i^2),i=1,2,\dotsb,n\)&lt;/span>，则&lt;span class="math">\(Y=\sum_{i=1}^n a_i X_i +b \sim N(\sum_{i=1}^n a_i \mu_i +b,\sum_{i=1}^n a_i^2 \sigma_i^2)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>(2)：证明伽马分布的可加性。首先指出&lt;span class="math">\(Z=X+Y\)&lt;/span>的取值范围仍然在&lt;span class="math">\((0,∞)\)&lt;/span>,所以当&lt;span class="math">\(z&amp;lt;0\)&lt;/span>时，有&lt;span class="math">\(p_{_Z}(z)=0\)&lt;/span>。而当&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时，可用卷积公式，此时被积函数&lt;span class="math">\(p_{_X}(z-y)p_{_Y}(y)\)&lt;/span>的非零区域为&lt;span class="math">\(0&amp;lt;y&amp;lt;z\)&lt;/span>,故 &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\int_{-\infty}^\infty p_{_Y}(y)p_{_X}(z-y)dy\\
&amp;amp;=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\int_0^z (z-y)^{\alpha_1-1}e^{-\lambda(z-y)}y^{\alpha_2-1}e^{-\lambda y} dy \\
&amp;amp;=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)} e^{-\lambda z} \int_0^z(z-y)^{\alpha_1-1}y^{\alpha_2-1}dy\\
\end{aligned}
\]&lt;/span> 令&lt;span class="math">\(y=zt,t\in(0,1)\)&lt;/span>，则&lt;span class="math">\(dy=zdt\)&lt;/span>带入原式可得 &lt;span class="math">\[
p_{_Z}(z)=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)} e^{-\lambda z} z^{\alpha_1+\alpha_2-1}\underbrace{\int_0^1(1-t)^{\alpha_1-1}t^{\alpha_2-1}dt}_{\text{贝塔函数}}
\]&lt;/span> 我们看到这个积分的式子和贝塔函数是一模一样的，贝塔函数&lt;span class="math">\(B(\alpha_1,\alpha_2)=\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)}{\Gamma(\alpha_1+\alpha_2)}\)&lt;/span>。最后，相乘我们得到： &lt;span class="math">\[
p_{_Z}(z)=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1+\alpha_2)} z^{\alpha_1+\alpha_2-1}e^{-\lambda z}\sim Ga(\alpha_1+\alpha_2,\lambda)
\]&lt;/span> 显然，这个结论可以推广到有限个尺度参数相同的独立伽马变量之和上。此外，在Gamma分布中，当我们令&lt;span class="math">\(\alpha=1\)&lt;/span>时，有&lt;span class="math">\(Ga(1,\lambda)=\exp(\lambda)\)&lt;/span>，那么我们可以做出如下命题：&lt;/p>
&lt;blockquote>
&lt;p>命题6-5：n个独立同分布的指数分布随机变量&lt;span class="math">\(X_i\sim \exp(\lambda),i=1,2,\dotsb,n\)&lt;/span>之和为Gamma分布，即&lt;span class="math">\(\sum_{i=1}^nX_i\sim \underbrace{\exp(\lambda)*\exp(\lambda)*\dotsb*\exp(\lambda)}_{n个}=Ga(\underbrace{1+1+\dotsb+1}_{n个},\lambda)=Ga(n,\lambda)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>(3)：由于卡方分布时伽马分布&lt;span class="math">\(\alpha=\frac{n}{2},\lambda=\frac{1}{2}\)&lt;/span>时的特例，因此有(2)的证明可知命题6-3也成立。&lt;/p>
&lt;h3 id="变量变换法">变量变换法&lt;/h3>
&lt;p>变量变换法是对于从&lt;span class="math">\(n\rightarrow n\)&lt;/span>个随机变量函数变换的概率分布的描述，和单个随机变量函数分布一样，也是&lt;strong>利用了反函数的特性，只不过求导变成了求多个偏导的雅各布行列式&lt;/strong>。在此我们仅介绍二维随机变量的变量变换的方法，更高维的方法也是类似的。&lt;/p>
&lt;p>设二维随机变量&lt;span class="math">\((X,Y)\)&lt;/span>的联合密度函数为&lt;span class="math">\(p(x,y)\)&lt;/span>,那么如果函数 &lt;span class="math">\[
\begin{cases} u = g_1(x,y)\\ v=g_2(x,y)\end{cases}
\]&lt;/span> 有连续偏导数，且存在唯一的反函数： &lt;span class="math">\[
\begin{cases} x = x(u,v)\\ y=y(x,y)\end{cases}
\]&lt;/span> 然后我们可以列出雅克比行列式（其中的第二项倒数也告诉我们如果一方的偏导数不好求，可以求其反函数的偏导数雅克布行列式再取倒数）： &lt;span class="math">\[
J= \frac{\partial(x,y)}{\partial(u,v)}=(\frac{\partial(u,v)}{\partial(x,y)})^{-1}=\begin{vmatrix}
\frac{\partial x}{\partial u} &amp;amp; \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} &amp;amp; \frac{\partial y}{\partial x}
\end{vmatrix}\neq 0
\]&lt;/span> 则二维随机变量&lt;span class="math">\((U,V)\)&lt;/span>的联合密度函数为 &lt;span class="math">\[
p(u,v)=p(x(u,v),y(u,v))|J|\tag{6}
\]&lt;/span> 此法为二维随机变量的变量变换法，其证明可参阅二重积分的坐标变换法。&lt;/p>
&lt;h4 id="box-muller变换">Box-muller变换&lt;/h4>
&lt;p>Box-muller变换在计算机领域是一个重要的变换，它能够用两个均匀分布生成正态分布，其数学原理就是变量变换法。我们通过证明以下命题：&lt;/p>
&lt;blockquote>
&lt;p>命题7：若两个独立的随机变量&lt;span class="math">\(U_1,U_2\)&lt;/span>都服从均匀分布&lt;span class="math">\(U(0,1)\)&lt;/span>，则其组成二维函数组的二维随机变量 &lt;span class="math">\[\begin{cases}
X=\cos(2\pi U_1)\sqrt{-2\ln U_2}\\
Y=\sin(2\pi U_1)\sqrt{-2\ln U_2}
\end{cases}\]&lt;/span> 都服从标准正态分布&lt;span class="math">\(N(0,1)\)&lt;/span>，即&lt;span class="math">\(X,Y\sim N(0,1)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>根据公式&lt;span class="math">\((6)\)&lt;/span>，根据二维均匀分布的概率密度函数有： &lt;span class="math">\[
\begin{aligned}
p(x,y)&amp;amp;=p(u_1(x,y),u_2(x,y))|J|\\
&amp;amp;= 1 \times |\begin{vmatrix}
\frac{\partial u_1}{\partial x}&amp;amp;\frac{\partial u_1}{\partial y}\\
\frac{\partial u_2}{\partial x}&amp;amp;\frac{\partial u_2}{\partial y}
\end{vmatrix}|
\end{aligned}
\]&lt;/span> 而根据&lt;span class="math">\((X,Y)\)&lt;/span>的表达式，其反函数为： &lt;span class="math">\[
\begin{cases}
U_1=\frac{1}{2\pi}\arctan(Y/X)\\
U_2=e^{-\frac{X^2+Y^2}{2}}\\
\end{cases}
\]&lt;/span> 将其代入雅可比行列式为有： &lt;span class="math">\[
\begin{aligned}
p(x,y)&amp;amp;=|\begin{vmatrix}
\frac{\partial u_1}{\partial x}&amp;amp;\frac{\partial u_1}{\partial y}\\
\frac{\partial u_2}{\partial x}&amp;amp;\frac{\partial u_2}{\partial y}
\end{vmatrix}|\\
&amp;amp;=|\begin{vmatrix}
\frac{1}{2\pi}\frac{-y}{x^2+y^2}&amp;amp;\frac{1}{2\pi}\frac{x}{x^2+y^2}\\
-xe^{-\frac{x^2+y^2}{2}}&amp;amp;-ye^{-\frac{x^2+y^2}{2}}\\
\end{vmatrix}|\\
&amp;amp;=\frac{y^2}{2\pi(x^2+y^2)}e^{-\frac{x^2+y^2}{2}}+\frac{y^2}{2\pi(x^2+y^2)}e^{-\frac{x^2+y^2}{2}}\\
&amp;amp;=\frac{1}{2\pi}e^{-\frac{x^2+y^2}{2}}=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}
\end{aligned}
\]&lt;/span> 分别求&lt;span class="math">\(x,y\)&lt;/span>的边际分布可证&lt;span class="math">\(X,Y\sim N(0,1)\)&lt;/span>。注意，本次证明省略了定义域的说明，不过由其函数关系可知&lt;span class="math">\(X,Y\in(-∞,∞)\)&lt;/span>。&lt;/p>
&lt;p>得到标准正态分布函数后，可以通过正态分布的线性变换得到其他参数的正态分布函数随机变量。&lt;/p>
&lt;h3 id="增补变量法">增补变量法&lt;/h3>
&lt;p>增补变量本质是变量变换法的一个推广：为了求出二维连续随机变量&lt;span class="math">\((X,Y)\)&lt;/span>只有一个函数&lt;span class="math">\(U=g(X,Y)\)&lt;/span>的密度函数，增补一个新的随机变量&lt;span class="math">\(V=h(X,Y)\)&lt;/span>，转化成变量变换法的场景，在通过一般的变量变换法解决。为了方便求解，我们通常令&lt;span class="math">\(V=X\)&lt;/span>或&lt;span class="math">\(V=Y\)&lt;/span>。先用变量变换法求出&lt;span class="math">\((U,V)\)&lt;/span>的联合密度函数&lt;span class="math">\(p(u,v)\)&lt;/span>，再对&lt;span class="math">\(p(u,v)\)&lt;/span>关于&lt;span class="math">\(v\)&lt;/span>积分，从而得出关于&lt;span class="math">\(U\)&lt;/span>的边际密度函数。&lt;/p>
&lt;h4 id="二维随机变量积的分布">二维随机变量积的分布&lt;/h4>
&lt;blockquote>
&lt;p>二维随机变量积的公式：设随机变量&lt;span class="math">\(X,Y\)&lt;/span>相互独立，其密度函数分别为&lt;span class="math">\(p_{_X}(x),p_{_Y}(y)\)&lt;/span>，则&lt;span class="math">\(U=XY\)&lt;/span>的密度函数为： &lt;span class="math">\[p_{_U}(u)=\int_{-∞}^∞p_{_X}(\frac{u}{v})p_{_Y}(v)\frac{1}{|v|}dv\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>第一步：增补变量，记&lt;span class="math">\(V=Y\)&lt;/span>，则&lt;span class="math">\(\begin{cases}u=xy\\v=y\end{cases}\)&lt;/span>，其反函数为&lt;span class="math">\(\begin{cases}x=u/v\\y=v\end{cases}\)&lt;/span>&lt;/p>
&lt;p>第二步：通过变量变换法求出&lt;span class="math">\((U,V)\)&lt;/span>的联合密度函数。记住雅可比行列式要取绝对值。 &lt;span class="math">\[
p(u,v)=p_{_X}(\frac{u}{v})p_{_Y}(v)|\begin{vmatrix}
\frac{1}{v}&amp;amp;\frac{-u}{v^2}\\
0&amp;amp;1
\end{vmatrix}|=p_{_X}(\frac{u}{v})p_{_Y}(v)\frac{1}{|v|}
\]&lt;/span>&lt;/p>
&lt;p>第三步：对&lt;span class="math">\(v\)&lt;/span>积分，求&lt;span class="math">\(u\)&lt;/span>的边际分布： &lt;span class="math">\[p_{_U}(u)=\int_{-∞}^∞p_{_X}(\frac{u}{v})p_{_Y}(v)\frac{1}{|v|}dv\]&lt;/span> 得证。&lt;/p>
&lt;h4 id="二维随机变量商的分布">二维随机变量商的分布&lt;/h4>
&lt;blockquote>
&lt;p>二维随机变量商的公式：设随机变量&lt;span class="math">\(X,Y\)&lt;/span>相互独立，其密度函数分别为&lt;span class="math">\(p_{_X}(x),p_{_Y}(y)\)&lt;/span>，则&lt;span class="math">\(U=X/Y\)&lt;/span>的密度函数为： &lt;span class="math">\[p_{_U}(u)=\int_{-∞}^∞p_{_X}(uv)p_{_Y}(v)|v|dv\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>第一步：增补变量，记&lt;span class="math">\(V=Y\)&lt;/span>，则&lt;span class="math">\(\begin{cases}u=x/y\\v=y\end{cases}\)&lt;/span>，其反函数为&lt;span class="math">\(\begin{cases}x=uv\\y=v\end{cases}\)&lt;/span>&lt;/p>
&lt;p>第二步：通过变量变换法求出&lt;span class="math">\((U,V)\)&lt;/span>的联合密度函数。记住雅可比行列式要取绝对值。 &lt;span class="math">\[
p(u,v)=p_{_X}(uv)p_{_Y}(v)|\begin{vmatrix}
v&amp;amp;u\\
0&amp;amp;1
\end{vmatrix}|=p_{_X}(uv)p_{_Y}(v){|v|}
\]&lt;/span>&lt;/p>
&lt;p>第三步：对&lt;span class="math">\(v\)&lt;/span>积分，求&lt;span class="math">\(u\)&lt;/span>的边际分布： &lt;span class="math">\[p_{_U}(u)=\int_{-∞}^∞p_{_X}(uv)p_{_Y}(v){|v|}dv\]&lt;/span> 得证。&lt;/p>
&lt;h4 id="二维随机变量积和商的分布直接推导">二维随机变量积和商的分布直接推导&lt;/h4>
&lt;p>上面两个说的都是两个独立的随机变量&lt;span class="math">\(X,Y\)&lt;/span>，实际上只要知道两个随机变量的联合概率分布，即使不是独立的，也有一样的结论。&lt;/p>
&lt;blockquote>
&lt;p>设二维随机变量&lt;span class="math">\((X,Y)\)&lt;/span>的联合概率密度为&lt;span class="math">\(f(x,y)\)&lt;/span>，那么二者的商的分布&lt;span class="math">\(Z=X/Y\)&lt;/span>的概率密度函数为：&lt;span class="math">\(p_{_Z}(z)=\int_{-\infty}^\infty f(zy,y)|y|\mathrm{d}y\)&lt;/span>。显然当&lt;span class="math">\(X,Y\)&lt;/span>独立时，有&lt;span class="math">\(f(zy,y)=p_{_X}(zy)p_{_Y}(y)\)&lt;/span>。&lt;/p>
&lt;p>设二维随机变量&lt;span class="math">\((X,Y)\)&lt;/span>的联合概率密度为&lt;span class="math">\(p(x,y)\)&lt;/span>，那么二者的积的分布&lt;span class="math">\(Z=XY\)&lt;/span>的概率密度函数为：&lt;span class="math">\(p_Z(z)=\int_{-\infty}^\infty f(\frac{z}{y},y)|\frac{1}{y}|\mathrm{d}y\)&lt;/span>。显然当&lt;span class="math">\(X,Y\)&lt;/span>独立时，有&lt;span class="math">\(f(\frac{z}{y},y)=p_{_X}(\frac{z}{y})p_{_Y}(y)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>我们先从随机变量商的分布开始证明。利用分布函数&lt;span class="math">\(F_{_Z}(z)\)&lt;/span>的定义，我们有： &lt;span class="math">\[
F_{_Z}(z)=P(Z\leq z)=P(\frac{X}{Y}\leq z)=\iint\limits_{\frac{x}{y}&amp;lt;z}f(x,y)\mathrm{d}x\mathrm{d}y
\]&lt;/span> 关键就是研究这个二重积分。我们先得确定积分范围。在&lt;span class="math">\(x/y\leq z\)&lt;/span>的场景中，&lt;span class="math">\(z\)&lt;/span>是一个给定的常数，因此积分的范围会被直线&lt;span class="math">\(x=zy\)&lt;/span>划分，针对&lt;span class="math">\(z\)&lt;/span>是否正负，还需要分成两种情况考虑：&lt;span class="math">\(z&amp;lt;0\)&lt;/span>和&lt;span class="math">\(z\geq 0\)&lt;/span>。&lt;/p>
&lt;p>当&lt;span class="math">\(z&amp;lt;0\)&lt;/span>时，&lt;span class="math">\(x/y\)&lt;/span>小于一个负数，那么二者必然一正一负，积分区域只能在第二、四象限；为了方便进一步确定积分范围，我们使用一个不严谨但是快速的方法。由于&lt;span class="math">\(-\infty&amp;lt; z\)&lt;/span>，那么&lt;span class="math">\(x\rightarrow -\infty,y&amp;gt;0\)&lt;/span>所在范围必然位于积分区域，即&lt;span class="math">\(x/y=z&amp;lt;0\)&lt;/span>在第二象限直线下方部分位于积分区域。同样的，&lt;span class="math">\(x\rightarrow \infty,y&amp;lt;0\)&lt;/span>所在范围必然位于积分区域，即&lt;span class="math">\(x/y=z&amp;lt;0\)&lt;/span>在第四象限直线上方部分位于积分区域。综上&lt;strong>得到&lt;span class="math">\(z&amp;lt;0\)&lt;/span>时下左图的红色阴影积分区域&lt;/strong>。&lt;/p>
&lt;p>当&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时，&lt;span class="math">\(x/y\)&lt;/span>小于一个正数，那么第二、四象限的&lt;span class="math">\(x/y\)&lt;/span>都是负数，必然属于积分区域；再看第一、三象限。采样上述类似的快速判断方法，由于&lt;span class="math">\(0&amp;lt;z\)&lt;/span>，那么&lt;span class="math">\(y\rightarrow \infty, x&amp;gt;0\)&lt;/span>所在范围必然位于积分区域，即&lt;span class="math">\(x/y=z&amp;gt;0\)&lt;/span>在第一象限直线上方部分位于积分区域。同样的，&lt;span class="math">\(y\rightarrow -\infty,x&amp;lt;0\)&lt;/span>所在范围必然位于积分区域，即&lt;span class="math">\(x/y=z&amp;gt;0\)&lt;/span>在第三象限直线下方部分位于积分区域。综上&lt;strong>得到&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时下右图的红色阴影积分区域&lt;/strong>。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/随机变量函数商的分布.jpg" alt="随机变量函数商的分布.jpg" />&lt;p class="caption">随机变量函数商的分布.jpg&lt;/p>
&lt;/div>
&lt;p>确定好了积分区域，我们下面选择积分次序。对于二重积分，可以先对&lt;span class="math">\(x\)&lt;/span>积分，也可以先对&lt;span class="math">\(y\)&lt;/span>积分，主要看那个方便。我们可以把两个次序的积分先写出来，比较一下。&lt;/p>
&lt;p>如果先对&lt;span class="math">\(y\)&lt;/span>积分，在对&lt;span class="math">\(x\)&lt;/span>积分。那么&lt;span class="math">\(z&amp;lt;0\)&lt;/span>和&lt;span class="math">\(z&amp;gt;0\)&lt;/span>的积分公式如下： &lt;span class="math">\[
\begin{aligned}
&amp;amp;z&amp;lt;0\\
&amp;amp;P(\frac{X}{Y}\leq z)=\int_{-\infty}^0\int_0^{x/z}f(x,y)\mathrm{d}y\mathrm{d}x+\int_{0}^{\infty}\int_{x/z}^0f(x,y)\mathrm{d}y\mathrm{d}x\\
&amp;amp;z&amp;gt;0\\
&amp;amp;P(\frac{X}{Y}\leq z)=\int_{0}^{\infty}\int_{x/z}^{\infty}f(x,y)\mathrm{d}y\mathrm{d}x+\int_{-\infty}^{0}\int_0^{\infty}f(x,y)\mathrm{d}y\mathrm{d}x\\
&amp;amp;\qquad\qquad\int_{-\infty}^{0}\int_{-\infty}^{z/x}f(x,y)\mathrm{d}y\mathrm{d}x+\int_{0}^{\infty}\int_{-\infty}^{0}f(x,y)\mathrm{d}y\mathrm{d}x
\end{aligned}
\]&lt;/span> 这种积分次序在&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时，被分成的四个积分区域，计算比较麻烦。我们再来尝试先对&lt;span class="math">\(x\)&lt;/span>积分，在对&lt;span class="math">\(y\)&lt;/span>积分。 &lt;span class="math">\[
\begin{aligned}
&amp;amp;z&amp;lt;0\\
&amp;amp;P(\frac{X}{Y}\leq z)=\int_{0}^{\infty}\int_{-\infty}^{yz}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^{0}\int_{yz}^{\infty}f(x,y)\mathrm{d}x\mathrm{d}y\\
&amp;amp;z&amp;gt;0\\
&amp;amp;P(\frac{X}{Y}\leq z)=\int_{0}^{\infty}\int_{-\infty}^{yz}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^{0}\int_{yz}^{\infty}f(x,y)\mathrm{d}x\mathrm{d}y\\
\end{aligned}
\]&lt;/span> 在此积分次序下，不仅都被只划分成了两个积分区域，并且在&lt;span class="math">\(z&amp;lt;0\)&lt;/span>和&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时，两个积分的公式是一样的，可以合并成一个场景，不用区分&lt;span class="math">\(z\)&lt;/span>的正负，十分有利于计算，因此先对&lt;span class="math">\(x\)&lt;/span>积分，在对&lt;span class="math">\(y\)&lt;/span>积分是合适的积分方式，即 &lt;span class="math">\[
F_{_Z}(z)=P(Z\leq z)\\
=P(\frac{X}{Y}\leq z)=\int_{0}^{\infty}\int_{-\infty}^{yz}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^{0}\int_{yz }^{\infty}f(x,y)\mathrm{d}x\mathrm{d}y\\
\]&lt;/span> 联系分布函数和概率密度函数的关系，&lt;span class="math">\(F_{_Z}(z)=\int_{-\infty}^z f(t)\mathrm{d}t\)&lt;/span>，我们希望在二重积分限中去除&lt;span class="math">\(y\)&lt;/span>，只保留&lt;span class="math">\(z\)&lt;/span>，因此我们在对&lt;span class="math">\(x\)&lt;/span>的积分中采用变量代换令&lt;span class="math">\(x=yt\)&lt;/span>，那么&lt;span class="math">\(\mathrm{d}x=y\mathrm{d}t\)&lt;/span>，代入有： &lt;span class="math">\[
F_{_Z}(z)=\int_{0}^{\infty}\int_{-\infty}^{z}f(yt,y)y\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{z}^{\infty}f(yt,y)y\mathrm{d}t\mathrm{d}y
\]&lt;/span> 上式中第一项中&lt;span class="math">\(y\)&lt;/span>恒大于0，第二项中&lt;span class="math">\(y\)&lt;/span>恒小于0，所以我们加上绝对值符号，统一积分内容： &lt;span class="math">\[
\begin{aligned}
F_{_Z}(z)&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(yt,y)|y|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{z}^{\infty}-f(yt,y)|y|\mathrm{d}t\mathrm{d}y\\
&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(yt,y)|y|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\underbrace{\int_{-\infty}^{z}}_{负号改变}f(yt,y)|y|\mathrm{d}t\mathrm{d}y\\
&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(yt,y)|y|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{-\infty}^{z}f(yt,y)|y|\mathrm{d}t\mathrm{d}y\\
&amp;amp;\overset{交互积分次序}{=}\int_{-\infty}^{z}\int_{0}^{\infty}f(yt,y)|y|\mathrm{d}y\mathrm{d}t+\int_{-\infty}^{z}\int_{-\infty}^{0}f(yt,y)|y|\mathrm{d}y\mathrm{d}t\\
&amp;amp;=\int_{-\infty}^{z}\int_{-\infty}^{\infty}f(yt,y)|y|\mathrm{d}y\mathrm{d}t
\end{aligned}
\]&lt;/span> 求导得到概率密度函数。 &lt;span class="math">\[
p_{_Z}(z)=F&amp;#39;_{_Z}(z)=\int_{-\infty}^{\infty}f(yz,y)|y|\mathrm{d}y
\]&lt;/span> 得证。&lt;/p>
&lt;p>我们再讨论随机变量积的分布。利用分布函数&lt;span class="math">\(F_{_Z}(z)\)&lt;/span>的定义，我们有： &lt;span class="math">\[
F_{_Z}(z)=P(Z\leq z)=P(XY\leq z)=\iint\limits_{xy&amp;lt;z}f(x,y)\mathrm{d}x\mathrm{d}y
\]&lt;/span> 同样我们要考虑其积分区域，需要分成两种情况考虑：&lt;span class="math">\(z&amp;lt;0\)&lt;/span>和&lt;span class="math">\(z\geq 0\)&lt;/span>。具体分析过程和随机变量商的方法类似，不再具体说明。可以得到如下图所示阴影积分区域。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/随机变量函数积的分布.jpg" alt="随机变量函数积的分布.jpg" />&lt;p class="caption">随机变量函数积的分布.jpg&lt;/p>
&lt;/div>
&lt;p>在考虑积分顺序时，同样也是先对&lt;span class="math">\(x\)&lt;/span>积分，再对&lt;span class="math">\(y\)&lt;/span>积分更适合计算，所以有： &lt;span class="math">\[
F_{_Z}(z)=P(Z\leq z)\\
=P(XY\leq z)=\int_{0}^{\infty}\int_{-\infty}^{z/y}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^{0}\int_{z/y}^{\infty}f(x,y)\mathrm{d}x\mathrm{d}y\\
\]&lt;/span> 之后变量代换（&lt;span class="math">\(x=\frac{t}{y}\)&lt;/span>）与交互积分顺序的方法也和随机变量商的分布一致，最后可以得到 &lt;span class="math">\[
\begin{aligned}
F_{_Z}(z)&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{z}^{\infty}-f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y\\
&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\underbrace{\int_{-\infty}^{z}}_{负号改变}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y\\
&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{-\infty}^{z}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y\\
&amp;amp;\overset{交互积分次序}{=}\int_{-\infty}^{z}\int_{0}^{\infty}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}y\mathrm{d}t+\int_{-\infty}^{z}\int_{-\infty}^{0}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}y\mathrm{d}t\\
&amp;amp;=\int_{-\infty}^{z}\int_{-\infty}^{\infty}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}y\mathrm{d}t
\end{aligned}
\]&lt;/span> 求导得到概率密度函数。 &lt;span class="math">\[
p_{_Z}(z)=F&amp;#39;_{_Z}(z)=\int_{-\infty}^{\infty}f(\frac{z}{y},y)|\frac{1}{y}|\mathrm{d}y
\]&lt;/span> 得证。&lt;/p></description></item><item><title>概率统计随机过程之马尔可夫过程</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B/</link><pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B/</guid><description>
&lt;h2 id="概率统计随机过程之马尔可夫过程">概率统计随机过程之马尔可夫过程&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="概论">概论&lt;/h2>
&lt;p>在概率论及统计学中，&lt;strong>马尔可夫过程&lt;/strong>（英语：Markov process）是一个具备了&lt;strong>马尔可夫性质&lt;/strong>的随机过程，因为俄国数学家安德雷·马尔可夫得名。马尔可夫过程是&lt;strong>不具备记忆特质的（memorylessness）&lt;/strong>。换言之，马尔可夫过程的条件概率仅仅与系统的&lt;strong>当前状态相关&lt;/strong>，而与它的过去历史或未来状态，都是独立、不相关的。&lt;/p>
&lt;p>具备&lt;strong>离散状态的马尔可夫过程，通常被称为马尔可夫链&lt;/strong>。马尔可夫链通常使用离散的时间集合定义，又称离散时间马尔可夫链。有些学者虽然采用这个术语，但允许时间可以取连续的值。&lt;/p>
&lt;p>马尔可夫过程根据时间与状态空间的连续性可分为下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;/th>
&lt;th align="center">可数或有限的状态空间&lt;/th>
&lt;th align="center">连续或一般的状态空间&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">离散时间&lt;/td>
&lt;td align="center">在可数且有限状态空间下的马尔可夫链&lt;/td>
&lt;td align="center">Harris chain (在一般状态空间下的马尔可夫链)&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">连续时间&lt;/td>
&lt;td align="center">Continuous-time Markov process&lt;/td>
&lt;td align="center">任何具备马尔可夫性质的连续随机过程，例如维纳过程&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>注：连续时间马尔可夫过程基本就是几何分布或者指数分布过程。&lt;/p>
&lt;h2 id="马尔可夫性质">马尔可夫性质&lt;/h2>
&lt;blockquote>
&lt;p>马尔可夫性质：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。&lt;/p>
&lt;/blockquote>
&lt;p>数学上，如果&lt;span class="math">\(X(t),t&amp;gt;0\)&lt;/span>为一个随机过程，则马尔可夫性质就是指 &lt;span class="math">\[
{\mathrm{Pr}}{\big [}X(t+h)=y\,|\,X(s)=x(s),s\leq t{\big ]}={\mathrm {Pr}}{\big [}X(t+h)=y\,|\,X(t)=x(t){\big ]},\quad \forall h&amp;gt;0.
\]&lt;/span>&lt;/p></description></item><item><title>概率统计随机过程之假设检验</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</guid><description>
&lt;h2 id="概率统计随机过程之假设检验">概率统计随机过程之假设检验&lt;!-- omit in toc -->&lt;/h2>
&lt;p>统计推断两大内容：&lt;strong>参数估计（点估计、区间估计）与假设检验&lt;/strong>。与参数估计不同，假设检验不需要去计算具体的数值或范围，只需要回答“Yes Or No”的问题。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#假设检验">假设检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#假设检验研究的问题">假设检验研究的问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#假设">“假设”&lt;/a>&lt;/li>
&lt;li>&lt;a href="#如何选择原假设h_0与备择假设h_1">如何选择原假设&lt;span class="math">\(H_0\)&lt;/span>与备择假设&lt;span class="math">\(H_1\)&lt;/span>？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基本方法与步骤">基本方法与步骤&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两类错误">两类错误&lt;/a>&lt;/li>
&lt;li>&lt;a href="#显著性水平">显著性水平&lt;/a>&lt;/li>
&lt;li>&lt;a href="#p值">P值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#显著性水平和p值的区别">显著性水平和P值的区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一个总体的参数假设检验">一个总体的参数假设检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总体均值的检验">总体均值的检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总体比例的检验">总体比例的检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总体方差的检验">总体方差的检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个总体的参数假设检验">两个总体的参数假设检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均值差异性检验">均值差异性检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个总体比例差的检验">两个总体比例差的检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个总体方差之比的检验">两个总体方差之比的检验&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="假设检验">假设检验&lt;/h2>
&lt;p>假设检验的基本思想是对&lt;strong>总体参数&lt;/strong>的具体数值进行假设性陈述（如使用=，≥，≤等符号），再利用样本或实验结果来推断此假设的可信度。通常，逻辑上采用&lt;strong>反证法&lt;/strong>，但只是概率性证伪，依据是统计上的&lt;strong>小概率原理&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>小概率原理:一个事件如果发生的概率很小的话，那么它在一次试验中是几乎不可能发生的，但在多次重复试验中几乎是必然发生的。&lt;/p>
&lt;/blockquote>
&lt;p>多小能称之为小概率呢？统计学上，一般认为地认为等于或小于0.05或0.01的概率为小概率。&lt;/p>
&lt;h3 id="假设检验研究的问题">假设检验研究的问题&lt;/h3>
&lt;p>总体分布存在未知内容，对总体分布的“某种推断”做出某些“假设”，再通过&lt;strong>抽样的样本&lt;/strong>进行对假设进行检验。主要分为以下两种：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>分布类型未知：非参数假设，例如假设服从正态分布，均匀分布……这类一般很难。针对分布类型未知的假设检验为非参数的假设检验。&lt;/li>
&lt;li>参数未知：参数假设。针对分布类型已知而其中某些参数未知的假设建议称为&lt;strong>参数的假设检验&lt;/strong>（重点）。&lt;/li>
&lt;/ol>
&lt;h3 id="假设">“假设”&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;/th>
&lt;th align="center">原假设(Null hypothesis)&lt;/th>
&lt;th align="center">备择假设(Alternative hypothesis)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">表示记号&lt;/td>
&lt;td align="center">&lt;span class="math">\(H_0\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(H_1\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">别称&lt;/td>
&lt;td align="center">零假设&lt;/td>
&lt;td align="center">研究假设&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">含义&lt;/td>
&lt;td align="center">待检验的假设&lt;/td>
&lt;td align="center">与原假设对立的假设&lt;br>二者相互对立，有且只有一个成立&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">目的&lt;/td>
&lt;td align="center">研究者想收集证据予以&lt;strong>反对&lt;/strong>的假设&lt;/td>
&lt;td align="center">研究者想收集证据予以&lt;strong>支持&lt;/strong>的假设&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">等号&lt;/td>
&lt;td align="center">带有，=，≥，≤&lt;/td>
&lt;td align="center">不带有，≠，&amp;lt;,&amp;gt;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>原假设与备择假设&lt;strong>二选一&lt;/strong>，接受&lt;span class="math">\(H_0\)&lt;/span>则拒绝&lt;span class="math">\(H_1\)&lt;/span>，反则反之。&lt;/p>
&lt;p>假设设计步骤：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>先确定备择假设，在确定原假设&lt;/li>
&lt;li>=，≥，≤放在原假设中&lt;/li>
&lt;li>检验目的是收集证据拒绝原假设&lt;/li>
&lt;/ol>
&lt;h3 id="如何选择原假设h_0与备择假设h_1">如何选择原假设&lt;span class="math">\(H_0\)&lt;/span>与备择假设&lt;span class="math">\(H_1\)&lt;/span>？&lt;/h3>
&lt;p>&lt;strong>科学的审慎原则与原假设的优势性&lt;/strong>。在科学研究中，通常要求保守主义，遵守习俗、惯例和延续性。比如新的工艺或技术默认无效、新的要没有疗效、变量无关联等。&lt;/p>
&lt;p>原假设是具有先天优势的，因为必须拿出充分的证据才可以推翻，具备先天受保护性，相对于备择假设更有优势。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>一般不能轻易推翻（否定）的假设为原假设，原假设不能轻易拒绝，除非有足够的证据。&lt;/li>
&lt;li>保守性的作为原假设。&lt;/li>
&lt;li>分析人员想证明正确的命题作为备择假设，把分析人员努力证明他不正确的命题作为原假设。&lt;/li>
&lt;li>如果命题成立，但是误判为不成立时会造成严重后果的；命题为原假设。&lt;/li>
&lt;/ol>
&lt;p>个人总结：惰性，严谨性（悲观性）&lt;/p>
&lt;h2 id="基本方法与步骤">基本方法与步骤&lt;/h2>
&lt;p>&lt;strong>基本方法&lt;/strong>：&lt;/p>
&lt;p>用&lt;strong>样本&lt;/strong>构造统计量&lt;span class="math">\(T\)&lt;/span>，在原假设&lt;span class="math">\(H_0\)&lt;/span>情况下，&lt;span class="math">\(T\)&lt;/span>的分布已知，&lt;span class="math">\(T\)&lt;/span>发生的概率与此次抽样相关，&lt;strong>关注小概率事件在一次抽样中是否发生&lt;/strong>。&lt;/p>
&lt;p>&lt;em>假设检验和区间估计方法是类似的&lt;/em>。&lt;/p>
&lt;p>&lt;strong>步骤&lt;/strong>：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>提出原假设&lt;span class="math">\(H_0\)&lt;/span>与备择假设&lt;span class="math">\(H_1\)&lt;/span>&lt;/li>
&lt;li>假定&lt;span class="math">\(H_0\)&lt;/span>成立，构造统计量&lt;span class="math">\(T\)&lt;/span>，其分布已知&lt;/li>
&lt;li>对于给定的小概率&lt;span class="math">\(\alpha\)&lt;/span>，找到对应的小概率区间&lt;span class="math">\(W\)&lt;/span>，使得&lt;span class="math">\(P(\{X_1\dotsb,X_n\}\in W)=\alpha\)&lt;/span>。我们也称&lt;span class="math">\(W\)&lt;/span>为拒绝域，对应的大概率&lt;span class="math">\(1-\alpha\)&lt;/span>对应的区间&lt;span class="math">\(\bar W\)&lt;/span>为接受域。&lt;/li>
&lt;li>由样本数据值&lt;span class="math">\(x_1,\dotsb,x_n\)&lt;/span>，求出统计量&lt;span class="math">\(T\)&lt;/span>的值&lt;/li>
&lt;li>若样本数据构造的&lt;span class="math">\(\hat T\in W\Rightarrow\)&lt;/span> 在拒绝域，拒绝&lt;span class="math">\(H_0\)&lt;/span>；若样本数据构造的&lt;span class="math">\(\hat T\in \bar W\Rightarrow\)&lt;/span>在接受域，接受&lt;span class="math">\(H_0\)&lt;/span>；&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>补充&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>单侧检验，备择假设有方向性，拒绝域只在左侧（&amp;lt;）或右侧（&amp;gt;）。&lt;/li>
&lt;li>双侧检验，备择假设没有方向性，拒绝域在两侧（=）&lt;/li>
&lt;/ul>
&lt;h2 id="两类错误">两类错误&lt;/h2>
&lt;p>我们根据概率做的决策未必是对的。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>第一类错误：弃真。原假设为真，但是推翻原假设。&lt;span class="math">\(P(拒绝H_0|H_0为真)=\alpha\)&lt;/span>。此&lt;span class="math">\(\alpha\)&lt;/span>就是上文中的&lt;span class="math">\(\alpha\)&lt;/span>，称为显著性水平。&lt;span class="math">\(1-\alpha\)&lt;/span>成为置信水平。&lt;/li>
&lt;li>第二类错误：纳伪。原假设为假，但是不推翻原假设。&lt;span class="math">\(P(接受H_0|H_0为假)=\beta\)&lt;/span>。&lt;span class="math">\(1-\beta\)&lt;/span>称为检测效率。&lt;/li>
&lt;/ol>
&lt;p>两类错误通常在样本一定时无法兼顾，第一类错误低，必然导致第二类错误升高。&lt;/p>
&lt;p>N-P原则：尽量保证不犯第一类错误的前提下，尽量减少第二类错误。&lt;/p>
&lt;p>注意，我们只能说不能拒绝原假设，而不能轻易说接受原假设。一般文中的接受域只是术语。&lt;/p>
&lt;h3 id="显著性水平">显著性水平&lt;/h3>
&lt;p>显著性水平&lt;span class="math">\(\alpha\)&lt;/span>是一个概率值，代表着&lt;strong>拒绝原假设$H_0率，其概率范围叫拒绝域&lt;/strong>。通常显著性水平的值比较小，如0.01，0.05，0.10，这种小概率也表明要拒绝原假设的概率应该很小，除非证据充分。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/拒绝域.png" alt="拒绝域.png" />&lt;p class="caption">拒绝域.png&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>双侧检验：|检测统计量值|&amp;gt;临界值，拒绝&lt;span class="math">\(H_0\)&lt;/span>.&lt;/li>
&lt;li>左侧检验：检测统计量值 &amp;lt; 临界值，拒绝&lt;span class="math">\(H_0\)&lt;/span>.&lt;/li>
&lt;li>右侧检验：检测统计量值 &amp;gt; 临界值，拒绝&lt;span class="math">\(H_0\)&lt;/span>.&lt;/li>
&lt;/ul>
&lt;p>显著性水平是人为规定的，其概率大小与犯第一类错误的大小有关，显著性水平&lt;span class="math">\(\alpha\)&lt;/span>越大，拒绝原假设的概率越大，那么发生第一类错误的概率&lt;span class="math">\(P(拒绝H_0|H_0为真)\)&lt;/span>也就越大。&lt;/p>
&lt;h2 id="p值">P值&lt;/h2>
&lt;p>在上一小结使用显著性性水平和置信度的体系中，我们人为地规定了置信度，然后根据置信度确定临界值划分接受域和拒绝域。这实际上只回答了Yes Or NO的问题，具体抽样出来的样本发生概率有多小，并没有计算出来。为了近似的计算这个样本发生的概率，我们使用&lt;strong>P值&lt;/strong>这个指标。&lt;/p>
&lt;blockquote>
&lt;p>P值（P value）就是&lt;strong>当原假设为真时，比所得到的样本观察结果更极端的结果出现的概率&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>需要注意P值不是说所得样本发生的概率，而是所有比样本更加离谱发生的概率的&lt;strong>总和&lt;/strong>，它是一组事件集合的概率。对于不同的检验，离谱/极端的定义是不一样的，比如：&lt;/p>
&lt;ul>
&lt;li>左侧检验：检验统计量小于或等于根据实际观测样本数据计算得到的检验统计量值的概率。&lt;/li>
&lt;li>右侧检验：检验统计量大于或等于根据实际观测样本数据计算得到的检验统计量值的概率。&lt;/li>
&lt;li>双侧检验：既要考虑小于等于又要考虑大于等于的情形。&lt;/li>
&lt;/ul>
&lt;p>如果P值很小，说明原假设情况的发生的概率很小，而如果出现了，根据小概率原理，我们就有理由拒绝原假设，P值越小，我们拒绝原假设的理由越充分。总之，P值越小，表明结果越显著。用公式表示为： &lt;span class="math">\[
P\leq P_{\alpha}，则拒绝H_0
\]&lt;/span> 其中，&lt;span class="math">\(P_{\alpha}\)&lt;/span>是我们选定的概率。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/P值.png" alt="P值.png" />&lt;p class="caption">P值.png&lt;/p>
&lt;/div>
&lt;h3 id="显著性水平和p值的区别">显著性水平和P值的区别&lt;/h3>
&lt;p>显著性水平是人为定的，我们根据显著性水平得到临界值，用临界值和统计量比较。&lt;/p>
&lt;p>P值是设计一个统计量，然后计算该统计量所有极端情况的概率和，再和一个人为定义的概率做比较。此外，P值能够得到极端情形的概率，能够提供更多的信息。&lt;/p>
&lt;p>二者的思路正好是互补的，只不过显著性水平和人为定义的小概率值通常取得一致，比如都是0.1，0.05，所以才容易搞混。&lt;/p>
&lt;p>通常在研究报告中，作者会求出P值，然后让读者去自行选择拒绝的小概率，而非使用显著性水平，直接拒绝原假设。&lt;/p>
&lt;h2 id="一个总体的参数假设检验">一个总体的参数假设检验&lt;/h2>
&lt;p>在总体分布类型已知的情况下，如果只有一个参数需要检验，会有以下常见情况。&lt;/p>
&lt;h3 id="总体均值的检验">总体均值的检验&lt;/h3>
&lt;p>根据中心极限定理，当样本数量足够多时，样本的均值服从渐进正态分布&lt;span class="math">\(N(\mu_0,\sigma^2/n)\)&lt;/span>，其中&lt;span class="math">\(\mu_0\)&lt;/span>是总体真正的均值，&lt;span class="math">\(\sigma\)&lt;/span>是总体的标准差，&lt;span class="math">\(n\)&lt;/span>是样本容量。通常，当样本容量&lt;span class="math">\(n&amp;gt;30\)&lt;/span>时，我们就可以使用正态分布作为样本均值的分布。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/一个参数均值检验.png" alt="一个参数均值检验.png" />&lt;p class="caption">一个参数均值检验.png&lt;/p>
&lt;/div>
&lt;p>当总体的标准差&lt;span class="math">\(\sigma\)&lt;/span>未知时，我们可以用样本的标准差近似代替总体的标准差。需要注意，样本方差公式中分母是&lt;span class="math">\(n-1\)&lt;/span> &lt;span class="math">\[
s^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-{\bar {X}})^2
\]&lt;/span> 其中&lt;span class="math">\(\bar{X}\)&lt;/span>为样本均值。&lt;/p>
&lt;p>z检验就是正态检验，t检验是student检验。&lt;/p>
&lt;h3 id="总体比例的检验">总体比例的检验&lt;/h3>
&lt;p>总体比例的检验多见于使用二项分布的场景。比如，总体中具有某一个特征的个体可能比例为&lt;span class="math">\(\epsilon\)&lt;/span>，现在抽样的样本容量为&lt;span class="math">\(n\)&lt;/span>，其中具有该特征的样本数量为&lt;span class="math">\(x\)&lt;/span>，要求检验比例&lt;span class="math">\(\epsilon\)&lt;/span>的准确性。本质上是检验&lt;span class="math">\(\epsilon\)&lt;/span>与&lt;span class="math">\(p=\frac{x}{n}\)&lt;/span>的关系。&lt;/p>
&lt;p>我们知道当&lt;span class="math">\(n\)&lt;/span>较大时，二项分布是不太好求的，因为存在计算量很大的阶乘项，但是根据二项分布的中心极限定理，当&lt;span class="math">\(n\)&lt;/span>较大，且&lt;span class="math">\(n\epsilon\)&lt;/span>的值不是很小（&lt;span class="math">\(n\epsilon&amp;gt;5, n(1-\epsilon)&amp;gt;5\)&lt;/span>）时，则可用正态分布去近似二项分布。&lt;/p>
&lt;p>根据抽样要求可知，具有某一个特征的个体数量服从二项分布&lt;span class="math">\(B(n,\epsilon)\)&lt;/span>，即 &lt;span class="math">\[
P(x=k)=C_n^{k}\epsilon^k(1-\epsilon)^{n-k}
\]&lt;/span> &lt;strong>当&lt;span class="math">\(n\)&lt;/span>很大时&lt;/strong>，我们可以用正态分布近似二项分布，即&lt;span class="math">\(x\sim N(n\epsilon,n\epsilon(1-\epsilon))\)&lt;/span>，那么根据随机变量的关系有&lt;span class="math">\(p=\frac{x}{n}\sim N(\epsilon,{\epsilon(1-\epsilon)\over{n}})\)&lt;/span>，将&lt;span class="math">\(p\)&lt;/span>标准化后可得 &lt;span class="math">\[
\frac{p-\epsilon}{\sqrt{\frac{\epsilon(1-\epsilon)}{n}}}\sim N(0,1)
\]&lt;/span> 这样我们可以通过正态（Z）检验来对比例进行假设检验。&lt;/p>
&lt;h3 id="总体方差的检验">总体方差的检验&lt;/h3>
&lt;p>根据笔记&lt;a href="概率统计随机过程之抽样的分布.md">概率统计随机过程之抽样的分布&lt;/a>中内容可知，样本方差&lt;span class="math">\(S^2\)&lt;/span>与总体方差&lt;span class="math">\(\sigma^2\)&lt;/span>有如下关系： &lt;span class="math">\[
\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)
\]&lt;/span> 因此，可以对总体的方差进行卡方检验，方法与Z检验类似，只是卡方检验多用于单侧检验，且分布需要使用卡方分布。&lt;/p>
&lt;h2 id="两个总体的参数假设检验">两个总体的参数假设检验&lt;/h2>
&lt;p>两个总体的检验和单个总体的检验分类一样，也是分为均值、比例和方差的检验，如下图所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/两个总体的检验.png" alt="两个总体的检验" />&lt;p class="caption">两个总体的检验&lt;/p>
&lt;/div>
&lt;p>通常我们研究的两个总体之间的差距，因此，一般研究的是&lt;strong>两个总体的均值之差、比例之差与方差之比&lt;/strong>。&lt;/p>
&lt;h3 id="均值差异性检验">均值差异性检验&lt;/h3>
&lt;div class="figure">
&lt;img src="../../images/两个总体均值差的检验.png" alt="两个总体均值差的检验" />&lt;p class="caption">两个总体均值差的检验&lt;/p>
&lt;/div>
&lt;div class="figure">
&lt;img src="../../images/两个正态总体的参数假设检验1.png" alt="两个正态总体的参数假设检验1.png" />&lt;p class="caption">两个正态总体的参数假设检验1.png&lt;/p>
&lt;/div>
&lt;h3 id="两个总体比例差的检验">两个总体比例差的检验&lt;/h3>
&lt;div class="figure">
&lt;img src="../../images/两个总体比例差的检验.png" alt="两个总体比例差的检验" />&lt;p class="caption">两个总体比例差的检验&lt;/p>
&lt;/div>
&lt;h3 id="两个总体方差之比的检验">两个总体方差之比的检验&lt;/h3>
&lt;div class="figure">
&lt;img src="../../images/两个总体方差之比的检验.png" alt="两个总体方差之比的检验" />&lt;p class="caption">两个总体方差之比的检验&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之参数估计</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</guid><description>
&lt;h2 id="概率统计随机过程之参数估计">概率统计随机过程之参数估计&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#点估计">点估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#点估计的优良性准则">点估计的优良性准则&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩估计">矩估计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#矩估计存在的要求">矩估计存在的要求&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩估计的一般步骤">矩估计的一般步骤&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩估的特点">矩估的特点&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#极大似然估计">极大似然估计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#最大似然估计的不变原理">最大似然估计的不变原理&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#点估计的有效性详解">点估计的有效性详解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一致最小方差无偏估计">一致最小方差无偏估计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#改进一个无偏估计">改进一个无偏估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#零无偏估计法">零无偏估计法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#充分完全统计量法">充分完全统计量法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#cramer-raoc-r不等式与界">Cramer-Rao（C-R）不等式与界&lt;/a>&lt;/li>
&lt;li>&lt;a href="#区间估计">区间估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#置信区间与枢轴变量">置信区间与枢轴变量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一个正态总体的期望和方差的区间估计">一个正态总体的期望和方差的区间估计&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>统计推断三大内容：&lt;strong>抽样分布、参数估计（点估计、区间估计）与假设检验&lt;/strong>。&lt;/p>
&lt;p>参数估计的核心思想：用抽样出来的样本构造函数（统计量）来尝试近似实际分布的参数。&lt;/p>
&lt;h2 id="点估计">点估计&lt;/h2>
&lt;blockquote>
&lt;p>估计量：在参数估计大类的点估计中，那么用于估计未知参数的&lt;strong>统计量&lt;/strong>称为&lt;strong>点估计(量)&lt;/strong>，简称为&lt;strong>估计(量)&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>估计一个&lt;strong>具体的数值&lt;/strong>，实际比较困难。我们用样本来构造一个函数 &lt;span class="math">\[
\hat\theta = \Theta(X),X=(x_1,\dotsb,x_n)
\]&lt;/span> 用以计算参数&lt;span class="math">\(\theta\)&lt;/span>，由于是估计值所以用&lt;span class="math">\(\hat\theta\)&lt;/span>表示。其中&lt;span class="math">\(X=(x_1,\dotsb,x_n)\)&lt;/span>为容量为n的样本。有时也用&lt;span class="math">\(\hat g(\theta)\)&lt;/span>表示，因为有时要估计的不是&lt;span class="math">\(\theta\)&lt;/span>，而是&lt;span class="math">\(\theta\)&lt;/span>的某个函数。&lt;/p>
&lt;p>具体可参见笔记《概率统计随机过程之数理统计常用概念》中统计量与估计量那一小节。&lt;/p>
&lt;p>具体方法：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>矩估计&lt;/li>
&lt;li>最大似然估计（MLE）&lt;/li>
&lt;li>贝叶斯估计（MAP等）&lt;/li>
&lt;li>LSE（MMSE）&lt;/li>
&lt;li>……&lt;/li>
&lt;/ol>
&lt;h3 id="点估计的优良性准则">点估计的优良性准则&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>无偏性。&lt;span class="math">\(E(\hat\theta)=\theta\)&lt;/span>（样本方差的系数&lt;span class="math">\(n-1\)&lt;/span>就是这里的无偏性得出来的）&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;span class="math">\(\hat\theta\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，但是&lt;span class="math">\(g(\hat\theta)\)&lt;/span>不一定是&lt;span class="math">\(g(\theta)\)&lt;/span>的无偏估计。例如说明1。&lt;/li>
&lt;/ul>
&lt;ol start="2" style="list-style-type: decimal">
&lt;li>有效性。估计值的方差越小越好。&lt;span class="math">\(D(\hat\theta_1)\leq D(\hat\theta_2)\)&lt;/span>则&lt;span class="math">\(\hat\theta_1\)&lt;/span>更好。波动性小，无偏情况下，更可能接近于真实值。&lt;/li>
&lt;li>相合性（一致性）。&lt;span class="math">\(\lim_{n\rightarrow \infty} P(|\hat\theta-\theta|&amp;lt;\varepsilon)=1\)&lt;/span>，取的样本越多，越趋近于真实值。&lt;/li>
&lt;li>渐进正态性（大样本性质）。体现了估计量随样本数量趋近于真实值的速度。&lt;/li>
&lt;/ol>
&lt;p>说明1：样本方差&lt;span class="math">\(S^2\)&lt;/span>是&lt;span class="math">\(\sigma^2\)&lt;/span>的无偏估计，但是&lt;span class="math">\(\sqrt{(S^2)}\)&lt;/span>不是&lt;span class="math">\(\sqrt{\sigma^2}\)&lt;/span>的无偏估计。&lt;/p>
&lt;p>说明2：无偏性+有效性（尽量小方差）=一致最小方差无偏估计（UMVUE）&lt;/p>
&lt;p>说明3：无偏性和相合性是两个方面的性质，无偏性是概率性质，相合性是统计性质，无偏不一定相合。&lt;/p>
&lt;p>具体可参见笔记&lt;a href="概率统计随机过程之数理统计常用概念.md">《概率统计随机过程之数理统计常用概念》&lt;/a>中估计量的评价指标那一小节。&lt;/p>
&lt;h3 id="矩估计">矩估计&lt;/h3>
&lt;p>核心思想：近似替代。&lt;strong>用样本的矩代替总体的矩，用样本矩的函数估计总体矩的相应函数&lt;/strong>。就把样本当总体。&lt;/p>
&lt;p>理论基础是&lt;strong>格涅坚科大数定理&lt;/strong>（原始数据用原点矩估计）&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">总体的矩&lt;/th>
&lt;th align="center">&amp;lt;-----&amp;gt;&lt;/th>
&lt;th align="center">样本的矩&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">一阶 &lt;span class="math">\(E(X)\)&lt;/span>&lt;/td>
&lt;td align="center">&amp;lt;-----&amp;gt;&lt;/td>
&lt;td align="center">一阶 &lt;span class="math">\(\bar X = {1\over n}\sum X_i\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">二阶 &lt;span class="math">\(E(X^2)\)&lt;/span>&lt;/td>
&lt;td align="center">&amp;lt;-----&amp;gt;&lt;/td>
&lt;td align="center">二阶 &lt;span class="math">\(A_2 = {1\over n}\sum X_i^2\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>同时有： &lt;span class="math">\[\begin{aligned}
&amp;amp;\hat \mu = \bar X\\
&amp;amp;\hat \sigma^2 = A_2 - \hat \mu^2 =B_2(二阶中心矩)
\end{aligned}
\]&lt;/span> 不难发现，中心矩可以用原点矩来表示。&lt;/p>
&lt;h4 id="矩估计存在的要求">矩估计存在的要求&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>该分布的k阶矩是存在的&lt;/li>
&lt;li>估计量或其函数&lt;span class="math">\(g(\theta)\)&lt;/span>可以写成各阶矩组成的函数，即&lt;span class="math">\(g(\theta)=G(\underbrace{\alpha_1,\alpha_2,\dotsb,\alpha_k}_{k原点矩},\underbrace{\mu_2,\mu_3,\dotsb,\mu_s}_{s阶中心矩})\)&lt;/span>，则&lt;span class="math">\(g(\theta)\)&lt;/span>可以用样本矩的函数&lt;span class="math">\(\hat g(X)=G(a_1,a_2,\dotsb,a_k,m_2,\dotsb,m_s)\)&lt;/span>进行矩估计，其中&lt;span class="math">\(a_i,s_i\)&lt;/span>分别是&lt;span class="math">\(i\)&lt;/span>阶样本原点矩和中心矩。&lt;/li>
&lt;/ol>
&lt;h4 id="矩估计的一般步骤">矩估计的一般步骤&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>求出一些矩。一般情况下，原分布中有几个未知参数，就需要几阶矩估计。（有时也确定需要哪些矩）&lt;/li>
&lt;li>将&lt;span class="math">\(\theta\)&lt;/span>或&lt;span class="math">\(g(\theta)\)&lt;/span>用矩表示（表示方法不唯一，可以根据UMVUE准则判断优劣，即要无偏，方差要小）。&lt;/li>
&lt;li>替换矩（样本矩→总体矩，&lt;span class="math">\(\hat\theta = \varTheta(X)→\theta\)&lt;/span>或&lt;span class="math">\(\hat g(X)→g(\theta)\)&lt;/span>）。&lt;/li>
&lt;/ol>
&lt;h4 id="矩估的特点">矩估的特点&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>简单。&lt;/li>
&lt;li>一般矩估计是渐进无偏的。由于原点矩都是无偏的，中心距是渐进无偏的；只有当估计量是原点矩的线性组合时，才是无偏估计。&lt;/li>
&lt;li>没有运用到总体的分布信息，准确性稍差。而且要求总体的矩一定存在。&lt;/li>
&lt;li>矩估计不唯一，取决于&lt;span class="math">\(\theta\)&lt;/span>或&lt;span class="math">\(g(\theta)\)&lt;/span>如何被各阶矩表示。&lt;/li>
&lt;li>相合性：略&lt;/li>
&lt;/ol>
&lt;h3 id="极大似然估计">极大似然估计&lt;/h3>
&lt;p>核心思想：概率大的事件比概率小的事件更容易发生。&lt;strong>要估计的参数能够使产生这个样本的概率最大&lt;/strong>。&lt;/p>
&lt;p>步骤：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>总体的概率/密度函数。&lt;/li>
&lt;li>写出似然估计函数&lt;span class="math">\(L(\lambda)\)&lt;/span>。&lt;/li>
&lt;li>两边取&lt;span class="math">\(\ln\)&lt;/span>，即为&lt;span class="math">\(\ln(L(\lambda))\)&lt;/span>。&lt;/li>
&lt;li>对&lt;span class="math">\(\lambda\)&lt;/span>求导（多参数估计就是偏导），令导数为0。&lt;/li>
&lt;/ol>
&lt;p>TIPS：对于有些分布的极大似然估计没法直接求，比如均匀分布。&lt;/p>
&lt;p>&lt;strong>似然函数取对数的原因&lt;/strong>:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>减少计算量。乘法变成加法，从而减少了计算量；同时，如果概率中含有指数项，如高斯分布，能把指数项也化为求和形式，进一步减少计算量；另外，在对联合概率求导时，和的形式会比积的形式更方便。&lt;/li>
&lt;li>计算时更准确。为概率值都在[0,1]之间，因此，概率的连乘将会变成一s个很小的值，可能会引起浮点数下溢，尤其是当数据集很大的时候，联合概率会趋向于0，非常不利于之后的计算。&lt;/li>
&lt;/ol>
&lt;p>需要指出的是：&lt;strong>取对数不影响单调性&lt;/strong>。 &lt;span class="math">\[
p(x|\theta_1)&amp;gt;p(x|\theta_2)\Leftrightarrow \ln(p(x|\theta_1))&amp;gt;\ln(p(x|\theta_2))
\]&lt;/span> 因为相同的单调性，它确保了概率的最大对数值出现在与原始概率函数相同的点上。因此，可以用更简单的对数似然来代替原来的似然。&lt;/p>
&lt;h4 id="最大似然估计的不变原理">最大似然估计的不变原理&lt;/h4>
&lt;p>我们介绍一个致使最大似然估计得到广泛应用的定理。&lt;/p>
&lt;blockquote>
&lt;p>定理1（不变定理）：设&lt;span class="math">\(X\sim p(x;\theta),\;\theta\in\mathcal{\Theta}\)&lt;/span>，若&lt;span class="math">\(\theta\)&lt;/span>的最大似然估计为&lt;span class="math">\(\hat\theta\)&lt;/span>，则对任意函数&lt;span class="math">\(\gamma=g(\theta)\)&lt;/span>，&lt;span class="math">\(\gamma\)&lt;/span>的最大似然估计为&lt;span class="math">\(\hat\gamma=g(\hat\theta)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>这个定理的条件很宽，致使最大似然估计的应用也会广泛。在函数&lt;span class="math">\(g(\theta)\)&lt;/span>非单调的时候，它的证明需要测度论的内容，暂时不予证明。&lt;/p>
&lt;h3 id="点估计的有效性详解">点估计的有效性详解&lt;/h3>
&lt;p>我们在前面提到过可以用估计值的方差来代表估计的有效性，但是这有一个前提条件：&lt;strong>需要该估计为无偏估计&lt;/strong>。这样估计值才能紧密散布在真值周围，如果是估计值偏差较大，方差很小，那只会猜到一个错误的位置。如果用图像表示无偏与方差的关系，如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/点估计偏差与方差.jpg" alt="点估计偏差与方差" />&lt;p class="caption">点估计偏差与方差&lt;/p>
&lt;/div>
&lt;p>从上图中，我们可以看出：第一幅子图中，当无偏估计且方差很小时，估计点密布在真值周围，我们可以用取平均的方法贴近真实值；如果像第二个子图中，是有偏估计，而方差又很小，那么我们有很大可能会得到一个错误的估计。第二行的两个子图是方差较大的情况，我们可以观察到，无偏估计在方差较大时，点比较散，因此会增大估计误差；而有偏估计在方差较大时，反而可能比小方差时表现的更好。&lt;/p>
&lt;p>下面我们给出无偏估计有效性的精确定义：&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(\hat\theta_1=\hat\theta_1(x_1,x_2,\dotsb,x_n)\)&lt;/span>与&lt;span class="math">\(\hat\theta_2=\hat\theta_2(x_1,x_2,\dotsb,x_n)\)&lt;/span>都是参数&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，如果： &lt;span class="math">\[\mathrm{Var}(\hat\theta_1)\leq \mathrm{Var}(\hat\theta_2),\forall \theta\in \varTheta\]&lt;/span> 且至少对一个&lt;span class="math">\(\theta_0\in\varTheta\)&lt;/span>，有严格不等号成立，则称&lt;span class="math">\(\hat\theta_1\)&lt;/span>比&lt;span class="math">\(\hat\theta_2\)&lt;/span>有效。&lt;/p>
&lt;/blockquote>
&lt;p>那么对于&lt;strong>有偏估计&lt;/strong>，我们要如何评价它的优劣呢？有偏估计与无偏估计相比，除了随机散布造成的方差时存在的，还有与真实值之间的系统性&lt;strong>偏差&lt;/strong>。由于随机造成的误差有正有负，因此我们用&lt;strong>平方的方式&lt;/strong>来去除正负的影响（用绝对值也可以叫平均绝对误差，不过平方有更好的运算性质）。&lt;/p>
&lt;p>在此，我们定义有偏估计的有效性：&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(\hat\theta_1=\hat\theta_1(x_1,x_2,\dotsb,x_n)\)&lt;/span>与&lt;span class="math">\(\hat\theta_2=\hat\theta_2(x_1,x_2,\dotsb,x_n)\)&lt;/span>都是参数&lt;span class="math">\(\theta\)&lt;/span>的估计量，如果： &lt;span class="math">\[E(\hat\theta_1-\theta)^2\leq E(\hat\theta_2-\theta)^2,\forall \theta\in \varTheta\]&lt;/span> 且至少对一个&lt;span class="math">\(\theta_0\in\varTheta\)&lt;/span>，有严格不等号成立，则称在&lt;strong>均方误差&lt;/strong>意义下&lt;span class="math">\(\hat\theta_1\)&lt;/span>优于&lt;span class="math">\(\hat\theta_2\)&lt;/span>。其中，&lt;span class="math">\(E(\hat\theta_i-\theta)^2\)&lt;/span>称为&lt;span class="math">\(\theta_i\)&lt;/span>d的&lt;strong>均方误差&lt;/strong>，常记为&lt;span class="math">\(MSE(\hat\theta_i)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>根据定义不难发现，如果&lt;span class="math">\(\hat\theta_i\)&lt;/span>是无偏估计，那么均方误差等于方差，即&lt;span class="math">\(MSE(\hat\theta)=Var(\hat\theta)\)&lt;/span>。我们可以将均方误差的式子做如下变换： &lt;span class="math">\[\begin{aligned}
MSE(\hat\theta)&amp;amp;=E(\hat\theta-\theta)^2=E\{[\hat\theta-E(\hat\theta)]+[E(\hat\theta)-\theta]\}^2\\
&amp;amp;=E[(\hat\theta-E(\hat\theta))^2]+[E(\hat\theta)-\theta]^2\\
&amp;amp;=\mathrm{Var}(\hat\theta)+\delta^2
\end{aligned}
\]&lt;/span> 其中，我们将&lt;span class="math">\(\delta=|E(\hat\theta)-\theta|\)&lt;/span>称为（绝对）偏差，它体现了估计&lt;span class="math">\(\hat\theta\)&lt;/span>与真实值&lt;span class="math">\(\theta\)&lt;/span>之间的系统误差。由此可见，均方误差&lt;span class="math">\(MSE(\hat\theta_i)\)&lt;/span>可以分解成&lt;strong>系统误差和随机误差两部分&lt;/strong>两部分组成。无偏性可以让偏差&lt;span class="math">\(\delta\)&lt;/span>为0，有效性指标等同于要求方差最小化，而有偏估计则要求二者之和越小越好。假如有一个有偏估计其均方误差比任一个无偏估计的方差还小，则此种有偏估计应予以肯定。如下例子所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/有偏估计MSE例子.png" alt="有偏估计MSE例子.png" />&lt;p class="caption">有偏估计MSE例子.png&lt;/p>
&lt;/div>
&lt;p>可惜的是，参数的一切可能估计组成的估计类中一致最小均方误差估计不存在。&lt;/p>
&lt;p>思想实验：为什么参数的一切可能估计组成的估计类中一致最小均方误差估计不存在？&lt;/p>
&lt;blockquote>
&lt;p>如果一个估计&lt;span class="math">\(\theta^*\)&lt;/span>为一致最小均方误差，那么对于其他任意对于&lt;span class="math">\(\theta\)&lt;/span>的估计方法&lt;span class="math">\(\tilde{\theta}\)&lt;/span>在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上都有&lt;span class="math">\(MSE(\theta^*)\leq MSE(\tilde{\theta})\)&lt;/span>。问题就出自两个任意上，一是任意估计方法，二是&lt;span class="math">\(\forall\theta\in\varTheta\)&lt;/span>；这两个要求太严格了。我们来设计这样一个场景：参数的真值为&lt;span class="math">\(\theta_0\)&lt;/span>，有一个奇葩的估计方式&lt;span class="math">\(\tilde{\theta}_0\)&lt;/span>，无论给出什么样本，其估计结果都是&lt;span class="math">\(\theta_0\)&lt;/span>（固定值完全消除了样本随机性带来的随机性，导致方差为0）。尽管这个估计方式对于&lt;span class="math">\(\varTheta\)&lt;/span>中除了真实值为&lt;span class="math">\(\theta_0\)&lt;/span>的情况，一无是处，但是我们不能否认在真实值为&lt;span class="math">\(\theta_0\)&lt;/span>，这个估计很完美（绝对误差为0）。此时，&lt;span class="math">\(MSE(\tilde{\theta}_0)=0\)&lt;/span>。而一致最小均方误差&lt;span class="math">\(\theta^*\)&lt;/span>要满足&lt;span class="math">\(MSE(\theta^*)\leq MSE(\tilde{\theta})\)&lt;/span>，那么&lt;span class="math">\(\theta^*\)&lt;/span>必须是方差为0，偏差为0，那么只能让&lt;span class="math">\(\theta^*=\tilde{\theta}_0\)&lt;/span>。那么对于&lt;span class="math">\(\varTheta\)&lt;/span>中的任意&lt;span class="math">\(\theta_i\)&lt;/span>都可以构造类似的奇葩估计：无论给出什么样本，其估计结果都是&lt;span class="math">\(\theta_i\)&lt;/span>。根据&lt;span class="math">\(MSE(\theta^*)\leq MSE(\tilde{\theta})\)&lt;/span>，又必须让&lt;span class="math">\(\theta^*=\tilde\theta_i\)&lt;/span>。那么对于不同&lt;span class="math">\(\theta_i\)&lt;/span>，一致最小均方误差估计&lt;span class="math">\(\theta^*\)&lt;/span>根本不是同一个估计方式，所以一致最小均方误差估计不存在。&lt;/p>
&lt;/blockquote>
&lt;p>对此，我们的处理方式是将需要将估计方式范围缩小一些，做出一些限制，例如要求估计方法都必须是无偏的。此时产生一类非常重要的估计类：一致最小方差无偏估计，简称UMVUE。&lt;/p>
&lt;h3 id="一致最小方差无偏估计">一致最小方差无偏估计&lt;/h3>
&lt;p>我们前面已经分析过，均方误差在无偏估计中会简化为方差，此时一致最小均方误差估计简化为一致最小方差无偏估计。仿照前面的形式，我们给出一致最小方差无偏估计的定义：&lt;/p>
&lt;blockquote>
&lt;p>一致最小方差无偏估计：在参数估计&lt;span class="math">\(\mathcal{F}=\{f(x,\theta),\theta\in\varTheta\}\)&lt;/span>中，如果&lt;span class="math">\(\hat\theta\)&lt;/span>是参数&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>无偏估计&lt;/strong>，如果对另外任意一个&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>无偏估计&lt;/strong>&lt;span class="math">\(\tilde{\theta}\)&lt;/span>，在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上都有： &lt;span class="math">\[\mathrm{Var}(\hat\theta)\leq \mathrm{Var}(\tilde{\theta})\]&lt;/span> 则称&lt;span class="math">\(\hat{\theta}\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>一致最小方差无偏估计，简记为UMVUE(Uniformly minimum variance unbiased estimation)&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>需要指出，有些参数可能不存在无偏估计，即UMVUE可能也不存在。如果参数的无偏估计存在，我们称此参数为&lt;strong>可估参数&lt;/strong>。为什么加上无偏之后，一致最小方差估计就可以存在了呢？前面我们提到两个任意上，一是任意估计方法，二是&lt;span class="math">\(\forall\theta\in\varTheta\)&lt;/span>，这两个要求太严格。无偏其实是对第一个任意的限制，缩小了估计类范围，把很多奇葩的估计方法（如上文提到的&lt;span class="math">\(\tilde{\theta}_0\)&lt;/span>方法）排除在外。&lt;/p>
&lt;p>显然，在给定样本数量后，从无偏性和有效性两个角度，UMVUE是最优解，下面我们给出三个求UMVUE的方法。第一个是零无偏估计法，第二个是充分完全统计量法，第三个用了C-R不等式（单拎出来介绍）。下面我们分别介绍。&lt;/p>
&lt;h4 id="改进一个无偏估计">改进一个无偏估计&lt;/h4>
&lt;p>假设我们已经知道了一个无偏估计，有没有办法能够优化它的方差呢？我们先介绍一种改进无偏估计的方法——Rao–Blackwell定理。&lt;/p>
&lt;blockquote>
&lt;p>定理2（Rao–Blackwell定理）：设&lt;span class="math">\(T=T(x)\)&lt;/span>是样本&lt;span class="math">\(x\)&lt;/span>关于参数&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>充分统计量&lt;/strong>，&lt;span class="math">\(\hat\theta(x)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个&lt;strong>无偏估计&lt;/strong>，即&lt;span class="math">\(E[\hat\theta(x)]=\theta\)&lt;/span>，则 &lt;span class="math">\[h(T)=E[\hat\theta(x)|T]\]&lt;/span> 是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，并且 &lt;span class="math">\[D[h(T)]\leq D[\hat\theta(x)]\]&lt;/span> 其中当且仅当&lt;span class="math">\(P(\hat\theta(x))=h(T)=1\)&lt;/span>，即&lt;span class="math">\(h(T)=\hat\theta(x)\)&lt;/span>，a.s. P成立。&lt;/p>
&lt;/blockquote>
&lt;p>我们先解释下&lt;span class="math">\(h(T)=E[\hat\theta(x)|T]\)&lt;/span>，从条件期望的&lt;span class="math">\(E(X|Y)\)&lt;/span>可知，我们是对&lt;span class="math">\(X\)&lt;/span>求期望，会将&lt;span class="math">\(X\)&lt;/span>的随机性抹去，&lt;span class="math">\(E(X|Y)\)&lt;/span>实际上是关于&lt;span class="math">\(Y\)&lt;/span>的随机变量函数，即&lt;span class="math">\(f(Y)=E(X|Y)\)&lt;/span>，当&lt;span class="math">\(Y=y\)&lt;/span>时，函数的值就确定了。因此&lt;span class="math">\(h(T)=E[\hat\theta(x)|T]\)&lt;/span>就是一个关于&lt;span class="math">\(T\)&lt;/span>的随机变量函数&lt;span class="math">\(h(T)\)&lt;/span>。这个定理的关键就是说，这样的复合函数&lt;span class="math">\(h\cdot T\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，且方差比原来的无偏估计&lt;span class="math">\(\hat\theta(x)\)&lt;/span>小。&lt;/p>
&lt;p>&lt;span class="math">\(h\cdot T\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计这点用重期望公式可以证明： &lt;span class="math">\[
E[h(T)]=E[E(\hat\theta(x)|T)]=E[\hat\theta(x)]
\]&lt;/span> 由于&lt;span class="math">\(\hat\theta(x)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计，所以&lt;span class="math">\(E[h(T)]=E[\hat\theta(x)]=\theta\)&lt;/span>，即&lt;span class="math">\(E[h(T)]\)&lt;/span>也是&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计。&lt;/p>
&lt;p>对于方差的证明，我们需要用一个小技巧： &lt;span class="math">\[
\begin{aligned}
D[\hat\theta(x)]&amp;amp;=E\{\hat\theta(x)-E[\hat\theta(x)]\}^2\\
&amp;amp;=E\{\hat\theta(x)\underbrace{-h(T)+h(T)}_{引入h(T)}-\underbrace{E[\hat\theta(x)]}_\theta\}^2\\
&amp;amp;=E[\hat\theta(x)-h(T)]^2+\underbrace{E[h(T)-\theta]^2}_{D[h(T)]}+2\{E[h(T)-\theta][\hat\theta(x)-h(T)]\}
\end{aligned}
\]&lt;/span> 前面两项好理解，需要处理一下交叉相乘的最后一项。我们可以根据重期望公式将其换种写法： &lt;span class="math">\[
E\{[h(T)-\theta][\hat\theta(x)-h(T)]\}=E_T\{E[(h(T)-\theta)(\hat\theta(x)-h(T))|T]\}\\
当T=t为给定的条件时，h(T)为一常数\\
原式=E_T\{(h(T)-\theta)E[\hat\theta(x)-h(T)|T]\}=E_T\{(h(T)-\theta)\{E[\hat\theta(x)|T]-h(T)\}\}\\
\]&lt;/span> 在式子的最后，我们发现有一项&lt;span class="math">\(E[\hat\theta(x)|T]\)&lt;/span>，而这正是&lt;span class="math">\(h(T)\)&lt;/span>的定义啊，所以必有 &lt;span class="math">\[
E_T\{(h(T)-\theta)\underbrace{\{E[\hat\theta(x)|T]-h(T)\}}_{E[\hat\theta(x)|T]=h(T)}\}=0
\]&lt;/span> 因此，&lt;span class="math">\(2\{E[h(T)-\theta][\hat\theta(x)-h(T)]\}=0\)&lt;/span>，&lt;span class="math">\(D[\hat\theta(x)]\)&lt;/span>可以写成： &lt;span class="math">\[
D[\hat\theta(x)]=E[\hat\theta(x)-h(T)]^2+D[h(T)]\geq D[h(T)],\forall \theta \in \varTheta
\]&lt;/span> 并且等号成立的条件是&lt;span class="math">\(E[\hat\theta(x)-h(T)]^2=0\)&lt;/span>，即&lt;span class="math">\(\hat\theta(x)=h(T)\)&lt;/span>。这里再多解释一句，充分统计量&lt;span class="math">\(T\)&lt;/span>也是&lt;span class="math">\(x\)&lt;/span>的函数，所以前式具体应写为&lt;span class="math">\(\hat\theta(x)=h(T(x))\)&lt;/span>或&lt;span class="math">\(\hat\theta=h\cdot T\)&lt;/span>。&lt;/p>
&lt;p>Rao–Blackwell定理的意义在于，&lt;strong>如果我们能够找到一个充分统计量，那么就用&lt;span class="math">\(E[\hat\theta(x)|T]\)&lt;/span>可以改进任何&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，得到&lt;span class="math">\(h(T(x))\)&lt;/span>&lt;/strong>。且在&lt;strong>充分统计量存在&lt;/strong>的情况下，UMVUE一定是充分统计量的函数（除非相等），否则我们可以通过&lt;span class="math">\(h(T)\)&lt;/span>构造一个方差更小的无偏估计。&lt;/p>
&lt;p>举个例子：设&lt;span class="math">\(X=(X_1,\dotsb,X_n)\)&lt;/span>是从两点分布族&lt;span class="math">\(\{b(1,p):0&amp;lt;p&amp;lt;1\}\)&lt;/span>中抽取的简单样本。显然&lt;span class="math">\(X_1\)&lt;/span>是&lt;span class="math">\(p\)&lt;/span>的一个无偏估计，从前文可知&lt;span class="math">\(T(X)=\sum_{i=1}^n X_i\)&lt;/span>是&lt;span class="math">\(p\)&lt;/span>的充分统计量，试利用&lt;span class="math">\(T\)&lt;/span>构造一个比&lt;span class="math">\(X_1\)&lt;/span>方差更小的无偏估计。&lt;/p>
&lt;p>利用Rao–Blackwell定理可知，通过条件期望构造的无偏估计如下： &lt;span class="math">\[
h(t)=E[X_1|T=t]=1\times P(X_1=1|T=t)+0\times P(X_1=0|T=t)\\
=E[X_1|T=t]=1\times P(X_1=1|T=t)=\frac{P(X_1=1,T=t)}{P(T=t)}\\
=\frac{P(X_1=1,X_2+\dots+X_n={t-1})}{P(T=t)}=\frac{p\cdot{n-1\choose{t-1}}p^{t-1}(1-p)^{n-t}}{{n\choose{t}}p^{t}(1-p)^{n-t}}\\
=\frac{t}{n}=\frac{\sum_{i=1}^n X_i}{n}=\bar{X}
\]&lt;/span> 显然，样本均值&lt;span class="math">\(h(T)=\bar{X}\)&lt;/span>也是&lt;span class="math">\(p\)&lt;/span>的无偏估计，且方差为&lt;span class="math">\(p(1-p)/n\)&lt;/span>，而&lt;span class="math">\(X_1\)&lt;/span>的方差为&lt;span class="math">\(p(1-p)\)&lt;/span>，只要当&lt;span class="math">\(n\geq 2\)&lt;/span>时，&lt;span class="math">\(\bar{X}\)&lt;/span>的方差就会小于&lt;span class="math">\(X_1\)&lt;/span>。然而，Rao-Blackwell定理能够通过充分统计量改进无偏估计统计量，却并没有告诉我们改进后的估计是否为UMVUE或者如何改进成UMVUE。为了判断一个统计量是否为UMVUE，我们先介绍一些判别方法。&lt;/p>
&lt;h4 id="零无偏估计法">零无偏估计法&lt;/h4>
&lt;blockquote>
&lt;p>定理3：设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自某总体的一个样本，&lt;span class="math">\(\hat\theta(X)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计，&lt;span class="math">\(D[\hat\theta(X)]&amp;lt;\infty\)&lt;/span>，则&lt;span class="math">\(\hat\theta\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE的&lt;strong>充要条件&lt;/strong>是：&lt;/p>
&lt;p>对任意一个满足&lt;span class="math">\(E[\varphi(X)]=0\)&lt;/span>且&lt;span class="math">\(\mathrm{Var}[\varphi(X)]&amp;lt;\infty\)&lt;/span>的&lt;span class="math">\(\varphi(X)\)&lt;/span>，都有 &lt;span class="math">\[\mathrm{Cov}(\hat\theta,\varphi)=0,\forall\theta\in\varTheta\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>其中，&lt;span class="math">\(\varphi(X)\)&lt;/span>是一个统计量，其期望为0，可称为零的无偏估计，这也是这个方法名字的由来。这个定理的常用解释为：&lt;span class="math">\(\theta\)&lt;/span>的UMVUE必与任何零的无偏估计&lt;strong>线性不相关&lt;/strong>。反之，与任何零的无偏估计&lt;strong>线性不相关&lt;/strong>的估计量，必然是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE。&lt;/p>
&lt;blockquote>
&lt;p>我们先证明这个定理的&lt;strong>充分性&lt;/strong>，即&lt;span class="math">\(\Rightarrow\)&lt;/span> UMVUE&lt;/p>
&lt;p>设有另一个&lt;span class="math">\(\theta\)&lt;/span>的无偏估计&lt;span class="math">\(\tilde{\theta}(X)\)&lt;/span>，那么构造函数&lt;span class="math">\(\varphi(X)=\hat\theta(X)-\tilde{\theta}(X)\)&lt;/span>，其期望为 &lt;span class="math">\[E[\varphi(X)]=E[\hat\theta(X)-\tilde{\theta}(X)]=E[\hat\theta(X)]-E[\tilde{\theta}(X)]=\theta-\theta=0\]&lt;/span> 方差为： &lt;span class="math">\[\mathrm{Var}[\tilde\theta(X)]=E[\tilde\theta(X)-\theta]^2=E[(\tilde{\theta}(X)-\hat\theta(X))+(\hat\theta(X)-\theta)]^2\\
=\underbrace{E[(\tilde{\theta}(X)-\hat\theta(X))^2]}_{E(\varphi^2(X))}+\underbrace{E[(\hat\theta(X)-\theta)^2]}_{\mathrm{Var}(\hat\theta)}+2\underbrace{\mathrm{Cov}(\varphi(X),\hat\theta(X))}_{定理充分性体现：=0}\\
=\underbrace{E(\varphi^2(X))}_{\geq 0}+\mathrm{Var}(\hat\theta)\geq \mathrm{Var}(\hat\theta)\]&lt;/span> 这表明，&lt;span class="math">\(\hat\theta\)&lt;/span>在&lt;span class="math">\(\theta\)&lt;/span>的无偏估计中方差最小，即为UMVUE。&lt;/p>
&lt;p>我们再证明定理的&lt;strong>必要性&lt;/strong>，即UMVUE&lt;span class="math">\(\Rightarrow \mathrm{Cov}(\hat\theta,\varphi)=0,\forall\theta\in\varTheta\)&lt;/span>。我们采用反证法证明必要性。&lt;/p>
&lt;p>设&lt;span class="math">\(\hat\theta(X)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE，存在一个&lt;span class="math">\(\varphi(X)\)&lt;/span>满足&lt;span class="math">\(E[\varphi(X)]=0\)&lt;/span>且&lt;span class="math">\(\mathrm{Var}[\varphi(X)]&amp;lt;\infty\)&lt;/span>。假设，在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>中存在一个&lt;span class="math">\(\theta_0\)&lt;/span>使得&lt;span class="math">\(\mathrm{Cov}(\varphi(X),\hat\theta(X))=a\neq 0\)&lt;/span>。下面就是一系列神仙操作，令： &lt;span class="math">\[
b=-\frac{\mathrm{Cov}(\varphi(X),\hat\theta(X))}{\mathrm{Var}[\varphi(X)])}=-\frac{a}{\mathrm{Var}[\varphi(X)])}\neq 0
\]&lt;/span> 现在，构造一个&lt;span class="math">\(X\)&lt;/span>的估计&lt;span class="math">\(\tilde{\theta}=\hat{\theta}+b\varphi(X)\)&lt;/span>，对其求期望可得&lt;span class="math">\(E[\tilde{\theta}]=E[\hat{\theta}+b\varphi(X)]=E[\hat{\theta}]+bE[\varphi(X)]\)&lt;/span>，由于&lt;span class="math">\(\varphi(X)\)&lt;/span>期望为0，所以&lt;span class="math">\(E[\tilde{\theta}]=\theta\)&lt;/span>，也是一个无偏估计，然而其方差为： &lt;span class="math">\[\begin{aligned}\mathrm{Var}(\tilde{\theta})&amp;amp;=E[\hat{\theta}+b\varphi(X)-\theta]^2\\
&amp;amp;=E(\hat{\theta}-\theta)^2+b^2E[\varphi^2(X)]+2bE[(\hat{\theta}-\theta)\varphi(X)]\\
\because E(\hat{\theta}-\theta)^2&amp;amp;=\mathrm{Var}(\hat{\theta});E[\varphi^2(X)]=\mathrm{Var}[\varphi(X)]\\
\because E[(\hat{\theta}-\theta)\varphi(X)]&amp;amp;=E[(\hat{\theta}-\theta)(\varphi(X)-0)]= \mathrm{Cov}[\varphi(X),\hat\theta(X)]=a\\
原式&amp;amp;=\mathrm{Var}(\hat{\theta})+b^2\mathrm{Var}[\varphi(X)]+2ab\end{aligned}\]&lt;/span> 由于&lt;span class="math">\(b=-\frac{a}{\mathrm{Var}[\varphi(X)])}\)&lt;/span>，因此&lt;span class="math">\(b^2\mathrm{Var}[\varphi(x)]=\frac{a^2}{\mathrm{Var}[\varphi(x)]},\quad 2ab=\frac{-2a^2}{\mathrm{Var}[\varphi(x)]}\)&lt;/span>。因此上式可化简为： &lt;span class="math">\[
原式=\mathrm{Var}(\hat{\theta})-\frac{a^2}{\mathrm{Var}[\varphi(X)]}&amp;lt;\mathrm{Var}(\hat{\theta})
\]&lt;/span> 显然，此时&lt;span class="math">\(\mathrm{Var}(\tilde{\theta})&amp;lt;\mathrm{Var}(\hat{\theta})\)&lt;/span>，即&lt;span class="math">\(\hat\theta(X)\)&lt;/span>并不是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE，与前提矛盾。所以必有&lt;span class="math">\(\mathrm{Cov}(\varphi(X),\hat\theta(X))=0\)&lt;/span>。&lt;/p>
&lt;p>得证。&lt;/p>
&lt;/blockquote>
&lt;p>对于统计量&lt;span class="math">\(\hat\theta(X)\)&lt;/span>，我们必须对任意的零无偏估计统计量&lt;span class="math">\(\varphi(x)\)&lt;/span>都要有&lt;span class="math">\(\mathrm{Cov}(\hat{\theta},\varphi)=0\)&lt;/span>，这在证明时需要很多技巧，而且很多时候很难证明。但如果，我们能够证明这个统计量是个&lt;strong>充分统计量&lt;/strong>，那么就可以有以下推论：&lt;/p>
&lt;blockquote>
&lt;p>推论3-1：设&lt;span class="math">\(T=T(X)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的充分统计量，&lt;span class="math">\(h(T(X))\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计，且方差&lt;span class="math">\(D[h(T(X))]&amp;lt;\infty\)&lt;/span>。对任何充分统计量&lt;span class="math">\(T\)&lt;/span>的函数&lt;span class="math">\(\delta(T)\)&lt;/span>，如果&lt;span class="math">\(E[\delta(T)]=0\)&lt;/span>，必有 &lt;span class="math">\[\mathrm{Cov}[h(T),\delta(T)]=E[h(T)\times\delta(T)]=0\]&lt;/span> 那么，&lt;span class="math">\(h(T(X))\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE。&lt;/p>
&lt;/blockquote>
&lt;p>此推论在充分统计量的条件下，将所有的零无偏估计缩小到零无偏的充分统计函数的函数。&lt;/p>
&lt;h4 id="充分完全统计量法">充分完全统计量法&lt;/h4>
&lt;p>充分完全统计量法的核心是LS定理，它不仅给出了UMVUE的充分条件，还给出了唯一性和如何构造UMVUE的线索。&lt;/p>
&lt;blockquote>
&lt;p>定理4（Lehmann-Scheff定理，简称LS定理）：设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自总体&lt;span class="math">\(\{f(x,\theta),\theta\in\varTheta\}\)&lt;/span>的一个样本，&lt;span class="math">\(\varTheta\)&lt;/span>为参数空间，&lt;span class="math">\(T(X)\)&lt;/span>是参数&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>充分完全统计量&lt;/strong>，若&lt;span class="math">\(\hat{g}(T(X))\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计，则&lt;span class="math">\(\hat{g}(T(X))\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>唯一的UMVUE&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>注意，此处唯一是依概率1的唯一，即设&lt;span class="math">\(\hat{g},\hat{g}_1\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的两个估计量，若&lt;span class="math">\(P(\hat{g}=\hat{g}_1)=1\)&lt;/span>，对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，则视&lt;span class="math">\(\hat{g},\hat{g}_1\)&lt;/span>为同一个估计量。&lt;/p>
&lt;p>定理的证明：&lt;/p>
&lt;blockquote>
&lt;p>先证利用统计量的完全性证明唯一性。&lt;/p>
&lt;p>设&lt;span class="math">\(\hat{g}_1(T(X))\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的任一无偏估计，令&lt;span class="math">\(\delta(T(X))=\hat{g}(T(X))-\hat{g}_1(T(X))\)&lt;/span>，则&lt;span class="math">\(E[\delta(T(X))]=E[\hat{g}(T(X))]-E[\hat{g}_1(T(X))]=0, \theta\in\varTheta\)&lt;/span>。由于&lt;span class="math">\(T(X)\)&lt;/span>是完全统计量，而&lt;span class="math">\(\delta(T(X))\)&lt;/span>是其函数，根据信息处理过程中的信息量不增定理，&lt;span class="math">\(\delta(T(X))\)&lt;/span>也是完全统计量。&lt;/p>
&lt;p>根据完全统计量的定义，&lt;span class="math">\(E[\delta(T(X))]=0\)&lt;/span>，几乎处处成立（a.s.）时，&lt;span class="math">\(\delta(T(X))=0\)&lt;/span>，即&lt;span class="math">\(\hat{g}(T(X))=\hat{g}_1(T(X))\)&lt;/span>几乎处处成立（a.s.）。又由于&lt;span class="math">\(\hat{g}_1(T(X))\)&lt;/span>的任意性，唯一性得证。&lt;/p>
&lt;p>在唯一性得证的基础上，我们使用充分性证明方差最小。设&lt;span class="math">\(\varphi(X)\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>任一&lt;/strong>无偏估计，令&lt;span class="math">\(h(T(X))=E[\varphi(X)|T]\)&lt;/span>，由于&lt;span class="math">\(T(X)\)&lt;/span>时充分统计量，因此&lt;span class="math">\(p(\varphi(X)|T)\)&lt;/span>的概率分布与&lt;span class="math">\(\theta\)&lt;/span>无关，那么其期望&lt;span class="math">\(h(T(X))=E[\varphi(X)|T]\)&lt;/span>也与待估计参数无关，因此&lt;span class="math">\(h(T(X))\)&lt;/span>也是一个统计量。而由定理2，Rao–Blackwell定理可知： &lt;span class="math">\[
E[h(T(X))]=E[E[\varphi(X)|T]]=E[\varphi(X)]=\theta,\forall \theta\in\varTheta\\
D[h(T(X))]\leq D[\varphi(X)],\forall \theta\in\varTheta
\]&lt;/span> 又因为由充分完全统计量构造出来的无偏估计是唯一的（唯一性），因此对任一&lt;span class="math">\(\varphi(X)\)&lt;/span>构造出来的&lt;span class="math">\(h(T(X))=E[\varphi(X)|T]\)&lt;/span>，都等于&lt;span class="math">\(\hat{g}(T(X))\)&lt;/span>。综上可知有： &lt;span class="math">\[\hat{g}(T(X)) \leq D[\varphi(X)],\forall \theta\in\varTheta\]&lt;/span> 最小方差特性得证。&lt;/p>
&lt;/blockquote>
&lt;p>这个方法的&lt;strong>核心是先找到个一个充分完全统计量&lt;/strong>，比如可以通过因子分解定理，指数族概率函数特性，定义等方式获得。当有了充分完全统计量后，可以有两个方法求UMVUE。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>使用Rao–Blackwell定理。首先找个一个无偏估计&lt;span class="math">\(\varphi(X)\)&lt;/span>，然后通过条件期望&lt;span class="math">\(h(T)=E[\varphi(x)|T]\)&lt;/span>，得到充分统计量&lt;span class="math">\(T\)&lt;/span>的函数，且根据Rao–Blackwell定理，&lt;span class="math">\(h(T)\)&lt;/span>是无偏的。&lt;/li>
&lt;li>直接使用充分统计量，构造&lt;span class="math">\(\theta\)&lt;/span>无偏估计的函数。适用于充分统计量与无偏估计关系比较简单的情形。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>推论4-1：设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自指数族总体 &lt;span class="math">\[f(\bm{x},\bm{\theta})=c(\bm\theta)\exp\left\{\sum_{j=1}^k \theta_j T_j(x)\right\}h(x),\bm\theta=(\theta_1,\dotsb,\theta_k)\in\varTheta^\star\]&lt;/span> 令&lt;span class="math">\(T(x)=(T_1(x),\dotsb,T_k(x))\)&lt;/span>，且自然参数空间&lt;span class="math">\(\varTheta^\star\)&lt;/span>作为&lt;span class="math">\(R_k\)&lt;/span>的自己有内点，且&lt;span class="math">\(g(T(X))\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，则&lt;span class="math">\(g(T(X))\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的唯一的UMVUE。&lt;/p>
&lt;/blockquote>
&lt;p>其实，根据指数族分布的性质可知，&lt;span class="math">\(T(X)\)&lt;/span>实际上就是充分完全统计量，因此只有找到一个函数&lt;span class="math">\(g\)&lt;/span>使&lt;span class="math">\(g(T(X))\)&lt;/span>无偏，那么根据定理4，它就是参数&lt;span class="math">\(\theta\)&lt;/span>的唯一的UMVUE。&lt;/p>
&lt;h3 id="cramer-raoc-r不等式与界">Cramer-Rao（C-R）不等式与界&lt;/h3>
&lt;p>Cramer-Rao不等式是另一个判别无偏估计是否为UMVUE的方法，但是Cramer-Rao不等式有更深层的含义。&lt;/p>
&lt;p>我们知道估计量始终会是一个随机变量，有自己的概率分布，而不是一个准确的值。Cramer-Rao除了给出了Cramer-Rao正则分布族这种费雪信息的存在条件，还有另一个更重要的贡献：&lt;strong>C-R不等式&lt;/strong>，可以说给了统计学理论上的绝望。&lt;/p>
&lt;p>C-R不等式，其实就是在说：统计，对真实的概率分布参数估计能力是有限的。举个不太恰当的类比，有点像量子理论中的测不准原理 （二者证明有相似之处哦）。C-R不等式告诉我们，无论我们如何抽样充足，无论我们统计方法如何科学，我们对参数的估计值，永远不可能无限逼近是逻辑上的真实值！&lt;/p>
&lt;p>回到C-R不等式和UMVUE的关系上来，其思想如下：设&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一切无偏估计构成的集合，所有的这些&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>中的无偏估计的方差必有一个下界（一定非负），这个下界称为C-R下界。如果&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>中某一个估计量&lt;span class="math">\(\hat g\)&lt;/span>的方差达到了这个下界，则&lt;span class="math">\(\hat{g}\)&lt;/span>就一定是参数的UMVUE，当然会对样本分布族和&lt;span class="math">\(\hat{g}\)&lt;/span>有一些正则条件。当时，使用这种下界的方法，都一个缺点，即&lt;strong>C-R不等式给出的下界经常比实际的下界更小一些&lt;/strong>。这一情况下，C-R不等式就无法判断UMVUE的存在性。此外，C-R不等式还有其他一些用处，比如计算估计的效率、有效估计等等。&lt;/p>
&lt;p>具体C-R不等式的细节，见笔记&lt;a href="概率统计随机过程之C-R不等式.md">《概率统计随机过程之C-R不等式》&lt;/a>&lt;/p>
&lt;h2 id="区间估计">区间估计&lt;/h2>
&lt;p>估计一个&lt;strong>数值范围&lt;/strong>，核心要求：&lt;strong>希望以尽可能大的概率落在尽可能小的区间内&lt;/strong>。&lt;/p>
&lt;h3 id="置信区间与枢轴变量">置信区间与枢轴变量&lt;/h3>
&lt;ul>
&lt;li>区间长度：越长概率越大，越不精确&lt;/li>
&lt;li>以多大概率落在区间中&lt;/li>
&lt;li>希望以尽可能大的概率落在尽可能小的区间内&lt;/li>
&lt;/ul>
&lt;p>需要指出的是，置信区间是指其能包括待估计参数的区间，而不是待估计参数落入区间的意思。待估计值是客观存在的。&lt;/p>
&lt;blockquote>
&lt;p>枢轴变量：枢轴变量一般满足特定的分布，枢轴变量和待估计参数之间存在确定的函数关系，因此通过枢轴变量可以求出置信区间。&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../../images/枢轴变量.jpg" alt="枢轴变量.jpg" />&lt;p class="caption">枢轴变量.jpg&lt;/p>
&lt;/div>
&lt;h3 id="一个正态总体的期望和方差的区间估计">一个正态总体的期望和方差的区间估计&lt;/h3>
&lt;p>当我们从一个含有未知参数的正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>抽样时，可以通过抽样的样本对参数&lt;span class="math">\(\mu,\sigma^2\)&lt;/span>进行区间估计，分为以下四种情况。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/一个正态总体的期望和方差的区间估计.jpg" alt="一个正态总体的期望和方差的区间估计" />&lt;p class="caption">一个正态总体的期望和方差的区间估计&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之最大似然估计拓展</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E6%8B%93%E5%B1%95/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E6%8B%93%E5%B1%95/</guid><description>
&lt;h2 id="概率统计随机过程之最大似然估计拓展">概率统计随机过程之最大似然估计拓展&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#最大似然估计概述">最大似然估计概述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计的不变原理">最大似然估计的不变原理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计服从渐进正态的条件">最大似然估计服从渐进正态的条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#渐近正态性">渐近正态性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计具有正态性定理">最大似然估计具有正态性定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#cramer-rao正则分布族下费雪信息量的另一种表示">Cramer-Rao正则分布族下费雪信息量的另一种表示&lt;/a>&lt;/li>
&lt;li>&lt;a href="#cramer-rao不等式与界">Cramer-Rao不等式与界&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计与相对熵kl散度交叉熵的等价性">最大似然估计与相对熵（KL散度）、交叉熵的等价性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计不具备的性质">最大似然估计不具备的性质&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>最大似然估计是参数点估计中非常常用且有效的估计方法，本文对其相关性质进行拓展介绍，包括渐进正态性，一致最小方差无偏估计，C-R界以及存在隐变量时采用的EM算法。&lt;/p>
&lt;p>本文阅读需建立在&lt;strong>已经理解最大似然估计基础上&lt;/strong>，首先我们精简的概述最大似然估计。&lt;/p>
&lt;h2 id="最大似然估计概述">最大似然估计概述&lt;/h2>
&lt;p>核心思想：概率大的事件比概率小的事件更容易发生。&lt;strong>要估计的参数能够使产生这个样本的概率最大&lt;/strong>。&lt;/p>
&lt;p>步骤：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>总体的概率/密度函数。&lt;/li>
&lt;li>写出似然估计函数&lt;span class="math">\(L(\theta)\)&lt;/span>，其中&lt;span class="math">\(\theta\)&lt;/span>是待估计参数。&lt;/li>
&lt;li>两边取&lt;span class="math">\(\ln\)&lt;/span>，即为&lt;span class="math">\(l(\theta)=\ln(L(\theta))\)&lt;/span>。(注意，对数似然函数是小写&lt;span class="math">\(l\)&lt;/span>)&lt;/li>
&lt;li>对&lt;span class="math">\(l(\theta)\)&lt;/span>的&lt;span class="math">\(\theta\)&lt;/span>求导（多参数估计就是偏导），令导数为0。&lt;/li>
&lt;li>求出使导数为0的&lt;span class="math">\(\theta\)&lt;/span>即为最大似然估计参数&lt;span class="math">\(\hat\theta_{MLE}\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;p>TIPS：对于有些分布的极大似然估计没法直接求，比如均匀分布。&lt;/p>
&lt;p>&lt;strong>似然函数取对数的原因&lt;/strong>:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>减少计算量。乘法变成加法，从而减少了计算量；同时，如果概率中含有指数项，如高斯分布，能把指数项也化为求和形式，进一步减少计算量；另外，在对联合概率求导时，和的形式会比积的形式更方便。&lt;/li>
&lt;li>计算时更准确。为概率值都在[0,1]之间，因此，概率的连乘将会变成一个很小的值，可能会引起浮点数下溢，尤其是当数据集很大的时候，联合概率会趋向于0，非常不利于之后的计算。&lt;/li>
&lt;li>取对数后，可以是一个上凸函数，更有利于求取最大值。&lt;/li>
&lt;/ol>
&lt;p>需要指出的是：&lt;strong>取对数不影响单调性&lt;/strong>。 &lt;span class="math">\[
p(x|\theta_1)&amp;gt;p(x|\theta_2)\Leftrightarrow \ln(p(x|\theta_1))&amp;gt;\ln(p(x|\theta_2))
\]&lt;/span> 因为相同的单调性，它确保了概率的最大对数值出现在与原始概率函数相同的点上。因此，可以用更简单的对数似然来代替原来的似然。&lt;/p>
&lt;h3 id="最大似然估计的不变原理">最大似然估计的不变原理&lt;/h3>
&lt;p>我们介绍一个致使最大似然估计得到广泛应用的定理：&lt;/p>
&lt;blockquote>
&lt;p>不变定理：设&lt;span class="math">\(X\sim p(x;\theta),\;\theta\in\mathcal{\Theta}\)&lt;/span>，若&lt;span class="math">\(\theta\)&lt;/span>的最大似然估计为&lt;span class="math">\(\hat\theta\)&lt;/span>，则对任意函数&lt;span class="math">\(\gamma=g(\theta)\)&lt;/span>，&lt;span class="math">\(\gamma\)&lt;/span>的最大似然估计为&lt;span class="math">\(\hat\gamma=g(\hat\theta)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h2 id="最大似然估计服从渐进正态的条件">最大似然估计服从渐进正态的条件&lt;/h2>
&lt;p>最大似然估计除了不变原理，还有另一个非常好的性质就是在一定条件下满足渐进正态性，从而保证了样本增加时的参数估计的收敛速度（通常为&lt;span class="math">\(\frac{1}{\sqrt{n}}\)&lt;/span>）。&lt;/p>
&lt;h3 id="渐近正态性">渐近正态性&lt;/h3>
&lt;p>渐进正态性（大样本性质）：估计量的渐进正态性来源于&lt;strong>中心极限定理&lt;/strong>，若统计量在样本容量&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>时，统计量的分布也渐近于正态分布，称为渐进正态性。具体可定义为：如果存在一序列&lt;span class="math">\(\{\sigma_n^2\}\)&lt;/span>，满足&lt;span class="math">\((\hat\theta_n-\theta)/\sigma_n(\theta)\overset{L}{\rightarrow}N(0,1)\)&lt;/span>，则称&lt;span class="math">\(\hat\theta_n\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的渐进正态估计，&lt;span class="math">\(\sigma_n^2\)&lt;/span>称为&lt;span class="math">\(\hat\theta_n\)&lt;/span>的渐进方差。&lt;/p>
&lt;p>严格定义如下：&lt;/p>
&lt;blockquote>
&lt;p>渐近正态性：设&lt;span class="math">\(\hat\theta_n=\hat\theta(x_1,x_2,\dotsb,x_n)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个相合估计序列，若存在一个趋于零的正数列&lt;span class="math">\(\sigma_n(\theta)\)&lt;/span>，使得规范变量&lt;span class="math">\(y_n=\frac{\hat\theta_n-\theta}{\sigma_n(\theta)}\)&lt;/span>的概率分布函数&lt;span class="math">\(F_n(y)\)&lt;/span>收敛于标准正态分布函数&lt;span class="math">\(\varPhi(y)\)&lt;/span>，即： &lt;span class="math">\[
F_n(y)=P(\frac{\hat\theta_n-\theta}{\sigma_n(\theta)}\leq y)\rightarrow \varPhi(y)(n\rightarrow \infty)
\]&lt;/span> 如果用依分布收敛符号&lt;span class="math">\(L\)&lt;/span>可表示为： &lt;span class="math">\[
\frac{\hat\theta_n-\theta}{\sigma_n(\theta)}\overset{L}{\rightarrow} N(0,1)(n\rightarrow\infty)
\]&lt;/span> 则称&lt;span class="math">\(\hat\theta_n\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的渐进正态估计，或称&lt;span class="math">\(\hat\theta_n\)&lt;/span>具有渐进正态性，即： &lt;span class="math">\[
\hat\theta_n\sim AN(\theta,\sigma^2_n(\theta))
\]&lt;/span> 其中，&lt;span class="math">\(\sigma^2_n(\theta)\)&lt;/span>称为&lt;span class="math">\(\hat\theta_n\)&lt;/span>的渐进方差。&lt;/p>
&lt;/blockquote>
&lt;p>从&lt;span class="math">\((\hat\theta_n-\theta)/\sigma_n(\theta)\)&lt;/span>来看，分子项依概率收敛于&lt;span class="math">\(\theta\)&lt;/span>的速度与分母项&lt;span class="math">\(\sigma_n(\theta)\)&lt;/span>趋近于0的速度相同时，其比值才会稳定与正态分布。因此，&lt;span class="math">\(\hat\theta_n\)&lt;/span>收敛速度与渐近方差直接相关，渐近方差越小，收敛越快。&lt;/p>
&lt;p>还需要指出，渐进方差并不是唯一的，如果存在另一&lt;span class="math">\(\tau_n(\theta)\)&lt;/span>有&lt;span class="math">\(\frac{\sigma_n(\theta)}{\tau_n(\theta)}\rightarrow 1(n\rightarrow\infty)\)&lt;/span>，根据依概率收敛定义可知&lt;span class="math">\(\tau_n(\theta)\)&lt;/span>也是&lt;span class="math">\(\theta\)&lt;/span>的渐近方差。&lt;/p>
&lt;p>渐进正态性和相合性的关系类似于中心极限定理和大数定律。相合性是对估计的一种较低要求，它只要求估计序列&lt;span class="math">\(\{\hat\theta_n\}\)&lt;/span>在样本数量&lt;span class="math">\(n\)&lt;/span>增加的时候也趋近于&lt;span class="math">\(\theta\)&lt;/span>，但是并没有指出趋近的速度（例如是&lt;span class="math">\(1/n,1/\sqrt{n}\)&lt;/span>或&lt;span class="math">\(1/\ln n\)&lt;/span>）。而渐进正态性补充了这一点，收敛速度与渐进方差相关。&lt;strong>经验来看，大多数渐进正态估计都是以&lt;span class="math">\(1/\sqrt{n}\)&lt;/span>的速度收敛于被估参数的&lt;/strong>。&lt;/p>
&lt;h3 id="最大似然估计具有正态性定理">最大似然估计具有正态性定理&lt;/h3>
&lt;p>在一定条件下，最大似然估计具有渐进正态性。我们将通过如下定理阐释。需要指出的是，定理是以连续分布的形式给出，但是对于离散场景也是适用的。&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(p(x;\theta)\)&lt;/span>是某密度函数，其参数空间&lt;span class="math">\(\varTheta=\{\theta\}\)&lt;/span>是直线上的非退化区间（即不是一个点），假如：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，&lt;span class="math">\(p(x;\theta)\)&lt;/span>对&lt;span class="math">\(\theta\)&lt;/span>如下偏导都存在：&lt;span class="math">\(\frac{\partial\ln p}{\partial\theta},\frac{\partial^2\ln p}{\partial\theta^2},\frac{\partial^3\ln p}{\partial\theta^3}\)&lt;/span>&lt;/li>
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，有&lt;span class="math">\(|\frac{\partial\ln p}{\partial\theta}|&amp;lt;F_1(x),|\frac{\partial^2\ln p}{\partial\theta^2}|&amp;lt;F_2(x),\frac{\partial^3\ln p}{\partial\theta^3}&amp;lt;H(x)\)&lt;/span>成立，其中&lt;span class="math">\(F_1(x)\)&lt;/span>与&lt;span class="math">\(F_2(x)\)&lt;/span>在实数轴上可积，而&lt;span class="math">\(H(x)\)&lt;/span>满足：&lt;span class="math">\(\int_{-\infty}^\infty H(x)p(x;\theta)&amp;lt;M\)&lt;/span>，这里&lt;span class="math">\(M\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关。&lt;/li>
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，有&lt;span class="math">\(0&amp;lt;I(\theta)=E[(\frac{\partial\ln p}{\partial \theta})^2]&amp;lt;+\infty\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>则在参数真值&lt;span class="math">\(\theta\)&lt;/span>为参数空间&lt;span class="math">\(\varTheta\)&lt;/span>内点的情况下，其似然方程有一个解存在，且此解&lt;span class="math">\(\hat\theta_n=\theta(x_1,x_2,\dotsb,x_n)\)&lt;/span>依概率收敛于&lt;span class="math">\(\theta\)&lt;/span>，且： &lt;span class="math">\[
\hat\theta_n\sim AN(\theta,[nI(\theta)]^{-1})
\]&lt;/span> 其中，&lt;span class="math">\(I(\theta)\)&lt;/span>为分布&lt;span class="math">\(p(x;\theta)\)&lt;/span>中含有&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>费雪信息量&lt;/strong>，简称信息量。&lt;/p>
&lt;/blockquote>
&lt;p>这个定理的意义在于给定了最大似然分布有渐进正态性的条件，其中渐进方差（体现大样本效率）完全由&lt;strong>样本数量&lt;span class="math">\(n\)&lt;/span>和分布的费雪信息量&lt;span class="math">\(I(\theta)\)&lt;/span>决定&lt;/strong>，且费雪信息量越大（分布中含有&lt;span class="math">\(\theta\)&lt;/span>）的信息越多，渐进方差在同等样本数量下越小，从而最大似然估计效果越好。&lt;/p>
&lt;p>我们在这个定理中引入了费雪信息量这一概念，因此还要&lt;strong>注意费雪信息量是否存在&lt;/strong>。对此，我们给出一个简单的概要：&lt;strong>Cramer-Rao正则分布族中的分布费雪信息都是存在的&lt;/strong>。Cramer-Rao正则分布族定义如下：&lt;/p>
&lt;blockquote>
&lt;p>Cramer-Rao正则分布族：分布&lt;span class="math">\(p(x;\theta)\)&lt;/span>，&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>属于Cramer-Rao正则分布族，则需要满足以下五个条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>参数空间&lt;span class="math">\(\varTheta\)&lt;/span>是直线上的开区间；&lt;/li>
&lt;li>&lt;span class="math">\(\frac{\partial\ln p}{\partial\theta}\)&lt;/span>对所有&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>都存在；&lt;/li>
&lt;li>分布的支撑&lt;span class="math">\(\{x:p(x;\theta)&amp;gt;0\}\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关；&lt;/li>
&lt;li>&lt;span class="math">\(p(x;\theta)\)&lt;/span>的微分与积分运算可交换；&lt;/li>
&lt;li>对所有&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，期望&lt;span class="math">\(0&amp;lt;E[(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2]&amp;lt;+\infty\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>常用的分布大多数都属于Cramer-Rao正则分布族，但是均匀分布&lt;span class="math">\(U(0,\theta)\)&lt;/span>不是，因为其分布的支撑与&lt;span class="math">\(\theta\)&lt;/span>相关。&lt;/p>
&lt;h3 id="cramer-rao正则分布族下费雪信息量的另一种表示">Cramer-Rao正则分布族下费雪信息量的另一种表示&lt;/h3>
&lt;p>若&lt;span class="math">\(p(x;\theta)\)&lt;/span>为Cramer-Rao正则分布族，若其二节偏导数&lt;span class="math">\(\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}\)&lt;/span>对一切&lt;span class="math">\(\theta\)&lt;/span>存在，其费雪信息量还可以写为： &lt;span class="math">\[
I(\theta)=-E[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}]
\]&lt;/span> 这种方式可以帮助我们简便计算费雪信息量。此外，这是理解费雪信息量的另一个角度。我们知道二阶导数衡量一个函数的“凸程度”，绝对值越大，越陡峭，说明真值集中性越强。意味着，费雪信息量越大，我们更可能在凸峰出取到真实值附近的值。下面我们证明这两种表示是相等的。&lt;/p>
&lt;p>&lt;strong>证明&lt;/strong>：由于&lt;span class="math">\(p(x;\theta)\)&lt;/span>为Cramer-Rao正则分布族，且其二阶偏导数总是存在，对于定积分 &lt;span class="math">\[
1=\int_{-\infty}^{+\infty}p(x;\theta)\mathrm{d}x
\]&lt;/span> 两侧对&lt;span class="math">\(\theta\)&lt;/span>求偏导，由于定积分是个常数，因此导数必为0： &lt;span class="math">\[
0=\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}p(x;\theta)\mathrm{d}x
\]&lt;/span> 根据Cramer-Rao正则分布族的第4条定义，我们交换上式中积分与微分的顺序： &lt;span class="math">\[
0=\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}p(x;\theta)\mathrm{d}x=\int_{-\infty}^{+\infty}\frac{\partial p(x;\theta)}{\partial\theta}\mathrm{d}x
\]&lt;/span> 下面用到了一个计算技巧：&lt;span class="math">\(\frac{\partial p(x;\theta)}{\partial\theta}=\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\)&lt;/span>，代入其中有： &lt;span class="math">\[
0=\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x=E[\frac{\partial\ln p(x;\theta)}{\partial\theta}]
\]&lt;/span> 我们得到一个副结论：Log似然函数的一阶导期望为0。但我们还没有得到的结论，结论中需要二阶导，所以我们再对&lt;span class="math">\(\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x\)&lt;/span>求&lt;span class="math">\(\theta\)&lt;/span>的偏导有： &lt;span class="math">\[
\begin{aligned}
0&amp;amp;=\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x(交互积分微分顺序)\\
&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)+\frac{\partial\ln p(x;\theta)}{\partial\theta}\frac{\partial p(x;\theta)}{\partial\theta}]\mathrm{d}x\\
&amp;amp;\because \frac{\partial p(x;\theta)}{\partial\theta}=\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\\
0&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)+(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2p(x;\theta)]\mathrm{d}x\\
0&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x+\underbrace{\int_{-\infty}^{+\infty}(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2p(x;\theta)]\mathrm{d}x}_{I(\theta)=E[(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2]}\\
\end{aligned}
\]&lt;/span> 从而有： &lt;span class="math">\[
\begin{aligned}
I(\theta)&amp;amp;=0-\int_{-\infty}^{+\infty}\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x\\
&amp;amp;=-E[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}]
\end{aligned}
\]&lt;/span> 得证。&lt;/p>
&lt;p>关于副结论：Log似然函数的一阶导期望为0。我们有时候也把&lt;span class="math">\(p\)&lt;/span>的Log似然函数称为分数函数，即分数函数的期望为0，而费雪信息量正好是分数函数的二阶原点矩，同时由于其期望为0，二阶原点矩等于二阶中心矩，即&lt;strong>费雪信息量也正好是分数函数的方差&lt;/strong>。&lt;/p>
&lt;h2 id="cramer-rao不等式与界">Cramer-Rao不等式与界&lt;/h2>
&lt;p>从最大似然估计的渐进正态性，我们知道估计量始终会是一个随机变量，有自己的概率分布，而不是一个准确的值。Cramer-Rao除了给出了Cramer-Rao正则分布族这种费雪信息的存在条件，还有另一个更重要的贡献：&lt;strong>C-R不等式&lt;/strong>，可以说给了统计学理论上的绝望。&lt;/p>
&lt;p>C-R不等式，其实就是在说：统计，对真实的概率分布参数估计能力是有限的。举个不太恰当的类比，有点像量子理论中的测不准原理 （二者证明有相似之处哦）。C-R不等式告诉我们，无论我们如何抽样充足，无论我们统计方法如何科学，我们对参数的估计值，永远不可能无限逼近是逻辑上的真实值！它有自己估计的上限！&lt;/p>
&lt;h2 id="最大似然估计与相对熵kl散度交叉熵的等价性">最大似然估计与相对熵（KL散度）、交叉熵的等价性&lt;/h2>
&lt;p>在机器学习、广义线性模型的应用场景中，常常使用最小（大）化交叉熵或相对熵，又叫KL散度来替换一些最大似然估计的操作，从理论上来看它们是具有等价性的。关于相对熵的概念，可以先看笔记《&lt;a href="无线通信之信息熵回忆总结.md">无线通信之信息熵回忆总结&lt;/a>》。最大似然估计的目标是最大化似然函数，我们将其写为： &lt;span class="math">\[
\argmax_{\theta} \ln(L(\theta))=\argmax_{\theta} \ln(\prod_{i=1}^n p(x_i;\theta))
\]&lt;/span> 显然，似然函数&lt;span class="math">\(L(\theta)\)&lt;/span>是由抽样出来的已知样本&lt;span class="math">\(x_i\)&lt;/span>的概率密度函数（pdf）连乘得来的。当参数&lt;span class="math">\(\theta\)&lt;/span>已知时，&lt;span class="math">\(\prod_{i=1}^n p(x_i;\theta)\)&lt;/span>就是&lt;span class="math">\(x=(x_1,x_2,\dotsb,x_n)\)&lt;/span>的联合概率密度值；如果&lt;span class="math">\(\theta\)&lt;/span>未知，那么就会得到一个关于&lt;span class="math">\(\theta\)&lt;/span>的函数&lt;span class="math">\(L(\theta)\)&lt;/span>，而找到让函数&lt;span class="math">\(L(\theta)\)&lt;/span>最大的&lt;span class="math">\(\theta^\star\)&lt;/span>就是最大似然估计的核心。&lt;/p>
&lt;p>如果将&lt;span class="math">\(\ln\)&lt;/span>符号和&lt;span class="math">\(\prod\)&lt;/span>符号交换，上式可改写成： &lt;span class="math">\[
\argmax_{\theta} \ln(\prod_{i=1}^n p(x_i;\theta))=\argmax_{\theta}\sum_{i=1}^n \ln(p(x_i;\theta))
\]&lt;/span> 简单地，我 们可以对似然函数加个负号，然后取最大值就变成了取最小值： &lt;span class="math">\[
\argmax_{\theta}\sum_{i=1}^n \ln(p(x_i;\theta))=\argmin_{\theta}\sum_{i=1}^n -\ln(p(x_i;\theta))
\]&lt;/span> 由于&lt;span class="math">\(n\)&lt;/span>是一个固定的数，因此在上式前面乘以一个常数&lt;span class="math">\(\frac{1}{n}\)&lt;/span>不改变&lt;span class="math">\(\argmin\limits_{\theta}\)&lt;/span>的结果，即&lt;span class="math">\(\argmin\limits_{\theta} \frac{1}{n}\sum\limits_{i=1}^n -\ln(p(x_i;\theta))\)&lt;/span>。&lt;/p>
&lt;p>接下来是体现抽样与概率关系的一步。我们知道&lt;span class="math">\(x_i\)&lt;/span>都是从总体抽出来的样本，概率密度大的地方抽样出来的样本多，概率密度小的地方抽出来的样本就少。那么，想想我们如何使用蒙特卡洛法求一个随机变量函数的期望&lt;span class="math">\(E[f(Y)]\)&lt;/span>的？其中&lt;span class="math">\(Y\)&lt;/span>是一个随机变量。理论上来说： &lt;span class="math">\[
E[f(Y)]=\int_{-\infty}^\infty p(y)f(y)\mathrm{d}y
\]&lt;/span> 但是，我们常常不知道&lt;span class="math">\(Y\)&lt;/span>精确的概率密度函数&lt;span class="math">\(p(y)\)&lt;/span>，因此我们会从总体&lt;span class="math">\(Y\)&lt;/span>中抽取一定量的&lt;span class="math">\(y_i\)&lt;/span>，然后用 &lt;span class="math">\[
\tilde{E}[f(Y)]=\frac{1}{n}\sum_{i=1}^n f(y_i)
\]&lt;/span> 来近似计算期望，我们抽样的数量越多，这个值越准确。当抽样的数量趋近于无穷时，&lt;span class="math">\(\tilde{E}[f(y)]{\rightarrow}E[f(y)](P=1)\)&lt;/span>。这个表述是不是看得眼熟，没错这就是强大数定律！由此我们可以得到一个结论：&lt;/p>
&lt;blockquote>
&lt;p>结论1：当样本数足够大时，可以使用&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n f(y_i)\)&lt;/span>替代期望&lt;span class="math">\(E[f(Y)]\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>回到原来&lt;span class="math">\(\argmin\limits_{\theta} \frac{1}{n}\sum\limits_{i=1}^n -\ln(p(x_i;\theta))\)&lt;/span>的场景，我们令&lt;span class="math">\(f(x)=-\ln(p(x_i;\theta))\)&lt;/span>，并且&lt;span class="math">\(x_i\)&lt;/span>都是从总体的抽样出来的，那么当样本数量较大时，我们同样可以用求期望的方式去近似函数均值，即： &lt;span class="math">\[
\argmin\limits_{\theta} \frac{1}{n}\sum\limits_{i=1}^n -\ln(p(x;\theta))\approx\begin{cases}
\argmin\limits_{\theta} \int\limits_x p(x) [(-\ln(p(x;\theta))]\mathrm{d}x,\quad x\text{为连续随机变量}\\
\argmin\limits_{\theta} \sum\limits_x p(x) [(-\ln(p(x;\theta))],\quad x\text{为离散随机变量}
\end{cases}\\
=\argmin_\theta E[-\ln(p(x;\theta))]=\argmin_\theta H(x;\theta)
\]&lt;/span> 需要注意的是，作为概率的&lt;span class="math">\(p(x)\)&lt;/span>是与&lt;span class="math">\(\theta\)&lt;/span>无关的事实概率密度函数。因为抽样是从真实分布中抽样的，抽样结果只和&lt;span class="math">\(x\)&lt;/span>的真实分布有关，或者说抽样结果已经是包含参数&lt;span class="math">\(\theta\)&lt;/span>信息之后，体现出来的结果。到这里，我们又发现了一个重要的结论：&lt;/p>
&lt;blockquote>
&lt;p>结论2：最大似然估计的效果等同于调节参数让信息熵最小。&lt;/p>
&lt;/blockquote>
&lt;p>这个直观上很好理解，最大似然估计是找出让事件发生概率最大的参数&lt;span class="math">\(\theta\)&lt;/span>，而熵是衡量事件不确定性的度量，越可能发生的事情，其信息含量越小，熵也越小。因此，最大似然估计会使得&lt;span class="math">\(\argmin\limits_\theta H(x;\theta)\)&lt;/span>。&lt;/p>
&lt;p>到这里，我们距离交叉熵与相对熵（KL散度）貌似还有一些距离。因为上式中，我们使用的是一般的熵，而不是交叉熵、相对熵，那么区别在哪里呢？&lt;/p>
&lt;p>答案就是似然函数&lt;span class="math">\(-\ln(p(x;\theta))\)&lt;/span>。我们在上文中都默认使用了真实的概率分布&lt;span class="math">\(p(x;\theta)\)&lt;/span>，然而我们实际上，我们并不知道！并且真实分布是客观存在的，实际上也不存在参数&lt;span class="math">\(\theta\)&lt;/span>。我们只是选取了一种带有参数&lt;span class="math">\(\theta\)&lt;/span>概率分布族&lt;span class="math">\(q(x;\theta)\)&lt;/span>的去近似真实的概率&lt;span class="math">\(p(x)\)&lt;/span>；或者假定数据&lt;span class="math">\(x_i\)&lt;/span>服从某一带有参数&lt;span class="math">\(\theta\)&lt;/span>概率分布族&lt;span class="math">\(q(x;\theta)\)&lt;/span>。然而，实际情况是&lt;span class="math">\(q(x;\theta)\)&lt;/span>与真实分布&lt;span class="math">\(p(x)\)&lt;/span>是有差距的。因此，我们写出实际情况: &lt;span class="math">\[
\argmin\limits_{\theta} \frac{1}{n}\sum\limits_{i=1}^n -\ln(p(x;\theta))\approx\begin{cases}
\argmin\limits_{\theta} \int\limits_x p(x) [(-\ln(q(x;\theta))]\mathrm{d}x,\quad x\text{为连续随机变量}\\
\argmin\limits_{\theta} \sum\limits_x p(x) [(-\ln(q(x;\theta))],\quad x\text{为离散随机变量}
\end{cases}\\
=\argmin_\theta E[-\ln(q(x;\theta))]
\]&lt;/span> 注意只是将&lt;span class="math">\(p(x;\theta)\)&lt;/span>换成了&lt;span class="math">\(q(x;\theta)\)&lt;/span>，抽样的&lt;span class="math">\(x_i\)&lt;/span>反应的是真实的概率分布，因此就是约等于&lt;span class="math">\(p(x)\)&lt;/span>，且根据大数定律，样本数越大，越接近真实分布。根据笔记《&lt;a href="无线通信之信息熵回忆总结.md">无线通信之信息熵回忆总结&lt;/a>》的定义，我们知道 &lt;span class="math">\[
E[-\ln(q(x;\theta))]=\sum_{x}p(x)(-\ln q(x;\theta))=H(p,q)
\]&lt;/span> 其中，&lt;span class="math">\(H(p,q)\)&lt;/span>成为交叉熵，只有我们设定的参数分布族&lt;span class="math">\(q(x;\theta)\)&lt;/span>才与&lt;span class="math">\(\theta\)&lt;/span>相关，真实分布作为客观存在与&lt;span class="math">\(\theta\)&lt;/span>无关。&lt;/p>
&lt;p>根据交叉熵和相对熵的关系 &lt;span class="math">\[
D_{KL}(p||q)=H(p,q)-H(p)
\]&lt;/span> 可知，相对熵和交叉熵相差一个真实熵&lt;span class="math">\(H(p)\)&lt;/span>。因为&lt;span class="math">\(p(x)\)&lt;/span>是客观存在的，因此&lt;span class="math">\(H(p)\)&lt;/span>也是一个客观存在的常数，虽然我们不知道具体是多少，但是它确实只是个常数，与&lt;span class="math">\(\theta\)&lt;/span>无关，因此 &lt;span class="math">\[
\argmin_\theta E[-\ln(q(x;\theta))]=\argmin_\theta H(p,q)=\argmin_\theta D_{KL}(p||q)
\]&lt;/span> 其中，&lt;span class="math">\(q\)&lt;/span>是与&lt;span class="math">\(\theta\)&lt;/span>相关的分布族，&lt;span class="math">\(p\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关。&lt;/p>
&lt;p>综上可知，最大似然估计等同于最小化交叉熵或相对熵（KL散度）。&lt;/p>
&lt;h2 id="最大似然估计不具备的性质">最大似然估计不具备的性质&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>最大似然估计不一定是无偏的。&lt;/li>
&lt;li>当待估计参数与自变量&lt;span class="math">\(x\)&lt;/span>相关时，无法用求导的方式求解。我们需要回到最大似然估计的基本定义，既让似然函数概率最大，来考虑问题。&lt;/li>
&lt;/ol></description></item><item><title>概率统计随机过程之蒙特卡洛方法</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</guid><description>
&lt;ul>
&lt;li>&lt;a href="#蒙特卡洛求定积分的基本方法">蒙特卡洛求定积分的基本方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#随机投点法">随机投点法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本平均值法">样本平均值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重要性采样">重要性采样&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分层抽样法">分层抽样法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大期望算法expectation-maximum-em算法">最大期望算法（Expectation Maximum, EM）算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#马尔科夫链蒙特卡洛mcmc">马尔科夫链蒙特卡洛MCMC&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概率统计随机过程之蒙特卡洛方法">概率统计随机过程之蒙特卡洛方法&lt;!-- omit in toc -->&lt;/h2>
&lt;p>蒙特卡方法洛（Monte Carlo Method）简称MC，又叫随机模拟方法，本质是随机抽样。MC方法的使用框架一般为&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>针对实际问题建立一个便于实现的概率统计模型，使所求解恰好是所建模型的概率分布或其某个数字特征，比如某个事件的概率或期望值。&lt;/li>
&lt;li>对模型中随机变量建立抽样，在计算机上进行随机试验，抽取足够的随机数，并对有关事件进行统计；&lt;/li>
&lt;li>对模拟实验结果加以分析，&lt;strong>给出所求解的估计及精度（方差）的估计&lt;/strong>；&lt;/li>
&lt;li>必要时，还应改进模型以降低估计方差和减少实验复杂度，提供模拟计算效率。&lt;/li>
&lt;/ol>
&lt;h2 id="蒙特卡洛求定积分的基本方法">蒙特卡洛求定积分的基本方法&lt;/h2>
&lt;p>蒙特卡洛方法近年来受到了广泛运用，不少统计问题到最后到可以归结为定积分的计算，如计算概率、各阶矩、贝叶斯机器学习等，因此我们首先介绍定积分的蒙特卡洛计算。考虑定积分的一个通用形式：&lt;/p>
&lt;p>&lt;span class="math">\[
\theta=\int_a^b f(x) \mathrm{d}x\tag{1}
\]&lt;/span>&lt;/p>
&lt;p>我们先考虑两种基础的蒙特卡洛积分方法，然后再给出一些给复杂而有效的办法。&lt;/p>
&lt;h3 id="随机投点法">随机投点法&lt;/h3>
&lt;p>随机投点法底层原理是伯努利大数定理（用频率接近概率），可以简单理解成几何概型的应用。简单起见，假设&lt;span class="math">\(a,b\)&lt;/span>有限，式（1）中的函数满足&lt;span class="math">\(0\leq f(x)\leq M\)&lt;/span>，令&lt;span class="math">\(\Omega=\{(x,y):a\leq x\leq b,0\leq y\leq M\}\)&lt;/span>。那么根据几何概型，在&lt;span class="math">\(\Omega\)&lt;/span>内取均匀分布的点，该点落在&lt;span class="math">\(f(x)\)&lt;/span>下方的概率为&lt;span class="math">\(p=\frac{S(\Omega)}{\theta}\)&lt;/span>，其中&lt;span class="math">\(S(\Omega)\)&lt;/span>是整体的面积，值为&lt;span class="math">\(M(b-a)\)&lt;/span>，&lt;span class="math">\(\theta\)&lt;/span>是&lt;span class="math">\(f(x)\)&lt;/span>函数下方的面积，根据积分的定义可知&lt;span class="math">\(\theta=\int_a^b f(x) \mathrm{d}x\)&lt;/span>。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/随机投点法.png" alt="随机投点法" />&lt;p class="caption">随机投点法&lt;/p>
&lt;/div>
&lt;p>显然，这就是高中学过的几何概型，概率等于面积的比值，由此，我们也可以反推&lt;span class="math">\(f(x)\)&lt;/span>下方的面积，即积分的值为： &lt;span class="math">\[
\theta=\int_a^b f(x) \mathrm{d}x=M(b-a)p\tag{2}
\]&lt;/span> 现在不知道的就是概率&lt;span class="math">\(p\)&lt;/span>。根据大数定律，如果我们往&lt;span class="math">\(\Omega\)&lt;/span>中投很多点&lt;span class="math">\(n\)&lt;/span>，若有&lt;span class="math">\(n_0\)&lt;/span>个点落在&lt;span class="math">\(f(x)\)&lt;/span>下方，那么可以通过频率逼近概率，即&lt;span class="math">\(\hat p=\frac{n_0}{n}\rightarrow p (a.s.\; n\rightarrow\infty)\)&lt;/span>，代入式（2）中有 &lt;span class="math">\[
\hat{\theta}=M(b-a)\frac{n_0}{n}\rightarrow \theta(a.s.\; n\rightarrow\infty) \tag{3}
\]&lt;/span> 上述思想是容易实现的，步骤如下&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>独立地产生&lt;span class="math">\(n\)&lt;/span>对服从&lt;span class="math">\(U(0,1)\)&lt;/span>的独立随机数&lt;span class="math">\((u_i,v_i),i=1,2,\dotsb,n\)&lt;/span>；&lt;/li>
&lt;li>根据随机变量的函数关系有&lt;span class="math">\(x_i=a+u_i(b-a),y_i=Mv_i\)&lt;/span>；&lt;/li>
&lt;li>统计&lt;span class="math">\(y_i\leq f(x_i)\)&lt;/span>的个数&lt;span class="math">\(n_0\)&lt;/span>(该点落在&lt;span class="math">\(f(x)\)&lt;/span>下方)；&lt;/li>
&lt;li>使用式（3）估算积分值。&lt;/li>
&lt;/ol>
&lt;p>随机投点法用了类似于舍选法的做法， 在非随机问题中引入随机性时用了二维均匀分布和二项分布，靠求二项分布概率来估计积分，随机投点法容易理解，但是效率较低。&lt;/p>
&lt;p>那么，&lt;strong>随机投点法精度提高一位数需要多大的代价呢&lt;/strong>？&lt;/p>
&lt;p>我们看每次投点的结果其实都是服从伯努利分布（也叫0-1分布），&lt;span class="math">\(b(1,p)\)&lt;/span>，而&lt;span class="math">\(n\)&lt;/span>次投点结果&lt;span class="math">\(n_0\)&lt;/span>则服从二项分布&lt;span class="math">\(b(n,p)\)&lt;/span>。显然，&lt;span class="math">\(\hat\theta\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计: &lt;span class="math">\[
E(\hat\theta)=E[M(b-a)\frac{n_0}{n}]=\frac{M(b-a)}{n}E[n_0]\\
=\frac{M(b-a)}{n}\times np=M(b-a)p=\theta\tag{4}
\]&lt;/span> 在无偏的情况下，我们可以用方差衡量精度： &lt;span class="math">\[
\begin{aligned}
Var(\hat\theta)&amp;amp;=Var[M(b-a)\frac{n_0}{n}]=\frac{M^2(b-a)^2}{n^2}Var(n_0)\\
&amp;amp;=\frac{M^2(b-a)^2}{n^2}\times np(1-p),其中p=\frac{\theta}{M(b-a)}\\
&amp;amp;=\frac{1}{n}\theta[M(b-a)-\theta]
\end{aligned}\tag{5}
\]&lt;/span> 其中，&lt;span class="math">\(\theta, M, b, a\)&lt;/span>都是定值，只有&lt;span class="math">\(n\)&lt;/span>是我们能够影响的数。也就是说，模拟次数&lt;span class="math">\(n\)&lt;/span>提高到&lt;span class="math">\(n\)&lt;/span>倍，方差缩小到&lt;span class="math">\(1/n\)&lt;/span>，如果看和精度直接相关的标准差形如（&lt;span class="math">\(E[x]\plusmn \mathrm{std}(x)\)&lt;/span>），那么精度每增加一位小数，实验量需要增加100倍。&lt;/p>
&lt;p>&lt;strong>随机模拟积分的精度一般都服从这样的规律&lt;/strong>。&lt;/p>
&lt;h3 id="样本平均值法">样本平均值法&lt;/h3>
&lt;p>随机投点法是一种很直觉，但是效率不高的方法，另一种效率更高的方法是利用&lt;strong>期望值(矩)的估计&lt;/strong>。取随机变量&lt;span class="math">\(U\sim U(a,b)\)&lt;/span>，则将&lt;span class="math">\(U\)&lt;/span>代入&lt;span class="math">\(f(x)\)&lt;/span>，得到一个随机数&lt;span class="math">\(Y=f(U)\)&lt;/span>： &lt;span class="math">\[
E[Y]=E[f(U)]=\int_a^b f(u) \frac{1}{b-a} \mathrm{d}u=\frac{\theta}{b-a}\\
\Rightarrow\theta=(b-a)E[Y]=(b-a)E[f(U)]\tag{6}
\]&lt;/span> 在范围&lt;span class="math">\(b-a\)&lt;/span>已知的情况下，我们需要知道&lt;span class="math">\(f(U)\)&lt;/span>的期望即可。我们知道&lt;strong>一个随机变量的期望即为一阶原点矩（一阶矩）&lt;/strong>，因此我们可以用&lt;strong>矩估计&lt;/strong>的方法来近似计算。一阶矩的估计方式很简单，&lt;strong>就是采样多个点，求函数均值，用样本均值代替一阶矩（总体期望）&lt;/strong>。&lt;/p>
&lt;p>我们取服从均匀分布&lt;span class="math">\(U(a,b)\)&lt;/span>的独立的随机变量&lt;span class="math">\(\{U_i,i=1,2,\dotsb,n\}\)&lt;/span>，则&lt;span class="math">\(Y=f(U)\)&lt;/span>的n个采样值为&lt;span class="math">\(\{Y_i=f(U_i),i=1,2,\dotsb,n\}\)&lt;/span>，根据矩估计方法， &lt;span class="math">\[
\hat Y={1\over n}\sum_{i=1}^nf(U_i) \rightarrow E[Y]=E[f(U)]\ a.s. n\rightarrow \infty\tag{7}
\]&lt;/span> 代入式（6）可得： &lt;span class="math">\[
\hat\theta=(b-a)\hat Y=\frac{b-a}{n}\sum_{i=1}^nf(U_i) \rightarrow (b-a)E[f(U)]=\theta\ a.s. n\rightarrow\infty\tag{8}
\]&lt;/span>&lt;/p>
&lt;p>简单来说，就是从&lt;span class="math">\(f(x)\)&lt;/span>上取足够多的点，用采样点的平均值代替函数值的均值，再乘以定义域长度，得到面积（积分值）。用一个图描述了蒙特卡洛求定积分的思想：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/蒙特卡洛求定积分.png" alt="蒙特卡洛求定积分.png" />&lt;p class="caption">蒙特卡洛求定积分.png&lt;/p>
&lt;/div>
&lt;p>显然，样本平均值法也是无偏估计。我们也可以用方差计算其精度。为了方便比较，我们令随机投点法的积分估计值为&lt;span class="math">\(\hat\theta_1\)&lt;/span>，样本平均值法的估计值为&lt;span class="math">\(\hat\theta_2\)&lt;/span>。那么可以计算&lt;span class="math">\(\mathrm{Var}(\hat\theta_2)\)&lt;/span>，根据式（8），有： &lt;span class="math">\[
\begin{aligned}
\mathrm{Var}(\hat\theta_2)&amp;amp;=\mathrm{Var}[\frac{b-a}{n}\sum_{i=1}^nf(U_i)]=(\frac{b-a}{n})^2\mathrm{Var}[\sum_{i=1}^nf(U_i)]\\
&amp;amp;\because U_i\quad i.i.d\quad \therefore 根据独立随机变量和的方差性质有：\\
&amp;amp;=(\frac{b-a}{n})^2\times n \times \mathrm{Var}[f(U)]\\
&amp;amp;=\frac{(b-a)^2}{n}\times \int_a^b (f(U)-E[f(U)])^2 \frac{1}{b-a} \mathrm{d}U\\
&amp;amp;\because E[f(U)]=\frac{\theta}{b-a}\\
&amp;amp;=\frac{1}{n}[(b-a)^2\int_a^b f^2(U)\frac{1}{b-a}\mathrm{d}U-\theta^2]\\
&amp;amp;=\frac{1}{n}[(b-a)\int_a^b f^2(U)\mathrm{d}U-\theta^2]
\end{aligned}\tag{9}
\]&lt;/span>&lt;/p>
&lt;p>直观来看无法比较随机投点法和样本均值法的差异，我们将&lt;span class="math">\(\hat\theta_1,\hat\theta_2\)&lt;/span>做差，进行比较，在&lt;span class="math">\(0\leq f(x) \leq M\)&lt;/span>时（随机投点法要求），可以证明&lt;span class="math">\(\mathrm{Var}(\hat\theta_1)\geq \mathrm{Var}(\hat\theta_2)\)&lt;/span>，二者相减结果如下：&lt;/p>
&lt;p>&lt;span class="math">\[
\begin{aligned}
\mathrm{Var}(\hat\theta_1)-\mathrm{Var}(\hat\theta_2)&amp;amp;=\frac{1}{n}\theta[M(b-a)]-\\
&amp;amp;=\frac{1}{n}[(b-a)\int_a^bf^2(U)\mathrm{d}U-\theta^2]\\
&amp;amp;=\frac{M(b-a)}{n}[\theta-\int_a^b \frac{f^2(U)}{M}\mathrm{d}U]\\
&amp;amp;\because 0\leq f(x) \leq M \therefore 0&amp;lt;\frac{f(U)}{M}\leq 1\\
&amp;amp;\geq \frac{M(b-a)}{n}[\theta-\int_a^b f(U)\mathrm{d}U]=0\\
\end{aligned}
\]&lt;/span> 由此可见，只要&lt;span class="math">\(\{U:f(U)&amp;lt;M\}\)&lt;/span>不是零测集，就有&lt;span class="math">\(\mathrm{Var}(\hat\theta_1)&amp;gt;\mathrm{Var}(\hat\theta_2)\)&lt;/span>。由此，样本均值法比随机投点法方差更小，更有效。此外，样本均值法不要求&lt;span class="math">\(f(x)\)&lt;/span>有上界&lt;span class="math">\(M\)&lt;/span>，可以推广至一般情形。&lt;/p>
&lt;h2 id="重要性采样">重要性采样&lt;/h2>
&lt;p>我们根据随机投点法和样本均值法已经能够进行蒙特卡洛积分的无偏估计，并且随着采样次数的增加最终能以&lt;span class="math">\(O(n^{-1/2})\)&lt;/span>收敛。我们现在需要的是优化蒙特卡洛积分的计算方法，使&lt;strong>估计的方差尽量小&lt;/strong>。&lt;/p>
&lt;p>我们再回看式（1），对他进行适当变形： &lt;span class="math">\[
\theta=\int_a^b f(x) \mathrm{d}x=\int_a^b \frac{f(x)}{g(x)} g(x) \mathrm{d}x=E[\frac{f(X)}{g(X)}]\\
g(x)\neq 0;x\in[a,b]\tag{10}
\]&lt;/span> 其中，&lt;span class="math">\(g(x)\)&lt;/span>是一个概率密度函数。那么原来的积分问题就变成了求期望，若有来自&lt;span class="math">\(g(x)\)&lt;/span>的样本（随机数）&lt;span class="math">\(\{x_1,x_2,\dotsb,x_n\}\)&lt;/span>（服从&lt;span class="math">\(g(x)\)&lt;/span>的分布），则&lt;span class="math">\(\theta\)&lt;/span>的估计可以用一阶矩估计来实现： &lt;span class="math">\[
\hat\theta=\frac{1}{n}\sum_{i=1}^n \frac{f(x_i)}{g(x_i)}\tag{11}
\]&lt;/span> 显然，如果当&lt;span class="math">\(g(x)=\frac{1}{b-a}\)&lt;/span>时，这种方法就是样本均值法，也可以说&lt;strong>样本均值法是此类方法的一个特例&lt;/strong>。&lt;/p>
&lt;p>估计量&lt;span class="math">\(\hat{\theta}\)&lt;/span>的&lt;strong>构造是很简单的，说白了就是一阶原点矩估计&lt;/strong>。我们对&lt;span class="math">\(\hat\theta\)&lt;/span>求期望易知此估计式无偏的，即&lt;span class="math">\(E[\hat{\theta}]=\theta\)&lt;/span>。而其方差为： &lt;span class="math">\[
\mathrm{Var}(\hat\theta)=\frac{1}{n}[E(\frac{f(x)}{g(x)})^2-E^2(\frac{f(x)}{g(x)})]=\frac{1}{n}[E(\frac{f(x)}{g(x)})^2-\theta^2]\tag{12}
\]&lt;/span> 那么根据式（12），&lt;strong>除了增加采样的次数，我们又多了一个可以用来降低方差的工具&lt;/strong>&lt;span class="math">\(g(x)\)&lt;/span>，我们可以通过合理设计&lt;span class="math">\(g(x)\)&lt;/span>使蒙特卡洛积分的效率更高。理论上来看，当&lt;span class="math">\(g(x)=\frac{f(x)}{\int_a^b f(x) \mathrm{d}x}\)&lt;/span>时，有&lt;span class="math">\(\mathrm{Var}(\hat\theta)=0\)&lt;/span>，此时结果最优，但是这个&lt;span class="math">\(g(x)\)&lt;/span>中必须已知式（1）&lt;span class="math">\(\int_a^b f(x) \mathrm{d}x\)&lt;/span>的结果，显然是无法直接使用的。不过，他也给我们一个启示，即&lt;span class="math">\(g(x)\)&lt;/span>与&lt;span class="math">\(f(x)\)&lt;/span>应该在形状相近的情况下，估计的方差更小。下面我们举一个例子： &lt;span class="math">\[
f(x)=-x^2+2x\quad x\in[0,2]\\
g_1(x)=\frac{1}{2};g_2(x)=e^{-x};g_3(x)=\frac{1}{\sqrt{2\pi}\times 0.5}e^{-\frac{(x-1)^2}{2\times 0.5^2}}
\]&lt;/span> 分别用3个&lt;span class="math">\(g(x)\)&lt;/span>函数，作为重要性采用的概率密度函数。首先，我们易算出&lt;span class="math">\(f(x)=-x^2+2x\quad x\in[0,2]\)&lt;/span>的积分结果为&lt;span class="math">\(\frac{4}{3}\)&lt;/span>，我们画出各个函数的图像以及&lt;span class="math">\(\frac{f(x)}{g(x)}\)&lt;/span>的图像。&lt;/p>
&lt;p>&lt;img src="../../images/importance_sampling1.png" alt="importance sampling" /> &lt;img src="../../images/importance_sampling2.png" alt="importance sampling" />&lt;/p>
&lt;p>我们使用matlab分别计算三个重要性采样的方差有（取&lt;span class="math">\(n=10\)&lt;/span>）： &lt;span class="math">\[
\mathrm{Var}(\frac{1}{10}\sum_{i=1}^{10} \frac{f(x)}{g_1(x)})=0.03556\\
\mathrm{Var}(\frac{1}{10}\sum_{i=1}^{10} \frac{f(x)}{g_2(x)})=0.1335\\
\mathrm{Var}(\frac{1}{10}\sum_{i=1}^{10} \frac{f(x)}{g_3(x)})=0.01205
\]&lt;/span> matlab计算结果显示&lt;span class="math">\(g_3(x)\)&lt;/span>能够缩减方差，而与&lt;span class="math">\(f(x)\)&lt;/span>形状差异大的&lt;span class="math">\(g_2(x)\)&lt;/span>反而使方差更大了。&lt;/p>
&lt;p>可以从第二个图中看出，如果&lt;span class="math">\(g(x)\)&lt;/span>与&lt;span class="math">\(f(x)\)&lt;/span>的形状相似，相除之后的函数图像更加贴近积分结果&lt;span class="math">\(\theta=\frac{4}{3}(\frac{f(x)}{g_3(x)})\)&lt;/span>，这样我们在抽样时结果接近&lt;span class="math">\(\theta\)&lt;/span>的概率也更大；如果形状相差太大，如&lt;span class="math">\(g_2(x)\)&lt;/span>积分的结果反而会变得更加不好统计。&lt;/p>
&lt;p>综合来说，&lt;strong>重要性采样让新的函数值&lt;span class="math">\(h(x)=\frac{f(x)}{g(x)}\)&lt;/span>取到接近于积分结果的概率变大了&lt;/strong>。我们现在的问题就转变成了&lt;strong>如何找到一个和&lt;span class="math">\(f(x)\)&lt;/span>形状相似的函数&lt;/strong>。&lt;/p>
&lt;h3 id="分层抽样法">分层抽样法&lt;/h3>
&lt;p>在重要性采样的结论中，我们知道当&lt;span class="math">\(f(x)\)&lt;/span>与&lt;span class="math">\(g(x)\)&lt;/span>比值为常数时，可以得到理解最小的方差0，那么我们可不可以近似的构造一个这样的函数呢？基于这个思想，我们有了分层抽样法。&lt;/p>
&lt;p>分层抽样法首先把样本空间&lt;span class="math">\(D\)&lt;/span>分成一些小区间&lt;span class="math">\(D_1,\dotsb,D_m\)&lt;/span>，且诸&lt;span class="math">\(D_i\)&lt;/span>不交，&lt;span class="math">\(\cup D_i =D\)&lt;/span>，然后在各小区间内的抽样数由其“贡献”大小决定。这里的“贡献”我们定义为在小区间&lt;span class="math">\(D_i\)&lt;/span>内的积分值，即 &lt;span class="math">\[p_i=\int_{D_i}f(x)\mathrm{d}x\tag{13}\]&lt;/span> 在区间&lt;span class="math">\(D_i\)&lt;/span>抽样数与&lt;span class="math">\(p_i\)&lt;/span>成正比。显然，分层抽样法是利用小区间构造一个离散的形状类似于原函数&lt;span class="math">\(f(x)\)&lt;/span>的&lt;strong>分段函数&lt;/strong>。&lt;/p>
&lt;p>我们继续以上个例子&lt;span class="math">\(f(x)=-x^2+2x\)&lt;/span>来说明，我们把积分区间&lt;span class="math">\([0,2]\)&lt;/span>划分成十等分，根据式（13）有以下表格： |区间|积分值&lt;span class="math">\(p_i\)&lt;/span>|归一化比例| |:-:|:-:|:-:| |[0,0.2)|0.0373|0.0280| |[0.2,0.4)|0.1013|0.0760| |[0.4,0.6)|0.1493|0.1120| |[0.6,0.8)|0.1813|0.1360| |[0.8,1.0)|0.1973|0.1480| |[1.0,1.2)|0.1973|0.1480| |[1.2,1.4)|0.1813|0.1360| |[1.4,1.6)|0.1493|0.1120| |[1.6,1.8)|0.1013|0.0760| |[1.8,2.0]|0.0373|0.0280&lt;/p>
&lt;p>我们将归一化&lt;span class="math">\(p_i\)&lt;/span>值与&lt;span class="math">\(f(x)\)&lt;/span>画到一张图上(图中黄线与红线)。此外，为了表现构造的分段函数与原函数形状是相似的，我们将分段函数线性放大6倍（蓝线）。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/分层抽样1.png" alt="分层抽样1" />&lt;p class="caption">分层抽样1&lt;/p>
&lt;/div>
&lt;p>可以看出，分层抽样构造出的函数与目标积分函数是类似的，我们区间划分的越细，分段函数越相似。根据分段函数比例分配抽样次数，可以大大降低方差。理论上，给定分层抽样函数后，每个区间的分配的抽样数&lt;strong>最优方式&lt;/strong>为: &lt;span class="math">\[
n_i=nl_i\sigma_i/(\sum_{i=1}^m l_i\sigma_i)\tag{14}
\]&lt;/span> 其中，&lt;span class="math">\(n_i\)&lt;/span>是区间&lt;span class="math">\(D_i\)&lt;/span>的抽样数，&lt;span class="math">\(n\)&lt;/span>为总抽样数，&lt;span class="math">\(m\)&lt;/span>为总区间数目，&lt;span class="math">\(l_i\)&lt;/span>是区间&lt;span class="math">\(D_i\)&lt;/span>的长度，&lt;span class="math">\(\sigma_i\)&lt;/span>是区间&lt;span class="math">\(D_i\)&lt;/span>的标准差，此时方差最小，为&lt;span class="math">\(\frac{1}{n}(\sum_{i=1}^m l_i\sigma_i)^2\)&lt;/span>。然而实际计算中，每个区间的标准差&lt;span class="math">\(\sigma_i\)&lt;/span>总是未知的，式(14)无法直接使用。即使如此，最简单的分配方案&lt;span class="math">\(n_i=nl_i/\sum l_i\)&lt;/span>方差也比样本均值法低。&lt;/p>
&lt;p>此外，其他缩减方差技术还有关联抽样法、控制变量法、对立变量法、条件期望法等等。&lt;/p>
&lt;h2 id="最大期望算法expectation-maximum-em算法">最大期望算法（Expectation Maximum, EM）算法&lt;/h2>
&lt;p>我们在参数估计中常用最大似然估计，不过实际问题中，除了需要估计的未知参数，还有别的&lt;/p>
&lt;h2 id="马尔科夫链蒙特卡洛mcmc">马尔科夫链蒙特卡洛MCMC&lt;/h2>
&lt;p>马尔可夫链蒙特卡洛（英语：Markov chain Monte Carlo，MCMC）方法（含随机游走蒙特卡洛方法）是一组用&lt;strong>马氏链从随机分布取样的算法&lt;/strong>，之前步骤的作为底本。&lt;strong>步数越多，结果越好&lt;/strong>。&lt;/p></description></item><item><title>概率统计随机过程之马尔可夫过程转移矩阵（随机矩阵）</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E8%BD%AC%E7%A7%BB%E7%9F%A9%E9%98%B5%E9%9A%8F%E6%9C%BA%E7%9F%A9%E9%98%B5/</link><pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E8%BD%AC%E7%A7%BB%E7%9F%A9%E9%98%B5%E9%9A%8F%E6%9C%BA%E7%9F%A9%E9%98%B5/</guid><description>
&lt;h2 id="概率统计随机过程之转移矩阵随机矩阵">概率统计随机过程之转移矩阵（随机矩阵）&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注：本文说的随机矩阵是转移矩阵，英文叫stochastic matrix不是random matrix。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#极限行为和不变概率稳态">极限行为和不变概率（稳态）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#状态分类">状态分类&lt;/a>&lt;/li>
&lt;li>&lt;a href="#周期性与非周期">周期性与非周期&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可约与不可约">可约与不可约&lt;/a>&lt;/li>
&lt;li>&lt;a href="#不可约非周期链遍历链">不可约非周期链→遍历链&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可约性非常返态和周期的影响">可约性、非常返态和周期的影响&lt;/a>&lt;/li>
&lt;li>&lt;a href="#随机矩阵的主特征1的简单证明">随机矩阵的主特征1的简单证明&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从线性代数的角度看马尔可夫矩阵">从线性代数的角度看马尔可夫矩阵&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="极限行为和不变概率稳态">极限行为和不变概率（稳态）&lt;/h2>
&lt;p>有转移矩阵&lt;span class="math">\(P\)&lt;/span>，但经过&lt;span class="math">\(N→ ∞\)&lt;/span>后，是否存在一个收敛的极限矩阵 &lt;span class="math">\[\Pi=\lim_{n→ ∞}P^n\]&lt;/span> 如果能有一个更好的性质： &amp;gt;极限矩阵每一行都是一样的。&lt;/p>
&lt;p>那么对于任一个概率向量&lt;span class="math">\(\vec v\)&lt;/span>(每一项非负且和为1)，都有： &lt;span class="math">\[\vec \pi=\vec v\lim_{n→ ∞}P^n\]&lt;/span> 则&lt;span class="math">\(\vec \pi\)&lt;/span>为不变概率分布或稳态概率分布。这个时候，&lt;span class="math">\(\Pi\)&lt;/span>的值应该是 &lt;span class="math">\[\Pi=\lim_{n→ ∞}P^n=\begin{bmatrix}
\vec \pi\\
\vec \pi\\
\vdots\\
\vec \pi
\end{bmatrix}\]&lt;/span> 因为向量左乘是行线性组合，&lt;span class="math">\(\vec \pi\)&lt;/span>之间的任何线性组合都是&lt;span class="math">\(\vec \pi\)&lt;/span>.&lt;/p>
&lt;p>那么什么样的矩阵能有稳态分布吧?&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>主特征值为1，且为单重（1）&lt;/li>
&lt;li>能选出一个非负的左特征向量（2）&lt;/li>
&lt;/ol>
&lt;p>随机矩阵的主特征值为1这点可以证明见章节&lt;a href="#随机矩阵的主特征1的简单证明">随机矩阵的主特征1的简单证明&lt;/a>。但是主特征值为单重和非负左特征向量需要更高等的数学技巧。我们用Perron–Frobenius theorem先给出一部分符合两点要求的矩阵。&lt;/p>
&lt;p>Perron–Frobenius theorem给出了一系列符合这两点的矩阵：&lt;/p>
&lt;blockquote>
&lt;p>定理1：Perron–Frobenius theorem：&lt;strong>所有元素均为正实数&lt;/strong>的正矩阵的基本性质的重要定理.该定理断言:若矩阵&lt;span class="math">\(A&amp;gt;0，ρ(A)为A\)&lt;/span>的谱半径，则:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(ρ(A)\)&lt;/span>为&lt;span class="math">\(A\)&lt;/span>的正特征值,其对应的一个特征向量为正特征向量。&lt;/li>
&lt;li>对于&lt;span class="math">\(A\)&lt;/span>的任意其他特征值，都有&lt;span class="math">\(|λ|&amp;lt;ρ(A)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(ρ(A)是A\)&lt;/span>的单重特征值。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>尽管Perron–Frobenius theorem给出了一系列矩阵，但是却不能覆盖所有具有适当极限性质的随机矩阵。因为随机矩阵的要求是非负，而不是全是正数。尽管随机矩阵&lt;span class="math">\(P\)&lt;/span>的元素不全是正数，但是我们注意到，&lt;span class="math">\(P^n\)&lt;/span>可能是全正的。而且根据矩阵的特征值分解，&lt;span class="math">\(P^n\)&lt;/span>的特征值为&lt;span class="math">\(P\)&lt;/span>特征值的n次方，且特征向量不变。而&lt;span class="math">\(P\)&lt;/span>主特征值1的任意次方都是1，所以只要&lt;span class="math">\(P^n\)&lt;/span>满足了Perron–Frobenius theorem，那么&lt;span class="math">\(P\)&lt;/span>也满足。&lt;/p>
&lt;p>鉴于此，我们给出以下结论：&lt;/p>
&lt;blockquote>
&lt;p>结论1：如果&lt;span class="math">\(P\)&lt;/span>是一个随机矩阵，且对于某一n，&lt;span class="math">\(P^n\)&lt;/span>的所有元素都是正数，则&lt;span class="math">\(P\)&lt;/span>满足条件（1）（2）。&lt;/p>
&lt;/blockquote>
&lt;h2 id="状态分类">状态分类&lt;/h2>
&lt;p>那么到底什么样的矩阵，当&lt;span class="math">\(n\)&lt;/span>充分大时，&lt;span class="math">\(P^n\)&lt;/span>的元素全都是正的呢？答案是&lt;strong>遍历的&lt;/strong>随机矩阵。遍历的随机矩阵是&lt;strong>不可约、非周期&lt;/strong>的矩阵。几个不能符合（1）（2）的矩阵包括：可约的，随机矩阵不收敛（周期性），有非常返态的（因为非常返态导致矩阵可约）。&lt;/p>
&lt;p>下面对几个概念进行介绍。&lt;/p>
&lt;h3 id="周期性与非周期">周期性与非周期&lt;/h3>
&lt;p>一种直观情况：如果有正整数&lt;span class="math">\(d,d&amp;gt;1\)&lt;/span>，只有当&lt;span class="math">\(n=d,2d,3d,…\)&lt;/span>时，有&lt;span class="math">\(p(n)_{ij}&amp;gt;0\)&lt;/span>，则状态i为&lt;strong>周期性的状态&lt;/strong>，周期为&lt;span class="math">\(d\)&lt;/span>。&lt;/p>
&lt;p>周期性所说不直观的定义：若集合&lt;span class="math">\(\{n: n≥1, p_{ii}&amp;gt; 0\}\)&lt;/span>非空，则称它的最大公约数，&lt;span class="math">\(d=d(i)\)&lt;/span>为状态&lt;span class="math">\(i\)&lt;/span>的周期。若&lt;span class="math">\(d&amp;gt;1\)&lt;/span>，称&lt;span class="math">\(i\)&lt;/span>是周期的;若&lt;span class="math">\(d=1\)&lt;/span>,称 &lt;span class="math">\(i\)&lt;/span>是非周期的。（引用：《应用随机过程（张波）》，第5章）&lt;/p>
&lt;p>我当初也纳闷，定义中取公约数和周期没啥关系，然而，这才是更加完善的周期。见下图（《应用随机过程（张波）》，图5-3）&lt;/p>
&lt;p>&lt;img src="../../images/马尔可夫链周期性.jpg" alt="马尔可夫链周期性" /> 由状态1出发返回状态1的步长为&lt;span class="math">\(T=\{4,6,8,10,12,...\}\)&lt;/span>，很直观的感受到，4步之后就是两步两步的间隔了。&lt;/p>
&lt;p>定义周期：方案一是取4，但是会丢失了&lt;span class="math">\(\{6,10,14...\}\)&lt;/span>这些步长；方案二就是取公约数2，就可以防止&lt;span class="math">\(\{6,10,14...\}\)&lt;/span>步长的丢失，弊端也许就是不如&lt;span class="math">\(\{d,2d,3d...\}\)&lt;/span>这种结构来得直观。（有点像&lt;span class="math">\(y=ax+b，a\)&lt;/span>是周期）。&lt;/p>
&lt;p>补充：当马尔可夫链中&lt;strong>任意一个状态都为周期性&lt;/strong>的状态时，则称马尔可夫链是周期的。但&lt;strong>非周期性的马尔可夫链才是我们想要的&lt;/strong>，它是构成遍历的马尔可夫链的必要条件，遍历的马尔可夫有很多好的性质。&lt;/p>
&lt;blockquote>
&lt;p>定理2：如果状态&lt;span class="math">\(i\)&lt;/span>和&lt;span class="math">\(j\)&lt;/span>互通，那么&lt;span class="math">\(d(i)=d(j)\)&lt;/span>（互通类中各状态周期相等。）&lt;/p>
&lt;/blockquote>
&lt;p>非周期定义：如果链从所有任意起点返回到自己所需要的步数集的最大公约数为1，则说明此链是非周期的，即所有状态不是定期返回的且所有返回的间隔的最大公约数为1。但是并不是说，返回自己的状态只需要一步，而是说，对于某个步数&lt;span class="math">\(M\)&lt;/span>，所有步数n，满足&lt;span class="math">\(n≥M\)&lt;/span>，都能够返回原状态。&lt;/p>
&lt;blockquote>
&lt;p>如何判别一个状态是非周期的？&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>若此状态带有自环，则必为非周期的（虽然非周期的状态不一定有自环）&lt;/li>
&lt;li>若此状态与一个非周期的状态互通，则必为非周期的。（定理2应用）&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>以上&lt;strong>只是两个充分&lt;/strong>条件。&lt;/p>
&lt;h3 id="可约与不可约">可约与不可约&lt;/h3>
&lt;p>互通：如果马尔可夫链里某状态&lt;span class="math">\(i\)&lt;/span>经过&lt;span class="math">\(n\)&lt;/span>步到达状态&lt;span class="math">\(j\)&lt;/span>的概率&lt;span class="math">\(&amp;gt;0\)&lt;/span>且相反的，状态&lt;span class="math">\(j\)&lt;/span>经过&lt;span class="math">\(n\)&lt;/span>步到&lt;span class="math">\(i\)&lt;/span>的概率也&lt;span class="math">\(&amp;gt;0\)&lt;/span>，即说明这俩状态是互通的. 如果对所有&lt;span class="math">\(n\)&lt;/span>上面都不成立，则证明&lt;span class="math">\(i,j\)&lt;/span>不通。&lt;/p>
&lt;p>可约的（reducible），意思就是在状态集中至少存在一个状态&lt;span class="math">\(i\)&lt;/span>到状态&lt;span class="math">\(j\)&lt;/span>是不通的！注意有可能&lt;span class="math">\(j\)&lt;/span>到&lt;span class="math">\(i\)&lt;/span>是通的，互通是一个相互的概念。&lt;span class="math">\(i→j\)&lt;/span>同时&lt;span class="math">\(j→i\)&lt;/span>才是互通。可约就是存在不互通的状态。&lt;/p>
&lt;p>不可约（irreducible），意思是马尔可夫链链从状态空间里一个位置经过有限步跳转到任何一个位置的概率大于0，即说的是马氏链里任何两个状态都是互通的。&lt;/p>
&lt;h3 id="不可约非周期链遍历链">不可约非周期链→遍历链&lt;/h3>
&lt;p>现在我们来证明一个非周期、不可约的转移矩阵满足结论（1），即 &amp;gt;如果&lt;span class="math">\(P\)&lt;/span>是非周期、不可约的转移矩阵(遍历的)，那么存在&lt;span class="math">\(M&amp;gt;0,\forall n≥M\)&lt;/span>，有&lt;span class="math">\(P^n\)&lt;/span>&amp;gt;0(每一个元素大于0)。&lt;span class="math">\(\Leftrightarrow\)&lt;/span>存在稳态概率&lt;span class="math">\(\vec\pi\)&lt;/span>的条件。&lt;/p>
&lt;p>证明：&lt;/p>
&lt;p>取任意&lt;span class="math">\(i,j\)&lt;/span>，由于矩阵&lt;span class="math">\(P\)&lt;/span>不可约，所以&lt;span class="math">\(i,j\)&lt;/span>属于同一个连通类。因此存在一个数，记为&lt;span class="math">\(m_{ij}\)&lt;/span>，使得&lt;span class="math">\(p_{m_{ij}}(i,j)&amp;gt;0\)&lt;/span>，即&lt;span class="math">\(i\)&lt;/span>通过&lt;span class="math">\(m_{ij}\)&lt;/span>次转移后到达&lt;span class="math">\(j\)&lt;/span>。&lt;/p>
&lt;p>此外，由于&lt;span class="math">\(P\)&lt;/span>是非周期的，因此存在一个&lt;span class="math">\(M_i\)&lt;/span>，使得所有&lt;span class="math">\(n≥M_i\)&lt;/span>，有&lt;span class="math">\(p_n(i,i)&amp;gt;0\)&lt;/span>（非周期的性质）。因此对所有的n≥M_i&lt;span class="math">\(，有 \)&lt;/span>&lt;span class="math">\(p_{n+m_{ij}}(i,j)≥p_n(i,i)p_{m_{ij}}(i,j)&amp;gt;0\)&lt;/span>$ 令&lt;span class="math">\(M\)&lt;/span>为所有&lt;span class="math">\(i,j\)&lt;/span>组合中&lt;span class="math">\(M_i+m_{ij}\)&lt;/span>的最大值（现在讨论有限状态马氏链，因此最大值存在），则对于所有&lt;span class="math">\(n≥M\)&lt;/span>，以及任意的&lt;span class="math">\(i,j\)&lt;/span>都有 &lt;span class="math">\[p_{n}(i,j)&amp;gt;0.\]&lt;/span> 因此，由&lt;span class="math">\(i,j\)&lt;/span>的任意性可知：存在&lt;span class="math">\(n\)&lt;/span>，使得&lt;span class="math">\(P^n\)&lt;/span>中每一项大于0。&lt;/p>
&lt;p>如果我们用更加数学的语言描述，将其归纳成定理：&lt;/p>
&lt;blockquote>
&lt;p>定理3：如果&lt;span class="math">\(P\)&lt;/span>是非周期、不可约的马尔可夫链转移矩阵，则存在一个唯一一个不变（稳态）概率向量&lt;span class="math">\(\vec\pi\)&lt;/span>，满足 &lt;span class="math">\[\vec\pi P=\vec\pi(这不是特征向量的转置吗？)\]&lt;/span> 如果&lt;span class="math">\(\phi_0\)&lt;/span>为任一初始概率向量，则有 &lt;span class="math">\[\lim_{n→ ∞}\phi_0P^n=\vec\pi\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这也给我们求不变概率提示了一个更简单的方法，求转置矩阵&lt;span class="math">\(P^T\)&lt;/span>特征值为1时的特征向量，并要求次特征向量1-范数为1。（矩阵的特征值在转置变换中不改变，转置矩阵与原矩阵是相似的。）&lt;/p>
&lt;h3 id="可约性非常返态和周期的影响">可约性、非常返态和周期的影响&lt;/h3>
&lt;p>可约性：可以当成几个不可约的子矩阵处理。&lt;/p>
&lt;p>非常返态：初始概率会对最终稳态概率有影响。&lt;/p>
&lt;p>周期：绝对值为1的特征值不只1个，周期是几，就有几个&lt;strong>不同&lt;/strong>的绝对值为1的复根。同样，周期是多少，就有多少个不同的状态组，每个状态组都有其极限值。有平均稳态，是各个状态组极限稳态概率的平均数。&lt;/p>
&lt;h2 id="随机矩阵的主特征1的简单证明">随机矩阵的主特征1的简单证明&lt;/h2>
&lt;p>按照定义&lt;span class="math">\(A*[1,1,1,...1,1,1]^T\)&lt;/span>得到一个列向量。&lt;/p>
&lt;p>而&lt;span class="math">\(A\)&lt;/span>是一个随机矩阵(这里以行的和为1为例)，即&lt;span class="math">\(A\)&lt;/span>的每一行和为1，&lt;span class="math">\(A*[1,1,1,...1,1,1]^T\)&lt;/span>的每一个元素按照定义展开，正好是A的每一行的和，即是1，所以&lt;/p>
&lt;p>&lt;span class="math">\(A*[1,1,1,...1,1,1]^T=[1,1,1,...1,1,1]^T\)&lt;/span>。按照特征值的定义，即得到一个特征值为1。&lt;/p>
&lt;p>然后你还要证明1是最大的特征值。&lt;/p>
&lt;p>矩阵的1范数是1（行和范数），而根据范数不等式，任何范数都大于等于谱半径（=最大特征值），所以该矩阵的最大特征值不大于1，而，已知有一个特征值为1，所以最大特征值就是1。&lt;/p>
&lt;p>（其实严谨点还需要证明这个最大特征值即谱半径的重数为1）&lt;/p>
&lt;p>严谨证明请看：&lt;a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem#Non-negative_matrices">Perron–Frobenius theorem&lt;/a>&lt;/p>
&lt;h2 id="从线性代数的角度看马尔可夫矩阵">从线性代数的角度看马尔可夫矩阵&lt;/h2>
&lt;p>如果我们从线性代数的角度看马尔可夫矩阵，会有一个整体的视角，获得更直观的认识。&lt;/p>
&lt;p>我们如果通过特征基来表示马尔可夫矩阵&lt;span class="math">\(A\)&lt;/span>，有： &lt;span class="math">\[
Av=c_1\lambda_1 x_1+c_2\lambda_2 x_2+\dotsb+c_n\lambda_n x_n\\
v=c_1x_1+c_2x_2+\dotsb+c_nx_n
\]&lt;/span> 其中，&lt;span class="math">\(\lambda_i\)&lt;/span>是矩阵&lt;span class="math">\(A\)&lt;/span>的特征值，且我们将其按从大到小顺序排好，向量&lt;span class="math">\(x_i\)&lt;/span>是对应的特征矩阵，我们将任意向量&lt;span class="math">\(v\)&lt;/span>用特征基表示，其系数为&lt;span class="math">\(c_i\)&lt;/span>。&lt;/p>
&lt;p>那么，初始状态向量经过&lt;span class="math">\(k\)&lt;/span>个周期的马尔可夫矩阵为： &lt;span class="math">\[v_k=A^kv=c_1\lambda_1^k x_1+c_2\lambda_2^k x_2+\dotsb+c_n\lambda_n^k x_n\]&lt;/span> 如果想让&lt;span class="math">\(v_k\)&lt;/span>趋于稳态，那么特征值的&lt;span class="math">\(|\lambda_i|\)&lt;/span>都必须小于等于1，否则矩阵就会发散。&lt;/p>
&lt;p>我们再看行列式&lt;span class="math">\(\det(A-I)\)&lt;/span>，由于&lt;span class="math">\(A\)&lt;/span>的每一列和为1，当对角元素减1后，列和必然为0，因此&lt;span class="math">\(A-I\)&lt;/span>的行向量是线性相关的，即&lt;span class="math">\(A-I\)&lt;/span>是奇异矩阵，所以&lt;span class="math">\(\det(A-I)=0\)&lt;/span>，这使得1是&lt;span class="math">\(A\)&lt;/span>的一个特征值。&lt;/p>
&lt;p>设特征值&lt;span class="math">\(\lambda_1\sim\lambda_i\)&lt;/span>为1，剩下的小于1，那么当&lt;span class="math">\(k\rightarrow ∞\)&lt;/span>时，有&lt;span class="math">\(\lambda_1^k,\dotsb,\lambda_i^k=1,\lambda_{i+1},\dotsb,\lambda_n\rightarrow 0\)&lt;/span>，此时 &lt;span class="math">\[
v_k=A^kv=c_1 x_1+c_2 x_2+\dotsb+c_ix_i
\]&lt;/span> 即为稳态。&lt;/p></description></item><item><title>概率统计随机过程之两两独立与相互独立</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E4%B8%A4%E4%B8%A4%E7%8B%AC%E7%AB%8B%E4%B8%8E%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B/</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E4%B8%A4%E4%B8%A4%E7%8B%AC%E7%AB%8B%E4%B8%8E%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B/</guid><description>
&lt;h2 id="概率统计随机过程之两两独立与相互独立">概率统计随机过程之两两独立与相互独立&lt;!-- omit in toc -->&lt;/h2>
&lt;p>&lt;span class="math">\(A,B,C\)&lt;/span>互相独立，说明&lt;span class="math">\(A,B,C\)&lt;/span>间无关联，是互相独立的，但两两独立指&lt;span class="math">\(A\)&lt;/span>和&lt;span class="math">\(B\)&lt;/span>间独立，&lt;span class="math">\(B\)&lt;/span>和&lt;span class="math">\(C\)&lt;/span>之间独立，&lt;span class="math">\(A\)&lt;/span>和&lt;span class="math">\(C\)&lt;/span>间独立，但三者放在一起，并不能判断他们是无关的。&lt;/p>
&lt;p>所以，&lt;strong>两两独立不一定相互独立，相互独立必然两两独立。&lt;/strong>&lt;/p>
&lt;p>例如：有三个随机变量&lt;span class="math">\(A,B,C\)&lt;/span>如果他们两两独立， 那么： &lt;span class="math">\[P(AB)=P(A)P(B)\\
P(AC)=P(A)P(C)\\
P(BC)=P(B)P(C)\]&lt;/span>&lt;/p>
&lt;p>但是&lt;span class="math">\(P(ABC)\)&lt;/span>不一定等于&lt;span class="math">\(P(A)P(B)P(C)\)&lt;/span>，&lt;/p>
&lt;p>如果相互独立的话，那么上式就是成立的。&lt;/p>
&lt;p>&lt;strong>请看下例&lt;/strong>:&lt;/p>
&lt;blockquote>
&lt;p>例1：设有四张外型一样的卡片,上分别写 有数字&lt;span class="math">\(2,3,5,30\)&lt;/span>今从中任取一张观察其上数字: &lt;span class="math">\[A=\{取到是2的倍数\};\\B=\{取到是3的倍数\};\\C=\{取到是5的倍数\};\]&lt;/span> 则&lt;span class="math">\(A,B,C\)&lt;/span>是两两独立而不是相互独立.&lt;/p>
&lt;p>因为: &lt;span class="math">\[A=\{2,30\}; B=\{3,30\}; C=\{5,30\};\\AB=AC=BC=ABC=\{30\}\]&lt;/span> 所以: &lt;span class="math">\(P(A)=P(B)=P(C)=2/4=1/2;P(AB)=P(A)P(B)=(1/2)*(1/2)=0.25,\)&lt;/span>&lt;/p>
&lt;p>类似&lt;span class="math">\(P(BC)=P(B)P(C))=0.25; P(AC)=P(A)P(C))=0.25\)&lt;/span>&lt;/p>
&lt;p>而&lt;span class="math">\(P(ABC)=(1/2)*(1/2)*(1/2)=0.125,\)&lt;/span>&lt;/p>
&lt;p>但&lt;span class="math">\(P(A)P(B)P(C)=0.5*0.5*0.5=0.125\)&lt;/span>两者不等.&lt;/p>
&lt;/blockquote>
&lt;p>下面这个例子来自维基百科： &amp;gt;例2： Suppose &lt;span class="math">\(X\)&lt;/span> and &lt;span class="math">\(Y\)&lt;/span> are two independent tosses of a fair coin, where we designate 1 for heads and 0 for tails. Let the third random variable &lt;span class="math">\(Z\)&lt;/span> be equal to 1 if exactly one of those coin tosses resulted in &amp;quot;heads&amp;quot;, and 0 otherwise(异或). Then jointly the triple &lt;span class="math">\((X, Y, Z)\)&lt;/span> has the following probability distribution: &amp;gt;&lt;span class="math">\[(X,Y,Z)=\left\{{\begin{matrix}(0,0,0)&amp;amp;{\text{with probability}}\ 1/4,\\(0,1,1)&amp;amp;{\text{with probability}}\ 1/4,\\(1,0,1)&amp;amp;{\text{with probability}}\ 1/4,\\(1,1,0)&amp;amp;{\text{with probability}}\ 1/4.\end{matrix}}\right.\]&lt;/span> &amp;gt;Here the marginal probability distributions are identical: &lt;span class="math">\(f_{X}(0)=f_{Y}(0)=f_{Z}(0)=1/2\)&lt;/span>, and &lt;span class="math">\(f_{X}(1)=f_{Y}(1)=f_{Z}(1)=1/2\)&lt;/span>. The bivariate distributions also agree: &lt;span class="math">\(f_{{X,Y}}=f_{{X,Z}}=f_{{Y,Z}}\)&lt;/span>, where &lt;span class="math">\(f_{{X,Y}}(0,0)=f_{{X,Y}}(0,1)=f_{{X,Y}}(1,0)=f_{{X,Y}}(1,1)=1/4\)&lt;/span>. &amp;gt; &amp;gt;Since each of the pairwise joint distributions equals the product of their respective marginal distributions, the variables are pairwise independent: &amp;gt; &amp;gt;- X and Y are independent, and &amp;gt;- X and Z are independent, and &amp;gt;- Y and Z are independent. &amp;gt; &amp;gt;However, &lt;span class="math">\(X, Y\)&lt;/span>, and &lt;span class="math">\(Z\)&lt;/span> are &lt;strong>not mutually independent&lt;/strong>, since &lt;span class="math">\({\displaystyle f_{X,Y,Z}(x,y,z)\neq f_{X}(x)f_{Y}(y)f_{Z}(z),}\)&lt;/span> the left side equalling for example 1/4 for &lt;span class="math">\((x, y, z) = (0, 0, 0)\)&lt;/span> while the right side equals 1/8 for &lt;span class="math">\((x, y, z) = (0, 0, 0)\)&lt;/span>. In fact, any of &lt;span class="math">\(\{X,Y,Z\}\)&lt;/span> is completely determined by the other two (any of &lt;span class="math">\(X, Y, Z\)&lt;/span> is the sum (modulo(模) 2) of the others). That is as far from independence as random variables can get.&lt;/p>
&lt;p>总的来说，有些随机变量可以由其他随机变量决定，因此，两两独立的随机变量之间可形成组合和其他随机变量形成联系。&lt;/p></description></item><item><title>概率统计随机过程之分析化</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%88%86%E6%9E%90%E5%8C%96%E4%B8%8E%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%88%86%E6%9E%90%E5%8C%96%E4%B8%8E%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</guid><description>
&lt;h2 id="概率统计随机过程之概率母函数矩母函数和特征函数">概率统计随机过程之概率母函数、矩母函数和特征函数&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#概率母函数">概率母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#性质">性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#例子">例子&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二项分布概率母函数">二项分布概率母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泊松分布概率母函数">泊松分布概率母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几何分布概率母函数">几何分布概率母函数&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#求概率">求概率&lt;/a>&lt;/li>
&lt;li>&lt;a href="#推广二维概率母函数">推广——二维概率母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#局限">局限&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩母函数">矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩母函数性质">矩母函数性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩母函数例子">矩母函数例子&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#离散型矩母函数">离散型矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数分布矩母函数">指数分布矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态分布矩母函数">正态分布矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#复合随机变量">复合随机变量&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#推广随机向量的矩母函数">推广——随机向量的矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#局限性">局限性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数">特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数性质">特征函数性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数例子">特征函数例子&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二项分布特征函数">二项分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泊松分布特征函数">泊松分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几何分布特征函数">几何分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态分布特征函数">正态分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均匀分布特征函数">均匀分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#柯西分布特征函数">柯西分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉普拉斯分布特征函数">拉普拉斯分布特征函数&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#更多的性质">更多的性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有趣的例题">有趣的例题&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概率母函数">概率母函数&lt;/h2>
&lt;blockquote>
&lt;p>定义：设&lt;span class="math">\(X\)&lt;/span>是非负整数的随机变量，定义其概率母函数 (probability-generating function)为 &lt;span class="math">\[g(s)=\mathbb{E}[s^X]=\sum_{j=0}^{\infty} s^j\mathbb{P}[X=j], s\in[-1,1]\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>其中约定 &lt;span class="math">\(0^0=1\)&lt;/span>。显然 &lt;span class="math">\(g(s)\)&lt;/span> 在 &lt;span class="math">\([-1,1]\)&lt;/span> 绝对收敛&lt;/p>
&lt;h3 id="性质">性质&lt;/h3>
&lt;ul>
&lt;li>&lt;span class="math">\(\mathbb{P}[X=k]=\frac{g^{(k)}(0)}{k!},\ k=0,1,\ldots\)&lt;/span>，这说明概率母函数和概率分布列一一对应&lt;/li>
&lt;li>&lt;span class="math">\(\mathbb{E}[X]=g^{(1)}(1)\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(\mathbb{E}[X]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(\mathrm{Var}[X]=g^{(2)}(1)+g^{(1)}(1)-[g^{(1)}(1)]^2\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(X_1,\ldots,X_n\)&lt;/span> 相互独立，&lt;span class="math">\(Y=X_1+\cdots+X_n\)&lt;/span>，则 &lt;span class="math">\(g_Y(s)=g_{X_1}(s)\cdots g_{X_n}(s),s\in[-1,1]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(X_1,X_2,\ldots\)&lt;/span> 是&lt;strong>独立同分布&lt;/strong>的非负整数随机变量，概率母函数为 &lt;span class="math">\(\psi(x)\)&lt;/span>; &lt;span class="math">\(N\)&lt;/span> 为取正整数值的随机变量且独立于 &lt;span class="math">\(X_i\)&lt;/span>，概率母函数为 &lt;span class="math">\(G(s)\)&lt;/span>。则 &lt;span class="math">\(Y=X_1+\cdots+X_N\)&lt;/span> 的概率母函数为 &lt;span class="math">\(H(s)=G[\psi(s)]\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>只给出性质五的证明 &lt;span class="math">\[\begin{aligned}
H(s)&amp;amp;=\mathbb{E}[\mathbb{E}[S^W\mid Y]]\\
&amp;amp;=\sum_{n=1}^{\infty}\mathbb{E}[s^{X_1+\cdots+X_n}]\mathbb{P}[Y=n]\\
&amp;amp;=\sum_{n=1}^{\infty}[\psi(s)]^n\mathbb{P}[Y=n]=G[\psi(s)]
\end{aligned}\]&lt;/span>&lt;/p>
&lt;h3 id="例子">例子&lt;/h3>
&lt;h4 id="二项分布概率母函数">二项分布概率母函数&lt;/h4>
&lt;p>二项分布 &lt;span class="math">\(B(n,p)\)&lt;/span> 的概率母函数为 &lt;span class="math">\(g(s)=(sp+q)^n\)&lt;/span>&lt;/p>
&lt;p>由此立得若 &lt;span class="math">\(X_i,\ldots,X_m\)&lt;/span> 独立，且&lt;span class="math">\(X_i\sim B(n_i,p)\)&lt;/span>，则&lt;span class="math">\(Y=X_1+\cdots+X_m\sim B(n_1+\cdots+n_m,p)\)&lt;/span>&lt;/p>
&lt;h4 id="泊松分布概率母函数">泊松分布概率母函数&lt;/h4>
&lt;p>泊松分布 &lt;span class="math">\(\mathcal{P}(\lambda)\)&lt;/span> 的概率母函数为 &lt;span class="math">\(g(s)=e^{\lambda(s-1)}\)&lt;/span>&lt;/p>
&lt;p>由此立得若 &lt;span class="math">\(X_i,\ldots,X_m\)&lt;/span> 独立，且&lt;span class="math">\(X_i\sim \mathcal{P}(\lambda_i)\)&lt;/span>，则&lt;span class="math">\(Y=X_1+\cdots+X_m\sim \mathcal{P}(\lambda_1+\cdots+\lambda_m)\)&lt;/span>&lt;/p>
&lt;h4 id="几何分布概率母函数">几何分布概率母函数&lt;/h4>
&lt;p>几何分布 &lt;span class="math">\(G(p)\)&lt;/span> 的概率母函数为 &lt;span class="math">\(g(s)=\frac{sp}{1-sq}\)&lt;/span>&lt;/p>
&lt;p>由此立得若 &lt;span class="math">\(X_i,\ldots,X_m\)&lt;/span> 独立，且&lt;span class="math">\(X_i\sim G(p)\)&lt;/span>，则&lt;span class="math">\(S_m=X_1+\cdots+X_m\)&lt;/span> 有概率母函数 &lt;span class="math">\[\begin{aligned}
g_{S_m}(s)&amp;amp;=\left(\frac{sp}{1-sq}\right)^m\\
&amp;amp;=(sp)^m\sum_{j=0}^{\infty}\frac{m(m+1)\cdots(m+j-1)}{j!}(sq)^j\\
&amp;amp;=(sp)^m\sum_{j=0}^{\infty}\binom{m+j-1}{j}(sq)^j\\
&amp;amp;=\sum_{k=m}^{\infty}\binom{k-1}{m-1}p^mq^{k-m}s^k\end{aligned}\]&lt;/span>&lt;/p>
&lt;p>于是得 Pascal 分布 &lt;span class="math">\[\mathbb{E}[S_m=k]=\binom{k-1}{m-1}p^mq^{k-m}\]&lt;/span>&lt;/p>
&lt;h3 id="求概率">求概率&lt;/h3>
&lt;p>求扔三颗骰子，总点数为 9 的概率。 记 &lt;span class="math">\(X_i\)&lt;/span> 为第 &lt;span class="math">\(i\)&lt;/span> 颗骰子的点数，其概率母函数&lt;/p>
&lt;p>&lt;span class="math">\[g(s)=\mathbb{E}[s^{X_1}]=\frac{1}{6}(s+s^2+\cdots+s^6)=\frac{1}{6}\frac{s(1-s^6)}{1-s}\]&lt;/span>&lt;/p>
&lt;p>则 &lt;span class="math">\(Y=X_1+X_2+X_3\)&lt;/span> 的概率母函数为&lt;/p>
&lt;p>&lt;span class="math">\[g_Y(s)=[g_X(s)]^3=\frac{s^3(1-s^6)^3}{6^3(1-s)^s}=\frac{1}{6^3}(s^3)(1-3s^6+3s^{12}-s^{18})\sum_{k=0}^{\infty}\binom{k+2}{2}s^k\]&lt;/span>&lt;/p>
&lt;p>则 &lt;span class="math">\(s^9\)&lt;/span> 的系数为 &lt;span class="math">\[\mathbb{P}(Y=9)=\frac{1}{6^3}[\binom{6+2}{2}-3]=\frac{25}{216}\]&lt;/span>&lt;/p>
&lt;h3 id="推广二维概率母函数">推广——二维概率母函数&lt;/h3>
&lt;blockquote>
&lt;p>设 &lt;span class="math">\((X,Y)\)&lt;/span> 是二维取非负整数值的随机向量，记 &lt;span class="math">\(p_{ik}=\mathbb{P}[X=i,Y=k]\)&lt;/span>，则其二维概率母函数为 &lt;span class="math">\[g(s,t)=\mathbb{E}[s^Xt^Y]=\sum_{i=0}^{\infty}\sum_{k=0}^{\infty}p_{ik}s^it^k,\quad s,t\in[-1,1]\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>其有如下性质&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\lvert g(s,t)\rvert\leq g(1,1)=1,\lvert s\rvert\leq 1,\lvert t\rvert\leq 1\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(g_{aX+bY+c}(s)=s^cg(s^a,s^b)\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(X,Y\)&lt;/span> 独立，则 &lt;span class="math">\(g(s,t)=g_X(s)g_Y(t)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(g(s,1)=g_X(s),g(1,t)=g_Y(t)\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(\mathbb{E}[X]&amp;lt;\infty,\mathbb{E}[Y]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\[\mathbb{E}[X]=\frac{\partial g(s,t)}{\partial s}\big|_{s=t=1},\mathbb{E}[Y]=\frac{\partial g(s,t)}{\partial t}\big|_{s=t=1}\]&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(\mathbb{E}[X^2]&amp;lt;\infty,\mathbb{E}[Y^2]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\[\mathbb{E}[X^2]=\frac{\partial^2 g(s,t)}{\partial s^2}\big|_{s=t=1},\mathbb{E}[Y^2]=\frac{\partial^2 g(s,t)}{\partial t^2}\big|_{s=t=1},\mathbb{E}[XY]=\frac{\partial^2 g(s,t)}{\partial s\partial t}\big|_{s=t=1}\]&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(p_{ik}=\frac{1}{i!k!}\frac{\partial^{i+k}g(s,t)}{\partial s^i\partial t^k}\big |_{s=t=0},\ i,k=0,1,\ldots\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="局限">局限&lt;/h3>
&lt;p>只能对取&lt;strong>非负整数值&lt;/strong>的随机变量定义&lt;/p>
&lt;h2 id="矩母函数">矩母函数&lt;/h2>
&lt;blockquote>
&lt;p>定义:设 &lt;span class="math">\(X\)&lt;/span> 是随机变量，定义其矩母函数 (moment-generating function)为 &lt;span class="math">\[M_X(s)=\mathbb{E}[e^{sX}]\]&lt;/span> 仅当 &lt;span class="math">\(\mathbb{E}[e^{sX}]&amp;lt;\infty\)&lt;/span> 时，我们称 &lt;span class="math">\(M_X(s)\)&lt;/span> 存在&lt;/p>
&lt;/blockquote>
&lt;h3 id="矩母函数性质">矩母函数性质&lt;/h3>
&lt;ul>
&lt;li>&lt;span class="math">\(M_{aX+b}(s)=e^{sb}M(sa)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathbb{E}[X^k]=M^{(k)}(0),k=1,2,\ldots\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(M(0)=1\)&lt;/span>&lt;/li>
&lt;li>可逆性：若&lt;span class="math">\(\exists a&amp;gt;0,\forall s\in[-a,a], M(s)&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(M(s)\)&lt;/span> 唯一地决定了 &lt;span class="math">\(X\)&lt;/span> 的分布函数&lt;/li>
&lt;li>若 &lt;span class="math">\(X_1,\ldots,X_n\)&lt;/span> 独立，&lt;span class="math">\(Y=X_1+\cdots+X_n\)&lt;/span>，则&lt;span class="math">\(M_{Y}(s)=M_{X_1}(s)\cdots M_{X_n}(s)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(X_1,X_2,\ldots\)&lt;/span> 独立同分布，矩母函数为 &lt;span class="math">\(M_X(s)\)&lt;/span>; &lt;span class="math">\(N\)&lt;/span> 为取正整数值的随机变量，矩母函数为 &lt;span class="math">\(M_N(s)\)&lt;/span>。则 &lt;span class="math">\(Y=X_1+\cdots+X_Y\)&lt;/span> 的矩母函数为 &lt;span class="math">\(M_Y(s)=\mathbb{E}[\mathbb{E}[e^{sY}\mid N=n]]=\mathbb{E}[(M_X(s))^n]=\sum_{n=1}^{\infty}(M_X(s))^n\mathbb{P}[N=n]\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>而 &lt;span class="math">\(M_N(s)=\sum_{n=1}^{\infty}[e^s]^n\mathbb{P}[N=n]\)&lt;/span> 二者有紧密的联系：将 &lt;span class="math">\(M_N(s)\)&lt;/span> 中出现的 &lt;span class="math">\(e^s\)&lt;/span> 替换为 &lt;span class="math">\(M_X(s)\)&lt;/span> 即可&lt;/p>
&lt;h3 id="矩母函数例子">矩母函数例子&lt;/h3>
&lt;h4 id="离散型矩母函数">离散型矩母函数&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">X&lt;/th>
&lt;th align="left">2&lt;/th>
&lt;th align="left">3&lt;/th>
&lt;th align="left">5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">&lt;span class="math">\(\mathbb{P}\)&lt;/span>&lt;/td>
&lt;td align="left">1/2&lt;/td>
&lt;td align="left">1/6&lt;/td>
&lt;td align="left">1/3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;span class="math">\[M(s)=\frac{1}{2}e^{2s}+\frac{1}{6}e^{3s}+\frac{1}{3}e^{5s}\]&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\[\mathbb{E}[X]=M^{(1)}(0)=(\frac{1}{2}2e^{2s}+\frac{1}{6}3e^{3s}+\frac{1}{3}5e^{5s})|_{s=0}=\frac{19}{6}\]&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(\mathbb{E}[X^2]=M^{(2)}(0)=(\frac{1}{2}4e^{2s}+\frac{1}{6}9e^{3s}+\frac{1}{3}25e^{5s})|_{s=0}=\frac{71}{6}\)&lt;/span>&lt;/p>
&lt;h4 id="指数分布矩母函数">指数分布矩母函数&lt;/h4>
&lt;p>设 &lt;span class="math">\(X\sim \mathcal{E}(\lambda)\)&lt;/span>，则当 &lt;span class="math">\(s&amp;lt;\lambda\)&lt;/span> 时，有 &lt;span class="math">\[M(s)=\lambda\int_0^\infty e^{sx}e^{-\lambda x}\,\mathrm{d}x=\frac{\lambda}{s-\lambda}e^{(s-\lambda)x}|{x=0}^{\infty}=\frac{\lambda}{\lambda-s}\]&lt;/span>&lt;/p>
&lt;p>而当 &lt;span class="math">\(s\ge\lambda\)&lt;/span> 时 &lt;span class="math">\(M(s)\)&lt;/span> 不存在&lt;/p>
&lt;p>&lt;span class="math">\[\mathbb{E}[X]=M^{(1)}(0)=\frac{\lambda}{(\lambda-s)^2}|_{s=0}=\frac{1}{\lambda}\]&lt;/span> &lt;span class="math">\[\mathbb{E}[X]=M^{(2)}(0)=\frac{2\lambda}{(\lambda-s)^3}|_{s=0}=\frac{2}{\lambda^2}\]&lt;/span>&lt;/p>
&lt;h4 id="正态分布矩母函数">正态分布矩母函数&lt;/h4>
&lt;p>&lt;span class="math">\(X\sim\mathcal{N}({\mu_1,\sigma_1^2}),Y\sim\mathcal{N}(\mu_2,\sigma_2^2), X,Y\)&lt;/span> 相互独立，求 &lt;span class="math">\(Z=X+Y\)&lt;/span> 的分布&lt;/p>
&lt;p>先计算标准正态分布的矩母函数，由定义求得 &lt;span class="math">\(M(s)=e^{s^2/2}\)&lt;/span>&lt;/p>
&lt;p>根据矩母函数的性质，有 &lt;span class="math">\[M_X(s)=e^{\mu_1 s}e^{\sigma_1^2s^2/2},\\
M_Y(s)=e^{\mu_2 s}e^{\sigma_2^2s^2/2},\\
M_Z(s)=e^{(\mu_1+\mu_2) s}e^{(\sigma_1^2+\sigma_2^2)s^2/2}\]&lt;/span>&lt;/p>
&lt;p>于是 &lt;span class="math">\(Z\sim\mathcal{N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)\)&lt;/span>&lt;/p>
&lt;h4 id="复合随机变量">复合随机变量&lt;/h4>
&lt;p>不断进行成功概率为 &lt;span class="math">\(p\)&lt;/span> 的伯努利实验直至成功，每次实验的耗时服从参数 &lt;span class="math">\(\lambda\)&lt;/span> 的指数分布，且完全独立。 求总耗时的分布&lt;/p>
&lt;p>首先 &lt;span class="math">\(X_i\sim\mathcal{E}(\lambda), N\sim G(p), Y=X_1+\cdots+X_N\)&lt;/span> , 当 &lt;span class="math">\(s&amp;lt;\lambda\)&lt;/span> 时有 &lt;span class="math">\[M_{X_i}(s)=\frac{\lambda}{\lambda-s}\]&lt;/span> 而 &lt;span class="math">\[M_N(s)=\frac{pe^s}{1-qe^s}\]&lt;/span> 故 &lt;span class="math">\[M_Y(s)=\frac{p M_X(s)}{1-qM_X(s)}=\frac{p\lambda}{\lambda-s-q\lambda}=\frac{p\lambda}{p\lambda-s}\]&lt;/span>&lt;/p>
&lt;h3 id="推广随机向量的矩母函数">推广——随机向量的矩母函数&lt;/h3>
&lt;p>设&lt;span class="math">\(\vec{X}=(X_1,X_2,\ldots,X_n)^\intercal\)&lt;/span>，则其矩母函数定义为&lt;/p>
&lt;p>&lt;span class="math">\[M_{\vec{X}}(\vec{s})=\mathbb{E}[e^{\vec{s}^\intercal\vec{X}}]=\mathbb{E}[e^{s_1X_1+\cdots+s_nX_n}] \]&lt;/span>&lt;/p>
&lt;h3 id="局限性">局限性&lt;/h3>
&lt;p>有些分布的&lt;strong>矩母函数不存在&lt;/strong>，因为&lt;strong>其积分发散&lt;/strong>，如 Cauchy 分布。为此我们引入特征函数来保证可积性。&lt;/p>
&lt;h2 id="特征函数">特征函数&lt;/h2>
&lt;blockquote>
&lt;p>定义：对随机变量 &lt;span class="math">\(X\)&lt;/span> ，定义其特征函数 (characteristic function) 为 &lt;span class="math">\[\phi(t)=\mathbb{E}[e^{itX}]=\mathbb{E}[\cos(tX)]+i\mathbb{E}[\sin(tX)], t\in \mathbb{R}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h3 id="特征函数性质">特征函数性质&lt;/h3>
&lt;ul>
&lt;li>&lt;span class="math">\(\lvert \phi(t)\rvert\leq \phi(0)=1,\quad\phi(-t)=\overline{\phi(t)}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\phi(t)\)&lt;/span> 在 &lt;span class="math">\((-\infty,\infty)\)&lt;/span> 一致连续 v若 &lt;span class="math">\(\mathbb{E}[\lvert X\rvert^k]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(\phi^{(k)}(t)=i^k\mathbb{E}[X^k e^{itX}],\phi^{(k)}(0)=i^k\mathbb{E}[X^k]\)&lt;/span>&lt;/li>
&lt;li>非负定性：&lt;span class="math">\(\forall t_1,\ldots,t_n\in\mathbb{R},\forall z_1,\ldots,z_n\in\mathbb{C},\sum_{k=1}^{n}\sum_{j=1}^{n}\phi(t_k-t_j)z_k\bar{z}_j\ge 0\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(X_1,\ldots,X_n\)&lt;/span> 相互独立， &lt;span class="math">\(X_k\)&lt;/span> 特征函数为 &lt;span class="math">\(\phi_k(t)\)&lt;/span>，则 &lt;span class="math">\(Y=X_1+\cdots+X_n\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi_Y(t)=\phi_1(t)\cdots\phi_{k}(t)\)&lt;/span>。注意，逆命题不成立，后面给出了例子。&lt;/li>
&lt;/ul>
&lt;p>特征函数与概率分布函数是有一一对应关系的，二者可以互相确定，在概率论中叫做&lt;strong>反演定理&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>在累积概率分布函数与特征函数之间存在&lt;strong>双射&lt;/strong>。也就是说，&lt;strong>两个不同的概率分布不能有相同的特征函数&lt;/strong>。&lt;/p>
&lt;p>给定一个特征函数&lt;span class="math">\(\phi\)&lt;/span>，可以用以下公式求得对应的累积概率分布函数 &lt;span class="math">\[F_{X}(y)-F_{X}(x)=\lim_{{\tau \to +\infty }}{\frac{1}{2\pi }}\int_{{-\tau }}^{{+\tau }}{\frac{e^{{-itx}}-e^{{-ity}}}{it}}\,\varphi_{X}(t)\,dt\]&lt;/span> 一般地，这是一个广义积分；被积分的函数可能只是条件可积而不是勒贝格可积的，也就是说，它的绝对值的积分可能是无穷大。&lt;/p>
&lt;/blockquote>
&lt;h3 id="特征函数例子">特征函数例子&lt;/h3>
&lt;h4 id="二项分布特征函数">二项分布特征函数&lt;/h4>
&lt;p>二项分布 &lt;span class="math">\(B(n,p)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=(e^{it}p+q)^n\)&lt;/span>&lt;/p>
&lt;p>(对比其概率母函数 &lt;span class="math">\(g(s)=(sp+q)^n\)&lt;/span>)&lt;/p>
&lt;h4 id="泊松分布特征函数">泊松分布特征函数&lt;/h4>
&lt;p>泊松分布 &lt;span class="math">\(\mathcal{P}(\lambda)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=e^{\lambda(e^{it}-1)}\)&lt;/span>&lt;/p>
&lt;p>(对比其概率母函数 &lt;span class="math">\(g(s)=e^{\lambda(s-1)}\)&lt;/span>)&lt;/p>
&lt;h4 id="几何分布特征函数">几何分布特征函数&lt;/h4>
&lt;p>几何分布 &lt;span class="math">\(G(p)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=\frac{pe^{it}}{1-qe^{it}}\)&lt;/span>&lt;/p>
&lt;p>(对比其概率母函数 &lt;span class="math">\(g(s)=\frac{sp}{1-sq}\)&lt;/span>)&lt;/p>
&lt;h4 id="正态分布特征函数">正态分布特征函数&lt;/h4>
&lt;p>正态分布 &lt;span class="math">\(\mathcal{N}(\mu,\sigma^2)\)&lt;/span> 有特征函数 &lt;span class="math">\(\phi(t)=e^{i\mu t}e^{-\frac{1}{2}\sigma^2t^2}\)&lt;/span>&lt;/p>
&lt;p>(对比其矩母函数 &lt;span class="math">\(M(s)=e^{\mu s}e^{\sigma^2s^2/2}\)&lt;/span>)&lt;/p>
&lt;p>先考察标准正态分布。正态分布的特征函数推导不太容易，一种不太严谨的做法是做形式化运算，将 &lt;span class="math">\(i\)&lt;/span> 视为常数，则&lt;/p>
&lt;p>&lt;span class="math">\(\phi(t)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{itx}e^{-x^2/2}\,\mathrm{d}x=e^{-t^2/2}\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-(x-it)^2/2}\,\mathrm{d}x=e^{-t^2/2}\)&lt;/span>&lt;/p>
&lt;p>严格的数学推导需要一定复变函数的背景知识。&lt;/p>
&lt;p>首先 &lt;span class="math">\[\phi(t)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{itx}e^{-x^2/2}\,\mathrm{d}x=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \cos(tx)e^{-x^2/2}\,\mathrm{d}x\]&lt;/span>&lt;/p>
&lt;p>对 &lt;span class="math">\(t\)&lt;/span> 求导得 &lt;span class="math">\[\begin{aligned}\phi&amp;#39;(t)&amp;amp;=-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty x\sin(tx)e^{-x^2/2}\,\mathrm{d}x\\&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}\sin(tx)\,\mathrm{d}e^{-x^2/2}\\&amp;amp;=-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty t\cos(tx)e^{-x^2/2}\,\mathrm{d}x\\&amp;amp;=-t\phi(t)\end{aligned}\]&lt;/span>&lt;/p>
&lt;p>即 &lt;span class="math">\(\frac{\mathrm{d}}{\mathrm{d}t}[\phi(t)e^{t^2/2}]=0\)&lt;/span>，则 &lt;span class="math">\(\phi(t)e^{t^2/2}=C=\phi(0)=1\)&lt;/span>，得 &lt;span class="math">\(\phi(t)=e^{-t^2/2}\)&lt;/span>&lt;/p>
&lt;p>由此再求一般正态的特征函数&lt;/p>
&lt;p>&lt;span class="math">\(\mathbb{E}[e^{it(\mu+\sigma X)}]=e^{it\mu}\mathbb{E}[e^{it\sigma X}]=e^{it\mu}e^{-\sigma^2t^2/2}\)&lt;/span>&lt;/p>
&lt;p>同时，若 &lt;span class="math">\(X_1,\ldots,X_m\)&lt;/span> 相互独立，&lt;span class="math">\(X_j\sim\mathcal{\mu_j,\sigma_j^2}\)&lt;/span>，则&lt;/p>
&lt;p>&lt;span class="math">\(Y=X_1+\cdots+X_m\sim\mathcal{N}(\sum_{j=1}^{m}\mu_j,\sum_{j=1}^{m}\sigma_j^2)\)&lt;/span>&lt;/p>
&lt;h4 id="均匀分布特征函数">均匀分布特征函数&lt;/h4>
&lt;p>均匀分布 &lt;span class="math">\(\mathcal{U}(a,b)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=\frac{e^{itb}-e^{ita}}{it(b-a)}\)&lt;/span>&lt;/p>
&lt;p>指数分布 指数分布 &lt;span class="math">\(\mathcal{E}(\lambda)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=(1-\frac{it}{\lambda})^{-1}\)&lt;/span>&lt;/p>
&lt;p>(与矩母函数 &lt;span class="math">\(M(s)=\frac{\lambda}{\lambda-s}=\frac{1}{1-s/\lambda}=(1-s/\lambda)^{-1}\)&lt;/span> 对比)&lt;/p>
&lt;h4 id="柯西分布特征函数">柯西分布特征函数&lt;/h4>
&lt;p>&lt;span class="math">\(f(x)=\frac{1}{\pi(1+x^2)}\)&lt;/span>，其特征函数为 &lt;span class="math">\(\phi(t)=e^{-\lvert t\rvert}\)&lt;/span>&lt;/p>
&lt;p>取 &lt;span class="math">\(Y=aX,(a&amp;gt;0)\)&lt;/span>，则 &lt;span class="math">\(\phi_Y(t)=\mathbb{E}[e^{i(at)X}]=e^{-a\lvert t\rvert}\)&lt;/span>，此时&lt;/p>
&lt;p>&lt;span class="math">\(\phi_{X+Y}(t)=\mathbb{E}[e^{it(1+a)X}]=e^{-(1+a)\lvert t\rvert}=\phi_X(t)\phi_Y(t)\)&lt;/span>，但显然 &lt;span class="math">\(X,Y\)&lt;/span> 不独立&lt;/p>
&lt;h4 id="拉普拉斯分布特征函数">拉普拉斯分布特征函数&lt;/h4>
&lt;p>&lt;span class="math">\(f(x)=\frac{1}{2}e^{-\lvert x\rvert}\)&lt;/span> ，其特征函数为 &lt;span class="math">\(\phi(t)=\frac{1}{1+t^2}\)&lt;/span>&lt;/p>
&lt;p>注意它和柯西分布的“对称性”&lt;/p>
&lt;h3 id="更多的性质">更多的性质&lt;/h3>
&lt;ul>
&lt;li>若 &lt;span class="math">\(\mathbb{E}[\lvert X\rvert^n]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(\phi(t)=\sum_{m=0}^{n}\frac{\mathbb{E}[(itX)^m]}{m!}+o(t^n)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>特别的，若二阶矩存在，则 &lt;span class="math">\(\phi(t)=1+it\mathbb{E}[X]-\frac{1}{2}t^2\mathbb{E}[X^2]+o(t^2)\)&lt;/span>&lt;/p>
&lt;ul>
&lt;li>&lt;p>逆转公式：若累积分布函数 &lt;span class="math">\(F(x)\)&lt;/span> 在 &lt;span class="math">\((a,b)\)&lt;/span> 连续，则 &lt;span class="math">\[\frac{1}{2\pi}\lim\limits_{T\rightarrow\infty}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\phi(t)\,\mathrm{d}t=F(b)-F(a)\]&lt;/span> 若 &lt;span class="math">\(\int_{-\infty}^{\infty}\lvert\phi(t)\rvert\,\mathrm{d}t&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(X\)&lt;/span> 有有界连续密度函数 &lt;span class="math">\(f(x)=\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{-itx}\phi(t)\,\mathrm{d}t\)&lt;/span>&lt;/p>
这些说明了随机变量和分布函数相互唯一决定&lt;/li>
&lt;li>&lt;p>先介绍收敛性：设 &lt;span class="math">\(X\)&lt;/span> 有分布函数 &lt;span class="math">\(F(x)\)&lt;/span>， &lt;span class="math">\(X_n\)&lt;/span> 有分布函数 &lt;span class="math">\(F_n(x)\)&lt;/span>。若在 &lt;span class="math">\(F(x)\)&lt;/span> 的连续点 &lt;span class="math">\(x\)&lt;/span> 处，有 &lt;span class="math">\(\lim\limits_{n\rightarrow\infty}F_n(x)=F(x)\)&lt;/span>，则称 &lt;span class="math">\(X_n\)&lt;/span> 依分布收敛 (convergence in distribution) 到 &lt;span class="math">\(X\)&lt;/span>，记为 &lt;span class="math">\(X_n\overset{d}{\rightarrow} X\)&lt;/span>;或称 &lt;span class="math">\(F_n\)&lt;/span> 弱收敛 (weak convergence) 到 &lt;span class="math">\(F\)&lt;/span>，记为 &lt;span class="math">\(F_n\overset{w}{\rightarrow}F\)&lt;/span>&lt;/p>
连续性定理指出，&lt;span class="math">\(X_n\)&lt;/span> 依分布收敛到 &lt;span class="math">\(X\)&lt;/span> 的充分必要条件是，其对应的特征函数满足 &lt;span class="math">\[\lim\limits_{n\rightarrow\infty}\phi_n(t)=\phi(t),\quad\forall t\in\mathbb{R}\]&lt;/span>&lt;/li>
&lt;li>判定一个函数是否为特征函数有如下定理&lt;/li>
&lt;li>&lt;p>S. Bochner-Khintchine 定理&lt;/p>
&lt;p>设 &lt;span class="math">\(\phi(t),t\in\mathbb{R}\)&lt;/span> 是连续函数且 &lt;span class="math">\(\phi(0)=1\)&lt;/span>，其是特征函数的充要条件是他是非负定的，即 &lt;span class="math">\(\forall t_1,\ldots,t_n\in\mathbb{R},\forall z_1,\ldots,z_n\in\mathbb{C},\sum_{k=1}^{n}\sum_{j=1}^{n}\phi(t_k-t_j)z_k\bar{z}_j\ge 0\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>Polya 定理&lt;/p>
&lt;p>设 &lt;span class="math">\(\phi(t)\)&lt;/span> 是连续函数、偶函数，在 &lt;span class="math">\((0,\infty)\)&lt;/span> 是凸函数，且满足&lt;span class="math">\(\phi(t)\ge 0\)&lt;/span>,&lt;span class="math">\(\phi(0)=1\)&lt;/span>,&lt;span class="math">\(\phi(t)\rightarrow 0\,(t\rightarrow\infty)\)&lt;/span> 则其是特征函数。&lt;/p>&lt;/li>
&lt;li>&lt;p>J. Marcinkiewiez 定理&lt;/p>
&lt;p>若 &lt;span class="math">\(\phi(t)\)&lt;/span> 具有形式 &lt;span class="math">\(e^{P(t)}\)&lt;/span>，其中 &lt;span class="math">\(P(t)\)&lt;/span> 是多项式，则其系数不能大于 2&lt;/p>&lt;/li>
&lt;/ul>
&lt;h3 id="有趣的例题">有趣的例题&lt;/h3>
&lt;p>（1）&lt;span class="math">\(\phi(t)\)&lt;/span> 是特征函数，证明 &lt;span class="math">\(\bar{\phi},\phi^2,\lvert\phi\rvert^2,Re[\phi]\)&lt;/span> 都是特征函数&lt;/p>
&lt;p>设 &lt;span class="math">\(X,Y\)&lt;/span>独立同分布且特征函数为 &lt;span class="math">\(\phi\)&lt;/span>，则前三个分别为 &lt;span class="math">\(-X,X+Y,X-Y\)&lt;/span>&lt;/p>
&lt;p>第四个构造独立于 &lt;span class="math">\(X\)&lt;/span> 的随机变量 &lt;span class="math">\(Z\)&lt;/span> 且 &lt;span class="math">\(\mathbb{P}[Z=\pm 1]=0.5\)&lt;/span>，则&lt;span class="math">\(XZ\)&lt;/span> 的特征函数为 &lt;span class="math">\(Re[\phi]\)&lt;/span>&lt;/p>
&lt;p>（2）&lt;span class="math">\(\phi(t)\)&lt;/span> 是特征函数，则 &lt;span class="math">\(\lvert\phi(t)\rvert\)&lt;/span> 未必是特征函数&lt;/p>
&lt;p>设 &lt;span class="math">\(X\sim B(1,1/3)\)&lt;/span>，&lt;span class="math">\(\phi(t)=\frac{2}{3}+\frac{1}{3}e^{it}\)&lt;/span> 。设 &lt;span class="math">\(Y\)&lt;/span> 的特征函数为 &lt;span class="math">\(\psi(t)=\lvert\phi(t)\rvert\)&lt;/span>,则 &lt;span class="math">\(\psi^2(t)=\phi(t)\phi(-t)\)&lt;/span>，即 &lt;span class="math">\(Y_1+Y_2\)&lt;/span> 和 &lt;span class="math">\(X_1-X_2\)&lt;/span> 同分布。其中 &lt;span class="math">\(Y_1,Y_2\)&lt;/span> 与 &lt;span class="math">\(Y\)&lt;/span> 独立同分布，&lt;span class="math">\(X_1,X_2\)&lt;/span> 与 &lt;span class="math">\(X\)&lt;/span> 独立同分布。由于 &lt;span class="math">\(X_1-X_2\in\{-1,0,1\}\)&lt;/span>，则 &lt;span class="math">\(Y_i\in\{-0.5,0.5\}\)&lt;/span>，记 &lt;span class="math">\(\mathbb{P}[Y_1=0.5]=\alpha\)&lt;/span>。则&lt;/p>
&lt;p>&lt;span class="math">\(\mathbb{P}[Y_1+Y_2=1]=\alpha^2=\mathbb{P}[X_1-X_2=1]=2/9\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(\mathbb{P}[Y_1+Y_2=-11]=(1-\alpha)^2=\mathbb{P}[X_2-X_1=1]=2/9\)&lt;/span>&lt;/p>
&lt;p>此时 &lt;span class="math">\(\alpha\)&lt;/span> 无解&lt;/p>
&lt;p>（3）&lt;span class="math">\(X_1,\ldots,X_4\)&lt;/span> 独立同标准正态分布，则&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X_1X_2\)&lt;/span>的特征函数为 &lt;span class="math">\(\frac{1}{\sqrt{1+t^2}}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(X_1X_2+X_3X_4\)&lt;/span> 的特征函数为 &lt;span class="math">\(\frac{1}{1+t^2}\)&lt;/span>，即服从拉普拉斯分布&lt;/li>
&lt;/ol>
&lt;p>（4）&lt;span class="math">\(X,Y\)&lt;/span>独立同标准正态分布，&lt;span class="math">\(U,V\)&lt;/span>独立于 &lt;span class="math">\(X,Y\)&lt;/span>，则&lt;span class="math">\(Z=\frac{UX+VY}{\sqrt{U^2+V^2}}\sim\mathcal{N}(0,1)\)&lt;/span>&lt;/p>
&lt;p>（5）利用已知结果若 &lt;span class="math">\(a&amp;gt;0,b&amp;gt;0\)&lt;/span>，则&lt;span class="math">\(I(a,b)=\int_0^\infty\exp\{-a^2u^2-b^2u^{-2}\}\,\mathrm{d}u=\frac{e^{-2ab}\sqrt{\pi}}{2a}\)&lt;/span>&lt;/p>
&lt;p>证明若 &lt;span class="math">\(f(x)=\frac{1}{2\pi x^3}\exp(-\frac{1}{2x}),x&amp;gt;0\)&lt;/span>， 则 &lt;span class="math">\(\mathbb{E}[e^{-tX}]=\exp(-\sqrt{2t})\)&lt;/span>&lt;/p>
&lt;p>（6）&lt;span class="math">\(X,Y,Z\)&lt;/span>独立同标准正态分布，则&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X/Y\)&lt;/span>服从柯西分布&lt;/li>
&lt;li>&lt;span class="math">\(1/X^2\)&lt;/span>的概率密度函数为 5. 中结果&lt;/li>
&lt;li>&lt;span class="math">\((XYZ)/\sqrt{X^2Y^2+Y^2Z^2+Z^2X^2}\sim\mathcal{N}(0,1/9)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>（7）&lt;span class="math">\(X_n\)&lt;/span> 有分布函数 &lt;span class="math">\(F_n(x)=x-\frac{\sin(2n\pi x)}{2n\pi},0\leq x\leq 1\)&lt;/span>&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X_n\)&lt;/span> 有密度函数 &lt;span class="math">\(f_n(x)=1-\cos(2n\pi x),0\leq x\leq 1\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(F_n(x)\)&lt;/span> 弱收敛于 &lt;span class="math">\(\mathcal{U}[0,1]\)&lt;/span>，但 &lt;span class="math">\(f_n(x)\)&lt;/span> 不收敛&lt;/li>
&lt;/ol></description></item><item><title>概率统计随机过程之经验函数分布</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%BB%8F%E9%AA%8C%E5%87%BD%E6%95%B0%E5%88%86%E5%B8%83/</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%BB%8F%E9%AA%8C%E5%87%BD%E6%95%B0%E5%88%86%E5%B8%83/</guid><description>
&lt;h2 id="概率统计随机过程之经验函数分布">概率统计随机过程之经验函数分布&lt;!-- omit in toc -->&lt;/h2>
&lt;p>设&lt;span class="math">\(x_1, x_2, \cdots, x_n\)&lt;/span>是取自总体&lt;span class="math">\(X\)&lt;/span>的样本, 其分布函数为&lt;span class="math">\(F(x)\)&lt;/span>,&lt;span class="math">\(F(x)\)&lt;/span>是未知的. 为了估计分布函数&lt;span class="math">\(F(x)=P(X\le x)\)&lt;/span>, 使用如下统计量 &lt;span class="math">\[
F_n(x)=\frac{\#\{i: x_i\leq x\}}{n},
\]&lt;/span> 其中&lt;span class="math">\(\#A\)&lt;/span>表示集合&lt;span class="math">\(A\)&lt;/span>中元素的个数, &lt;span class="math">\(F_n(x)\)&lt;/span>称为&lt;strong>经验分布函数 (empirical distribution function)&lt;/strong>. 上式中经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>的定义体现了用&lt;strong>频率近似概率&lt;/strong>的想法.&lt;/p>
&lt;p>如果用&lt;span class="math">\(I_A(x)\)&lt;/span>表示集合&lt;span class="math">\(A\)&lt;/span>的特征函数（示性函数）, 即 &lt;span class="math">\[
I_A(x):=\begin{cases}
1,x \in A,\\
0,x \notin A,
\end{cases}
\]&lt;/span> 则经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>可以改写成 &lt;span class="math">\[
F_n(x)=\frac{1}{n}\sum_{i=0}^nI_{[x_i,\infty]}(x).\\
I_{[x_i,\infty]}(x)=\begin{cases}
1,x_i \le x,\\
0,x_i &amp;gt; x,
\end{cases}
\]&lt;/span> 将样本&lt;span class="math">\(x_1,x_2,⋯,x_n\)&lt;/span>理解成样本值时, &lt;span class="math">\(F_n(x)\)&lt;/span>是一个分布函数. 设随机变量&lt;span class="math">\(W∼F_n(x)\)&lt;/span>, 则&lt;span class="math">\(W\)&lt;/span>服从离散分布, 在&lt;span class="math">\(\{x_1,x_2,⋯,x_n\}\)&lt;/span>内取值, 如果各&lt;span class="math">\(x_i\)&lt;/span>互不相同则&lt;span class="math">\(W\)&lt;/span>服从&lt;span class="math">\(\{x_1,x_2,⋯,x_n\}\)&lt;/span>上的离散均匀分布&lt;span class="math">\(P(W=x_i)=1/n, i=1,2,⋯,n.\)&lt;/span>如果&lt;span class="math">\(\{x_1,x_2,⋯,x_n\}\)&lt;/span>中有相同的观测值则其相应的取值概率是&lt;span class="math">\(1/n\)&lt;/span>乘以重复次数.&lt;/p>
&lt;p>对样本&lt;span class="math">\(x_1,x_2,⋯,x_n\)&lt;/span>从小到大排序得到&lt;span class="math">\(x_{(1)}≤x_{(2)}≤⋯≤x_{(n)}\)&lt;/span>, 称为样本的次序统计量. 如果&lt;span class="math">\(x_{(1)}≤x_{(2)}≤⋯≤x_{(n)}\)&lt;/span>, 易见 &lt;span class="math">\[
F_n(x)=\begin{cases}
0, &amp;amp; \textrm{当}\, x&amp;lt; x_{(1)},\\
\dfrac{i}{n}, &amp;amp; \textrm{当}\,x_{(i)}\leq x&amp;lt; x_{(i+1)},\quad i=1,2,\cdots, n-1,\\
1, &amp;amp; \textrm{当}\, x\geq x_{(n)}.
\end{cases}
\]&lt;/span> 将样本&lt;span class="math">\(x_1,x_2,⋯,x_n\)&lt;/span>看成随机变量时, &lt;span class="math">\(F_n(x)\)&lt;/span>是样本统计量.&lt;/p>
&lt;p>&lt;span class="math">\(I_{[x_i,∞)}(x)\)&lt;/span>是独立同分布的随机变量, 其共同分布为两点分布&lt;span class="math">\(b(1,F(x))\)&lt;/span>. 由Glivenko-Cantelli定理可知, 当&lt;span class="math">\(n→∞\)&lt;/span>时, &lt;span class="math">\[
\sup_{x\in\mathbb R}|F_n(x)-F(x)| \xrightarrow[]{\;\;{\rm a.s.}\;\;} 0.
\]&lt;/span> 此结果表明&lt;span class="math">\(F_n(x)\)&lt;/span>是&lt;span class="math">\(F(x)\)&lt;/span>的一致强相合估计(uniformly and strongly consistent estimator). 于是当样本容量&lt;span class="math">\(n\)&lt;/span>充分大时, &lt;span class="math">\(F_n(x)\)&lt;/span>能良好地逼近总体分布函数&lt;span class="math">\(F(x)\)&lt;/span>. 这是在统计学中以样本推断总体的依据.&lt;/p>
&lt;h2 id="经验分布函数与样本均值的关系">经验分布函数与样本均值的关系&lt;/h2>
&lt;p>如果随机变量&lt;span class="math">\(W∼F_n(x)\)&lt;/span>, 显然&lt;span class="math">\(W\)&lt;/span>的期望 &lt;span class="math">\[
E(W)=\frac{1}{n}\sum_{i=1}^nx_i=\bar x,
\]&lt;/span> 即样本均值. 所以样本均值可以理解成服从经验分布的随机变量的数学期望. &lt;strong>样本均值&lt;span class="math">\(\bar x\)&lt;/span>用于估计总体均值&lt;span class="math">\(E(X)\)&lt;/span>, 其本质上是用经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>近似总体分布函数&lt;span class="math">\(F(x)\)&lt;/span>&lt;/strong>. 用经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>近似总体分布函数&lt;span class="math">\(F(x)\)&lt;/span>的一个应用是bootstrap方法.&lt;/p>
&lt;h2 id="经验分布函数与直方图的关系">经验分布函数与直方图的关系&lt;/h2>
&lt;p>直方图 (histogram) 是估计分布密度非常直观简单的方法.&lt;/p>
&lt;h3 id="直方图作法">直方图作法&lt;/h3>
&lt;p>&lt;a href="https://www.zybuluo.com/lyc102/note/1311776">参考文献&lt;/a>&lt;/p></description></item><item><title>读书笔记之小白统计学</title><link>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%B0%8F%E7%99%BD%E7%BB%9F%E8%AE%A1%E5%AD%A6/</link><pubDate>Sat, 23 Nov 1991 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%B0%8F%E7%99%BD%E7%BB%9F%E8%AE%A1%E5%AD%A6/</guid><description>
&lt;h2 id="读书笔记之小白统计学">读书笔记之小白统计学&lt;!-- omit in toc -->&lt;/h2>
&lt;p>本篇为读书笔记，内容为微信公众号：小白统计学：以通俗易懂的语言介绍并推广统计学，让即使完全不懂统计的小白也能够看懂。公众号简介：&lt;/p>
&lt;p>公众号：stats_for_dummy。文章汇总&lt;a href="https://mp.weixin.qq.com/s/fvyKnVZ1G6sBVFNBl2abAA">https://mp.weixin.qq.com/s/fvyKnVZ1G6sBVFNBl2abAA&lt;/a>&lt;/p>
&lt;p>该公众平台不是以盈利为主，旨在推广医学统计学，让众多的统计小白能够真正了解统计学。下面是关于本平台的简单介绍：&lt;/p>
&lt;p>（1）所有文章均为作者原创，可能有的文章的部分内容作者在其它地方也曾发表过，但都是作者自己的原创内容。如果摘录、引用等请注明出处，尊重作者版权。&lt;/p>
&lt;p>（2）由于文章主要走的是通俗、浅显的路子，因此有的文章中的一些概念可能通俗有余，严谨不足。如果想了解对某些概念的严谨定义，请参考相应的统计学教材。&lt;/p>
&lt;p>（3）本平台只是抛砖引玉，将作者多年对统计的理解以通俗的形式表达出来，面向对象主要是对统计感兴趣的各位同道，希望达到相互交流的目的。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#概念和观点理解">概念和观点理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#小白学统计系列之三p值到底是个什么东西">小白学统计系列之三：p值到底是个什么东西&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从女士品茶到假设检验">从“女士品茶”到假设检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#小白学统计系列之五法官的艰难抉择统计学中的两类错误">小白学统计系列之五：法官的艰难抉择——统计学中的两类错误&lt;/a>&lt;/li>
&lt;li>&lt;a href="#貌合神离的标准差与标准误">貌合神离的标准差与标准误&lt;/a>&lt;/li>
&lt;li>&lt;a href="#p005真的值得庆贺吗兼谈置信区间">P&amp;lt;0.05真的值得庆贺吗？——兼谈置信区间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态分布的理解">正态分布的理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分类资料与计数资料">分类资料与计数资料&lt;/a>&lt;/li>
&lt;li>&lt;a href="#计数资料离散资料和连续资料">计数资料、离散资料和连续资料&lt;/a>&lt;/li>
&lt;li>&lt;a href="#什么是虚拟变量应用中应注意什么问题">什么是虚拟变量，应用中应注意什么问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于抽样误差">关于抽样误差&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量大于30就算正态了吗">样本量大于30就算正态了吗？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#判断正态性的一些简易方法">判断正态性的一些简易方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多变量与多因素的区别">多变量与多因素的区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#信度与效度评价">信度与效度评价&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#问卷的信度和效度评价">问卷的信度和效度评价&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#问卷用不用做信度和效度评价">问卷用不用做信度和效度评价&lt;/a>&lt;/li>
&lt;li>&lt;a href="#传染病模型中的拐点">传染病模型中的拐点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一些基础统计方法">一些基础统计方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#定量资料的组间比较">定量资料的组间比较&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分类资料的组间比较">分类资料的组间比较&lt;/a>&lt;/li>
&lt;li>&lt;a href="#t检验应用的注意事项">t检验应用的注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析的变异分解思想">方差分析的变异分解思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析的一些特殊情况">方差分析的一些特殊情况&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析后的两两比较方法选择">方差分析后的两两比较方法选择&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析的应用注意事项">方差分析的应用注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析中的随机效应与固定效应">方差分析中的随机效应与固定效应&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多重校正的fdr方法">多重校正的FDR方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#卡方检验的注意事项">卡方检验的注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#卡方检验等于单因素logistic回归吗">卡方检验等于单因素logistic回归吗&lt;/a>&lt;/li>
&lt;li>&lt;a href="#协方差与相关系数的关系">协方差与相关系数的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相关分析的注意事项">相关分析的注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#临床常见的疑惑解答之一">临床常见的疑惑解答之一&lt;/a>&lt;/li>
&lt;li>&lt;a href="#临床常见的疑惑解答之二">临床常见的疑惑解答之二&lt;/a>&lt;/li>
&lt;li>&lt;a href="#统计描述与绘图">统计描述与绘图&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相关性的可视化">相关性的可视化&lt;/a>&lt;/li>
&lt;li>&lt;a href="#组间差异比较的另类绘图">组间差异比较的另类绘图&lt;/a>&lt;/li>
&lt;li>&lt;a href="#文章中的统计描述">文章中的统计描述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#比例和率的区别">比例和率的区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#率和比例的介绍">率和比例的介绍&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计与调查的相关内容">实验设计与调查的相关内容&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计之随机">实验设计之随机&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计之均衡">实验设计之均衡&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计之重复">实验设计之重复&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计之对照">实验设计之对照&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常见的实验设计方法">常见的实验设计方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#利用excel进行随机分组">利用excel进行随机分组&lt;/a>&lt;/li>
&lt;li>&lt;a href="#利用excel进行随机抽样">利用excel进行随机抽样&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于复杂抽样的介绍">关于复杂抽样的介绍&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算相关内容">样本量估算相关内容&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量计算需要考虑的因素">样本量计算需要考虑的因素&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之一">样本量估算需要考虑的因素之一&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之二">样本量估算需要考虑的因素之二&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之三">样本量估算需要考虑的因素之三&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之四">样本量估算需要考虑的因素之四&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之五">样本量估算需要考虑的因素之五&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之六">样本量估算需要考虑的因素之六&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之七">样本量估算需要考虑的因素之七&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之八">样本量估算需要考虑的因素之八&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归方法大家族">回归方法大家族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归方法杂谈之一">回归方法杂谈之一&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归方法杂谈之二">回归方法杂谈之二&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归方法杂谈之三">回归方法杂谈之三&lt;/a>&lt;/li>
&lt;li>&lt;a href="#苹果案例之logistic回归">苹果案例之logistic回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#苹果案例之poisson回归">苹果案例之Poisson回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#苹果案例之负二项回归">苹果案例之负二项回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deming回归与passing-bablok回归">Deming回归与Passing-Bablok回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样条回归之一">样条回归之一&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样条回归之二">样条回归之二&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性回归分析思路">线性回归分析思路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#混杂因素与统计学悖论">混杂因素与统计学悖论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归系数的理解">回归系数的理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单因素与多因素的不同">单因素与多因素的不同&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于线性的理解">关于“线性”的理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#box-cox变换">Box-Cox变换&lt;/a>&lt;/li>
&lt;li>&lt;a href="#box-tidwell变换">Box-Tidwell变换&lt;/a>&lt;/li>
&lt;li>&lt;a href="#要不要用逐步回归">要不要用逐步回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单因素分析有没有必要做">单因素分析有没有必要做？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#先做单因素再做多因素是否正确">先做单因素、再做多因素，是否正确？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#先做单因素再做多因素的理解">先做单因素再做多因素的理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多因素筛选策略">多因素筛选策略&lt;/a>&lt;/li>
&lt;li>&lt;a href="#置信区间与预测区间">置信区间与预测区间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性回归中的方差齐性">线性回归中的方差齐性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#异常值的问题">异常值的问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#缺失值的类型">缺失值的类型&lt;/a>&lt;/li>
&lt;li>&lt;a href="#缺失值的简单处理">缺失值的简单处理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#用多重插补法处理缺失值">用多重插补法处理缺失值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#自变量对数变换如何解释">自变量对数变换如何解释&lt;/a>&lt;/li>
&lt;li>&lt;a href="#因变量对数变换如何解释">因变量对数变换如何解释&lt;/a>&lt;/li>
&lt;li>&lt;a href="#生存分析的相关内容">生存分析的相关内容&lt;/a>&lt;/li>
&lt;li>&lt;a href="#生存分析方法简介">生存分析方法简介&lt;/a>&lt;/li>
&lt;li>&lt;a href="#中位生存时间与中位随访时间">中位生存时间与中位随访时间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#log-rank检验应用注意事项">log-rank检验应用注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数回归">指数回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#weibull回归">Weibull回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#等比例cox回归">等比例cox回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时依协变量与分层cox回归">时依协变量与分层cox回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于重复测量数据的分析">关于重复测量数据的分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据介绍">重复测量数据介绍&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量方差分析">重复测量方差分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据的趋势探索">重复测量数据的趋势探索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据的趋势比较">重复测量数据的趋势比较&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据之重复测量方差分析">重复测量数据之重复测量方差分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据之广义估计方程">重复测量数据之广义估计方程&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据之多水平模型">重复测量数据之多水平模型&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量方法的比较">重复测量方法的比较&lt;/a>&lt;/li>
&lt;li>&lt;a href="#本人对统计学方法的观点">本人对统计学方法的观点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#专家说我的方法过时了怎么办">专家说我的方法过时了怎么办&lt;/a>&lt;/li>
&lt;li>&lt;a href="#预测建模需要注意点什么问题">预测建模需要注意点什么问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大数据和医工交叉时代下对医学统计的思考">大数据和医工交叉时代下对医学统计的思考&lt;/a>&lt;/li>
&lt;li>&lt;a href="#危险因素探索中的几个要点">危险因素探索中的几个要点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#横断面与病例对照研究的区别">横断面与病例对照研究的区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#如何学习统计学的体会">如何学习统计学的体会&lt;/a>&lt;/li>
&lt;li>&lt;a href="#统计方法的算法与软件操作哪个重要">统计方法的算法与软件操作哪个重要&lt;/a>&lt;/li>
&lt;li>&lt;a href="#预测建模的基本思路">预测建模的基本思路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一些数据分析思路">一些数据分析思路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#我学统计学的经历">我学统计学的经历&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于课题申请或论文撰写的方法">关于课题申请或论文撰写的方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#课题申请中的方法学描述">课题申请中的方法学描述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#课题申请中方法学的注意事项">课题申请中方法学的注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于课题设计的一些问题">关于课题设计的一些问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#论文撰写中的统计方法">论文撰写中的统计方法&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概念和观点理解">概念和观点理解&lt;/h2>
&lt;p>这部分主要是一些统计学概念和一些观点的通俗介绍，如标准误到底是什么意思，P值如何理解，中心极限定理是在说什么，等等。&lt;/p>
&lt;h3 id="小白学统计系列之三p值到底是个什么东西">小白学统计系列之三：p值到底是个什么东西&lt;/h3>
&lt;p>p值（p value）就是当原假设为真时&lt;strong>所得到的样本观察结果或更极端结果出现&lt;/strong>的概率。p值表示一个事件发生的可能性，在假设检验中是对原假设的设定，我们希望这个p值比较小，说明原假设是个小概率事件。当小概率事件发生时，我们就有理由认为我们的原假设是有问题的，从而选择备择假设（原假设与备择假设为互斥事件）。p值只能用来拒绝原假设，而不能肯定原假设，即当p值比较大时，不能说明原假设是对的，只能说统计学上不能证明原假设是错的，这时同样没有证明原假设是对的，其依旧处于不能说明对，也不能证明错的状态。只有当拒绝原假设，即统计学上认为原假设是错的是，我们才能认为原假设的互斥事件——备择假设是对的。&lt;/p>
&lt;h3 id="从女士品茶到假设检验">从“女士品茶”到假设检验&lt;/h3>
&lt;p>假设检验的理解，来源自《女士品茶：20 世纪统计怎样变革了科学》，这本书还是不错的。&lt;/p>
&lt;h3 id="小白学统计系列之五法官的艰难抉择统计学中的两类错误">小白学统计系列之五：法官的艰难抉择——统计学中的两类错误&lt;/h3>
&lt;p>统计学中两类错误：第一类：去真、第二类：取伪。&lt;strong>给定采样样品后&lt;/strong>，采用假设检验，设定拒绝域&lt;span class="math">\(\alpha\)&lt;/span>的做法让两类错误无法同时降低，此消彼长。只有通过提供样品、试验次数或者提高实验准确性才能同时降低二者。&lt;/p>
&lt;h3 id="貌合神离的标准差与标准误">貌合神离的标准差与标准误&lt;/h3>
&lt;p>标准差是“样本原始数据的标准差”。标准误是“样本统计量的标准差”。标准差和标准误的区别，最实质的地方至少是两点：一是针对计算的对象不同，标准差是根据一次抽样的原始数据计算的，而标准误是根据多次抽样的样本统计量（可以是均值，也可以是率等）计算的。二是标准差只是一个描述性指标，只是描述原始数据的波动情况，而标准误是跟统计推断有关的指标，大多数的统计量计算都需要用到标准误。&lt;/p>
&lt;p>举例：对于抽取的10个样本，每个样本容量为100，每个样本都有各自的标准差，每个样本也都可以计算一个均值，这样10个样本就可以计算出10个均值。将这10个均值作为原始数据，仍然可以计算出均值和标准差，这里作为统计量的均值的标准差就是标准误，它是用10个均值计算的，而标准差是用一次样本中的原始数据计算的。&lt;/p>
&lt;p>如果真要严格定义，应该说标准误是“样本统计量的标准差”更加合适，而不是非要局限于均数或率等。而标准差是“样本原始数据的标准差”。&lt;/p>
&lt;h3 id="p0.05真的值得庆贺吗兼谈置信区间">P&amp;lt;0.05真的值得庆贺吗？——兼谈置信区间&lt;/h3>
&lt;p>p值提供的信息不够丰富，只能说明得到该抽样统计量发生（以及更极端情况）的概率，例如，抽样20个样品，计算得到方差是5.5，p值为0.01，这意味着我随机抽样20个样品，从总体抽样1000次，那么发生样本方差大于等于5.5的概率为0.01，即大约10次。p值算是点估计在假设检验中体现，为了获得更丰富的信息，我们将区间估计的思想也用到假设检验中，就能得到&lt;strong>置信区间&lt;/strong>。&lt;/p>
&lt;p>置信区间，就是用样本数据计算两个值，用这两个数确定一个区间，这个区间以一定的可信程度包含被估计的参数。根据上面的定义，可以将置信区间这个词拆成两个部分理解：置信和区间。先说区间，比较容易理解，就是一段数值范围，如果确认这个范围呢？它是根据样本数据计算的点估计和标准误来计算的，表现为（参数估计值±边际误差），所谓边际误差，就是考虑到了样本与总体是有一定差异的。由于现实中几乎所有抽样分布都会近似呈正态分布，因此边际误差通常都是用正态分位数的一个“z值&lt;em>标准误”来表示，也就是我们通常见到的“1.96&lt;/em>标准误”（1.96是双边Z检验，单侧累积概率为0.975时的值）。再说置信，他表示“一定的可信程度”，例如95%的置信区间就是计算一个区间，我们有95%的信心认为这个区间包含了被估计的参数。&lt;/p>
&lt;p>置信区间的前缀数字95%称为置信系数，我们习惯用95%，但不一定非要用这个。也可以用90%、99%等。一般来说，置信系数越大，所得的区间越宽，也就是越可信；置信系数越小，所得区间越窄，越不可信。&lt;/p>
&lt;div class="figure">
&lt;embed src="../../images/置信区间大小.webp" />&lt;p class="caption">置信区间大小.webp&lt;/p>
&lt;/div>
&lt;p>如果要说置信区间的理论意义，也是只存在于理论中。比如95%的置信区间，意思是：如果从一个总体中重复多次抽取不同的样本，对每一个样本都可以计算一个置信区间，那么理论上有95%的置信区间包含了总体参数。一个总体参数总是固定的，对于每次抽样计算的置信区间，要么包含这个参数，要么不包含这个参数，但总的来说，100次抽样样本中，大概会有95次包含了这个参数。&lt;/p>
&lt;p>置信区间的宽窄反映了对参数估计的精确度，置信区间越窄，说明越精确，置信区间越宽，说明越不精确。一般来说，样本量越大，计算的置信区间越窄。因为样本量越大，标准误越小。当样本量跟总体一样多时，计算的置信区间就窄成了只有一个值了。这时就是最精确了。&lt;/p>
&lt;h3 id="正态分布的理解">正态分布的理解&lt;/h3>
&lt;p>我自己的笔记更好。&lt;a href="../../学习笔记/概率统计随机过程之如何推导得到正态分布—正态分布的理解角度.md">概率统计随机过程之如何推导得到正态分布—正态分布的理解角度&lt;/a>&lt;/p>
&lt;h3 id="分类资料与计数资料">分类资料与计数资料&lt;/h3>
&lt;ul>
&lt;li>分类资料（数据）：没有单位，可以是数字，也可以是其他类型，比如 是否、男女、实验组/对照组等&lt;/li>
&lt;li>计数数据：有单位，一般是有单位的频数，如天数、次数&lt;/li>
&lt;li>有些使用数值作为数据时，分类数据和计数数据是差不多的，不太需要严格区分&lt;/li>
&lt;/ul>
&lt;h3 id="计数资料离散资料和连续资料">计数资料、离散资料和连续资料&lt;/h3>
&lt;ul>
&lt;li>计数资料一般是频数，通过数数得来的，而离散资料只是数值是离散的，并不一定通过计数得来的。&lt;/li>
&lt;li>离散数据并不一定都是整数，例如取值结果为&lt;span class="math">\({0,0.5,1,1.5,\dotsb}\)&lt;/span>的数据也是离散数据。&lt;/li>
&lt;li>在一定区间内可以任意取值的数据叫连续数据，其数值是连续不断的，相邻两个数值可作无限分割，即可取无限个数值。&lt;/li>
&lt;li>计数资料一般都是非负整数，当数据值比较大，且近似取值不会影响最终结果时，可以近似当成连续数据来处理。&lt;/li>
&lt;li>连续数据：没有正负、离散或整数的限制。&lt;/li>
&lt;/ul>
&lt;h3 id="什么是虚拟变量应用中应注意什么问题">什么是虚拟变量，应用中应注意什么问题&lt;/h3>
&lt;p>在线性回归中，我们的自变量一般都是数值型数据，如果分类数据也会对回归结果产生影响，那么如何将其引入回归方程中呢？就需要使用虚拟变量。说白了，&lt;strong>虚拟变量就是将分类数据引入回归方程的一种方法&lt;/strong>。&lt;/p>
&lt;p>比如，人每天的基础代谢（BMR）和身高、体重、年龄以及性别有关，前面三项都好说，一般的多元线性回归都实用，现在需要将性别因素也考虑进去，性别是个分类变量，无法用具体数值表示，因此我们就是用虚拟变量表示。对于性别女，我们使用0表示，男用1表示，这样带入回归方程有： &lt;span class="math">\[
BMR=\alpha_0+\alpha_1×\text{身高}+\alpha_2×\text{体重}+\alpha_3×\text{年龄}+\alpha_4×\text{性别}\\
性别=\begin{cases}
1,男\\
0,女
\end{cases}
\]&lt;/span> 实际上，虚拟变量就是用0，1表示分类变量，得到一个包含离散变量的回归方程。一般用0表示属性或特征不存在，1表示属性或特征存在。&lt;/p>
&lt;p>上面使用的虚拟变量指标是了两种可能，所以能用一个0，1虚拟变量表示，当分类变量的水平数大于两个，我们要用0，1，2……表示吗？答案是否定的。理由是：若我们用连续的数字表达分类变量，那么就隐藏地给这个变量添加了一个很强的假设：每个分类之间依顺序有完美的线性关系，因此，对于有&lt;span class="math">\(m\)&lt;/span>个水平的分类变量，我们添加&lt;span class="math">\(m-1\)&lt;/span>个0-1虚拟变量来替代（一个例外是，如果回归方程无截距项，可引入&lt;span class="math">\(m\)&lt;/span>个0-1虚拟变量）。之所以要少引入一个虚拟变量，是为了防止多重共线性，又叫虚拟变量陷阱。&lt;/p>
&lt;p>此外，上面的例子，虚拟变量通过与其他变量&lt;strong>相加&lt;/strong>融入回归方程中，还有通过&lt;strong>相乘&lt;/strong>融入进去的例子。例如，查看一个家庭的总体教育支出&lt;span class="math">\(Y_e\)&lt;/span>，除了受家庭收入水平&lt;span class="math">\(X_1\)&lt;/span>、孩子年龄&lt;span class="math">\(X_2\)&lt;/span>影响外，一个非常重要的因素就是有没有孩子，而有没有孩子是一个分类变量&lt;span class="math">\(\beta\)&lt;/span>。我们需要通过乘法将其引入回归方程： &lt;span class="math">\[
Y_3=\alpha_0+\alpha_1×X_1+\alpha_2×\beta×X_2\\
\beta=\begin{cases}
1,有孩子\\
0,无孩子
\end{cases}
\]&lt;/span>&lt;/p>
&lt;p>虚拟变量应用场景：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>分段回归&lt;/li>
&lt;li>稳定性检验。稳定行：利用不同样本的得到的回归方程系数没有显著性差异。&lt;/li>
&lt;li>季节性波动调整&lt;/li>
&lt;/ol>
&lt;h3 id="关于抽样误差">关于抽样误差&lt;/h3>
&lt;p>抽样误差的来源：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>没有做到真正的随机&lt;/li>
&lt;li>抽取的样本没有代表性&lt;/li>
&lt;/ol>
&lt;p>当存在系统性抽样错误，即使样本数量再大，可能得出结论也是错的。&lt;/p>
&lt;h3 id="样本量大于30就算正态了吗">样本量大于30就算正态了吗？&lt;/h3>
&lt;p>这个说法其实是将中心极限定理和抽样的原始数据搞混了，而且没注意到样本均值也是个随机变量。中心极限定理说的是：不管原始数据的分布是什么样的（可能是正态，也可能偏态，还可能超级变态），如果从这个原始数据中多次抽样的话，对于每个样本计算出&lt;strong>均值&lt;/strong>，如果每个样本中的例数大于30，这些计算出的&lt;strong>样本均值的分布&lt;/strong>接近正态。而不是说：一个样本中的原始数据的个数大于30，这个原始数据的分布接近正态。&lt;/p>
&lt;p>&lt;strong>如果针对原始数据，无论如何都是要进行正态性检验的&lt;/strong>。&lt;/p>
&lt;h3 id="判断正态性的一些简易方法">判断正态性的一些简易方法&lt;/h3>
&lt;p>几种简易的判断正态性的方法（统计专业人士请绕行）：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>根据均值和标准差。首先，分别计算均值和标准差，然后看一下数据中有百分之多少的人在均值±1个标准差、均值±2个标准差、均值±3个标准差之内。如果分别大概是68%、95%、99%左右，说明差不多是正态的。&lt;/li>
&lt;li>计算四分位数间距和标准差，如果四分位数间距/标准差的值大约在1.35左右，可以认为满足正态分布。比如上面的10个数中，四分位数间距是1.9，标准差是1.3，1.9/1.3大约为1.4左右，比较接近1.35，可以认为是正态的。&lt;/li>
&lt;li>通过几幅图来判断，最常用的图有箱式图、直方图、茎叶图、QQ图等。（再在有计算机辅助情形下最直观）&lt;/li>
&lt;/ol>
&lt;h3 id="多变量与多因素的区别">多变量与多因素的区别&lt;/h3>
&lt;p>回归中的多变量、多因素、多重、多元有什么区别？&lt;/p>
&lt;p>多变量线性回归或多重线性回归（multivariable or multiple linear regression）是一回事，多因素线性回归或多重线性回归则是有多个自变量。但它们都是只有&lt;strong>1个因变量&lt;/strong>。 &lt;span class="math">\[
y=\alpha_0+\alpha_1 x_1+\dotsb+\alpha_n x_n+\varepsilon
\]&lt;/span> 多元或多变量线性回归模型（multivariate linear regression model）是指&lt;strong>多个因变量&lt;/strong>的回归模型。&lt;/p>
&lt;p>读者注：数理统计有时候确实不那么区分。甚至线性回归默认自变量或因变量可以是一维乃至多维矢量。&lt;/p>
&lt;h3 id="信度与效度评价">信度与效度评价&lt;/h3>
&lt;ul>
&lt;li>信度代表的是数据的可靠性程度和一致性程度，它能够反映数据的&lt;strong>稳定性和集中程度&lt;/strong>。所谓“信”，意思是不会偏离太多、行为在可预想范围内。&lt;/li>
&lt;li>效度是指测量工具能够准确测量出事物真实情况的能力，它能够反映数据的&lt;strong>准确性&lt;/strong>。所谓“效”，意味着效果、准确性，达到目的的程度。&lt;/li>
&lt;/ul>
&lt;div class="figure">
&lt;img src="../../images/信度效度.png" alt="信度效度.png" />&lt;p class="caption">信度效度.png&lt;/p>
&lt;/div>
&lt;p>如果用射击来类比：&lt;/p>
&lt;p>右下图的弹孔是散布在整个靶图上的，有两个特点：1、点与点之间的距离很大，说明运动员的稳定性差；2、几乎没有弹孔落在靶心，说明运动员的准确性也差。说明该射击运动员既没有稳定性，也没有准确性。如果将每个弹孔看作一个数据信息（个案），那么该数据集合是既没有信度（稳定性）也没有效度（准确性）。&lt;/p>
&lt;p>左下图的弹孔密集的落在一个狭小的区域内，但是偏离了靶心，说明该运动员的射击稳定性很好，但是准确性则不足。同样的，如果弹孔看作数据，那么该数据集合的特点是具有高信度，效度却很低。&lt;/p>
&lt;p>右上图的弹孔是分散的，但是大部分的弹孔落在了靶心，说明运动员的稳定性不足，但是准确性还是不错的。形容数据集合的话，那么该数据集合是高效度和低信度的。&lt;/p>
&lt;p>左上图的弹孔密集的落在了靶心，说明该运动员的稳定性和准确性都很好。用来形容数据集合则说明该数据集合是高信度和高效度的。&lt;/p>
&lt;h4 id="问卷的信度和效度评价">问卷的信度和效度评价&lt;/h4>
&lt;p>问卷的信度在于评价收集上来的数据是否真实可靠，也就是检查填写问卷的这些人是不是认真的填写了问卷，还是乱填的。大家可以想象一下，如果一个人胡乱的填写数据，那么有很大的可能他的答案与其它人的答案是南辕北辙的，差异很大，那么就会影响到整份问卷在的信度。因此，大家在收集问卷数据时，应该想各种办法让大家能够认证回答。&lt;/p>
&lt;p>问卷的效度是用来研究题目的设置是否能够有效的测量问卷设计者当初设计的初衷，也就是说检验问卷题目的设计是否合理。如果题目是合理的，那么它就能够有效地测量出问卷设计者的设计目的和初衷。&lt;/p>
&lt;p>信度的分析类型：信度分析的目的是检验受访者是否真实的回答了问卷的问题，收集上来的数据是否真实可靠。根据测量工具的不同，信度指标可以分成四类，如下图所示：&lt;/p>
&lt;div class="figure">
&lt;embed src="../../images/信度类型.jfif" />&lt;p class="caption">信度类型.jfif&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>重测信度：是指用同样的测量工具，对同一组被测者隔一定时间重复测量，考察两次测量结果的相关程度，可以直接采用相关分析，得到的相关系数即为重测信度系数。重复信度能够检验时间差异带来的数据误差，该误差不是测量工具不直接有测量工具造成，而且测量的是同一组被测者，因此称为外在信度。&lt;/li>
&lt;li>复本信度是指让同一组被测者一次填写两份平行问卷，计算两份数据的相关系数，复本信度要求两份问卷除了在问题表述不同之外，其余方面要完全一致，实际操作比较困难。&lt;/li>
&lt;li>折半信度是指将一份问卷分成两部分，计算这两部分的相关系数，即折半信度系数，以此来衡量整份问卷的信度。&lt;/li>
&lt;li>α系数又称克朗巴哈系数，是最常用的测量内部一致性信度的方法，计算出的克朗巴哈α系数是所有可能折半信度的均值，取值在0-1之间，系数越高一致性越好，说明数据的真实性越高。α系数是问卷数据真实性检验的最重要指标。&lt;/li>
&lt;/ul>
&lt;p>效度分析类型：效度是指测量工具能够准确测量出事物所要测量特性的程度，效度越高表示测量真实性越高，由于真实值往往未知，所以我们对于效度的评价也不可能永恒不变的，随着设定和对比的“真实值”变化，效度指标也会发生变化。也就是说，效度的指标的种类会有很多。效度分析可以分成以下三种内容：&lt;/p>
&lt;div class="figure">
&lt;embed src="../../images/效度类型.jfif" />&lt;p class="caption">效度类型.jfif&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>内容效度：是一种定性的评价标准，主要通过经验判断进行，主要是通过专家和有经验的业内人士进行评价的方法。还可以对问卷在正式使用前进行小范围的使用，结合结果进行题项的修正以说明问卷的有效性。内容效度一般不需要使用SPSS进行数据分析，但是需要专家和权威老师的指导，前测修改过程，最后对问卷的内容效度进行说明。&lt;/li>
&lt;li>结构效度：是指测量题项与测量方向之间的对应关系，其测量方法是因子分析。因子分析的数据计算理论比较复杂。&lt;/li>
&lt;li>校标效度：人为指定确定的结果作为“准确标准”，考察其他待测结果与其是否一致。例如，考试卷中的选择题都会有准确答案，那么每题的正确率就是这个班级在该题上的效度。&lt;/li>
&lt;/ul>
&lt;h3 id="问卷用不用做信度和效度评价">问卷用不用做信度和效度评价&lt;/h3>
&lt;p>作者认为：问卷的客观特征和行为问题不用做信效度评价。量表需要信度和效度分析。&lt;/p>
&lt;p>作者所说的问卷的强调目的是为了获取（客观的、直接的信息），而量表是为了测量（主观的、隐藏的信息）。但是调查问卷的范围有些泛化，将一些量表例如人格分析、心理量表也当成了调查问卷，因此在遇到后需要仔细甄别。&lt;/p>
&lt;h3 id="传染病模型中的拐点">传染病模型中的拐点&lt;/h3>
&lt;p>拐点：从微积分的概念来讲，函数f的图像上改变凹性的点叫做函数f的拐点。也就是说，从上凹到下凹或者从下凹到上凹，这一个点就是拐点。&lt;/p>
&lt;p>因此，在医学上拐点说的不是病例数的正负变化，而是病例的增长速度的正负变化（二阶导）。&lt;/p>
&lt;p>传染病模型，一般病例数的增长都是呈S型曲线（logistic生长曲线），前面增长越来越快，过了中间拐点，后面增长越来越慢。&lt;/p>
&lt;h2 id="一些基础统计方法">一些基础统计方法&lt;/h2>
&lt;p>这部分主要是一些基础统计学方法的介绍，如t检验、方差分析、卡方检验、相关分析等方法的正确应用。&lt;/p>
&lt;h3 id="定量资料的组间比较">定量资料的组间比较&lt;/h3>
&lt;div class="figure">
&lt;img src="../../images/分类数据统计分析方法.png" alt="分类数据统计分析方法" />&lt;p class="caption">分类数据统计分析方法&lt;/p>
&lt;/div>
&lt;h3 id="分类资料的组间比较">分类资料的组间比较&lt;/h3>
&lt;div class="figure">
&lt;img src="../../images/数值数据统计分析方法.png" alt="数值数据统计分析方法" />&lt;p class="caption">数值数据统计分析方法&lt;/p>
&lt;/div>
&lt;h3 id="t检验应用的注意事项">t检验应用的注意事项&lt;/h3>
&lt;p>t检验，亦称student t检验（Student's t test），主要用于样本含量较小（例如&lt;span class="math">\(n &amp;lt; 30\)&lt;/span>），总体标准差&lt;span class="math">\(σ\)&lt;/span>&lt;strong>未知&lt;/strong>的正态分布。&lt;/p>
&lt;p>t检验前置要求：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>已知一个总体均数；&lt;/li>
&lt;li>可得到一个样本均数及该样本标准差；&lt;/li>
&lt;li>样本来自正态或近似正态总体，t检验鲁棒性较强。&lt;/li>
&lt;/ol>
&lt;p>t分布的概率密度函数： &lt;span class="math">\[
p(t)=\frac{\Gamma
(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{t^2}{\nu})^{-(\nu+1)2}
\]&lt;/span> 其中，&lt;span class="math">\(\nu\)&lt;/span>表示t分布的自由度。对于从（近似）正态总体&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>随机抽样的i.i.d样本&lt;span class="math">\(X=\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>，其样本均值为&lt;span class="math">\(\bar{X}\)&lt;/span>，样本方差为&lt;span class="math">\(S^2\)&lt;/span>，统计量： &lt;span class="math">\[
\frac{\bar{X}-\mu}{S/\sqrt{n}} \sim t(n-1)
\]&lt;/span> 服从自由度为&lt;span class="math">\(n-1\)&lt;/span>的t分布。&lt;/p>
&lt;p>t检验主要用法：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;strong>单总体t检验&lt;/strong>是检验一个样本&lt;strong>平均数&lt;/strong>与一个已知的总体平均数的差异是否显著。&lt;/li>
&lt;li>&lt;strong>双总体t检验&lt;/strong>是检验两个样本&lt;strong>平均数&lt;/strong>与其各自所代表的总体的差异是否显著。注意此时要求两个总体有&lt;strong>方差齐性&lt;/strong>，方差不齐时则使用&lt;strong>Welch检验&lt;/strong>。而根据两个样本是&lt;strong>独立的还是配对的&lt;/strong>，又分为一是独立样本t检验（各实验处理组之间毫无相关存在，即为独立样本），该检验用于检验两组非相关样本被试所获得的数据的差异性；一是配对样本t检验，用于检验匹配而成的两组被试获得的数据或同组被试在不同条件下所获得的数据的差异性，这两种情况组成的样本即为相关样本。&lt;/li>
&lt;li>检验同一统计量的两次测量值之间的差异是否为零。&lt;/li>
&lt;li>回归系数的显著性检验。&lt;/li>
&lt;/ol>
&lt;p>作者提出的3重常见t检验使用错误：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>不考虑数据的正态性，只要是两组比较就直接用t检验。&lt;/li>
&lt;li>将t检验用于多组的两两比较，增加假阳性错误。&lt;/li>
&lt;li>不考虑资料是否独立，采用独立资料的t检验分析非独立数据。&lt;/li>
&lt;/ol>
&lt;h3 id="方差分析的变异分解思想">方差分析的变异分解思想&lt;/h3>
&lt;p>统计学中描述变异，一般是使用方差、标准差这类表述。而方差的分解来给我们带来很多有用信息。&lt;/p>
&lt;p>首先，方差分析就是基于变易分解，将总体方差分解为组间方差和组内方差，分别体现不同自变量和随机性对结果的影响。一般由于数据量越大，累积的方差越大，因此还需要除以数据量，就是自由度。方差一般服从卡方分布，因此方差的比值服从F分布。如果组间变异远远大于组内变异，那么组间均方除以组内均方的值肯定很大，反之，这一值就会很小。但是，到底大到什么程度才认为有统计学意义呢，那就得根据F分布了。&lt;/p>
&lt;p>此外，双总体t检验的公式实际上也是一种方差分析。以两组独立样本比较的公式为例，分子是什么？组间差异。分母又是什么？均数差值的标准误。两者的比值就是一种服从t分布的方差分析。&lt;/p>
&lt;p>对于线性回归，首先因变量y的值各不相同，这就是变异，线性回归就是为了弄明白，为什么这些y值不一样。所以才要有自变量x，看看哪个自变量对y的变异解释的更多。很明显，哪个解释的多，哪个自变量就对y的影响大。所以，为什么线性回归的结果中会出现方差分析的字眼，因为它也在方差分解啊，把总的y的变异分解为模型所能解释的部分，以及不能解释的部分。&lt;/p>
&lt;h3 id="方差分析的一些特殊情况">方差分析的一些特殊情况&lt;/h3>
&lt;p>&lt;strong>方差分析与实验设计是密切关联的&lt;/strong>。有一种实验设计方案，可能就有一种对应的方差分析。比如完全随机设计采用单因素方差分析，随机区组设计采用随机区组方差分析，析因设计采用析因设计的方差分析，交叉设计采用交叉设计的方差分析，嵌套设计采用嵌套设计的方差分析，裂区设计采用裂区设计的方差分析。&lt;/p>
&lt;h3 id="方差分析后的两两比较方法选择">方差分析后的两两比较方法选择&lt;/h3>
&lt;p>事后分析的两两分析一大作用就是&lt;strong>控制假阳性&lt;/strong>，因为两两比较次数多了，容易产生假阳性的结果。&lt;/p>
&lt;p>作者给出的各种两两比较方法总结：&lt;/p>
&lt;p>如果各组例数相等，建议首选Tukey法；如果例数不等，建议首选Scheffe法（如果比较组数不多，如3组，Bonferroni法也可以作为首选）；如果要分别比较每个试验组与对照组，建议采用Dunnett法；如果各组方差相差较大，建议采用Games-Hotwell法。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/方差分析事后分析.jpg" alt="方差分析事后分析.jpg" />&lt;p class="caption">方差分析事后分析.jpg&lt;/p>
&lt;/div>
&lt;h3 id="方差分析的应用注意事项">方差分析的应用注意事项&lt;/h3>
&lt;p>（1）缺乏对数据的正态性检验，组间比较都采用方差分析，而不考虑秩和检验。&lt;/p>
&lt;p>（2）两两比较直接采用t检验，而不是专门的两两比较方法。&lt;/p>
&lt;p>（3）采用方差分析处理重复测量资料，增加假阳性错误。&lt;/p>
&lt;p>（4）实验设计考虑不周，误用其它设计的统计分析方法。&lt;/p>
&lt;h3 id="方差分析中的随机效应与固定效应">方差分析中的随机效应与固定效应&lt;/h3>
&lt;h3 id="多重校正的fdr方法">多重校正的FDR方法&lt;/h3>
&lt;h3 id="卡方检验的注意事项">卡方检验的注意事项&lt;/h3>
&lt;h3 id="卡方检验等于单因素logistic回归吗">卡方检验等于单因素logistic回归吗&lt;/h3>
&lt;h3 id="协方差与相关系数的关系">协方差与相关系数的关系&lt;/h3>
&lt;h3 id="相关分析的注意事项">相关分析的注意事项&lt;/h3>
&lt;h3 id="临床常见的疑惑解答之一">临床常见的疑惑解答之一&lt;/h3>
&lt;h3 id="临床常见的疑惑解答之二">临床常见的疑惑解答之二&lt;/h3>
&lt;h2 id="统计描述与绘图">统计描述与绘图&lt;/h2>
&lt;h3 id="相关性的可视化">相关性的可视化&lt;/h3>
&lt;h3 id="组间差异比较的另类绘图">组间差异比较的另类绘图&lt;/h3>
&lt;h3 id="文章中的统计描述">文章中的统计描述&lt;/h3>
&lt;h3 id="比例和率的区别">比例和率的区别&lt;/h3>
&lt;h3 id="率和比例的介绍">率和比例的介绍&lt;/h3>
&lt;h2 id="实验设计与调查的相关内容">实验设计与调查的相关内容&lt;/h2>
&lt;p>这部分主要是介绍实验设计、流行病学调查有关的一些内容，包括各种概念的介绍、如何实现随机分组、随机抽样等&lt;/p>
&lt;h3 id="实验设计之随机">实验设计之随机&lt;/h3>
&lt;h3 id="实验设计之均衡">实验设计之均衡&lt;/h3>
&lt;h3 id="实验设计之重复">实验设计之重复&lt;/h3>
&lt;h3 id="实验设计之对照">实验设计之对照&lt;/h3>
&lt;h3 id="常见的实验设计方法">常见的实验设计方法&lt;/h3>
&lt;h3 id="利用excel进行随机分组">利用excel进行随机分组&lt;/h3>
&lt;h3 id="利用excel进行随机抽样">利用excel进行随机抽样&lt;/h3>
&lt;h3 id="关于复杂抽样的介绍">关于复杂抽样的介绍&lt;/h3>
&lt;h2 id="样本量估算相关内容">样本量估算相关内容&lt;/h2>
&lt;p>这部分主要是关于样本量估算的一些介绍，重点是介绍样本量估算需要考虑哪些因素。&lt;/p>
&lt;h3 id="样本量计算需要考虑的因素">样本量计算需要考虑的因素&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之一">样本量估算需要考虑的因素之一&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之二">样本量估算需要考虑的因素之二&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之三">样本量估算需要考虑的因素之三&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之四">样本量估算需要考虑的因素之四&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之五">样本量估算需要考虑的因素之五&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之六">样本量估算需要考虑的因素之六&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之七">样本量估算需要考虑的因素之七&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之八">样本量估算需要考虑的因素之八&lt;/h3>
&lt;h2 id="回归方法大家族">回归方法大家族&lt;/h2>
&lt;p>这部分主要是介绍了一些常见的各种回归方法，并不是很深入，侧重对各种方法的一些归类介绍，让大家明白有哪些回归方法。&lt;/p>
&lt;h3 id="回归方法杂谈之一">回归方法杂谈之一&lt;/h3>
&lt;h3 id="回归方法杂谈之二">回归方法杂谈之二&lt;/h3>
&lt;h3 id="回归方法杂谈之三">回归方法杂谈之三&lt;/h3>
&lt;h3 id="苹果案例之logistic回归">苹果案例之logistic回归&lt;/h3>
&lt;h3 id="苹果案例之poisson回归">苹果案例之Poisson回归&lt;/h3>
&lt;h3 id="苹果案例之负二项回归">苹果案例之负二项回归&lt;/h3>
&lt;h3 id="deming回归与passing-bablok回归">Deming回归与Passing-Bablok回归&lt;/h3>
&lt;h3 id="样条回归之一">样条回归之一&lt;/h3>
&lt;h3 id="样条回归之二">样条回归之二&lt;/h3>
&lt;h2 id="线性回归分析思路">线性回归分析思路&lt;/h2>
&lt;p>这部分主要是一些与线性回归分析有关的概念、分析思路、技巧等的介绍，如混杂因素的理解、要不要做单因素分析、单因素和多因素结果不同时应该怎么办，等等。&lt;/p>
&lt;h3 id="混杂因素与统计学悖论">混杂因素与统计学悖论&lt;/h3>
&lt;h3 id="回归系数的理解">回归系数的理解&lt;/h3>
&lt;h3 id="单因素与多因素的不同">单因素与多因素的不同&lt;/h3>
&lt;h3 id="关于线性的理解">关于“线性”的理解&lt;/h3>
&lt;h3 id="box-cox变换">Box-Cox变换&lt;/h3>
&lt;h3 id="box-tidwell变换">Box-Tidwell变换&lt;/h3>
&lt;h3 id="要不要用逐步回归">要不要用逐步回归&lt;/h3>
&lt;h3 id="单因素分析有没有必要做">单因素分析有没有必要做？&lt;/h3>
&lt;h3 id="先做单因素再做多因素是否正确">先做单因素、再做多因素，是否正确？&lt;/h3>
&lt;h3 id="先做单因素再做多因素的理解">先做单因素再做多因素的理解&lt;/h3>
&lt;h3 id="多因素筛选策略">多因素筛选策略&lt;/h3>
&lt;h3 id="置信区间与预测区间">置信区间与预测区间&lt;/h3>
&lt;h3 id="线性回归中的方差齐性">线性回归中的方差齐性&lt;/h3>
&lt;h3 id="异常值的问题">异常值的问题&lt;/h3>
&lt;h3 id="缺失值的类型">缺失值的类型&lt;/h3>
&lt;h3 id="缺失值的简单处理">缺失值的简单处理&lt;/h3>
&lt;h3 id="用多重插补法处理缺失值">用多重插补法处理缺失值&lt;/h3>
&lt;h3 id="自变量对数变换如何解释">自变量对数变换如何解释&lt;/h3>
&lt;h3 id="因变量对数变换如何解释">因变量对数变换如何解释&lt;/h3>
&lt;h2 id="生存分析的相关内容">生存分析的相关内容&lt;/h2>
&lt;p>这部分主要是一些与生存分析有关的内容，包括常见回归方法的介绍，一些常见问题的处理等。&lt;/p>
&lt;h3 id="生存分析方法简介">生存分析方法简介&lt;/h3>
&lt;h3 id="中位生存时间与中位随访时间">中位生存时间与中位随访时间&lt;/h3>
&lt;h3 id="log-rank检验应用注意事项">log-rank检验应用注意事项&lt;/h3>
&lt;h3 id="指数回归">指数回归&lt;/h3>
&lt;h3 id="weibull回归">Weibull回归&lt;/h3>
&lt;h3 id="等比例cox回归">等比例cox回归&lt;/h3>
&lt;h3 id="时依协变量与分层cox回归">时依协变量与分层cox回归&lt;/h3>
&lt;h2 id="关于重复测量数据的分析">关于重复测量数据的分析&lt;/h2>
&lt;p>这部分主要是一些与重复测量数据有关的方法和结果解读介绍。&lt;/p>
&lt;h3 id="重复测量数据介绍">重复测量数据介绍&lt;/h3>
&lt;h3 id="重复测量方差分析">重复测量方差分析&lt;/h3>
&lt;h3 id="重复测量数据的趋势探索">重复测量数据的趋势探索&lt;/h3>
&lt;h3 id="重复测量数据的趋势比较">重复测量数据的趋势比较&lt;/h3>
&lt;h3 id="重复测量数据之重复测量方差分析">重复测量数据之重复测量方差分析&lt;/h3>
&lt;h3 id="重复测量数据之广义估计方程">重复测量数据之广义估计方程&lt;/h3>
&lt;h3 id="重复测量数据之多水平模型">重复测量数据之多水平模型&lt;/h3>
&lt;h3 id="重复测量方法的比较">重复测量方法的比较&lt;/h3>
&lt;h2 id="本人对统计学方法的观点">本人对统计学方法的观点&lt;/h2>
&lt;p>这部分主要是本人对如何学习统计学、统计方法应用等的一些观点，未必正确，纯属个人观点。&lt;/p>
&lt;h3 id="专家说我的方法过时了怎么办">专家说我的方法过时了怎么办&lt;/h3>
&lt;h3 id="预测建模需要注意点什么问题">预测建模需要注意点什么问题&lt;/h3>
&lt;h3 id="大数据和医工交叉时代下对医学统计的思考">大数据和医工交叉时代下对医学统计的思考&lt;/h3>
&lt;p>作者认为统计学是大数据的基础之一。面对不断出现的新名词，打好统计学基础，再去尝试接触自己需要的方法，你会发现很多方法其实都是触类旁通的，就像你学好了易筋经，你会发现再练其它招式都是事半功倍。&lt;/p>
&lt;p>读者注：我很认同这一点，算然有些老派，但是符合规律。&lt;/p>
&lt;h3 id="危险因素探索中的几个要点">危险因素探索中的几个要点&lt;/h3>
&lt;h3 id="横断面与病例对照研究的区别">横断面与病例对照研究的区别&lt;/h3>
&lt;h3 id="如何学习统计学的体会">如何学习统计学的体会&lt;/h3>
&lt;h3 id="统计方法的算法与软件操作哪个重要">统计方法的算法与软件操作哪个重要&lt;/h3>
&lt;h3 id="预测建模的基本思路">预测建模的基本思路&lt;/h3>
&lt;h3 id="一些数据分析思路">一些数据分析思路&lt;/h3>
&lt;h3 id="我学统计学的经历">我学统计学的经历&lt;/h3>
&lt;p>总结：少玩手机多读书、多琢磨，加以应用，在实践中学习。&lt;/p>
&lt;h2 id="关于课题申请或论文撰写的方法">关于课题申请或论文撰写的方法&lt;/h2>
&lt;p>这部分主要是本人根据课题标书、论文等撰写中的一些问题，将其总结，提出一些建议。&lt;/p>
&lt;h3 id="课题申请中的方法学描述">课题申请中的方法学描述&lt;/h3>
&lt;h3 id="课题申请中方法学的注意事项">课题申请中方法学的注意事项&lt;/h3>
&lt;h3 id="关于课题设计的一些问题">关于课题设计的一些问题&lt;/h3>
&lt;h3 id="论文撰写中的统计方法">论文撰写中的统计方法&lt;/h3></description></item><item><title>读书笔记之计量、统计必读的好文章50篇.md</title><link>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E8%AE%A1%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%BF%85%E8%AF%BB%E7%9A%84%E5%A5%BD%E6%96%87%E7%AB%A050%E7%AF%87/</link><pubDate>Sat, 23 Nov 1991 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E8%AE%A1%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%BF%85%E8%AF%BB%E7%9A%84%E5%A5%BD%E6%96%87%E7%AB%A050%E7%AF%87/</guid><description>
&lt;h2 id="读书笔记计量统计必读的好文章50篇">读书笔记计量、统计必读的好文章50篇&lt;!-- omit in toc -->&lt;/h2>
&lt;p>经管之家于2018年2月发表在微信公账号上的列出了50篇计量、统计领域必读的50篇好文章，本篇为50篇读书笔记的综合。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#一个外行的计量经济学学习之路">一个外行的计量经济学学习之路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一位美国top10统计专业本科生的经验别走我这样的弯路">一位美国TOP10统计专业本科生的经验：别走我这样的弯路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#牛人整理的统计学教材全">牛人整理的统计学教材（全）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#原创国外流行的计量经济学著作-知多少">原创｜国外流行的计量经济学著作 知多少&lt;/a>&lt;/li>
&lt;li>&lt;a href="#jg系列no006写给统计学初学者的心得体会胖胖小龟宝">【JG系列•NO.006】写给统计学初学者的心得体会｜胖胖小龟宝&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="一个外行的计量经济学学习之路">一个外行的计量经济学学习之路&lt;/h2>
&lt;p>第一阶段：计量就是统计——很难很高深；第二阶段：学习统计/计量还得从实践中来，上课学习SPSS，根据需求学东西；不求甚解；第三阶段：查缺补漏——计量似乎也不是想象中的那么难，复习回顾，系统性再次学习，但是很多新的东西还是不会；第四阶段：躲不过了，硬着头皮也要上，学习真正难的东西，看了比较深的书；第五阶段：计量也不是想象中的那么难，完成学习再看遇到的问题轻松了很多。&lt;/p>
&lt;h2 id="一位美国top10统计专业本科生的经验别走我这样的弯路">一位美国TOP10统计专业本科生的经验：别走我这样的弯路&lt;/h2>
&lt;p>略，参考意义对我不大&lt;/p>
&lt;h2 id="牛人整理的统计学教材全">牛人整理的统计学教材（全）&lt;/h2>
&lt;p>整理了119本国外统计学教材，原文如下&lt;/p>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;amp;mid=2650200437&amp;amp;idx=2&amp;amp;sn=e974480697d64c5f62cfe93524b0a542&amp;amp;chksm=bed6bdf289a134e4e057b82a969b977efbf99cec3a6e9710e94a6c0f4ec388279c5155c92a5e&amp;amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;amp;mid=2650200437&amp;amp;idx=2&amp;amp;sn=e974480697d64c5f62cfe93524b0a542&amp;amp;chksm=bed6bdf289a134e4e057b82a969b977efbf99cec3a6e9710e94a6c0f4ec388279c5155c92a5e&amp;amp;scene=21#wechat_redirect&lt;/a>&lt;/p>
&lt;h2 id="原创国外流行的计量经济学著作-知多少">原创｜国外流行的计量经济学著作 知多少&lt;/h2>
&lt;p>同上一个，但是偏重经济学&lt;/p>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;amp;mid=2650200086&amp;amp;idx=3&amp;amp;sn=48cfb7b0b6c2701748e625f3c2ddec31&amp;amp;chksm=bed6bc9189a13587ff12326f81eebf2af2bb51ac83b0f63a0ef6e4e7eb11299f6d435a34b096&amp;amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;amp;mid=2650200086&amp;amp;idx=3&amp;amp;sn=48cfb7b0b6c2701748e625f3c2ddec31&amp;amp;chksm=bed6bc9189a13587ff12326f81eebf2af2bb51ac83b0f63a0ef6e4e7eb11299f6d435a34b096&amp;amp;scene=21#wechat_redirect&lt;/a>&lt;/p>
&lt;h2 id="jg系列no.006写给统计学初学者的心得体会胖胖小龟宝">【JG系列•NO.006】写给统计学初学者的心得体会｜胖胖小龟宝&lt;/h2></description></item></channel></rss>