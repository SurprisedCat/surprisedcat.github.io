<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>优化理论 on SurprisedCat</title><link>https://surprisedcat.github.io/categories/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/</link><description>Recent content in 优化理论 on SurprisedCat</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2020–2021, SurprisedCat; all rights reserved.</copyright><lastBuildDate>Wed, 20 Nov 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://surprisedcat.github.io/categories/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/index.xml" rel="self" type="application/rss+xml"/><item><title>优化理论之-凸优化-拟凸函数</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B-%E5%87%B8%E4%BC%98%E5%8C%96-%E6%8B%9F%E5%87%B8%E5%87%BD%E6%95%B0/</link><pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B-%E5%87%B8%E4%BC%98%E5%8C%96-%E6%8B%9F%E5%87%B8%E5%87%BD%E6%95%B0/</guid><description>
&lt;h2 id="拟凸函数">拟凸函数&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#下水平集与上水平集">下水平集与上水平集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拟凸函数引入">拟凸函数引入&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拟凸函数与凸函数联系">拟凸函数与凸函数联系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#扩展拟凹函数拟线性函数">扩展：拟凹函数，拟线性函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拟凸函数与jensen不等式">拟凸函数与Jensen不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拟凸优化问题">拟凸优化问题&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>我也有&lt;strong>全局最小值&lt;/strong>呦~~&lt;/p>
&lt;h2 id="下水平集与上水平集">下水平集与上水平集&lt;/h2>
&lt;blockquote>
&lt;p>&lt;span class="math">\(\alpha-\)&lt;/span>下水平集(sub-level set)：&lt;span class="math">\(f:R^n→R, C_\alpha-=\{x|x\in \mathop{dom}f \cap f(x)\leq \alpha\}\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(\alpha-\)&lt;/span>上水平集(super-level set)：&lt;span class="math">\(f:R^n→R, S_\alpha=\{x|x\in \mathop{dom}f \cap f(x)\geq \alpha\}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>如果函数&lt;span class="math">\(f\)&lt;/span>是&lt;strong>凸&lt;/strong>函数，则&lt;span class="math">\(f\)&lt;/span>的&lt;span class="math">\(\alpha-\)&lt;/span>&lt;strong>下水平集是凸集&lt;/strong>。对应的，如果函数&lt;span class="math">\(f\)&lt;/span>是&lt;strong>凹&lt;/strong>函数，则&lt;span class="math">\(f\)&lt;/span>的&lt;span class="math">\(\alpha-\)&lt;/span>&lt;strong>上水平集是凸集&lt;/strong>。&lt;/p>
&lt;p>逆命题不成立，例如：&lt;span class="math">\(f(x)=-e^x\)&lt;/span>的&lt;span class="math">\(\alpha-\)&lt;/span>下水平集是凸集，但是&lt;span class="math">\(f(x)=-e^x\)&lt;/span>是凹函数。&lt;/p>
&lt;h2 id="拟凸函数引入">拟凸函数引入&lt;/h2>
&lt;blockquote>
&lt;p>标准定义（定义1）&lt;span class="math">\(f:R^n→R, \mathop{dom}f\)&lt;/span>是凸集，&lt;span class="math">\(\alpha-\)&lt;/span>下水平集是凸集，则&lt;span class="math">\(f(x)\)&lt;/span>为拟凸函数。即 &lt;span class="math">\[f:R^n→R,s.t.\, S_\alpha=\{x\in \mathop{dom} f| f(x)\geq \alpha\}为凸集\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>解释:对于任意的&lt;span class="math">\(\alpha\)&lt;/span>一下的值(像),其对应的原像集合都是凸集。根据第一节中的例子，我们发现&lt;strong>凹函数也可能是拟凸的&lt;/strong>。&lt;/p>
&lt;img src="../../images/Quasiconvex_function.png" alt="Quasiconvex_function.png" />
&lt;center>
这个函数不是凸的，但是是拟凸的
&lt;/center>
&lt;p>观察上图二维图像：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(f(x)\)&lt;/span>既不是非增也不是非减&lt;/li>
&lt;li>有全局最小值时（可能是点也可能是集合），&lt;strong>左侧非增，右侧非减&lt;/strong>。&lt;/li>
&lt;/ol>
&lt;h3 id="拟凸函数与凸函数联系">拟凸函数与凸函数联系&lt;/h3>
&lt;p>凸函数&lt;span class="math">\(\Rightarrow\)&lt;/span>拟凸函数&lt;/p>
&lt;p>拟凸函数&lt;strong>不一定&lt;/strong>是凸函数，甚至可能是凹函数。&lt;/p>
&lt;h2 id="扩展拟凹函数拟线性函数">扩展：拟凹函数，拟线性函数&lt;/h2>
&lt;blockquote>
&lt;p>&lt;span class="math">\(f\)&lt;/span>是拟凸函数，我们称&lt;span class="math">\(-f\)&lt;/span>为&lt;strong>拟凹&lt;/strong>函数,拟凹函数的&lt;span class="math">\(\alpha-\)&lt;/span>&lt;strong>上水平集是凸集&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;hr />
&lt;blockquote>
&lt;p>拟线性函数：既是拟凸也是拟凹。等同于&lt;span class="math">\(f:R^n→R,s.t.\, S_\alpha=\{x\in \mathop{dom} f| f(x)=\alpha\}为凸集\)&lt;/span>（类似线性函数是既凸又凹）&lt;/p>
&lt;/blockquote>
&lt;p>一般情况下，拟线性函数是一个非增或非减函数。&lt;/p>
&lt;h2 id="拟凸函数与jensen不等式">拟凸函数与Jensen不等式&lt;/h2>
&lt;h2 id="拟凸优化问题">拟凸优化问题&lt;/h2></description></item><item><title>优化理论之KKT与拉格朗日</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8Bkkt%E4%B8%8E%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8Bkkt%E4%B8%8E%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5/</guid><description>
&lt;h2 id="拉格朗日乘子法lagrange-multiplier-和kkt条件">拉格朗日乘子法（Lagrange Multiplier) 和KKT条件&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#应用场景及使用">应用场景及使用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉格朗日乘子法与等式条件">拉格朗日乘子法与等式条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kkt条件与强对偶">KKT条件与强对偶&lt;/a>&lt;/li>
&lt;li>&lt;a href="#原始问题">原始问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶问题">对偶问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#原始问题与对偶问题的关系">原始问题与对偶问题的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kkt条件">KKT条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总结">总结&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="应用场景及使用">应用场景及使用&lt;/h2>
&lt;p>拉格朗日乘子法（Lagrange Multiplier) 和KKT条件是求解有约束优化问题非常重要的两个求取方法。对于等式约束的优化问题，可以应用拉格朗日乘子法去求取最优值；如果含有不等式约束，可以应用KKT条件去求取。当然，这两个方法求出的点只满足必要条件（也就是常说的极值点不一定是最值点，但最值点一定是极值点），当目标函数是凸函数时可转化为充分必要条件。（凸函数对应min类优化问题）&lt;/p>
&lt;p>通常我们需要求解的最优化问题有如下几类：&lt;/p>
&lt;p>（1）无约束优化问题，形如 &lt;span class="math">\[\min f(x)\]&lt;/span> （2）有等式约束的优化问题，形如 &lt;span class="math">\[\min f(x)\\
s.t.\quad h_i(x)=0;i=1,2,\dotsb\]&lt;/span> （3）有不等式约束的优化问题，形如 &lt;span class="math">\[\min f(x)\\
s.t.\quad h_i(x)=0;i=1,2,\dotsb\\
\qquad g_j(x)\le 0;j=1,2,\dotsb\\\]&lt;/span>&lt;/p>
&lt;p>对于第(1)类的优化问题，常常使用的方法就是Fermat引理，即使用求取&lt;span class="math">\(f(x)\)&lt;/span>的导数，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。&lt;/p>
&lt;p>对于第(2)类问题，利用拉格朗日系数将约束与目标函数按如下形式组合： &lt;span class="math">\[L(a,x)=f(x)+\sum_{i=1}^n a_i h_i(x),其中x是向量\]&lt;/span> 然后求取最优值， &lt;span class="math">\(L(a,x)对x\)&lt;/span>求导取零，联立&lt;span class="math">\(h_i(x)=0(a_i≠0)\)&lt;/span>进行求取，这个在高等数学里面有讲，但是没有讲为什么这么做就可以，在后面，将简要介绍其思想。&lt;/p>
&lt;p>对于第(3)类问题，同样将所有条件和目标函数写成一个式子： &lt;span class="math">\[L(a,b,x)=f(x)+\sum_{i=1}^m a_i g_i(x)+\sum_{j=1}^n b_j h_j(x)\]&lt;/span> 利用KKT条件求解最优值，&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(L(a,b,x)\)&lt;/span>对&lt;span class="math">\(x\)&lt;/span>求导为0&lt;/li>
&lt;li>&lt;span class="math">\(h_j(x)=0;j=1,2,\dotsb\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(g_i(x)\le 0;i=1,2,\dotsb\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(a_i \ge 0;b_j \neq 0\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(a_i g_i(x)=0\)&lt;/span>(互补松弛)&lt;/li>
&lt;/ol>
&lt;p>其中第4个式子是SVM很多重要性质的来源，如支持向量的概念。&lt;/p>
&lt;h2 id="拉格朗日乘子法与等式条件">拉格朗日乘子法与等式条件&lt;/h2>
&lt;p>为什么拉格朗日乘子法（Lagrange Multiplier)能够得到最优值？&lt;/p>
&lt;p>设我们的目标函数为&lt;span class="math">\(z=f(x)\)&lt;/span>，&lt;span class="math">\(x\)&lt;/span>为向量，将&lt;span class="math">\(z\)&lt;/span>投影到自变量&lt;span class="math">\(x\)&lt;/span>构成的平面上，即形成等高线。如下图，目标函数是&lt;span class="math">\(f(x,y)\)&lt;/span>，这里&lt;span class="math">\(x，y\)&lt;/span>是标量，虚线是等高线，箭头为导数方向，显然范围越小的等高线越接近无约束条件下的最值。现在假设我们的约束&lt;span class="math">\(g(x)=0\)&lt;/span>，&lt;span class="math">\(x\)&lt;/span>是向量，在&lt;span class="math">\(x\)&lt;/span>构成的平面或者曲面上是一条曲线，假设&lt;span class="math">\(g(x)\)&lt;/span>与等高线相交，交点就是同时满足等式约束条件和目标函数的可行域的值。但肯定不是最优值，&lt;strong>因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与约束函数的交点的值更大或者更小&lt;/strong>，也就是存在两种变化方向的可能性。而只有当等高线与约束条件&lt;strong>相切&lt;/strong>时，可能取值最优值，即此时单向不可更优。如下图1所示，等高线与约束函数的曲线在该点的法向量必须有相同方向，所以最优点满足：&lt;span class="math">\(f(x)的梯度=a_i∗g_i(x)的梯度;i=1,…,n\)&lt;/span>&lt;/p>
&lt;img src="../../images/拉格朗日乘子法.jpg" alt="拉格朗日乘子法原理" />
&lt;center>
图1 拉格朗日乘子法原理
&lt;/center>
&lt;h2 id="kkt条件与强对偶">KKT条件与强对偶&lt;/h2>
&lt;p>KKT条件是满足强对偶条件的优化问题的必要条件，需要结合拉格朗日对偶（Lagrange duality）进行理解。&lt;/p>
&lt;h3 id="原始问题">原始问题&lt;/h3>
&lt;p>假设&lt;span class="math">\(f(x),c_i(x),h_j(x)\)&lt;/span>是定义在&lt;span class="math">\(R^n\)&lt;/span>上的连续可微函数（为什么要求连续可微呢，后面再说，这里不用多想），考虑约束最优化问题： &lt;span class="math">\[\min_{x\in R^n} f(x)\\
s.t.\quad c_i(x)\le 0;i=1,2,\dotsb,k\\
\qquad h_j(x)=0;j=1,2,\dotsb,l\\\]&lt;/span> 称为约束最优化问题的原始问题。&lt;/p>
&lt;p>现在&lt;strong>如果不考虑约束条件&lt;/strong>，原始问题就是： &lt;span class="math">\[\min_{x\in R^n} f(x)\]&lt;/span> 因为假设其连续可微，利用高中的知识，对&lt;span class="math">\(f(x)\)&lt;/span>求导数，然后令导数为0，就可解出最优解，很easy. 那么，问题来了（呵呵。。。），偏偏有约束条件，好烦啊，要是能想办法把&lt;strong>约束条件去掉&lt;/strong>就好了，bingo! 拉格朗日函数就是干这个的。&lt;/p>
&lt;p>引进&lt;strong>广义拉格朗日函数&lt;/strong>（generalized Lagrange function）: &lt;span class="math">\[L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_i c_i(x)+\sum_{j=1}^l\beta_j h_j(x)\\
x=(x^{(1)},x^{(2)},\dotsb,x^{(n)})^T\in R^n\]&lt;/span> 不要怕这个式子，也不要被拉格朗日这个高大上的名字给唬住了，让我们慢慢剖析！这里&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>是拉格朗日乘子（名字高大上，其实就是上面函数中的参数而已），特别要求&lt;span class="math">\(\alpha_i&amp;gt;0\)&lt;/span>。&lt;/p>
&lt;p>现在，&lt;strong>如果把&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>看作是关于&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的函数&lt;/strong>，要求其最大值，即 &lt;span class="math">\[\max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\]&lt;/span> 再次注意&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>是一个关于&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的函数，经过我们优化（不要管什么方法），就是确定&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的值使得&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>取得最大值（此过程中把&lt;span class="math">\(x\)&lt;/span>看做常量），确定了&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的值，就可以得到&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>的最大值，因为&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>&lt;strong>已经确定&lt;/strong>，显然最大值&lt;span class="math">\(\max\limits_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\)&lt;/span>就是&lt;strong>只和&lt;span class="math">\(x\)&lt;/span>有关的函数&lt;/strong>，定义这个函数为： &lt;span class="math">\[\theta_p(x)=\max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\]&lt;/span>&lt;/p>
&lt;p>对于原式 &lt;span class="math">\[L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_i c_i(x)+\sum_{j=1}^l\beta_j h_j(x)\]&lt;/span> 通过&lt;span class="math">\(x\)&lt;/span>是否满足约束条件两方面来分析这个函数:&lt;/p>
&lt;p>(1).&lt;strong>考虑某个&lt;span class="math">\(x\)&lt;/span>违反了原始的约束&lt;/strong>，即&lt;span class="math">\(c_i(x)&amp;gt;0\)&lt;/span>或者&lt;span class="math">\(h_j(x)\neq 0\)&lt;/span>，那么： &lt;span class="math">\[\theta_p(x)=\max\limits_{\alpha,\beta,\alpha_i&amp;gt;0}[f(x)+\sum_{i=1}^k\alpha_i c_i(x)\\
+\sum_{j=1}^l\beta_j h_j(x)]=+\infty\]&lt;/span> 注意中间的最大化式子就是确定&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的之后的结果，若&lt;span class="math">\(c_i(x)&amp;gt;0\)&lt;/span>，则令&lt;span class="math">\(\alpha_i\rightarrow+\infty\)&lt;/span>，如果&lt;span class="math">\(h_j(x)\neq 0\)&lt;/span>，很容易取值&lt;span class="math">\(\beta_j\)&lt;/span>使得&lt;span class="math">\(\beta_j h_j(x)\rightarrow +\infty\)&lt;/span>。&lt;/p>
&lt;p>(2).&lt;strong>考虑&lt;span class="math">\(x\)&lt;/span>满足原始的约束&lt;/strong>，&lt;span class="math">\(c_i(x)\)&lt;/span>都是小于等于0的。如果&lt;span class="math">\(c_i(x)\)&lt;/span>严格小于0，那么我们必要让&lt;span class="math">\(\alpha_i\)&lt;/span>等于0，这时才能取最大值。等于0的情况就无所谓了，所以对于满足原始约束的情况，&lt;span class="math">\(\alpha_ic_i(x)=0，\)&lt;/span>即互补松弛条件。因此，&lt;span class="math">\(\theta_p(x)=\max\limits_{\alpha,\beta,\alpha_i&amp;gt;0}[f(x)]=f(x)\)&lt;/span>，注意中间的最大化是确定&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的过程，&lt;span class="math">\(f(x)\)&lt;/span>就是个常量，常量的最大值显然是本身。&lt;/p>
&lt;p>通过上面两条分析可以得出： &lt;span class="math">\[\theta_p(x)=
\begin{cases}
f(x), &amp;amp;x\text{满足原始问题约束}\\
+\infty, &amp;amp;\text{其他}
\end{cases}\]&lt;/span> 那么在满足约束条件下： &lt;span class="math">\[\min_x\theta_p(x)=\min_x \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\\
=\min_x f(x)\quad\]&lt;/span> 即&lt;span class="math">\(\min\limits_x\theta_p(x)\)&lt;/span>与原始优化问题等价,所以常用&lt;span class="math">\(\min\limits_x\theta_p(x)\)&lt;/span>代表原始问题，下标&lt;span class="math">\(p\)&lt;/span>表示原始问题，定义&lt;strong>原始问题的最优值&lt;/strong>： &lt;span class="math">\[p^\ast=\min_x\theta_p(x)\]&lt;/span> 原始问题讨论就到这里，做一个总结：&lt;strong>通过拉格朗日这位大神的办法重新定义一个无约束问题（大家都喜欢无拘无束），这个无约束问题等价于原来的约束优化问题，从而将约束问题无约束化！&lt;/strong>&lt;/p>
&lt;p>到这里，如果原问题是可解的，那么我们就可以使用求导的方式，求解问题。&lt;/p>
&lt;h3 id="对偶问题">对偶问题&lt;/h3>
&lt;p>现在我们先将&lt;span class="math">\(x\)&lt;/span>看做参数，定义关于&lt;span class="math">\(\alpha,\beta\)&lt;/span>的函数 &lt;span class="math">\[θ_D(\alpha,\beta)=\min_x L(x,\alpha,\beta)\]&lt;/span> 注意等式右边是关于&lt;span class="math">\(x\)&lt;/span>的函数的最小化，&lt;span class="math">\(x\)&lt;/span>确定以后，最小值就只与&lt;span class="math">\(\alpha,\beta\)&lt;/span>有关，所以是一个关于&lt;span class="math">\(\alpha,\beta\)&lt;/span>的函数。&lt;/p>
&lt;p>考虑极大化&lt;span class="math">\(θ_D(\alpha,\beta)=\min\limits_x L(x,\alpha,\beta)\)&lt;/span>，即 &lt;span class="math">\[\max_{\alpha,\beta:\alpha_i \ge0}θ_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i \ge0}\min\limits_x L(x,\alpha,\beta)\]&lt;/span> 这就是原始问题的对偶问题，再把原始问题写出来： &lt;span class="math">\[\min_x\theta_p(x)=\min_x \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\]&lt;/span> 形式上可以看出很对称，只不过原始问题是先固定&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>中的&lt;span class="math">\(x\)&lt;/span>，优化出参数&lt;span class="math">\(\alpha,\beta\)&lt;/span>，再优化最优&lt;span class="math">\(x\)&lt;/span>，而对偶问题是先固定&lt;span class="math">\(\alpha,\beta\)&lt;/span>，优化出最优&lt;span class="math">\(x\)&lt;/span>，然后再确定参数&lt;span class="math">\(\alpha,\beta\)&lt;/span>。&lt;/p>
&lt;p>定义对偶问题的最优值： &lt;span class="math">\[d^\ast=\max_{\alpha,\beta:\alpha_i \ge0}θ_D(\alpha,\beta)\]&lt;/span>&lt;/p>
&lt;h3 id="原始问题与对偶问题的关系">原始问题与对偶问题的关系&lt;/h3>
&lt;blockquote>
&lt;p>定理：若原始问题与对偶问题都有最优值，则 &lt;span class="math">\[d^\ast=\max_{\alpha,\beta:\alpha_i \ge0}\min\limits_x L(x,\alpha,\beta)\\
\le \min_x \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)=p^\ast\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：对于任意的&lt;span class="math">\(\alpha,\beta,x\)&lt;/span>，有 &lt;span class="math">\[θ_D(\alpha,\beta)=\min_xL(x,\alpha,\beta) \le L(x,\alpha,\beta)\\
\le \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)=\theta_p(x)\\
\Rightarrow θ_D(\alpha,\beta)\le \theta_p(x)恒成立\]&lt;/span> 由于原始问题与对偶问题都有最优值，所以 &lt;span class="math">\[\max_{\alpha,\beta:\alpha_i \ge0}θ_D(\alpha,\beta) \le \min_x \theta_p(x)\]&lt;/span> 即 &lt;span class="math">\[d^\ast=\max_{\alpha,\beta:\alpha_i \ge0}\min\limits_x L(x,\alpha,\beta)\\
\le \min_x \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)=p^\ast\]&lt;/span> 也就是说&lt;strong>原始问题的最优值不小于对偶问题的最优值&lt;/strong>，但是我们要通过对偶问题来求解原始问题，就必须使得&lt;strong>原始问题的最优值与对偶问题的最优值相等&lt;/strong>，于是可以得出下面的推论：&lt;/p>
&lt;blockquote>
&lt;p>推论：设&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>分别是原始问题和对偶问题的可行解，如果&lt;span class="math">\(d^\ast=p^\ast\)&lt;/span>，那么&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>分别是原始问题和对偶问题的最优解。&lt;/p>
&lt;/blockquote>
&lt;p>所以，当原始问题和对偶问题的最优值相等：&lt;span class="math">\(d^\ast=p^\ast\)&lt;/span>时，可以用&lt;strong>求解对偶问题来求解原始问题&lt;/strong>（当然是对偶问题求解比直接求解原始问题简单的情况下），但是到底满足什么样的条件才能使得&lt;span class="math">\(d^\ast=p^\ast\)&lt;/span>呢，这就是下面要阐述的&lt;strong>KKT&lt;/strong>条件。&lt;/p>
&lt;h3 id="kkt条件">KKT条件&lt;/h3>
&lt;p>强对偶性：存在&lt;span class="math">\(a^∗,b^∗,x^\ast\)&lt;/span>使得&lt;span class="math">\(d^∗=p^∗\)&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理：对于原始问题和对偶问题，假设函数&lt;span class="math">\(f(x)\)&lt;/span>和&lt;span class="math">\(c_i(x)\)&lt;/span>是凸函数，&lt;span class="math">\(h_j(x)\)&lt;/span>是仿射函数（即由一阶多项式构成的函数，&lt;span class="math">\(f(x)=Ax+b\)&lt;/span>,&lt;span class="math">\(A\)&lt;/span>是矩阵，&lt;span class="math">\(x,b\)&lt;/span>是向量）；并且假设不等式约束&lt;span class="math">\(c_i(x)\)&lt;/span>是严格可行的，即存在&lt;span class="math">\(x\)&lt;/span>，对所有&lt;span class="math">\(i\)&lt;/span>有&lt;span class="math">\(c_i(x)&amp;lt;0\)&lt;/span>，则存在&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>，使得&lt;span class="math">\(x^\ast\)&lt;/span>是原始问题的最优解，&lt;span class="math">\(\alpha^\ast,\beta^\ast\)&lt;/span>是对偶问题的最优解，并且 &lt;span class="math">\[d^\ast=p^\ast=L(x^\ast,\alpha^\ast,\beta^\ast)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>如果简单点说要求&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>分别是原始问题和对偶问题的最优解的&lt;strong>充分必要条件&lt;/strong>是&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>满足下面的Karush-Kuhn-Tucker(KKT)条件: &lt;span class="math">\[
\nabla_x L(x^\ast,\alpha^\ast,\beta^\ast)=0\\
\alpha_i^\ast\ge 0;i=1,2,\dotsb,k\\
h_j(x^\ast)=0;j=1,2,\dotsb,l\\
\alpha_i^\ast c_i(x^\ast)=0;i=1,2,\dotsb,k\]&lt;/span>&lt;/p>
&lt;p>关于KKT 条件的理解：前面三个条件是由解析函数的知识，对于各个变量的偏导数为0（这就解释了一开始为什么假设三个函数连续可微，如果不连续可微的话，这里的偏导数存不存在就不能保证），后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束。&lt;/p>
&lt;p>特别注意,最后一个条件称为&lt;strong>KKT对偶互补条件&lt;/strong>。当&lt;span class="math">\(\alpha_i^\ast&amp;gt;0\)&lt;/span>时，由KKT对偶互补条件可知：&lt;span class="math">\(c_i(x^\ast)=0\)&lt;/span>。&lt;/p>
&lt;p>证明：关于互补松弛条件，我们可以回顾一下这样一个式子： &lt;span class="math">\[\begin{aligned}
\theta_p(x)&amp;amp;=\max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\\
&amp;amp;=f(x)+\max_{\alpha,:\alpha_i \ge0}\sum_{i=1}^k\alpha_i c_i(x)+\max_{\beta}\sum_{j=1}^l\beta_j h_j(x)\\
&amp;amp;=f(x)
\end{aligned}\]&lt;/span> 显然&lt;span class="math">\(\sum\limits_{i=1}^k\alpha_i c_i(x)=0\)&lt;/span>时取得最大，因此这也是隐含的&lt;span class="math">\(d^∗=p^∗\)&lt;/span>时的条件。 这个知识点会在 SVM 的推导中用到。&lt;/p>
&lt;p>最后说明一下&lt;/p>
&lt;p>slater条件：存在&lt;span class="math">\(x\)&lt;/span>，使得不等式约束&lt;span class="math">\(c(x)&amp;lt;=0\)&lt;/span>严格成立(不取等号)。&lt;/p>
&lt;p>slater条件性质： slater条件是原问题可以等价于对偶问题的一个充分条件，满足他就说明存在最优解。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>一句话，某些条件下，把原始的约束问题通过拉格朗日函数转化为无约束问题，如果原始问题求解棘手，在满足KKT的条件下用求解对偶问题来代替求解原始问题，使得问题求解更加容易。&lt;/p>
&lt;p>其他参考资料： &lt;a href="http://xiaoyc.com/duality-theory-for-optimization/#citem_30">优化问题中的对偶性理论&lt;/a>，强烈推荐看一看&lt;/p></description></item><item><title>优化理论之共轭梯度法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95/</guid><description/></item><item><title>优化理论之内点法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%86%85%E7%82%B9%E6%B3%95/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%86%85%E7%82%B9%E6%B3%95/</guid><description/></item><item><title>优化理论之牛顿法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%89%9B%E9%A1%BF%E6%B3%95/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%89%9B%E9%A1%BF%E6%B3%95/</guid><description>
&lt;h2 id="牛顿法">牛顿法&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#基础牛顿法">基础牛顿法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#牛顿法求解函数根">牛顿法求解函数根&lt;/a>&lt;/li>
&lt;li>&lt;a href="#牛顿法求根的几何解释">牛顿法求根的几何解释&lt;/a>&lt;/li>
&lt;li>&lt;a href="#函数最优值与求根">函数最优值与求根&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#一元函数情况">一元函数情况&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多元函数的情况">多元函数的情况&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#实现算法">实现算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#牛顿法修正">牛顿法修正&lt;/a>&lt;/li>
&lt;li>&lt;a href="#goldstein-price修正法g-p法">Goldstein-Price修正法(G-P法)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#负曲率方向法">负曲率方向法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#goldfeld修正">Goldfeld修正&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gill-murray改进cholesky分解法的稳定牛顿法">Gill-Murray改进Cholesky分解法的稳定牛顿法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#信赖域法">信赖域法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#信赖域算法的基本思想">信赖域算法的基本思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基本信赖域法">基本信赖域法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#levenbergmarquardt法">Levenberg–Marquardt法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dogleg法">Dogleg法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#拟牛顿法">拟牛顿法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dfp算法">DFP算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#sr1算法">SR1算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bfgs算法">BFGS算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#l-bfgs算法">L-BFGS算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#共轭梯度法">共轭梯度法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总结">总结&lt;/a>&lt;/li>
&lt;li>&lt;a href="#参考文献">参考文献&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>牛顿法和梯度下降法都是求解非线性&lt;strong>无约束&lt;/strong>最优化问题的常用方法，但是效果速度比梯度法更好（但是计算量增加了，需要二阶信息）。牛顿法是&lt;strong>迭代算法&lt;/strong>，每一步需要求解目标函数的 Hessian 矩阵的逆矩阵，计算过程复杂。虽然牛顿法收敛速度很快，但是有两个很难处理的问题。一是要保证Hessian 矩阵正定。二是求Hessian 矩阵是很耗计算量的。因此，有许多&lt;strong>修正牛顿法&lt;/strong>来保证矩阵正定或是在矩阵不正定时提供候选方向。同时为了简化计算，诞生了&lt;strong>拟牛顿法&lt;/strong>通过正定矩阵近似 Hessian 矩阵的逆矩阵或 Hessian 矩阵，简化计算过程。DFP方法、BFGS方法和L-BFGS方法都属于拟牛顿法。&lt;/p>
&lt;p>牛顿法、修正的牛顿法和拟牛顿法解决的问题可以用如下&lt;strong>无约束最小化&lt;/strong>问题概括： &lt;span class="math">\[\mathop{min}\limits_{x}\quad f(x)\]&lt;/span> 其中，&lt;span class="math">\(x=(x_1，x_2，\ldots，x_N)^T\in\mathbb{R}^N\)&lt;/span>。由于本文不讨论收敛性，没有特殊说明时，假设&lt;span class="math">\(f\)&lt;/span>为&lt;strong>凸函数，且两阶连续可微&lt;/strong>，此外，记极小问题的解是&lt;span class="math">\(x^\ast\)&lt;/span>。&lt;/p>
&lt;h2 id="基础牛顿法">基础牛顿法&lt;/h2>
&lt;h3 id="牛顿法求解函数根">牛顿法求解函数根&lt;/h3>
&lt;p>牛顿法最核心的功能是&lt;strong>迭代求函数根&lt;/strong>，其他函数优化功能都是从这里衍生出去的。我们知道并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以&lt;strong>迭代求解&lt;/strong>。 原理是利用泰勒公式。我们将任一函数在&lt;span class="math">\(x_0\)&lt;/span>处展开，且展开到一阶，即： &lt;span class="math">\[f(x)=f(x_0)+(x–x_0)f&amp;#39;(x_0)+\underbrace{\frac{1}{2}(x-x_0)^2f&amp;#39;&amp;#39;(\varepsilon)}_{余项}\]&lt;/span> 其中，&lt;span class="math">\(\varepsilon\)&lt;/span>处于&lt;span class="math">\(x,x_0\)&lt;/span>之间，由于后面的余项是&lt;span class="math">\((x-x_0)\)&lt;/span>的高阶数，&lt;strong>先忽略&lt;/strong>。因此要求解方程&lt;span class="math">\(f(x)=0\)&lt;/span>，即是&lt;span class="math">\(f(x_0)+(x–x_0)f&amp;#39;(x_0)=0\)&lt;/span>，解为： &lt;span class="math">\[x=x_1=x_0–f(x_0)/f&amp;#39;(x_0)\]&lt;/span> 因为这是利用泰勒公式的一阶展开，且忽略了余项，余项的存在导致&lt;span class="math">\(f(x)与f(x_0)+(x–x_0)f&amp;#39;(x_0)\)&lt;/span>在&lt;span class="math">\(x_1\)&lt;/span>并不是完全相等，而是近似相等。因此，这里求得的&lt;span class="math">\(x_1\)&lt;/span>并不能让&lt;span class="math">\(f(x)=0\)&lt;/span>，只能说&lt;span class="math">\(f(x_1)\)&lt;/span>的值比&lt;span class="math">\(f(x_0)\)&lt;/span>更接近&lt;span class="math">\(f(x)=0\)&lt;/span>，于是乎，迭代求解的想法就很自然了，可以进而推出 &lt;span class="math">\[x_{n+1}=x_n–f(x_n)/f&amp;#39;(x_n)\]&lt;/span> 通过迭代，这个式子必然在&lt;span class="math">\(f(x^∗)=0\)&lt;/span>的时候收敛. 整个过程如下图：&lt;/p>
&lt;img src="../../images/newton_root.gif" alt="newton_root.gif" />
&lt;center>
图1牛顿法求根
&lt;/center>
&lt;h3 id="牛顿法求根的几何解释">牛顿法求根的几何解释&lt;/h3>
&lt;p>牛顿法求根其实是利用了切线的性质。如果我们查看过点&lt;span class="math">\(x_0\)&lt;/span>处的切线方程： &lt;span class="math">\[
y-f(x_0)=f&amp;#39;(x_0)(x-x_0)\\
\]&lt;/span> 当&lt;span class="math">\(y=0\)&lt;/span>时，有： &lt;span class="math">\[
-f(x_0)=f&amp;#39;(x_0)(x-x_0)\\
\Rightarrow x=x_0-\frac{f(x_0)}{f&amp;#39;(x_0)}
\]&lt;/span> 这个式子和牛顿法求根的迭代公式一致。说白了，牛顿法求根就是利用当前点所在直线与&lt;span class="math">\(y=0\)&lt;/span>的交点，这个交点&lt;span class="math">\(x_1\)&lt;/span>会比&lt;span class="math">\(x_0\)&lt;/span>更接近让函数值为0。&lt;/p>
&lt;h3 id="函数最优值与求根">函数最优值与求根&lt;/h3>
&lt;h4 id="一元函数情况">一元函数情况&lt;/h4>
&lt;p>从简单的一元函数开始，我们先列出函数的二阶泰勒展开式： &lt;span class="math">\[f(x)=f(x_0)+f&amp;#39;(x_0)(x-x_0)+\frac{1}{2}f&amp;#39;&amp;#39;(x_0)(x-x_0)^2+R_2(x)\]&lt;/span>&lt;/p>
&lt;p>然后，我们再来讨论我们的问题，求函数最优值。我们知道一个可微函数的最值点，一定是它的驻点（&lt;em>或边界点，假设定义域范围暂不考虑&lt;/em>）。那我们就要求驻点，即&lt;span class="math">\(f&amp;#39;(x)=0\)&lt;/span>的点。我们如果对&lt;span class="math">\(f(x)\)&lt;/span>和其二阶泰勒展开式求导（忽略余项）： &lt;span class="math">\[f&amp;#39;(x)=f&amp;#39;(x_0)+f&amp;#39;&amp;#39;(x_0)(x-x_0)\]&lt;/span> 根据驻点性质，要求&lt;span class="math">\(f&amp;#39;(x)=0\)&lt;/span>，则根据上一节&lt;strong>牛顿法求解函数根&lt;/strong>可知： &lt;span class="math">\[x_1=x_0-\frac{f&amp;#39;(x_0)}{f&amp;#39;&amp;#39;(x_0)}\]&lt;/span> 这样我们就得到了下一点的位置&lt;span class="math">\(x_1\)&lt;/span>。&lt;span class="math">\(f&amp;#39;(x_1)\)&lt;/span>比&lt;span class="math">\(f&amp;#39;(x_0)\)&lt;/span>更接近0。接下来重复这个过程，直到到达导数为0的点，由此得到求极值的牛顿法的迭代公式： &lt;span class="math">\[x_{n+1}=x_n-\frac{f&amp;#39;(x_n)}{f&amp;#39;&amp;#39;(x_n)}\]&lt;/span> 给定初始迭代点&lt;span class="math">\(x_0\)&lt;/span>，反复用上面的公式进行迭代，直到达到导数为0的点或者达到最大迭代次数。&lt;/p>
&lt;h4 id="多元函数的情况">多元函数的情况&lt;/h4>
&lt;p>下面推广到多元函数的情况，根据多元函数的泰勒展开公式（&lt;em>参考矩阵论教材&lt;/em>），我们对目标函数在&lt;span class="math">\(x_0\)&lt;/span>点处做二阶泰勒展开，有（自变量皆为列向量，忽略余项）： &lt;span class="math">\[f(x)=f(x_0)+\nabla f(x_0)^T(x-x_0)+\frac{1}{2}(x-x_0)^T\nabla^2f(x_0)(x-x_0)\]&lt;/span> 其中，&lt;span class="math">\(\nabla^2f(x_0)\)&lt;/span>为多元实值函数&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>处的海森矩阵（Hessian Matrix），我们以后用&lt;span class="math">\(H\)&lt;/span>表示。那么如果函数梯度为0，且海森矩阵&lt;span class="math">\(H\)&lt;/span>可逆，则有： &lt;span class="math">\[\nabla f(x)=\vec{0} \Rightarrow \\
\nabla f(x_0)+\nabla^2f(x_0)(x-x_0)=\vec{0}\Rightarrow \\
x_1=x_0-(\nabla^2f(x_0))^{-1}\nabla f(x_0)\Rightarrow \\
x_1=x_0-H^{-1}[f(x_0)]\nabla f(x_0)\]&lt;/span> 注意矩阵左右乘不可颠倒,&lt;span class="math">\(H^{-1}[f(x_0)]\)&lt;/span>，为海森矩阵的逆矩阵。&lt;strong>特别地，若是极小值点则Hessian是正定矩阵&lt;/strong>。同理，我们可以得到如下递推公式： &lt;span class="math">\[x_{n+1}=x_n-H^{-1}[f(x_n)]\nabla f(x_n) \\
可简写为：x_{n+1}=x_n-H^{-1}_n{g_n}\]&lt;/span> 最终会到达函数的驻点处。其中&lt;span class="math">\(-H^{-1}g\)&lt;/span>称为&lt;strong>牛顿方向&lt;/strong>。迭代终止的条件是梯度的模接近于0，或者函数值下降小于指定阈值。&lt;/p>
&lt;p>注意，我们目前默认牛顿法的步长是1，这样可以保证2阶收敛速度，但是并不是对所有函数都有效。现不加证明的指出，带步长的牛顿法是收敛的。&lt;/p>
&lt;h3 id="实现算法">实现算法&lt;/h3>
&lt;p>1.给定初始值&lt;span class="math">\(x_0\)&lt;/span>和精度阈值&lt;span class="math">\(\epsilon\)&lt;/span>，设置&lt;span class="math">\(k = 0\)&lt;/span>&lt;/p>
&lt;p>2.计算梯度&lt;span class="math">\(g_k\)&lt;/span>和矩阵&lt;span class="math">\(H_k\)&lt;/span>&lt;/p>
&lt;p>3.如果&lt;span class="math">\(\|g_k\|&amp;lt;\epsilon\)&lt;/span>即在此点处梯度的值接近于0，则达到极值点处，停止迭代&lt;/p>
&lt;p>4.计算搜索方向&lt;span class="math">\(d_k=-H^{-1}_k{g_k}\)&lt;/span>&lt;/p>
&lt;p>5.计算新的迭代点&lt;span class="math">\(x_{k+1}=x_k-\alpha H^{-1}_k{g_k}\)&lt;/span>(一般可以取&lt;span class="math">\(\alpha=1\)&lt;/span>)&lt;/p>
&lt;p>6.令&lt;span class="math">\(k = k + 1\)&lt;/span>，返回步骤2&lt;/p>
&lt;p>其中&lt;span class="math">\(\epsilon\)&lt;/span>是一个人工设定的接近于0的常数，和梯度下降法一样，需要这个参数的原因是保证&lt;span class="math">\(x_{k+1}\)&lt;/span>在&lt;span class="math">\(x_k\)&lt;/span>的邻域内，从而可以忽略泰勒展开的高次项。如果目标函数是二次函数，Hessian矩阵是一个常数矩阵，对于任意给定的初始点，牛顿法只需要一步迭代就可以收敛到极值点。&lt;/p>
&lt;p>牛顿法收敛速度快，具有二阶收敛性。但是和梯度下降法一样，牛顿法寻找的也是导数为0的点，这不一定是极值点，因此会面临局部极小值和鞍点问题。另外，它需要Hessian矩阵，计算量大，并且Hessian矩阵并不是一直能够求出。因此出现了一些&lt;strong>降低计算量&lt;/strong>得&lt;strong>拟牛顿法&lt;/strong>。&lt;/p>
&lt;p>除此之外，牛顿法在每次迭代时序列&lt;span class="math">\(x_i\)&lt;/span>可能不会收敛到一个最优解，它甚至不能保证函数值会按照这个序列递减。解决第一个问题可以通过&lt;strong>调整牛顿方向的步长&lt;/strong>来实现，目前常用的方法有两种：直线搜索和可信区域法。加了步长搜索的牛顿法也叫&lt;strong>阻尼牛顿法&lt;/strong>。&lt;/p>
&lt;h2 id="牛顿法修正">牛顿法修正&lt;/h2>
&lt;p>牛顿法的一大困难在于某一k步的Hessian 矩阵&lt;span class="math">\(H_k\)&lt;/span>可能不正定。这时二次型模型不一定有极小点，甚至没有平稳点。此时，牛顿方向&lt;span class="math">\(-H_k^{-1}g_k\)&lt;/span>不一定会让目标函数值下降。对应的各种改进方案应运而生，各位接着往下看。&lt;/p>
&lt;h3 id="goldstein-price修正法g-p法">Goldstein-Price修正法(G-P法)&lt;/h3>
&lt;p>针对&lt;span class="math">\(H_k\)&lt;/span>可能不正定的问题，如其不正定，就用“最速下降方向”来作为搜索方向。 &lt;span class="math">\[d_{k+1}=\begin{cases}-H_kg_k，H_k正定\\-g_k，H_k非正定
\end{cases}\]&lt;/span> 确定方向后，采用下列精确一维搜索（Armijo-Goldstein准则）（PS：其他准则也可以） &lt;span class="math">\[1:f(\vec x_k+\alpha\vec d_k)≤f(\vec x_k)+\alpha c\nabla f(\vec x_k)\vec d_k\]&lt;/span> &lt;span class="math">\[2:f(\vec x_k+\alpha\vec d_k)≥f(\vec x_k)+\alpha (1-c)\nabla f(\vec x_k)\vec d_k\]&lt;/span> 其中，&lt;span class="math">\(c ∈ (0,0.5)\)&lt;/span>&lt;/p>
&lt;p>在一定条件下，G-P法全局收敛。但当海森矩阵&lt;span class="math">\(H\)&lt;/span>非正定情况较多时，收敛速度降为接近线性（梯度法）。&lt;/p>
&lt;h3 id="负曲率方向法">负曲率方向法&lt;/h3>
&lt;p>在鞍点处，我们有梯度为0，同时H矩阵不定，G-P等修正牛顿法无法在鞍点处继续，需要额外的方式：采用&lt;strong>负曲率方向&lt;/strong>作为搜索方向，可使目标函数值下降。&lt;/p>
&lt;p>负曲率方向，即&lt;span class="math">\(d^T_k∇_2f(x_k)d_k&amp;lt;0\)&lt;/span>。这里涉及到负曲率方向的定义，在张贤达的《矩阵分析与应用》中说到，当矩阵H是非线性函数&lt;span class="math">\(f(x)\)&lt;/span>的Hessian矩阵时，称满足&lt;span class="math">\(p^HHp&amp;gt;0\)&lt;/span> 的向量&lt;span class="math">\(p\)&lt;/span>为函数&lt;span class="math">\(f\)&lt;/span>的正曲率方向，满足&lt;span class="math">\(p^HHp&amp;lt;0\)&lt;/span>的向量&lt;span class="math">\(p\)&lt;/span>为函数&lt;span class="math">\(f\)&lt;/span>的负曲率方向。标量&lt;span class="math">\(p^HHp\)&lt;/span>称为函数&lt;span class="math">\(f\)&lt;/span>沿着方向&lt;span class="math">\(p\)&lt;/span>的曲率。沿着负曲率方向进行一维搜索必能使目标函数值下降。&lt;/p>
&lt;p>因此，在鞍点时，我们只要找到任意负曲率方向，使函数值继续减小，就可以摆脱鞍点。&lt;/p>
&lt;h3 id="goldfeld修正">Goldfeld修正&lt;/h3>
&lt;p>与上面的Goldstein-Price修正的思路不同，Goldfeld在1966年也提出了一种方法，他的方法虽然还是在搜索方向&lt;span class="math">\(d_k\)&lt;/span> 上动手，但是当&lt;span class="math">\(H_k\)&lt;/span> 不正定时，他不是用最速下降方向&lt;span class="math">\(-g_k\)&lt;/span>来作为搜索方向，而是将&lt;span class="math">\(d_k\)&lt;/span>修正成下降方向——用下面的式子： &lt;span class="math">\[{d_k} = - B_k^{ - 1}{g_k}\]&lt;/span> 其中，&lt;span class="math">\({B_k} = {H_k} + {E_k}\)&lt;/span>是一个正定矩阵，&lt;span class="math">\(E_k\)&lt;/span>称为修正矩阵。在&lt;span class="math">\(E_k\)&lt;/span>满足一定条件的时候，（Goldfeld修正）牛顿法具有整体收敛性。&lt;/p>
&lt;p>接下来我们使用&lt;span class="math">\(B_k\)&lt;/span>表示经过各种修正后的海森矩阵。&lt;/p>
&lt;p>我们可以设&lt;span class="math">\(E_k=v_kI\)&lt;/span>，使得使得&lt;span class="math">\(H_k+v_kI\)&lt;/span>正定。比较理想的是，&lt;span class="math">\(v_k\)&lt;/span>为使&lt;span class="math">\(H_k+v_kI\)&lt;/span>正定的最小&lt;span class="math">\(v\)&lt;/span>。特征值的估计可以用盖尔圆盘定理或者Rayleigh商方法。&lt;/p>
&lt;h3 id="gill-murray改进cholesky分解法的稳定牛顿法">Gill-Murray改进Cholesky分解法的稳定牛顿法&lt;/h3>
&lt;p>前一个小节里说过加上一个&lt;span class="math">\(v_kI\)&lt;/span>使修正的Hessian矩阵正定，本节具体介绍一种高效稳定方法。Gill-Murray改进Cholesky分解法的稳定牛顿法可以说是&lt;strong>修正牛顿法中的集大成&lt;/strong>。基本思想是当&lt;span class="math">\(H_k\)&lt;/span>不定矩阵时，采用修改Cholesky分解强迫矩阵正定；当&lt;span class="math">\(g_k\)&lt;/span>趋于0时，采用负曲率方向使函数值下降。如果矩阵正定且负曲率方向不存在，则是最优解。其大体流程如下&lt;/p>
&lt;blockquote>
&lt;ol style="list-style-type: decimal">
&lt;li>给定初始点&lt;span class="math">\(x_0,\varepsilon&amp;gt;0,k=1\)&lt;/span>&lt;/li>
&lt;li>计算&lt;span class="math">\(g_k,H_k\)&lt;/span>&lt;/li>
&lt;li>对&lt;span class="math">\(H_k\)&lt;/span>进行修改cholesky分解强迫矩阵正定，&lt;span class="math">\(L_kD_kL_k^\ast=H_k+E_k\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(||g_k||&amp;gt;\varepsilon\)&lt;/span>（梯度不为0的计算机表达），则解方程&lt;span class="math">\(L_kD_kL_k^\ast d_k=-g_k\Rightarrow d_k=-(H_k+E_k)^{-1}g_k\)&lt;/span>，转步骤6&lt;/li>
&lt;li>若&lt;span class="math">\(||g_k||&amp;gt;\varepsilon\approx0\)&lt;/span>，构造负曲率方向，如果构造不出负曲率方向，&lt;strong>得到局部最优&lt;/strong>。&lt;/li>
&lt;li>一维搜 索步长，可用精确法或不精确准则得到步长&lt;span class="math">\(\alpha_k\)&lt;/span>，令&lt;span class="math">\(x_{k+1}=x_k+\alpha_k d_k\)&lt;/span>&lt;/li>
&lt;li>若第5步满足终止条件，则OK；否则&lt;span class="math">\(k=k+1\)&lt;/span>，转2.&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>可以证明该方法总体收敛。现在还有其中最重要的一步：&lt;strong>修正的Cholesky分解&lt;/strong>，请看笔记《海森矩阵校正》，更加详细的关于修正的Cholesky分解可见教程《Modified Cholesky Decomposition and Applications》&lt;/p>
&lt;h3 id="信赖域法">信赖域法&lt;/h3>
&lt;p>信赖域和line search同为最优化算法的基础算法，但是，从“Trust Region”这个名字你就可以看出，它是没有line search过程的，它是直接在一个region中“search”。 在一维搜索中，从&lt;span class="math">\(x_k\)&lt;/span>点移动到下一个点的过程，可以描述为：&lt;span class="math">\(x_k + α_k d_k\)&lt;/span>，此处&lt;span class="math">\(α_k d_k\)&lt;/span>就是在&lt;span class="math">\(d_k\)&lt;/span>方向上的位移，可以记&lt;span class="math">\(s_k\)&lt;/span>，总的来说，&lt;strong>线搜索把复杂问题改编成一系列简单的一维优化问题&lt;/strong>。而信赖域算法是根据一定的原则，直接确定位移&lt;span class="math">\(s_k\)&lt;/span>，同时，与一维搜索不同的是，它并没有先确定搜索方向&lt;span class="math">\(d_k\)&lt;/span>。如果根据“&lt;strong>某种原则&lt;/strong>”确定的位移能使目标函数值充分下降，则扩大信赖域；若不能使目标函数值充分下降，则缩小信赖域。如此迭代下去，直到收敛。总的来说，&lt;strong>信赖域方法是把复杂问题转化为一系列相对简单的局部优化问题&lt;/strong>。&lt;/p>
&lt;h4 id="信赖域算法的基本思想">信赖域算法的基本思想&lt;/h4>
&lt;p>在每次迭代中给出一个信赖域，这个信赖域一般是当前迭代点 的一个小邻域。然后，在这个邻域内求解一个子问题，得到&lt;strong>试探步长（trial step）&lt;span class="math">\(s_k\)&lt;/span>&lt;/strong>，接着用某一&lt;strong>评价函数&lt;/strong>来决定是否接受该试探步以及确定下一次迭代的信赖域。 如果试探步长被接受，则：&lt;span class="math">\(x_{k+1}=x_k+s_k\)&lt;/span>，否则，&lt;span class="math">\(x_{k+1}=x_k\)&lt;/span>。 新的信赖域的大小取决于试探步的好坏，&lt;em>粗略地说，如果试探步较好，在下一步信赖域扩大或者保持不变，否则减小信赖域&lt;/em>。&lt;/p>
&lt;h4 id="基本信赖域法">基本信赖域法&lt;/h4>
&lt;p>设当前点&lt;span class="math">\(x_k\)&lt;/span>的邻域定义为：&lt;span class="math">\(\Omega_k\)&lt;/span>。目标函数在极值点附近近似一个二次函数，因此对于无约束优化问题，利用二次逼近，构造如下信赖域子问题： 设当前点&lt;span class="math">\(x_k\)&lt;/span>的邻域定义为：&lt;span class="math">\(\Omega_k=\{x ∈ R^n|\Vert x-x_k\Vert≤\Delta_k\}\)&lt;/span>，其中，&lt;span class="math">\(\Delta_k\)&lt;/span>称为信赖域半径。 目标函数在极值点附近&lt;strong>近似一个二次函数&lt;/strong>(有点类似插值法)，因此对于无约束优化问题，利用二次逼近，构造如下&lt;strong>信赖域子问题&lt;/strong>： &lt;span class="math">\[\min q^{(k)}(s)=f(x_k)+g_k^Ts+\frac{1}{2}(s^TB_ks)\\
s.t. \quad \|s\|_2≤\Delta_k\]&lt;/span> 其中，&lt;span class="math">\(s=x-s_k\)&lt;/span>，&lt;span class="math">\(g_k\)&lt;/span>是目标函数&lt;span class="math">\(f(x)\)&lt;/span>在当前迭代点&lt;span class="math">\(x_k\)&lt;/span>处的梯度，&lt;span class="math">\(B_k ∈ R^{n*n}\)&lt;/span>对称，是&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(x_k\)&lt;/span>处Hessian矩阵或Hessian矩阵的近似。&lt;/p>
&lt;p>设&lt;span class="math">\(s_k\)&lt;/span>是信赖域子问题的解。目标函数&lt;span class="math">\(f(x)\)&lt;/span>在第k步的实际下降量（真实下降量）： &lt;span class="math">\[Ared_k=f(x_k)-f(x_k+s_k)\]&lt;/span> 二次模型函数&lt;span class="math">\(q^{(k)}(s)\)&lt;/span>的下降量（预测下降量）： &lt;span class="math">\[Pred_k=q^{(k)}(0)-q^{(k)}(s_k)&amp;gt;0\]&lt;/span> 定义比值： &lt;span class="math">\[r=\frac{Ared_k}{Pred_k}\]&lt;/span> 它衡量了二次模型与目标函数的逼近程度，&lt;span class="math">\(r_k\)&lt;/span>越接近于1，表明接近程度越好。因此，我们也用这个量来确定下次迭代的信赖域半径。&lt;/p>
&lt;ul>
&lt;li>越接近与1，表明接近程度越好。这时可以增大&lt;span class="math">\(\Delta_k\)&lt;/span>以扩大信赖域；&lt;/li>
&lt;li>&lt;span class="math">\(r_k&amp;gt;0\)&lt;/span>但是不接近于1，保持&lt;span class="math">\(\Delta_k\)&lt;/span>不变；&lt;/li>
&lt;li>如果&lt;span class="math">\(r_k\)&lt;/span>接近于0，减小&lt;span class="math">\(\Delta_k\)&lt;/span>，缩小信赖域。 &lt;a href="https://www.codelast.com/%E5%8E%9F%E5%88%9B%E4%BF%A1%E8%B5%96%E5%9F%9Ftrust-region%E7%AE%97%E6%B3%95%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/">https://www.codelast.com/%E5%8E%9F%E5%88%9B%E4%BF%A1%E8%B5%96%E5%9F%9Ftrust-region%E7%AE%97%E6%B3%95%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="levenbergmarquardt法">Levenberg–Marquardt法&lt;/h4>
&lt;p>&lt;a href="https://www.codelast.com/%e5%8e%9f%e5%88%9blm%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%9e%e7%8e%b0/">https://www.codelast.com/%e5%8e%9f%e5%88%9blm%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%9e%e7%8e%b0/&lt;/a>&lt;/p>
&lt;h4 id="dogleg法">Dogleg法&lt;/h4>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/99392484">https://zhuanlan.zhihu.com/p/99392484&lt;/a> &lt;a href="https://zhuanlan.zhihu.com/p/101124802">https://zhuanlan.zhihu.com/p/101124802&lt;/a>&lt;/p>
&lt;h2 id="拟牛顿法">拟牛顿法&lt;/h2>
&lt;p>拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。&lt;/p>
&lt;p>拟牛顿法的本质思想是&lt;strong>改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度&lt;/strong>。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。&lt;/p>
&lt;h3 id="dfp算法">DFP算法&lt;/h3>
&lt;h3 id="sr1算法">SR1算法&lt;/h3>
&lt;h3 id="bfgs算法">BFGS算法&lt;/h3>
&lt;h3 id="l-bfgs算法">L-BFGS算法&lt;/h3>
&lt;div class="figure">
&lt;img src="../../images/拟牛顿法的迭代公式.jpg" alt="拟牛顿法的迭代公式" />&lt;p class="caption">拟牛顿法的迭代公式&lt;/p>
&lt;/div>
&lt;h2 id="共轭梯度法">共轭梯度法&lt;/h2>
&lt;h2 id="总结">总结&lt;/h2>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>[1] &lt;a href="https://zhuanlan.zhihu.com/p/37588590">https://zhuanlan.zhihu.com/p/37588590&lt;/a>&lt;/p>
&lt;p>[2] &lt;a href="https://blog.csdn.net/itplus/article/details/21896619">https://blog.csdn.net/itplus/article/details/21896619&lt;/a>&lt;/p>
&lt;p>[3] &lt;a href="https://blog.csdn.net/itplus/article/details/21896453">https://blog.csdn.net/itplus/article/details/21896453&lt;/a>&lt;/p></description></item><item><title>优化理论之非线性方法-梯度法与牛顿法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95-%E6%A2%AF%E5%BA%A6%E6%B3%95%E4%B8%8E%E7%89%9B%E9%A1%BF%E6%B3%95/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95-%E6%A2%AF%E5%BA%A6%E6%B3%95%E4%B8%8E%E7%89%9B%E9%A1%BF%E6%B3%95/</guid><description>
&lt;h2 id="梯度下降法gradient-descent与牛顿法newtons-method求解最小值">梯度下降法(gradient descent)与牛顿法(newton’s method)求解最小值&lt;!-- omit in toc -->&lt;/h2>
&lt;p>梯度下降法与牛顿法是求解最小值/优化问题的两种经典算法。本文的目标是介绍两种算法的推导思路与流程，并且从初学者的角度就一些容易混淆的话题如：梯度下降法(gradient descent)与最速下降法(steepest descent)的联系与区别、牛顿求根迭代方法(Newton–Raphson method) 与牛顿法求解最小值算法的联系(来自 Andrew Ng 机器学习课程第四讲)进行说明。本文的内容将对高斯牛顿法(Gauss–Newton algorithm) ,Levenberg-Marquardt算法(LM算法)等非线性最小二乘问题解法起到引出作用。&lt;/p>
&lt;h2 id="梯度下降法">梯度下降法&lt;/h2>
&lt;p>已知多元函数&lt;span class="math">\(f(x_1,x_2,\dots,x_n)\)&lt;/span>在定义域上可微，如果将&lt;span class="math">\(f(\mathbf{x})\)&lt;/span>在&lt;span class="math">\(\mathbf{x}\)&lt;/span>处一阶泰勒展开(tayler expansion),可得到：(说明：为了编辑方便下文中统一以&lt;span class="math">\(x={\begin{bmatrix} x_1,x_2,\dots,x_n \end{bmatrix}}^T\)&lt;/span>代替&lt;span class="math">\(\mathbf{x}\)&lt;/span> )。 &lt;span class="math">\[f(x+\epsilon) = f(x)+\epsilon ^T\nabla_x f + O(||\epsilon||)\approx f(x)+\epsilon ^T\nabla_x f\]&lt;/span> 其中&lt;span class="math">\(\nabla_x f={\begin{bmatrix} \frac{\partial f}{\partial x_1},\dots,\frac{\partial f}{\partial x_n} \end{bmatrix}}^T\)&lt;/span>为&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(x\)&lt;/span>处的梯度向量。&lt;/p>
&lt;p>这个式子我们可以解读为当&lt;span class="math">\(x\)&lt;/span>增加&lt;span class="math">\(\epsilon\)&lt;/span>时，&lt;span class="math">\(f(x)\)&lt;/span>增加&lt;span class="math">\(\epsilon ^T\nabla_x f\)&lt;/span>，即&lt;span class="math">\(\epsilon\)&lt;/span>与梯度&lt;span class="math">\(\nabla_x f\)&lt;/span>的內积。如果我们限定&lt;span class="math">\(\epsilon\)&lt;/span>的模长为定值，其方向怎样才能获得&lt;span class="math">\(f(x+\epsilon)\)&lt;/span>的最小值呢？答案当然是与&lt;span class="math">\(\nabla_x f\)&lt;/span>方向相反的时候，此时&lt;span class="math">\(\epsilon ^T\nabla_x f\)&lt;/span>获得最小值。&lt;/p>
&lt;p>我们也可以利用直觉上较好理解的爬山的例子来解释梯度下降法。假设你位于山上某一点坐标为&lt;span class="math">\(\mathbf{\theta}(\theta_1,\theta_2)\)&lt;/span>，那么在此处(注意，是在这一点)下山最快的方向当然是沿着此处的梯度方向。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/steepest_descent_1.png" alt="steepest_descent_1.png" />&lt;p class="caption">steepest_descent_1.png&lt;/p>
&lt;/div>
&lt;p>所以说，将整个故事串起来，梯度下降法的思路可以总结如下：欲求多元函数&lt;span class="math">\(f(x)\)&lt;/span>的最小值，可以采用如下步骤：&lt;/p>
&lt;p>给定初始值&lt;span class="math">\(x_0\)&lt;/span>。 按照如下方式“下山”：&lt;span class="math">\(x_{i+1} = x_i-\eta\nabla_{x_i} f\)&lt;/span>。其中&lt;span class="math">\(\eta&amp;gt;0\)&lt;/span>，在机器学习领域，&lt;span class="math">\(\eta\)&lt;/span>也被称之为学习率(learning rate)。 直到&lt;span class="math">\(x\)&lt;/span>满足收敛条件为止。如&lt;span class="math">\(\|f(x_{i+1}) – f(x_i)\|&amp;lt;\epsilon\)&lt;/span>或&lt;span class="math">\(||\nabla_{x_i}f||\approx 0\)&lt;/span>。&lt;/p>
&lt;p>学习率的重要性：&lt;/p>
&lt;p>学习率作为控制下降步长的参数，影响函数下降的速度。学习率是我们根据经验确定的一个参数，因此在机器学习领域中这样的参数也被成为超参数(hyperparameter)。学习率的选取不能过大或者过小，如下图，不同的学习率导致函数不同的收敛速度，甚至可能导致函数不收敛。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/different_learning_rate.png" alt="different_learning_rate.png" />&lt;p class="caption">different_learning_rate.png&lt;/p>
&lt;/div>
&lt;h3 id="梯度下降法的优势">梯度下降法的优势&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>时间复杂度低，在每一个迭代中，只需要计算梯度，不需要对二阶导数矩阵（即海森矩阵(Hessian Matrix)）进行计算。&lt;/li>
&lt;li>空间复杂度低，因为梯度向量为一个&lt;span class="math">\(n\times 1\)&lt;/span>的向量，比起Hessian Matrix来，占用存储空间小n倍。在实际应用中，&lt;span class="math">\(\mathbf{x}\)&lt;/span>的维度可能非常高。&lt;/li>
&lt;/ol>
&lt;h3 id="梯度下降法的局限">梯度下降法的局限&lt;/h3>
&lt;p>对于部分求解函数，梯度下降法可能会出现下降非常缓慢的情形。其收敛速度也较其他方法低（其他文献分析其收敛速度为线性，本文不作推导）。如下图，梯度下降法的路径出现了z字型。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/Banana-SteepDesc.gif" alt="Banana-SteepDesc.gif" />&lt;p class="caption">Banana-SteepDesc.gif&lt;/p>
&lt;/div>
&lt;p>究其原因，我认为，某一点的梯度只能作为这一点的一个极小的领域处的最快下降方向，一旦梯度变化较快，梯度下降法会出现因为学习率不合适而出现”zigzag”现象。而且，如果我们将梯度下降法与下文的牛顿法做对比，你会发现，一直沿着梯度方向下降的速度不一定是最快的。如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/Newton_optimization_vs_grad_descent.png" alt="Newton_optimization_vs_grad_descent" />&lt;p class="caption">Newton_optimization_vs_grad_descent&lt;/p>
&lt;/div>
&lt;h3 id="最速下降法steepest-decent-与-梯度下降法gradient-descent的联系">最速下降法(steepest decent) 与 梯度下降法(gradient descent)的联系&lt;/h3>
&lt;p>总结一下就是 梯度下降法是最速下降法的一种特例。在最速下降法中，对于某一范数下&lt;span class="math">\(\epsilon\)&lt;/span>的取值根据以下原则：&lt;/p>
&lt;p>&lt;span class="math">\[\bigtriangleup \epsilon_{nsd}=argmin_v(\nabla f(x)^T\epsilon\mid \|\epsilon\|=1)\]&lt;/span>&lt;/p>
&lt;p>当我们指定的范数为欧几里得范数时，最速下降法给出的下降方向就是梯度的负方向，即梯度下降法给出的方向。&lt;/p>
&lt;p>在wikipedia中说明，梯度下降法也被称为最速下降法(Gradient descent is also known as steepest descent )。&lt;/p>
&lt;h2 id="牛顿法">牛顿法&lt;/h2>
&lt;p>如同根据一阶泰勒展开推导出梯度下降法一样，根据二阶泰勒展开可以推导出牛顿优化法(newton’s method in optimization)。将&lt;span class="math">\(f(\mathbf{x})\)&lt;/span>在&lt;span class="math">\(\mathbf{x}\)&lt;/span>处一阶泰勒展开(tayler expansion),可得到： &lt;span class="math">\[f(x+\epsilon) = f(x)+\epsilon ^T\nabla_x f + \frac{1}{2}\epsilon ^TH\epsilon +O(||\epsilon||^2)\\
\approx f(x)+\epsilon ^T\nabla_x f + \frac{1}{2}\epsilon ^TH\epsilon\]&lt;/span>&lt;/p>
&lt;p>如果我们将&lt;span class="math">\(x\)&lt;/span>看做固定的已知量，将&lt;span class="math">\(f\)&lt;/span>看做关于&lt;span class="math">\(\epsilon\)&lt;/span>的函数，那么欲求&lt;span class="math">\(f(\epsilon | x)\)&lt;/span>的最小值，必要条件(注意：不是充要条件)是&lt;span class="math">\(\frac{\partial f}{\partial \epsilon}=0\)&lt;/span>其中 &lt;span class="math">\[H = \begin{bmatrix}\frac{\partial^2f}{\partial x_1 \partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial^2f}{\partial x_1 \partial x_n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{\partial^2f}{\partial x_n \partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial^2f}{\partial x_n \partial x_n}\end{bmatrix}\]&lt;/span> 称之为&lt;span class="math">\(f\)&lt;/span>的&lt;span class="math">\(Hessian\)&lt;/span>矩阵。因为二阶连续混合编导数具备性质 &lt;span class="math">\[\frac{\partial^2f}{\partial x_i \partial x_j} = \frac{\partial^2f}{\partial x_j \partial x_i}\]&lt;/span> 因此&lt;span class="math">\(Hessian\)&lt;/span>矩阵为对称矩阵。根据矩阵求导法则，可以得到 &lt;span class="math">\[\frac{\partial f}{\partial \epsilon}={\nabla_x f}^T+\epsilon^TH=0\\
\epsilon = -H^{-1}{\nabla_x f}\]&lt;/span>&lt;/p>
&lt;p>可见，牛顿法的思路是将函数f在x处展开为多元二次函数，再通过求解二次函数最小值的方法得到本次迭代的下降方向&lt;span class="math">\(\epsilon\)&lt;/span>。那么问题来了，多元二次函数在梯度为0的地方一定存在最小值么？直觉告诉我们是不一定的。以 一元二次函数&lt;span class="math">\(g(x)=ax^2+bx+c\)&lt;/span>为例，我们知道当&lt;span class="math">\(a&amp;gt;0\)&lt;/span>时，&lt;span class="math">\(g(x)\)&lt;/span>可以取得最小值，否则&lt;span class="math">\(g(x)\)&lt;/span>不存在最小值。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/shape-of-the-graph.png" alt="shape-of-the-graph.png" />&lt;p class="caption">shape-of-the-graph.png&lt;/p>
&lt;/div>
&lt;p>推广到多元的情况，可以得出二次项矩阵必须是正定(positive definite)的，对应上式即&lt;span class="math">\(Hessian\)&lt;/span>为正定矩阵时，函数&lt;span class="math">\(f(\epsilon | x)\)&lt;/span>的最小值才存在。&lt;/p>
&lt;p>因此，牛顿法首先需要计算&lt;span class="math">\(Hessian\)&lt;/span>矩阵并且判断其正定性，当&lt;span class="math">\(Hessian\)&lt;/span>矩阵正定，此时其所有特征值均&amp;gt;0,当然&lt;span class="math">\(Hessian\)&lt;/span>矩阵也是可逆的，最小值存在。&lt;/p>
&lt;p>需要指出的是，当多元函数 f 本身就是二次函数并且存在最小值时，牛顿法可以一步解出最小值。&lt;/p>
&lt;h3 id="牛顿法的优点">牛顿法的优点&lt;/h3>
&lt;p>因为目标函数在接近极小值点附近接近二次函数，因此在极小值点附近，牛顿法的收敛速度较梯度下降法快的多。其他文献分析其收敛速度为2次收敛，本文不给出推导。下图是牛顿法应用在Rosenbrock函数上的效果：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/Rosenbrock_newton.png" alt="Rosenbrock_newton.png" />&lt;p class="caption">Rosenbrock_newton.png&lt;/p>
&lt;/div>
&lt;h3 id="牛顿法的缺点">牛顿法的缺点&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(Hessian\)&lt;/span>矩阵的计算难度非常的大。因此在高维度应用案例中，通常不会计算&lt;span class="math">\(Hessian\)&lt;/span>矩阵。因此牛顿法也产生了很多变种，主要的思想就是采用其他矩阵近似&lt;span class="math">\(Hessian\)&lt;/span>矩阵，降低计算复杂度。&lt;/li>
&lt;li>牛顿法当&lt;span class="math">\(Hessian\)&lt;/span>矩阵为正定矩阵时，最小值才存在。牛顿法经常会因为&lt;span class="math">\(Hessian\)&lt;/span>矩阵不正定而发散(diverage)。因此 牛顿法并不是非常的稳定。&lt;/li>
&lt;/ol>
&lt;h3 id="牛顿法求根公式与牛顿优化法之间的联系">牛顿法求根公式与牛顿优化法之间的联系&lt;/h3>
&lt;p>在说道牛顿优化方法的时候，上过《计算方法》这门课的同学经常会说，牛顿法不是用来求根的么？实际上，牛顿优化法还真可以用牛顿求根法推导得出。我看到的材料是 Andrew Ng在《机器学习课程》中给出的一种推导。在牛顿求根公式中，&lt;span class="math">\(f(x)=0\)&lt;/span>的解由迭代式 &lt;span class="math">\[x_{i+1}=x_{i}-\frac{f(x_i)}{f\prime(x_i)}\]&lt;/span> 给出。在牛顿优化法中，我们欲求得梯度&lt;span class="math">\(g(x)=f&amp;#39;(x)=0\)&lt;/span>对应的&lt;span class="math">\(x\)&lt;/span>。&lt;/p>
&lt;p>因此&lt;span class="math">\(x\)&lt;/span>可以根据求根公式 &lt;span class="math">\[x_{i+1}=x_{i}-\frac{f\prime(x_i)}{f\prime\prime(x_i)}\]&lt;/span> 求出。推广到多元函数上，&lt;span class="math">\({1}/{f\prime\prime(x_i)}\)&lt;/span>演变为&lt;span class="math">\(H^{-1}\)&lt;/span>，&lt;span class="math">\(f\prime(x_i)\)&lt;/span>演变为&lt;span class="math">\(\nabla_x f(x_i)\)&lt;/span>因此 &lt;span class="math">\[x_{i+1}=x_{i}-H^{-1}{\nabla_x f(x_i)}\]&lt;/span> 与根据二阶泰勒展开并求&lt;span class="math">\(f(\epsilon | x)\)&lt;/span>的最小值得到的结论一致。&lt;/p>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;a href="https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0">gradient descent in a nutshell – towardsdatascience.com&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s method in Optimization-wikipedia&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://people.rit.edu/lxwast/Linwei_Wangs_HomePage/CIS820_files/C3_GD.pdf">Gradient Descent Method – Rochester Institute of Technology&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml2-2008/graddescent.pdf">Using Gradient Descent in Optimization and Learning – University Collage London&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://math.stackexchange.com/questions/1659452/difference-between-gradient-descent-method-and-steepest-descent">Difference between Gradient Descent method and Steepest Descent – stack exchange&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.quora.com/In-optimization-why-is-Newtons-method-much-faster-than-gradient-descent">In optimization, why is Newton’s method much faster than gradient descent?&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>优化理论之非线性方法-高斯牛顿法与莱文贝格-马夸特方法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95-%E9%AB%98%E6%96%AF%E7%89%9B%E9%A1%BF%E6%B3%95%E4%B8%8E%E8%8E%B1%E6%96%87%E8%B4%9D%E6%A0%BC-%E9%A9%AC%E5%A4%B8%E7%89%B9%E6%96%B9%E6%B3%95/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95-%E9%AB%98%E6%96%AF%E7%89%9B%E9%A1%BF%E6%B3%95%E4%B8%8E%E8%8E%B1%E6%96%87%E8%B4%9D%E6%A0%BC-%E9%A9%AC%E5%A4%B8%E7%89%B9%E6%96%B9%E6%B3%95/</guid><description>
&lt;h2 id="高斯-牛顿法guass-newton-algorithm与莱文贝格-马夸特方法levenbergmarquardt-algorithm求解非线性最小二乘问题">高斯-牛顿法(Guass-Newton Algorithm)与莱文贝格-马夸特方法(Levenberg–Marquardt algorithm)求解非线性最小二乘问题&lt;!-- omit in toc -->&lt;/h2>
&lt;p>众所周知，最小二乘法通过最小化误差平方和获得最佳函数。有时候你可能产生疑问，为什么不能通过其他方式获得最优函数，比如说最小化误差的绝对值的和？本文中，我将会从概率的角度解释最小二乘法的依据（参考自andrew ng 《机器学习课程》 第三讲）。最小二乘问题可以分为线性最小二乘和非线性最小二乘两类，本文的目标是介绍两种经典的最小二乘问题解法：高斯牛顿法与莱文贝格-马夸特方法。实际上，后者是对前者以及梯度下降法的综合。&lt;/p>
&lt;h2 id="最小二乘法的概率解释probabilistic-interpretation">最小二乘法的概率解释(probabilistic interpretation)&lt;/h2>
&lt;p>以线性回归为例，假设最佳函数为 &lt;span class="math">\(y=\mathbf{\theta}^T\mathbf{x}\)&lt;/span> (&lt;span class="math">\(\theta,x\)&lt;/span>为向量), 对于每对观测结果&lt;span class="math">\((x^{(i)},y^{(i)})\)&lt;/span>，都有 &lt;span class="math">\[y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}\]&lt;/span> 其中 &lt;span class="math">\(\epsilon\)&lt;/span>为误差，基于一种合理的假设（中心极限定理），我们可以认为误差的分布服从正态分布(又称高斯分布)，即 &lt;span class="math">\(\epsilon \sim N(0,\sigma^2)\)&lt;/span> ，那么，我们可以认为&lt;span class="math">\(y^{(i)} \sim N(\theta^Tx^{(i)},\sigma^2)\)&lt;/span>,根据正态分布的概率公式 &lt;span class="math">\[P(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\]&lt;/span> 在统计学中，将所有的&lt;span class="math">\(P(y|x)\)&lt;/span>累乘作为&lt;span class="math">\(\theta\)&lt;/span>的似然函数,用以衡量&lt;span class="math">\(\theta\)&lt;/span>的或然性(likelihood) &lt;span class="math">\[L(\theta)=P(y|x;\theta)=\prod _{i=0}^m \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\]&lt;/span> 最佳的参数&lt;span class="math">\(\theta\)&lt;/span>应该是使得所有数据出现的概率最大的那个，这个过程称之为最大似然估计(maximum likelihood estimation)。为了数学计算上的便利，采用单调函数:log函数&lt;span class="math">\(l(\theta)=log(L(\theta))\)&lt;/span>，称为对数似然函数代表&lt;span class="math">\(L(\theta)\)&lt;/span>: &lt;span class="math">\[l(\theta)=log\prod_{i=0}^m \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\ =\sum_{i=0}^mlog\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\=mlog\frac{1}{\sqrt{2\pi}\sigma} – \frac{1}{2\sigma^2}\sum_{i=0}^m(y^{(i)}-\theta^Tx^{(i)})^2\]&lt;/span> 需要指出的是，在Andrew ng的讲解认为&lt;span class="math">\(\sigma\)&lt;/span>不影响&lt;span class="math">\(\theta\)&lt;/span>的决定，因此，最大化&lt;span class="math">\(l(\theta)\)&lt;/span>等同于最小化&lt;span class="math">\(\sum_{i=0}^m(y^{(i)}-\theta^Tx^{(i)})^2\)&lt;/span>。最小二乘法通过最小化误差平方和得到最佳函数的方法存在概率论方面的基础。&lt;/p>
&lt;h2 id="高斯-牛顿法guass-newton-algorithm">高斯-牛顿法(Guass-Newton Algorithm)&lt;/h2>
&lt;p>高斯-牛顿法是在牛顿法基础上进行修改得到的，用来(仅用于)解决非线性最小二乘问题。高斯-牛顿法相较牛顿法的最大优点是不需要计算二阶导数矩阵(Hessian矩阵)，当然，这项好处的代价是其仅适用于最小二乘问题。如下是其推导过程：&lt;/p>
&lt;p>最小二乘方法的目标是令残差的平方和最小： &lt;span class="math">\[f(\theta) = \frac{1}{2}\sum_{i=0}^mr(\mathbf{x_i})^2\]&lt;/span> 采用牛顿法求解&lt;span class="math">\(f(\theta)\)&lt;/span>的最小值，需要计算其梯度向量与Hessian矩阵。 &lt;span class="math">\[\nabla_\theta f =\frac{\partial f}{\partial \theta}=\sum r_i\frac{\partial r_i}{\partial \theta}\\
=\begin{bmatrix}
r(x_1)&amp;amp;r(x_2)&amp;amp;\dots&amp;amp;r(r_m)\end{bmatrix} \begin{bmatrix}
\nabla_\theta r(x_1)^T \\
\nabla_\theta r(x_2)^T \\
\vdots\\
\nabla_\theta r(x_m)^T \\
\end{bmatrix}\]&lt;/span>&lt;/p>
&lt;p>其中 &lt;span class="math">\[J_r(\theta)=\begin{bmatrix}\frac{\partial r_j}{\partial\theta_i}\end{bmatrix}_{j=1,\dots,m;i=1,\dots,n}=\begin{bmatrix}
\nabla_\theta r(x_1)^T \\
\nabla_\theta r(x_2)^T \\
\vdots\\
\nabla_\theta r(x_m)^T \\\
\end{bmatrix}\]&lt;/span> 称为&lt;span class="math">\(r\)&lt;/span>的雅各比(Jacobian)矩阵。因此上式可以写作 &lt;span class="math">\[\nabla_\theta f = r^TJ_r=J_r^Tr\]&lt;/span> 其中&lt;span class="math">\(r=\begin{bmatrix}r(x_1)&amp;amp;r(x_2)&amp;amp;\dots&amp;amp;r_m\end{bmatrix}^T\)&lt;/span>。再看Hessian矩阵的计算： &lt;span class="math">\[H = \begin{bmatrix}\frac{\partial^2f}{\partial \theta^2} \end{bmatrix}=\sum\begin{bmatrix}r_i\frac{\partial^2r_i}{\partial\theta^2}+(\frac{\partial r_i}{\partial \theta})(\frac{\partial r_i}{\partial \theta})^T \end{bmatrix}\]&lt;/span> 观察二阶导数项&lt;span class="math">\(r_i\frac{\partial^2r_i}{\partial\theta^2}\)&lt;/span>，因为残差 &lt;span class="math">\(r_i\approx 0\)&lt;/span>,因此我们可以认为此项接近于0而舍去。所以Hessian矩阵可以近似写成： &lt;span class="math">\[H\approx\sum\begin{bmatrix}(\frac{\partial r_i}{\partial\theta})(\frac{\partial r_i}{\partial\theta})^T\end{bmatrix}=J_r^TJ_r\]&lt;/span> 这里我们可以看到高斯-牛顿法相对于牛顿法的不同就是在于采用了近似的Hessian矩阵降低了计算的难度，但是同时，舍去项仅适用于最小二乘问题中残差较小的情形。&lt;/p>
&lt;p>将梯度向量，Hessian矩阵(近似)带入牛顿法求根公式，得到高斯-牛顿法的迭代式： &lt;span class="math">\[\theta_i = \theta_{i-1}-{(J_r^TJ_r)}^{-1}J_r^Tr\]&lt;/span> 只需要计算出&lt;span class="math">\(m\times n\)&lt;/span>的Jacobian矩阵便可以进行高斯-牛顿法的迭代，计算已经算是非常简便的了。&lt;/p>
&lt;h2 id="levenberg-marquart-算法">Levenberg-Marquart 算法&lt;/h2>
&lt;p>与牛顿法一样，当初始值距离最小值较远时，高斯-牛顿法的并不能保证收敛。并且当&lt;span class="math">\(J_r^TJ_r\)&lt;/span>近似奇异的时候，高斯牛顿法也不能正确收敛。Levenberg-Marquart 算法是对上述缺点的改进。L-M方法是对梯度下降法与高斯-牛顿法进行线性组合以充分利用两种算法的优势。通过在Hessian矩阵中加入阻尼系数&lt;span class="math">\(\lambda\)&lt;/span>来控制每一步迭代的步长以及方向： &lt;span class="math">\[(H+\lambda I)\epsilon = -J_r^Tr\]&lt;/span>&lt;/p>
&lt;ul>
&lt;li>当&lt;span class="math">\(\lambda\)&lt;/span>增大时，&lt;span class="math">\(H+\lambda I\)&lt;/span>趋向于&lt;span class="math">\(\lambda I\)&lt;/span>，因此&lt;span class="math">\(\epsilon\)&lt;/span>趋向于 &lt;span class="math">\(-\lambda J_r^Tr\)&lt;/span>，也就是梯度下降法给出的迭代方向；&lt;/li>
&lt;li>当&lt;span class="math">\(\lambda\)&lt;/span>减小时，&lt;span class="math">\(H+\lambda I\)&lt;/span>趋向于&lt;span class="math">\(H\)&lt;/span>，&lt;span class="math">\(\epsilon\)&lt;/span>趋向于&lt;span class="math">\(-H^{-1}J_r^Tr\)&lt;/span>，也就是高斯-牛顿法给出的方向。&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="math">\(\lambda\)&lt;/span>的大小通过如下规则调节，也就是L-M算法的流程：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/L-Mprocessure.png" alt="L-Mprocessure.png" />&lt;p class="caption">L-Mprocessure.png&lt;/p>
&lt;/div>
&lt;ol style="list-style-type: decimal">
&lt;li>初始化 &lt;span class="math">\(\theta_0\)&lt;/span>，&lt;span class="math">\(\lambda_0\)&lt;/span>。&lt;/li>
&lt;li>计算当前点&lt;span class="math">\(\theta_i\)&lt;/span>处的残差向量&lt;span class="math">\(r_i\)&lt;/span>与雅各比矩阵&lt;span class="math">\(J_r\)&lt;/span>。&lt;/li>
&lt;li>通过求解&lt;span class="math">\((H_i+\lambda I)\epsilon = -J_r^Tr_i\)&lt;/span>求解迭代方向&lt;span class="math">\(\epsilon\)&lt;/span>。&lt;/li>
&lt;li>计算&lt;span class="math">\(\theta_i^\prime=\theta_i+\epsilon\)&lt;/span>点处的残差向量&lt;span class="math">\(r_i^\prime\)&lt;/span>。&lt;/li>
&lt;li>如果&lt;span class="math">\(\|r_i\prime\|^2&amp;gt;\|r_i\|^2\)&lt;/span>?,即残差没有下降，则更新&lt;span class="math">\(\lambda = \beta\lambda\)&lt;/span>，增大&lt;span class="math">\(\lambda\)&lt;/span>重新回到第三步重新求解新的&lt;span class="math">\(\epsilon\)&lt;/span>。如果残差下降，则更新&lt;span class="math">\(\theta_{i+1} = \theta_i+\epsilon\)&lt;/span> ，到第二步，并且降低&lt;span class="math">\(\lambda=\alpha\lambda\)&lt;/span>，增大迭代步长。&lt;/li>
&lt;/ol>
&lt;p>在曲线拟合实践中，&lt;span class="math">\(\alpha\)&lt;/span>通常选取 0.1，&lt;span class="math">\(\beta\)&lt;/span>选取10。&lt;/p>
&lt;p>相比于高斯-牛顿法，L-M算法的优势在于非常的鲁棒，很多情况下即使初始值距离(局部)最优解非常远，仍然可以保证求解成功。作为一种阻尼最小二乘解法，LMA(Levenberg-Marquart Algorithm)的收敛速度要稍微低于GNA(Guass-Newton Algorithm)。L-M算法作为求解非线性最小二乘问题最流行的算法广泛被各类软件包实现，例如google用于求解优化问题的库Ceres Solver。后续，我会通过最小二乘圆拟合的案例给出L-M算法的实现细节。&lt;/p>
&lt;h2 id="最小二乘法的改进">最小二乘法的改进&lt;/h2>
&lt;p>传统的最小二乘法是针对线性函数与线性场景，典型的场景如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/Linear_regression.svg" alt="Linear_regression.svg" />&lt;p class="caption">Linear_regression.svg&lt;/p>
&lt;/div>
&lt;p>其需要优化的方程是： &lt;span class="math">\[
\min\|X\theta-y\|^2
\]&lt;/span> 其中，&lt;span class="math">\(\theta\)&lt;/span>表示参数向量，参数的个数与特征的个数一样（算上常数项的），&lt;span class="math">\(X\)&lt;/span>表示&lt;span class="math">\(m\)&lt;/span>个观察到的数据，每个数据有&lt;span class="math">\(n\)&lt;/span>个特征（算上常数项的），因此&lt;span class="math">\(X\)&lt;/span>是一个&lt;span class="math">\(m\times n\)&lt;/span>维矩阵。&lt;span class="math">\(y\)&lt;/span>是实际观察到的结果向量。详细表示起来则是： &lt;span class="math">\[
X=\begin{bmatrix}
——(x^{(1)})^T——\\
——(x^{(2)})^T——\\
\vdots\\
——(x^{(m)})^T——\\
\end{bmatrix},
x^{(i)}=\begin{bmatrix}
x^{(i)}_0(=1)\\
x^{(i)}_1\\
\vdots\\
x^{(i)}_{n-1}\\
\end{bmatrix},
\theta=\begin{bmatrix}
\theta_0\\
\theta_1\\
\vdots\\
\theta_{n-1}\\
\end{bmatrix},
y=\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_m\\
\end{bmatrix}
\]&lt;/span> 根据矩阵计算结果： &lt;span class="math">\[
\theta=(X^TX)^{-1}X^Ty
\]&lt;/span> 显然，&lt;span class="math">\(X\theta\)&lt;/span>只能表示直线结果，对于曲线结果无能为力。针对曲线场景，通常有两种改进思路，第一个就是将特征&lt;span class="math">\(x_j\)&lt;/span>的高阶项加入其中，举个简单例子，比如除了常数项，只有一个特征的场景： &lt;span class="math">\[
y=\theta_0+\theta_0 x_1+\theta_2 x^2_1+\theta_3 x^3_1
\]&lt;/span> 这样通过高阶多项式就可以拟合曲线，且根据拉格朗日插值法，只有阶数够多，那么就能够很好的拟合任意曲线。然而，高阶项数都是超参数，而且如果让每个特征都有高阶参数，那么参数数量就是几何数量级增长，难以控制参数规模。比如，有两个特征时，引入二次项就会有&lt;span class="math">\(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2+\theta_5x_1x_2\)&lt;/span>共6项，如果到三次方项，那会有更多参数。我们知道，当参数少时会有欠拟合现象，偏差大；参数过多就是过拟合，方差大，因此引入高次方项后，情况变得难以控制。同时，高次函数拟合会出现龙格现象。&lt;/p>
&lt;blockquote>
&lt;p>在数值分析领域中，龙格现象是在一组等间插值点上使用具有高次多项式的多项式插值时出现的区间边缘处的振荡问题。&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../../images/曲线拟合.png" alt="曲线拟合" />&lt;p class="caption">曲线拟合&lt;/p>
&lt;/div>
&lt;center>
左图：一阶函数欠拟合；中图：二阶函数适当拟合；右图：5阶函数过拟合
&lt;/center>
&lt;p>第二个改进思路就是使用&lt;strong>局部加权线性回归算法&lt;/strong>(Locally Weighted Linear Regression, LWLR)。它的核心思想就是弯曲点附近的局部曲线，接近整体曲线。我们知道传统的线性规划的代价函数为 &lt;span class="math">\[\sum_i (y^{(i)} − θ^T x^{(i)})^2\]&lt;/span> 而局部加权线性回归算法需要体现加权特性，它在不同线性函数之间进行加权，代价函数修改为： &lt;span class="math">\[\sum_i w^{(i)}(y^{(i)} − θ^T x^{(i)})^2\]&lt;/span> 这里&lt;span class="math">\(w^{(i)}\)&lt;/span>是非负的加权值，可以令&lt;span class="math">\(w^{(i)}=\exp{(-\frac{(x^{(i)}-x)^2}{2\tau^2})}\)&lt;/span>。虽然&lt;span class="math">\(w^{(i)}\)&lt;/span>有高斯分布形式，但是和高斯分布没有直接联系。显然，当&lt;span class="math">\(|x^{(i)}-x|\)&lt;/span>比较小时，&lt;span class="math">\(|x^{(i)}-x|\rightarrow 0\)&lt;/span>，此时&lt;span class="math">\(w^{(i)}\rightarrow 1\)&lt;/span>；当&lt;span class="math">\(|x^{(i)}-x|\)&lt;/span>比较大时，&lt;span class="math">\(w^{(i)}\rightarrow 0\)&lt;/span>，权重较小。&lt;/p>
&lt;p>局部加权线性回归的优化函数写成矩阵形式为： &lt;span class="math">\[
J(\theta)=(X\theta-y)^TW(X\theta-y)
\]&lt;/span> 多了一个权重矩阵&lt;span class="math">\(W\)&lt;/span>，它是一个对角矩阵，对角元素&lt;span class="math">\(w_{ii}={1\over 2}w^{(i)}\)&lt;/span>。根据矩阵求导公式可得： &lt;span class="math">\[
\begin{aligned}
\nabla J(\theta)&amp;amp;=\nabla (X\theta-y)^TW(X\theta-y)\\
&amp;amp;=2\frac{\partial{(X\theta-y)}}{\partial{\theta}}W(X\theta-y)\\
&amp;amp;=2X^TW(X\theta-y)
\end{aligned}
\]&lt;/span> 当取极值时，&lt;span class="math">\(\nabla J(\theta)=0\)&lt;/span>，即&lt;span class="math">\(2X^TW(X\theta-y)=0\)&lt;/span>，所以有： &lt;span class="math">\[
2X^TW(X\theta-y)=0\\
X^TWX\theta=X^TWy\\
\theta=(X^TWX)^{-1}X^TWy
\]&lt;/span> 关于矩阵求导的细节，请查阅网页资料&lt;a href="../../网页资料/线性代数与矩阵-矩阵求导表.html">线性代数与矩阵-矩阵求导表&lt;/a>&lt;/p>
&lt;p>此外，整个代价函数中只有一个超参数&lt;span class="math">\(\tau\)&lt;/span>，称为&lt;strong>带宽参数&lt;/strong>。它控制着加权参数随着对&lt;span class="math">\(x\)&lt;/span>的远离，衰减的速度。当&lt;span class="math">\(\tau\)&lt;/span>值比较大时，&lt;span class="math">\(w^{(i)}\)&lt;/span>衰减得慢，容易出现欠拟合；当&lt;span class="math">\(\tau\)&lt;/span>值比较小时，&lt;span class="math">\(w^{(i)}\)&lt;/span>衰减得快，容易出现过拟合。从词义上来看，带宽参数意味着能够容纳周围参数的多少，容纳的越多函数越平缓，当&lt;span class="math">\(\tau\rightarrow \infty\)&lt;/span>时，&lt;span class="math">\(w^{(i)}\rightarrow 1\)&lt;/span>此时，带宽无限大，容纳了所有数据点，局部加权线性回归算法退化为传统线性规划算法。当&lt;span class="math">\(\tau\)&lt;/span>很小时，&lt;span class="math">\(w^{(i)}\)&lt;/span>衰减很快，只有在当前点才有效，此时局部加权线性回归算法退化为插值算法。这个算法的问题在于，对于每一个要预测的点，都要重新依据整个数据集计算一个线性回归模型出来，使得算法代价极高。局部加权线性回归效果如下&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/局部加权线性规划.png" alt="局部加权线性规划.png" />&lt;p class="caption">局部加权线性规划.png&lt;/p>
&lt;/div>
&lt;center>
上图：带宽参数过大，欠拟合；中图：适当拟合；下图：带宽参数过小，过拟合
&lt;/center>
&lt;p>在局部加权线性回归算法中，参数数量&lt;span class="math">\(w^{(i)}\)&lt;/span>是随着数据量增加而线性增加的，这个参数不固定的监督学习算法成为&lt;strong>非参数学习算法&lt;/strong>；相对的参数固定的监督学习算法，比如传统的线性规划就称作&lt;strong>参数学习算法&lt;/strong>。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;a href="http://ece.eng.umanitoba.ca/undergraduate/ECE4850T02/Lecture%20Slides/MLRegression.pdf">maximum likelihood regression-university of manitoba&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://open.163.com/special/opencourse/machinelearning.html">过拟合与欠拟合-网易公开课：斯坦福大学机器学习课程&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml2-2008/graddescent.pdf">Using Gradient Descent for Optimization and Learning&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mads.lanl.gov/presentations/Leif_LM_presentation_m.pdf">Numerical Optimization using the Levenberg-Marquardt Algorithm-Los Alamos National Laboratory&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://people.cas.uab.edu/~mosya/cl/">Circular and Linear Regression Fitting Circles and Lines by Least Squares – Nikolai Chernov – UAB&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>优化理论之对偶单纯形法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%AF%B9%E5%81%B6%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%AF%B9%E5%81%B6%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</guid><description>
&lt;h2 id="对偶单纯行法">对偶单纯行法&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#前期准备">前期准备&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶单纯形法的基本思想">对偶单纯形法的基本思想&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="前期准备">前期准备&lt;/h2>
&lt;p>线性规划标准形式 &lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\sum_{j=1}^n c_jx_j\\
\mathop{s.t.}\; &amp;amp;\sum_{j=1}^n \alpha_{ij}x_j=b_i,\ &amp;amp;i=1,\dotsb,m\\
&amp;amp;x_j\geq 0,\ &amp;amp;j=1,\dotsb,n\\
\end{aligned}
\]&lt;/span> 用矩阵表示为： &lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\boldsymbol c^T \boldsymbol x\\
\mathop{s.t.}\; &amp;amp;\boldsymbol A \boldsymbol x = \boldsymbol b\\
&amp;amp;\boldsymbol x \geq \boldsymbol 0\\
\end{aligned}\tag{2.1}
\]&lt;/span>&lt;/p>
&lt;h2 id="对偶单纯形法的基本思想">对偶单纯形法的基本思想&lt;/h2>
&lt;p>前面用单纯形法求解问题&lt;span class="math">\((2.1)\)&lt;/span>时，往往需要引进人工变量，通过解一阶段问题求初始基本可行解。现在利用&lt;strong>对偶性质&lt;/strong>给出一种&lt;strong>不需引进人工变量&lt;/strong>的求解方法，这就是&lt;strong>对偶单纯形法&lt;/strong>。为介绍这种方法的基本思想，先引入&lt;strong>对偶可行的基本解&lt;/strong>的概念。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>对偶可行的基本解&lt;/strong>:设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>是&lt;span class="math">\((2.1)\)&lt;/span>式的一个基本解（不一定是可行解），它对应的基矩阵为&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>，若其对应的单纯性乘子&lt;span class="math">\(\boldsymbol{w^T=c^T_B B^{-1}}\)&lt;/span>是&lt;span class="math">\((2.1)\)&lt;/span>式的对偶问题的&lt;strong>可行解&lt;/strong>，即对所有&lt;span class="math">\(j\)&lt;/span>，&lt;span class="math">\(\boldsymbol{w^T p_j}-c_j\leq 0\)&lt;/span>成立，则称&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>为&lt;strong>原问题的‘对偶可行’的基本解&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>显然，&lt;strong>对偶可行的基本解不一定是原问题的可行解&lt;/strong>。当对偶可行的基本解&lt;strong>是&lt;/strong>原问题的可行解时，由于&lt;strong>判别数均小于或等于零&lt;/strong>，因此它就是原问题的&lt;strong>最优解&lt;/strong>。即对偶可行的基本解如果是原问题的可行解，那么它是最优解。&lt;/p>
&lt;p>对偶单纯形法的基本思想是，&lt;strong>从原问题的一个对偶可行的基本解出发&lt;/strong>，求改进的对偶可行的基本解，当得到的对偶可行的基本解是原问题的可行解时，就达到最优解。那么现在有两个问题&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>如何先找到一个对偶可行的基本解（&lt;strong>解扩充问题&lt;/strong>）&lt;/li>
&lt;li>如何改进这个对偶可行的基本解使得其在原问题可行（&lt;strong>改进对偶问题&lt;/strong>）&lt;/li>
&lt;/ol></description></item><item><title>优化理论之最优化条件</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E6%9C%80%E4%BC%98%E5%8C%96%E6%9D%A1%E4%BB%B6/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E6%9C%80%E4%BC%98%E5%8C%96%E6%9D%A1%E4%BB%B6/</guid><description>
&lt;h2 id="优化理论最优化条件">优化理论最优化条件&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#最优解的存在性">最优解的存在性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束问题最优化条件">无约束问题最优化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束问题最优化条件">有约束问题最优化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化一阶条件">有约束数值优化一阶条件&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#可行方向法">可行方向法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切锥下的一阶条件">切锥下的一阶条件&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#kkt点">KKT点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化二阶条件">有约束数值优化二阶条件&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二阶必要条件">二阶必要条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二阶充分条件">二阶充分条件&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;p>我们在本文主要谈论最小化： &lt;span class="math">\[f:\mathbb{R}^n→R，f为连续函数。对于f的可行域\Omega，求\\
\min_{x ∈ \Omega} f(x)\]&lt;/span>&lt;/p>
&lt;h2 id="最优解的存在性">最优解的存在性&lt;/h2>
&lt;p>我们需要指出的是，函数并没有指出在可行域内有最优解。即使有最优解，也未必在可行域内。因此，我们需要明确哪些情况最优解是一定存在的。&lt;/p>
&lt;p>根据基本的维尔思特拉斯极值定理有&lt;/p>
&lt;blockquote>
&lt;p>定理1：维尔思特拉斯极值定理：每一定义在紧集上的连续函数，都可以在定义域内取到极值点。这也表明：紧集上的连续函数都是有界的。&lt;/p>
&lt;/blockquote>
&lt;p>在此基础上，我们介绍一个更深的定理，但是现阶段不太会用到：&lt;/p>
&lt;blockquote>
&lt;p>定理2： 若函数&lt;span class="math">\(f\)&lt;/span>在n维闭区域&lt;span class="math">\(S\)&lt;/span>上连续并且向正无穷发散(coercive，即如果&lt;span class="math">\(lim||x||⇒∞,f(x)=+∞\)&lt;/span>)，那么&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(S\)&lt;/span>上一定有全局最小值。&lt;/p>
&lt;/blockquote>
&lt;p>以上定理只能保证最小值&lt;strong>存在&lt;/strong>，没有建立最小值和极小值之间的关系。但是，对于一类特殊的函数，它在一定区域内的极小值一定是最小值，这类函数即&lt;strong>凸函数&lt;/strong>。凸函数为定义在凸区间上的一种函数，它满足任意两点的连线位于抽象的函数曲面之下；而凸区间则满足任意两点连线仍然在区间中。定义在凸区间内的严格凸函数有唯一的极小值，该极小值为该函数在该区间上的最小值。&lt;/p>
&lt;h2 id="无约束问题最优化条件">无约束问题最优化条件&lt;/h2>
&lt;p>本节研究无约束问题：（基于费马引理） &lt;span class="math">\[\min f(x),x\in R^n（1）\]&lt;/span> 的最优性条件，包括一阶条件和二阶条件。&lt;/p>
&lt;p>应该指出，实际上我们只是求一个局部（局部严格）极小点，而非总体最小点。尽管我们也会考虑总体最小值的情况，但是一般来说这是一个相当困难的任务。在很多实际应用中，求局部最小值已经满足了问题的要求。因此本文章所指的极小值是指局部极小值。仅当问题具有某种凸性质，局部最小值才是总体最小值。&lt;/p>
&lt;p>设&lt;span class="math">\(f\)&lt;/span>的一阶导数和二阶导数存在，且分别表示为： &lt;span class="math">\[g(x)=\nabla f(x),G(x)=\nabla^2f(x),\]&lt;/span> 则我们有以下三个定理：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理1：&lt;/strong>（一阶必要条件） 设&lt;span class="math">\(f:D\subset R^n\rightarrow R^1\)&lt;/span>在开集D上连续可微，若&lt;span class="math">\(x^\ast\in D\)&lt;/span>是（1）上的局部极小点，则 &lt;span class="math">\[g(x^\ast)=0(2)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>证明1：&lt;/strong> 反证法&lt;/p>
&lt;p>假定&lt;span class="math">\(g(x^\ast)\neq0\)&lt;/span>，即存在非0梯度，那么取下降的负梯度&lt;span class="math">\(d=-g(x^\ast)\)&lt;/span>，则 &lt;span class="math">\[g(x^\ast)^Td=-g(x^\ast)^Tg(x^\ast)&amp;lt;0\]&lt;/span> 由于d是下降法向，从而在以&lt;span class="math">\(x^\ast\)&lt;/span>为中心邻域存在&lt;span class="math">\(\delta&amp;gt;0\)&lt;/span>，使得 &lt;span class="math">\[f(x^\ast+\alpha d)&amp;lt;f(x^\ast),\forall\alpha\in(0,\delta).\]&lt;/span> 那我们可以取&lt;span class="math">\(\overline{\delta}=\delta\|d\|\)&lt;/span>，因&lt;span class="math">\(\alpha&amp;lt;\delta\)&lt;/span>，故有： &lt;span class="math">\[\|\alpha d\|=\alpha\|d\|=\overline{\delta}\]&lt;/span> 因此，存在&lt;span class="math">\(x^\ast+\alpha d\in N(x^\ast)\)&lt;/span>，使得 &lt;span class="math">\[f(x^\ast+\alpha d)&amp;lt;f(x^\ast)，\]&lt;/span> 这与&lt;span class="math">\(x^\ast\)&lt;/span>是局部极小点矛盾。&lt;/p>
&lt;p>证明1完。&lt;/p>
&lt;p>我们也可以用费马引理来更简单的说明这个定理：&lt;/p>
&lt;blockquote>
&lt;p>引理1：费马引理：设函数&lt;span class="math">\(f(x)\)&lt;/span>在点&lt;span class="math">\(x_0\)&lt;/span>的某邻域&lt;span class="math">\(U(x_0)\)&lt;/span>内有定义，并且在&lt;span class="math">\(x_0\)&lt;/span>处可导，如果对任意的&lt;span class="math">\(x\in U(x_0)\)&lt;/span>，有 &lt;span class="math">\[f(x)\le f(x_0)或f(x)\ge f(x_0)\]&lt;/span> 那么&lt;span class="math">\(f^\prime(x_0)=0\)&lt;/span>。&lt;/p>
&lt;p>推论1：函数&lt;span class="math">\(f\)&lt;/span>在定义域&lt;span class="math">\(\Omega\)&lt;/span>内的最大值和最小值只能在&lt;strong>边界上，不可导的点，或驻点&lt;/strong>取得。&lt;/p>
&lt;/blockquote>
&lt;p>因此，对于连续可微函数，我们只需要在驻点和端点查找最小值即可。驻点即为一阶导数为0的点。&lt;/p>
&lt;p>如果只是一阶导数是0，那么到底能不能确定这个点是极小值点呢？这就需要二阶导数。如果二阶导数的海森矩阵半正定那就是极小值点，半负定就是极大值点，不定就是鞍点。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理2：&lt;/strong>（二阶必要条件）设&lt;span class="math">\(f:D\subset R^n\rightarrow R^1\)&lt;/span>在开集&lt;span class="math">\(D\)&lt;/span>上二阶连续可微，若&lt;span class="math">\(x^\ast\in D\)&lt;/span>是（1）的局部极小点，则： &lt;span class="math">\[g(x^\ast)=0\quad G(x^\ast)\geq0 (3)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>证明2：&lt;/strong> 反证法&lt;/p>
&lt;p>(3)的前半部分已经在&lt;strong>证明1&lt;/strong>得证。为了证明第二部分，即半正定，我们先假设&lt;span class="math">\(G(x^\ast)\)&lt;/span>不定，并设&lt;span class="math">\(N_\delta(x^\ast)\)&lt;/span>是&lt;span class="math">\(x^\ast\)&lt;/span>的邻域。有连续性可知，对所有&lt;span class="math">\(x\in N_\delta(x^\ast)\)&lt;/span>，&lt;span class="math">\(G(x)\)&lt;/span>不定。今选择&lt;span class="math">\(\epsilon\)&lt;/span>和向量d，使得&lt;span class="math">\(x^\ast+\epsilon d\in N_\delta(x^\ast)\)&lt;/span>，且满足&lt;span class="math">\(d^TG(x^\ast+\epsilon d)d&amp;lt;0\)&lt;/span>。利用&lt;span class="math">\(g(x^\ast)=0\)&lt;/span>，有&lt;span class="math">\(f(x+\epsilon d)\)&lt;/span>在&lt;span class="math">\(x^\ast\)&lt;/span>处的泰勒展开式： &lt;span class="math">\[f(x^\ast+\epsilon d)=f(x^\ast)+\frac{1}{2}\epsilon^2d^TG(x^\ast+\theta\epsilon d)d\]&lt;/span> 其中，&lt;span class="math">\(0\leq\theta\leq1\)&lt;/span>，从而，&lt;span class="math">\(f(x^\ast+\epsilon d)&amp;lt;f(x^\ast)\)&lt;/span>。这与&lt;span class="math">\(x^\ast\)&lt;/span>是局部极小点矛盾。所以&lt;span class="math">\(G(x^\ast)\)&lt;/span>在邻域内必须大于等于0，即半正定。&lt;/p>
&lt;p>满足&lt;span class="math">\(g(x^\ast)=0\)&lt;/span>的点&lt;span class="math">\(x^\ast\)&lt;/span>称为函数&lt;span class="math">\(f\)&lt;/span>的平稳点或驻点。如果&lt;span class="math">\(g(x^\ast)=0\)&lt;/span>，则有可能是极小点，也可能是极大点，也可能不是极值点，即鞍点。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理3：&lt;/strong>（二阶充分条件）设&lt;span class="math">\(f:D\subset R^n\rightarrow R^1\)&lt;/span>在开集&lt;span class="math">\(D\)&lt;/span>上二阶连续可微，则&lt;span class="math">\(x^\ast\in D\)&lt;/span>是&lt;span class="math">\(f\)&lt;/span>的一个严格局部极小点的充分条件是： &lt;span class="math">\[g(x^\ast)=0 且G(x^\ast)是正定矩阵(4)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>一般地，目标函数的平稳点不一定是极小值点。但若目标函数是凸函数，则其平稳点就是其极小点，且为最小点。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理4：&lt;/strong>（凸充分条件）设&lt;span class="math">\(f:D\subset R^n\rightarrow R^1\)&lt;/span>在开集&lt;span class="math">\(D\)&lt;/span>上为凸函数，且&lt;span class="math">\(f\in C^1\)&lt;/span>，则&lt;span class="math">\(x^\ast\)&lt;/span>是总体极小点的充分必要条件是：&lt;span class="math">\(g(x^\ast)=0\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="有约束问题最优化条件">有约束问题最优化条件&lt;/h2>
&lt;p>有约束条件的可行域不再是整个空间&lt;span class="math">\(R^n\)&lt;/span>，我们记可行域是&lt;span class="math">\(\Omega\)&lt;/span>，即 &lt;span class="math">\[\min_{\vec x} f(\vec x)\\
subject\quad to:\quad \vec x ∈ \Omega\]&lt;/span> 我们假定&lt;span class="math">\(f\)&lt;/span>是连续可微函数，并且可行域是非空闭集。&lt;/p>
&lt;h3 id="有约束数值优化一阶条件">有约束数值优化一阶条件&lt;/h3>
&lt;h4 id="可行方向法">可行方向法&lt;/h4>
&lt;p>我们在研究无约束优化一阶条件的基础上，特别想知道有哪些结论可以直接用到有约束情况下的。首先我们考虑可行方向：&lt;/p>
&lt;blockquote>
&lt;p>定义1：可行方向：对于可行域&lt;span class="math">\(\Omega \subset R^n\)&lt;/span>，有一个点&lt;span class="math">\(\vec x∈ \Omega\)&lt;/span>，我们说向量&lt;span class="math">\(\vec d\)&lt;/span>是&lt;strong>可行方向&lt;/strong>，如果存在一个步长&lt;span class="math">\(\bar t&amp;gt;0，\forall t ∈[0,\bar t]，\vec x+t\vec d ∈ \Omega\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>对于存在可行方向的点，我们存在以下定理&lt;/p>
&lt;blockquote>
&lt;p>定理5：有可行方向的有约束一阶条件：对于一个局部最优点&lt;span class="math">\(\vec x\)&lt;/span>，在这个点所有存在的可行方向&lt;span class="math">\(f&amp;#39;(\vec x+t\vec d)\)&lt;/span>，我们都有&lt;span class="math">\(f&amp;#39;(\vec x+t\vec d)≥0\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>显然这个定理是无约束条件的类比版，只不过可行方向受到可行域限制。对于凸多边形，可行方向法是充分的，我觉得单纯性法就是可行方向法的一个应用。但是对于很多曲线，可行方向法就不适用了，例如： &lt;span class="math">\[\Omega=\{(x_1,x_2):x_1^2+x_2^2=1\}\]&lt;/span> 每个点的可行方向都是&lt;span class="math">\(\vec 0\)&lt;/span>，这就没法做了。&lt;/p>
&lt;h4 id="切锥下的一阶条件">切锥下的一阶条件&lt;/h4>
&lt;p>为了克服可行方向法的弊端，我们使引入切锥。切锥是通过序列极限定义的：&lt;/p>
&lt;blockquote>
&lt;p>定义2：切锥：给定非空闭集&lt;span class="math">\(C \subset R^n\)&lt;/span>。如果在&lt;span class="math">\(C\)&lt;/span>内，存在收敛到&lt;span class="math">\(\vec x\)&lt;/span>的序列&lt;span class="math">\(\{\vec x^k\}\)&lt;/span>和收敛到&lt;span class="math">\(0\)&lt;/span>的正数序列&lt;span class="math">\(\{\tau^k\}\)&lt;/span>，使得向量&lt;span class="math">\(\vec d^k=(\vec x^k-x)/\tau^k→\vec d\)&lt;/span>。那么&lt;span class="math">\(\vec d\)&lt;/span>就是点&lt;span class="math">\(\vec x\)&lt;/span>的切线方向。点&lt;span class="math">\(\vec x\)&lt;/span>所有的切线方向组成的锥，就是切锥。 &lt;span class="math">\[T_c(\vec x)=\{\vec d ∈ R^n:\exists\{\vec x^k\}∈ C\}和\{\tau^k\}→0,d=\lim_{k→ ∞}\frac{\vec x^k-\vec x}{\tau^k}\}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这个定义主要在说什么呢？假设有一个闭集，然后有一点&lt;span class="math">\(\vec x\)&lt;/span>在闭集内，与此同时闭集内还有一组逐渐收敛于&lt;span class="math">\(\vec x点的序列。在点\)&lt;/span>x附近（neighborhood）所有可以收敛的方向都是这个点的tangent direction。看完这个定义会发现，点$x在该闭集上的tangent cone一般来讲是从该点作切线，并包含闭集内部的向外延伸的锥。下图是几个例子：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/切锥.jpg" alt="切锥" />&lt;p class="caption">切锥&lt;/p>
&lt;/div>
&lt;p>可行方向的约束作用在一些情况下略显严格，会导致空集的产生（例如非凸集合）。因此，我们需要适当放宽这个约束。因此，我们考虑用切锥来替代。在约束条件是&lt;strong>凸集&lt;/strong>的情况下，切锥等同于可行方向，而对于非凸情况，切锥避免了空集的产生。因此，我们可以放宽&lt;strong>定理5&lt;/strong>：&lt;/p>
&lt;blockquote>
&lt;p>定理6：切锥下的有约束一阶条件：对于一个局部最优点&lt;span class="math">\(\vec x\)&lt;/span>，有 &lt;span class="math">\[\nabla f(\vec x)^T\vec d≥0,\forall d ∈ T(\vec x|\Omega)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>显然，无约束一阶条件是定理6的特殊情况。&lt;/p>
&lt;h3 id="kkt点">KKT点&lt;/h3>
&lt;p>详见《KKT与拉格朗日》&lt;/p>
&lt;h3 id="有约束数值优化二阶条件">有约束数值优化二阶条件&lt;/h3>
&lt;p>我们知道，满足&lt;span class="math">\(p^T∇f(x^⋆)&amp;gt;0\)&lt;/span>的方向&lt;span class="math">\(p\)&lt;/span>可以使函数值增大，满足&lt;span class="math">\(p^T∇f(x^⋆)&amp;lt;0\)&lt;/span>的方向&lt;span class="math">\(p\)&lt;/span>可以使函数值减小，但对于&lt;span class="math">\(p^T∇f(x^⋆)=0\)&lt;/span>的方向&lt;span class="math">\(p\)&lt;/span>，我们不知道它到底使函数值增大还是减小。如下图所示，&lt;span class="math">\(p^T∇f(x^⋆)=0\)&lt;/span>可以使函数值增大、减小或不变。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/IXMRBAL.png" alt="IXMRBAL.png" />&lt;p class="caption">IXMRBAL.png&lt;/p>
&lt;/div>
&lt;p>将这些模棱两可的方向的集合称为“关键锥”（critical cone） &lt;span class="math">\[\mathcal{C}(x^{\star},\gamma)=\{w\in\mathcal{F}(x)|\nabla d_j(x^{\star})^Tw=0,\forall j\in\mathcal{A}(x^{\star})\text{ with }\lambda_j&amp;gt;0\}\]&lt;/span> 其中 &lt;span class="math">\[\mathcal{F}(x)=\left\{p|p \text{ 满足 }\begin{cases}\nabla c_i(x)^Tp=0\quad \forall i \\ \nabla d_j(x)^Tp\ge 0 \quad\forall d_j\in\mathcal{A}(x)\end{cases}\right\}\]&lt;/span>&lt;/p>
&lt;h4 id="二阶必要条件">二阶必要条件&lt;/h4>
&lt;ul>
&lt;li>D1. 满足KKT条件&lt;/li>
&lt;li>D2. &lt;span class="math">\(p^T\nabla^2\mathcal{L}(x^{\star},\gamma)p\ge 0 \quad \forall p\in\mathcal{C}(x^{\star},\gamma)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h4 id="二阶充分条件">二阶充分条件&lt;/h4>
&lt;ul>
&lt;li>E1. 满足KKT条件&lt;/li>
&lt;li>E2. &lt;span class="math">\(p^T\nabla^2\mathcal{L}(x^{\star},\gamma)p\gt 0 \quad \forall p\in\mathcal{C}(x^{\star},\gamma)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>类似于无约束的二阶条件，只不过约束问题的二阶条件是“有选择地”正定（或半正定）即可。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/最优化二阶条件.png" alt="最优化二阶条件" />&lt;p class="caption">最优化二阶条件&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>在Case I中，&lt;span class="math">\(f(x)\)&lt;/span>的斜率变化快于&lt;span class="math">\(d_1(x)\)&lt;/span>，所以拉格朗日函数的二次型严格大于零&lt;/li>
&lt;li>在Case II中，&lt;span class="math">\(f(x)\)&lt;/span>的斜率变化等于&lt;span class="math">\(d_1(x)\)&lt;/span>，所以拉格朗日函数的二次型等于零&lt;/li>
&lt;li>在Case III中，&lt;span class="math">\(f(x)\)&lt;/span>的斜率变化慢于&lt;span class="math">\(d_1(x)\)&lt;/span>，所以拉格朗日函数的二次型严格小于零&lt;/li>
&lt;/ul></description></item><item><title>优化理论之线性规划的对偶</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%AF%B9%E5%81%B6/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%AF%B9%E5%81%B6/</guid><description>
&lt;h2 id="线性规划的对偶原理">线性规划的对偶原理&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#对偶问题的表达">对偶问题的表达&lt;/a>&lt;/li>
&lt;li>&lt;a href="#1对称形式的对偶">（1）对称形式的对偶&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2非对称形式的对偶">（2）非对称形式的对偶&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3一般情形">（3）一般情形&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶问题的对偶是原问题">对偶问题的对偶是原问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶规划的一般规则重要">对偶规划的一般规则（重要）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶定理">对偶定理&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="对偶问题的表达">对偶问题的表达&lt;/h2>
&lt;p>线性规划中的对偶可以概括为&lt;strong>三种形式&lt;/strong>：&lt;/p>
&lt;h3 id="对称形式的对偶">（1）对称形式的对偶&lt;/h3>
&lt;p>原问题 &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax≥b}（注意这里是≥）\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.1}
\]&lt;/span> 对偶问题 &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{w^Tb}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{w^TA≤c^T}（注意这里是≤）\\
&amp;amp;\boldsymbol{w^T≥0}
\end{aligned}\tag{1.2}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{A=(p_1,p_2,\dotsb,p_n)}\)&lt;/span>是&lt;span class="math">\(m×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{b}=(b_1,b_2,\dotsb,b_m)^T\)&lt;/span>是&lt;strong>m维列&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{c^T}=(c_1,c_2,\dotsb,c_n)\)&lt;/span>是&lt;strong>n维行&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{x}=(x_1,x_2,\dotsb,x_n)^T\)&lt;/span>是原问题的变量组成的&lt;strong>n维列&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{w^T}=(w_1,w_2,\dotsb,w_m)\)&lt;/span>是由对偶问题变量组成的&lt;strong>m维行&lt;/strong>向量。&lt;/p>
&lt;p>在原问题&lt;span class="math">\((1.1)\)&lt;/span>中，目标函数是&lt;span class="math">\(\boldsymbol{c^T}\)&lt;/span>和&lt;span class="math">\(x\)&lt;/span>的内积，&lt;span class="math">\(\boldsymbol{Ax≥b}\)&lt;/span>包含m个不等式约束，若其中每个不等式约束记作&lt;span class="math">\(\boldsymbol{A_ix≥}b_i\)&lt;/span>，&lt;span class="math">\(\boldsymbol{A_i}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>的第i行，变量&lt;span class="math">\(\boldsymbol{x}\)&lt;/span>有非负限制。&lt;/p>
&lt;p>在对偶问题中&lt;span class="math">\((1.2)\)&lt;/span>中，目标函数是&lt;span class="math">\(\boldsymbol{b^T}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w}\)&lt;/span>的内积，&lt;span class="math">\(\boldsymbol{w^TA≤c^T}\)&lt;/span>包含n个不等式约束，每个约束条件记作&lt;span class="math">\(\boldsymbol{w^Tp_j}≤c_j\)&lt;/span>，对偶变量&lt;span class="math">\(w_i\)&lt;/span>也有非负限制。&lt;/p>
&lt;p>根据对称对偶的定义，原问题中约束条件&lt;span class="math">\(\boldsymbol{A_ix≥}b_i\)&lt;/span>个数，敲好等于对偶变量的个数；原问题中变量的个数，恰好等于对偶问题中约束条件&lt;span class="math">\(\boldsymbol{w^Tp_j}≤c_j\)&lt;/span>的个数。&lt;/p>
&lt;p>&lt;span class="math">\[
\begin{aligned}
对称形式：&amp;amp;原问题约束条件\rightarrow 对偶变量\\
&amp;amp;原问题变量\rightarrow 对偶约束\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>对于对偶问题引入理解，&lt;a href="../文档/优化理论/为何引入对偶问题.pdf">为何引入对偶问题&lt;/a>从经济学和数学两个方面给出了解释。（但是强对偶性即原问题最优解=对偶问题最优解，并没有被证明）。以下非对称形式对可以转换成对称形式，因此引入他们的根本理由都可以通过对称问题的对偶理解。&lt;/p>
&lt;h3 id="非对称形式的对偶">（2）非对称形式的对偶&lt;/h3>
&lt;p>考虑具有&lt;strong>等式约束条件&lt;/strong>的线性规划问题（这个反而是线性规划的标准形式） &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax=b}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.3}
\]&lt;/span> 我们已经引入了对称形式的对偶问题，现在我们可以通过一个trick将其&lt;strong>转换成对称形式&lt;/strong>：&lt;span class="math">\(=\Leftrightarrow ≥ \&amp;amp;\&amp;amp; ≤\)&lt;/span> &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax≥b}\\
\quad&amp;amp;\boldsymbol{Ax≤b\Rightarrow -Ax≥-b}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.4}
\]&lt;/span> 其中对前两行限制条件做一下变形： &lt;span class="math">\[
\left . \begin{aligned}
\quad&amp;amp;\boldsymbol{Ax≥b}\\
\quad&amp;amp;\boldsymbol{-Ax≥-b}\end{aligned} \right \}\Rightarrow \begin{bmatrix}\boldsymbol{A} \\ \boldsymbol{-A}\end{bmatrix}\boldsymbol{x}≥\begin{bmatrix}\boldsymbol{b} \\ \boldsymbol{-b}\end{bmatrix}
\]&lt;/span> 现在一个有&lt;span class="math">\(2m\)&lt;/span>个限制条件，因此对偶问题有&lt;span class="math">\(2m\)&lt;/span>个变量记为&lt;span class="math">\(\boldsymbol{[w&amp;#39;^T,w&amp;#39;&amp;#39;^T]},其中\boldsymbol{w&amp;#39;^T,w&amp;#39;&amp;#39;^T}\)&lt;/span>都是m维行向量。依照对称形式对偶问题的模样，我们可以写出&lt;span class="math">\((1.4)\)&lt;/span>的对偶形式 &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{[w&amp;#39;^T,w&amp;#39;&amp;#39;^T]}\begin{bmatrix}\boldsymbol{b} \\ \boldsymbol{-b}\end{bmatrix}=\boldsymbol{(w&amp;#39;^T-w&amp;#39;&amp;#39;^T)b}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{[w&amp;#39;^T,w&amp;#39;&amp;#39;^T]}\begin{bmatrix}\boldsymbol{A} \\ \boldsymbol{-A}\end{bmatrix}=\boldsymbol{(w&amp;#39;^T-w&amp;#39;&amp;#39;^T)A}≤\boldsymbol{c}\\
&amp;amp;\boldsymbol{w&amp;#39;^T,w&amp;#39;&amp;#39;^T≥0}
\end{aligned}
\]&lt;/span> 如果我们令&lt;span class="math">\(\boldsymbol{w^T=w&amp;#39;^T-w&amp;#39;&amp;#39;^T}\)&lt;/span>，我们可以得到和对称形式结构类似的对偶问题，变量数量也从&lt;span class="math">\(2m\rightarrow m\)&lt;/span>，但是&lt;strong>非负限制条件没有了&lt;/strong>： &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{w^Tb}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{w^TA}≤\boldsymbol{c}\\
\end{aligned}\tag{1.5}
\]&lt;/span> 到此，&lt;span class="math">\((1.5)\)&lt;/span>是&lt;span class="math">\((1.3)\)&lt;/span>的对偶问题。&lt;strong>他们的结构很相似，唯一的不同在于原问题的m个等式约束对应的对偶问题的m个变量没有正负号限制，它们称为非对称对偶&lt;/strong>。&lt;/p>
&lt;h3 id="一般情形">（3）一般情形&lt;/h3>
&lt;p>实际问题中，线性规划问题常常同时含有&lt;span class="math">\(≥，≤，=\)&lt;/span>多种限制条件，它们组成的原问题为： &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{A_1x≥b_1}\\
\quad&amp;amp;\boldsymbol{A_2x=b_2}\\
\quad&amp;amp;\boldsymbol{A_3x≤b_3}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.6}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{A_1}\)&lt;/span>是&lt;span class="math">\(m_1×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{A_2}\)&lt;/span>是&lt;span class="math">\(m_2×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{A_3}\)&lt;/span>是&lt;span class="math">\(m_3×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{b_1,b_2,b_3}\)&lt;/span>分别是&lt;span class="math">\(m_1,m_2,m_3\)&lt;/span>维列向量，&lt;span class="math">\(\boldsymbol{c^T}\)&lt;/span>是n维行向量，&lt;span class="math">\(\boldsymbol{x}\)&lt;/span>是n维列向量。&lt;/p>
&lt;p>显然，我们可以使用&lt;strong>添加松弛变量的方式将一般形式&lt;span class="math">\((1.6)\)&lt;/span>转换为非对称形式&lt;span class="math">\((1.3)\)&lt;/span>&lt;/strong>，即 &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{A_1 x-x_s=b_1}\\
\quad&amp;amp;\boldsymbol{A_2x=b_2}\\
\quad&amp;amp;\boldsymbol{A_3 x+-x_t=b_3}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{x_s}\)&lt;/span>是由&lt;span class="math">\(m_1\)&lt;/span>个松弛变量组成的&lt;span class="math">\(m_1\)&lt;/span>维列向量，&lt;span class="math">\(\boldsymbol{x_t}\)&lt;/span>是由&lt;span class="math">\(m_3\)&lt;/span>个松弛变量组成的&lt;span class="math">\(m_3\)&lt;/span>维列向量，如果用矩阵方式描述上述问题可写为 &lt;span class="math">\[
\begin{aligned}
\min \quad &amp;amp; \boldsymbol{c^Tx+0\cdot x_s+0\cdot x_t}\\
\mathop{s.t.} \quad &amp;amp; \begin{bmatrix}\boldsymbol{A_1} &amp;amp; \boldsymbol{-I_{m_1}} &amp;amp; \boldsymbol{0}\\
\boldsymbol{A_2} &amp;amp; \boldsymbol{0} &amp;amp; \boldsymbol{0}\\
\boldsymbol{A_3} &amp;amp; \boldsymbol{0} &amp;amp; \boldsymbol{I_{m_3}}\\ \end{bmatrix}
\begin{bmatrix}\boldsymbol{x}\\ \boldsymbol{x_s}\\ \boldsymbol{x_t}\end{bmatrix}=
\begin{bmatrix}\boldsymbol{b_1}\\ \boldsymbol{b_2}\\ \boldsymbol{b_3}\end{bmatrix},\\
\quad &amp;amp; \boldsymbol{x,x_s,x_t≥0},
\end{aligned}\tag{1.7}
\]&lt;/span> 按照非对称形式的对偶套路，&lt;span class="math">\((1.7)\)&lt;/span>的对偶问题可以描述为： &lt;span class="math">\[
\begin{aligned}
\max \quad &amp;amp; \boldsymbol{w^T b}=\boldsymbol{w^T_1 b_1+w^T_2 b_2+w^T_3 b_3}\\
\mathop{s.t.} \quad &amp;amp; \boldsymbol{[w^T_1,w^T_2,w^T_3]}
\begin{bmatrix}\boldsymbol{A_1} &amp;amp; \boldsymbol{-I_{m_1}} &amp;amp; \boldsymbol{0}\\
\boldsymbol{A_2} &amp;amp; \boldsymbol{0} &amp;amp; \boldsymbol{0}\\
\boldsymbol{A_3} &amp;amp; \boldsymbol{0} &amp;amp; \boldsymbol{I_{m_3}}\\ \end{bmatrix} ≤ \boldsymbol{[c^T,0,0]}
\end{aligned}
\]&lt;/span> 我们把矩阵的内容展开可得 &lt;span class="math">\[
\begin{aligned}
\max \quad &amp;amp; \boldsymbol{w^T_1 b_1+w^T_2 b_2+w^T_3 b_3}\\
\mathop{s.t.} \quad &amp;amp; \boldsymbol{w^T_1A_1+w^T_2A_2+w^T_3A_3≤c^T},\\
\quad &amp;amp; \boldsymbol{w_1≥0},\\
\quad &amp;amp; \boldsymbol{w_3≤0},\\
\quad &amp;amp; \boldsymbol{w_2}无限制,\\
\end{aligned}\tag{1.8}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{w_1,w_2,w_3}\)&lt;/span>分别是由对偶变量组成的&lt;span class="math">\(m_1\)&lt;/span>维，&lt;span class="math">\(m_2\)&lt;/span>维，&lt;span class="math">\(m_3\)&lt;/span>维行向量。定义&lt;span class="math">\((1.8)\)&lt;/span>式为&lt;span class="math">\((1.6)\)&lt;/span>式的对偶问题。&lt;/p>
&lt;p>由&lt;span class="math">\((1.8)\)&lt;/span>式可知，原问题中的约束&lt;span class="math">\(\boldsymbol{A_1 x≥b_1}\)&lt;/span>所对应的对偶变量&lt;span class="math">\(\boldsymbol{w_1}\)&lt;/span>有&lt;strong>非负限制&lt;/strong>，&lt;span class="math">\(\boldsymbol{A_2 x=b_2}\)&lt;/span>所对应的对偶变量&lt;span class="math">\(\boldsymbol{w_2}\)&lt;/span>&lt;strong>无正负限制&lt;/strong>，&lt;span class="math">\(\boldsymbol{A_3 x≤b_3}\)&lt;/span>所对应的对偶变量&lt;span class="math">\(\boldsymbol{w_3}\)&lt;/span>有&lt;strong>非正限制&lt;/strong>。从矩阵形式的对偶描述我们可以发现对偶变量的正负限制来自于&lt;strong>松弛变量的添加&lt;/strong>。&lt;/p>
&lt;h2 id="对偶问题的对偶是原问题">对偶问题的对偶是原问题&lt;/h2>
&lt;p>&lt;strong>原问题和对偶问题是相对的&lt;/strong>，显然原问题的对偶问题也是线性规划，它也有对偶问题，显然它的对偶问题就是原来对偶中的原问题。因此互相对偶的两个问题中，任何一个问题均可以作为原问题，而把另一个问题作为对偶问题。&lt;/p>
&lt;h2 id="对偶规划的一般规则重要">对偶规划的一般规则（重要）&lt;/h2>
&lt;p>为了方便讨论，我们下面所说的约束均指 &lt;span class="math">\[
\begin{aligned}
\boldsymbol{A_ix}&amp;amp;≥b_i\quad 及\quad\boldsymbol{w^T p_j}&amp;amp;≤c_j\\
(&amp;amp;=)&amp;amp;(=)\\
(&amp;amp;≥)&amp;amp;(≥)\\
\end{aligned}
\]&lt;/span> 型约束，不包含变量非负或非正限制。我们总结的一般规则有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>若原问题是极大化问题，那么对偶问题就是极小化问题；若原问题是极小化问题，那么对偶问题就是极大化问题。（&lt;strong>极小化&lt;span class="math">\(\leftrightarrow\)&lt;/span>极大化&lt;/strong>）&lt;/li>
&lt;li>在原问题和对偶问题中，约束右端变量向量与目标函数中系数向量恰好对换。（&lt;strong>约束向量&lt;span class="math">\(\leftrightarrow\)&lt;/span>系数向量&lt;/strong>）&lt;/li>
&lt;li>对于极小化问题的“&lt;span class="math">\(≥\)&lt;/span>”型约束（极大化问题中的“&lt;span class="math">\(≤\)&lt;/span>”型约束），相应的对偶变量有非负限制；对于极小化问题中的“&lt;span class="math">\(≤\)&lt;/span>”型约束（极大化问题中的“&lt;span class="math">\(≥\)&lt;/span>”型约束），相应的对偶变量有非正限制；对于原问题的“&lt;span class="math">\(=\)&lt;/span>”型约束，对应的对偶变量无正负限制。（&lt;strong>这一点可以通过矩阵形式添加的松弛变量体现出来&lt;/strong>）&lt;/li>
&lt;li>对于极小化问题的具有非负限制的变量（极大化问题的具有非正限制的变量），在其对偶问题中， 相应的约束为”&lt;span class="math">\(≥\)&lt;/span>“型不等式； 对小极小化问题的具有非正限制的变量（极大化问题的具有非负限制的变量），在其对偶问题中，相应的约束为“&lt;span class="math">\(≥\)&lt;/span>”型不等式；对于原问题中无正负限制的变量，在其对偶问题中，相应的约束为等式。（&lt;strong>根据对偶性，从第3点可以推出&lt;/strong>）&lt;/li>
&lt;/ol>
&lt;p>例子&lt;span class="math">\((1.9)\)&lt;/span>见《最优化理论与算法（第2版）》p126.&lt;/p>
&lt;h2 id="对偶定理">对偶定理&lt;/h2>
&lt;p>下面研究对偶的基本性质，由于不同形式的对偶问题可以相互转化，&lt;strong>在此仅叙述并证明关于对称对偶的几个重要定理，其结论对于其他形式的对偶仍成立&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理1.1&lt;/strong> 设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是&lt;span class="math">\((1.1),(1.2)\)&lt;/span>式的可行解,则&lt;span class="math">\(\boldsymbol{c^Tx^{(0)}}≥\boldsymbol{w^{(0)T}b}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明:利用对偶问题和原问题的定义有&lt;span class="math">\(\boldsymbol{Ax^{(0)}≥b}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}≥0}\)&lt;/span>,则有 &lt;span class="math">\[
\boldsymbol{w^{(0)T}Ax^{(0)}≥w^{(0)T}b}\tag{1.10}
\]&lt;/span> 同时,原问题和对偶问题还需要满足&lt;span class="math">\(\boldsymbol{w^{(0)T}A≤c^T}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{x^{(0)T}≥0}\)&lt;/span>，则有 &lt;span class="math">\[
\boldsymbol{c^{(0)T}x≥w^{(0)T}Ax^{(0)}}\tag{1.11}
\]&lt;/span> 由&lt;span class="math">\((1.10),(1.11)\)&lt;/span>可以推出&lt;span class="math">\(\boldsymbol{c^{(0)T}x≥w^{(0)T}b}\)&lt;/span>&lt;strong>恒成立&lt;/strong>。&lt;/p>
&lt;p>上述定理表明，就原问题和对偶问题的可行解而言，对于对偶中的两个问题，每一个问题的&lt;strong>任何一个可行解处的目标函数值都给出另一个问题的目标函数值的界&lt;/strong>。极小化问题给出极大化问题的目标函数值的上界；极大化问题给出极小化问题的目标函数值的下界。&lt;/p>
&lt;p>由定理1.1可以得到以下几个重要推论：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>推论1：&lt;/strong> 若&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的可行解，且&lt;span class="math">\(\boldsymbol{c^Tx^{(0)}=w^{(0)T}b}\)&lt;/span>，则&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解。(&lt;span class="math">\(\boldsymbol{c^{(0)T}x≥w^{(0)T}b}\)&lt;/span>&lt;strong>恒成立&lt;/strong>因此相等的情况分别是二者的最优解)&lt;/p>
&lt;p>&lt;strong>推论2：&lt;/strong> 对偶规划&lt;span class="math">\((1.1)\)&lt;/span>和&lt;span class="math">\((1.2)\)&lt;/span>有最优解的充要条件是它们同时有可行解。（最优解必然是可行解；可行解里面必然能选出至少一个最优解）&lt;/p>
&lt;p>&lt;strong>推论3：&lt;/strong> 若原问题&lt;span class="math">\((1.1)\)&lt;/span>的目标函数值在可行域上&lt;strong>无下界&lt;/strong>， 则对偶问题&lt;span class="math">\((1.2)\)&lt;/span>&lt;strong>无可行解&lt;/strong>；反之，若对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的目标函数值在可行域上&lt;strong>无上界&lt;/strong>， 则原问题&lt;span class="math">\((1.1)\)&lt;/span>&lt;strong>无可行解&lt;/strong>。（&lt;span class="math">\((1.1)\)&lt;/span>无下界说明下界值为&lt;span class="math">\(-\infty\)&lt;/span>，没有比&lt;span class="math">\(-\infty\)&lt;/span>更小的值，所以对偶问题&lt;span class="math">\((1.2)\)&lt;/span>无解，反之同理）&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>推论1&lt;/code>粗略的阐明了原问题和对偶问题最优解的关系，下面我们通过&lt;code>定理1.2&lt;/code>详细证明一下，&lt;code>定理1.2&lt;/code>可以说是对偶理论的&lt;strong>核心&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理1.2：&lt;/strong>设原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>中&lt;strong>有一个问题存在最优解， 则另一个问题也存在最优解&lt;/strong>，且两个问题的目标函数的&lt;strong>最优值相等&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：设原问题&lt;span class="math">\((1.1)\)&lt;/span>存在最优解，引进松弛变量，可以把原问题写成等价形式： &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax-v=b}\\
&amp;amp;\boldsymbol{x≥0}\\
&amp;amp;\boldsymbol{v≥0}
\end{aligned}\tag{1.12}
\]&lt;/span> 由于线性规则&lt;span class="math">\((1.12)\)&lt;/span>存在最优解， 因此能够用单纯形方法（包括使用能避免循环发生的摄动法）求出它的一个最优基本可行解，不妨设这个最优解是 &lt;span class="math">\[
\boldsymbol{y^{(0)}}=\begin{bmatrix}
\boldsymbol{x^{(0)}}\\
\boldsymbol{v^{(0)}}
\end{bmatrix}
\]&lt;/span> 相应的最优基是&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>。由单纯形法步骤可知，这时所有判别数&lt;span class="math">\(z_j-c_j\)&lt;/span>均非正， 即 &lt;span class="math">\[\boldsymbol{w^{(0)T}p_j}-c_j≤0\quad\forall j, \tag{1.13}\]&lt;/span> 其中&lt;span class="math">\(\boldsymbol{w^{(0)T}}=\boldsymbol{c_B^T B^{-1}}\)&lt;/span>，&lt;span class="math">\(\boldsymbol{c_B}\)&lt;/span>是目标函数中基变量（包括松弛变量中的基变量）的系数组成的向量。考虑所有原来变量（不包括松弛变量）在基&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>下的判别数，把它们所满足的条件&lt;span class="math">\((1.13)\)&lt;/span>用矩阵形式同时写出，得到 &lt;span class="math">\[
\boldsymbol{w^{(0)T}A-c^T≤0}\Rightarrow\\
\boldsymbol{w^{(0)T}A≤c^T} \tag{1.14}
\]&lt;/span> 对于松弛变量对应的矩阵列向量&lt;span class="math">\(\boldsymbol{p_j}\)&lt;/span>，只有松弛变量对应元素位置为-1（&lt;span class="math">\(\boldsymbol{Ax-v=b}\)&lt;/span>），其他位置为0，而&lt;span class="math">\((1.13)\)&lt;/span>对任意&lt;span class="math">\(j\)&lt;/span>成立；同时，松弛变量对应&lt;span class="math">\(\boldsymbol{c}\)&lt;/span>中系数皆为0，所以对任意松弛变量在基&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>下对应的判别数所满足的条件&lt;span class="math">\((1.13)\)&lt;/span>用矩阵形式表示，得到 &lt;span class="math">\[
\boldsymbol{w^{(0)T}(-I_m)≤0}\Rightarrow\\
\boldsymbol{w^{(0)T}≥0} \tag{1.15}
\]&lt;/span> 我们惊喜的发现：在单纯形法中构造出来的&lt;span class="math">\(\boldsymbol{w^{(0)T}}=\boldsymbol{c_B^T B^{-1}}\)&lt;/span>是满足对偶问题&lt;span class="math">\((1.2)\)&lt;/span>约束&lt;span class="math">\((1.14),(1.15)\)&lt;/span>的&lt;strong>可行解&lt;/strong>！！！&lt;/p>
&lt;p>根据单纯性法步骤，有&lt;span class="math">\(\boldsymbol{w^{(0)T}}=\boldsymbol{c_B^T B^{-1}}\)&lt;/span>，&lt;span class="math">\(\boldsymbol{y_B^{(0)}=B^{-1} b}\)&lt;/span>，因此有： &lt;span class="math">\[
\boldsymbol{w^{(0)T}b=c^T_B \underbrace{B^{-1} b}_{y_B^{(0)}}=c^T_B y_B^{(0)}}
\]&lt;/span> 由于非基变量取值为零及目标函数中松弛变量的系数为零，因此 &lt;span class="math">\[
\boldsymbol{c^T_B y_B^{(0)}=c^Tx^{(0)}}
\]&lt;/span> 这里&lt;span class="math">\(\boldsymbol{y^{(0)}_B}\)&lt;/span>表示&lt;span class="math">\(\boldsymbol{y^{(0)}}\)&lt;/span>中基变量的取值，其余非基变量都是0.根据&lt;code>定理1.1&lt;/code>的&lt;code>推论1&lt;/code>:&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>是对偶问题&lt;span class="math">\((1.2)\)&lt;/span>最优解，且原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的目标函数的最优值相等。类似的，可以证明，如果对偶问题&lt;span class="math">\((1.2)\)&lt;/span>存在最优解，则原问题&lt;span class="math">\((1.1)\)&lt;/span>也存在最优解，且两个问题目标函数的最优值相等。（定理1.2也常用凸集分离定理证明）&lt;/p>
&lt;p>由上述定理的证明过程可以得到下面一个推论：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>推论4&lt;/strong>：若线性规划&lt;span class="math">\((1.1)\)&lt;/span>存在一个对应基&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>的最优基本可行解， 则单纯形乘子&lt;span class="math">\(\boldsymbol{w^{(0)T}}=\boldsymbol{c_B^T B^{-1}}\)&lt;/span>是对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的一个最优解。&lt;/p>
&lt;/blockquote>
&lt;p>根据这个推论，我们能够&lt;strong>从原问题的最优单纯形表中直接获得对偶问题的一个最优解&lt;/strong>。&lt;/p></description></item><item><title>优化理论之线性规划的对偶的互补松弛定理</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%AF%B9%E5%81%B6%E7%9A%84%E4%BA%92%E8%A1%A5%E6%9D%BE%E5%BC%9B%E5%AE%9A%E7%90%86/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%AF%B9%E5%81%B6%E7%9A%84%E4%BA%92%E8%A1%A5%E6%9D%BE%E5%BC%9B%E5%AE%9A%E7%90%86/</guid><description>
&lt;h2 id="线性规划的对偶的互补松弛定理">线性规划的对偶的互补松弛定理&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#前情提要">前情提要&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对称形式对偶问题定义">对称形式对偶问题定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非对称形式对偶问题定义">非对称形式对偶问题定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶定理">对偶定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#互补松弛性质">互补松弛性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对称形式">对称形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非对称形式">非对称形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#强互补松弛定理">强互补松弛定理&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="前情提要">前情提要&lt;/h2>
&lt;h3 id="对称形式对偶问题定义">对称形式对偶问题定义&lt;/h3>
&lt;p>原问题 &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax≥b}（注意这里是≥）\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.1}
\]&lt;/span> 对偶问题 &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{w^Tb}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{w^TA≤c^T}（注意这里是≤）\\
&amp;amp;\boldsymbol{w^T≥0}
\end{aligned}\tag{1.2}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{A=(p_1,p_2,\dotsb,p_n)}\)&lt;/span>是&lt;span class="math">\(m×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{b}=(b_1,b_2,\dotsb,b_m)^T\)&lt;/span>是&lt;strong>m维列&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{c^T}=(c_1,c_2,\dotsb,c_n)\)&lt;/span>是&lt;strong>n维行&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{x}=(x_1,x_2,\dotsb,x_n)^T\)&lt;/span>是原问题的变量组成的&lt;strong>n维列&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{w^T}=(w_1,w_2,\dotsb,w_m)\)&lt;/span>是由对偶问题变量组成的&lt;strong>m维行&lt;/strong>向量。&lt;/p>
&lt;h3 id="非对称形式对偶问题定义">非对称形式对偶问题定义&lt;/h3>
&lt;p>原问题 &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax=b}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.3}
\]&lt;/span> 对偶问题 &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{w^Tb}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{w^TA}≤\boldsymbol{c}\\
\end{aligned}\tag{1.5}
\]&lt;/span>&lt;/p>
&lt;h3 id="对偶定理">对偶定理&lt;/h3>
&lt;p>&lt;strong>定理1.1&lt;/strong> 设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是&lt;span class="math">\((1.1),(1.2)\)&lt;/span>式的可行解,则&lt;span class="math">\(\boldsymbol{c^Tx^{(0)}}≥\boldsymbol{w^{(0)T}b}\)&lt;/span>&lt;/p>
&lt;p>&lt;strong>推论1：&lt;/strong> 若&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的可行解，且&lt;span class="math">\(\boldsymbol{c^Tx^{(0)}=w^{(0)T}b}\)&lt;/span>，则&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解。(&lt;span class="math">\(\boldsymbol{c^{(0)T}x≥w^{(0)T}b}\)&lt;/span>&lt;strong>恒成立&lt;/strong>因此相等的情况分别是二者的最优解)&lt;/p>
&lt;p>&lt;strong>定理1.2：&lt;/strong>设原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>中&lt;strong>有一个问题存在最优解， 则另一个问题也存在最优解&lt;/strong>，且两个问题的目标函数的&lt;strong>最优值相等&lt;/strong>。&lt;/p>
&lt;h2 id="互补松弛性质">互补松弛性质&lt;/h2>
&lt;p>利用对偶定理可以证明原问题和对偶问题的最优解满足重要的互补松弛关系。&lt;strong>互补松弛定理可以了解到变量在限制条件中是否取等号或者是取上下限值&lt;/strong>。&lt;/p>
&lt;h3 id="对称形式">对称形式&lt;/h3>
&lt;blockquote>
&lt;p>&lt;strong>定理1.3&lt;/strong>：设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的可行解，那么&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>都是最优解的充要条件是，对所有&lt;span class="math">\(i和j\)&lt;/span>，下列关系成立：&lt;/p>
&lt;blockquote>
&lt;ol style="list-style-type: decimal">
&lt;li>如果&lt;span class="math">\(x^{(0)}_j&amp;gt;0\)&lt;/span>，就有&lt;span class="math">\(\boldsymbol{w^{(0)T}p_j}=c_j\)&lt;/span>;&lt;/li>
&lt;li>如果&lt;span class="math">\(\boldsymbol{w^{(0)T}p_j}&amp;lt;c_j\)&lt;/span>，就有&lt;span class="math">\(x^{(0)}_j=0\)&lt;/span>;&lt;/li>
&lt;li>如果&lt;span class="math">\(w^{(0)}_i&amp;gt;0\)&lt;/span>，就有&lt;span class="math">\(\boldsymbol{A_i x^{(0)}}=b_i\)&lt;/span>;&lt;/li>
&lt;li>如果&lt;span class="math">\(\boldsymbol{A_i x^{(0)}}&amp;gt;b_i\)&lt;/span>，就有&lt;span class="math">\(w^{(0)}_i=0\)&lt;/span>;&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>其中&lt;span class="math">\(\boldsymbol{p_j}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>的第&lt;span class="math">\(j\)&lt;/span>列，&lt;span class="math">\(\boldsymbol{A_i}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>的第&lt;span class="math">\(i\)&lt;/span>行。&lt;/p>
&lt;/blockquote>
&lt;p>证明：先证必要性。最优解&lt;span class="math">\(\Rightarrow\)&lt;/span> (1,2,3,4)&lt;/p>
&lt;p>设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解。由于&lt;span class="math">\(\boldsymbol{c^T≥w^{(0)T}A}\)&lt;/span>以及&lt;span class="math">\(\boldsymbol{x^{(0)}≥0}\)&lt;/span>，则有 &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}≥\boldsymbol{w^{(0)T}A x^{(0)}}\tag{1.16}
\]&lt;/span> 由于&lt;span class="math">\(\boldsymbol{Ax^{(0)}≥b}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)T}≥0}\)&lt;/span>，则 &lt;span class="math">\[
\boldsymbol{w^{(0)T}A x^{(0)}}≥\boldsymbol{w^{(0)T}b}\tag{1.17}
\]&lt;/span> 由于&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解，根据&lt;code>定理1.2&lt;/code>必有： &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}=\boldsymbol{w^{(0)T}b}\tag{1.18}
\]&lt;/span> 用&lt;span class="math">\((1.18)\)&lt;/span>夹逼&lt;span class="math">\((1.16),(1.17)\)&lt;/span>可得 &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}=\boldsymbol{w^{(0)T}A x^{(0)}}=\boldsymbol{w^{(0)T}b} \tag{1.19}\\
\]&lt;/span> &lt;span class="math">\[
\boldsymbol{(c^T-w^{(0)T}A) x^{(0)}}=0 \tag{1.20}\\
\]&lt;/span> &lt;span class="math">\[
\boldsymbol{w^{(0)T}(A x^{(0)}-b)}=0\tag{1.21}\\
\]&lt;/span> 由于&lt;span class="math">\(\boldsymbol{c^T-w^{(0)T}A≥0}，\boldsymbol{x^{(0)}≥0}\)&lt;/span>，因此对于两项分量非0时都为正，因此对于每一个分量都有 &lt;span class="math">\[
(c_j-\boldsymbol{w^{0T}p_j})x_j^{(0)}=0,\quad j=1,2,\dotsb,n
\]&lt;/span> 故&lt;code>定理1.3&lt;/code>中关系（1）（2）成立。&lt;/p>
&lt;p>由于&lt;span class="math">\(\boldsymbol{Ax^{(0)}-b≥0}，\boldsymbol{w^{(0)}≥0}\)&lt;/span>，因此对于两项分量非0时都为正，因此对于每一个分量都有 &lt;span class="math">\[
w_i^{(0)}(\boldsymbol{A_ix^{(0)}}-b_i)=0,\quad i=1,2,\dotsb,m
\]&lt;/span> 故&lt;code>定理1.3&lt;/code>中关系（3）（4）成立。必要性得证。&lt;/p>
&lt;p>再证充分性。最优解&lt;span class="math">\(\Leftarrow\)&lt;/span> (1,2,3,4)&lt;/p>
&lt;p>设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的可行解，且关系（1）（2）（3）（4）成立。&lt;/p>
&lt;p>由于关系（1）和（2）成立，则对每一个&lt;span class="math">\(j\)&lt;/span>，有 &lt;span class="math">\[
(c_j-\boldsymbol{w^{0T}p_j})x_j^{(0)}=0,\quad j=1,2,\dotsb,n\tag{1.22}
\]&lt;/span> 由此可推出&lt;span class="math">\(\boldsymbol{(w^{(0)T}A-c^T)x^{(0)}=0}\)&lt;/span>，即 &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}=\boldsymbol{w^{(0)T}A x^{(0)}}\tag{1.23}
\]&lt;/span> 由于关系（3）（4）成立，则对于每一个&lt;span class="math">\(i\)&lt;/span>，有 &lt;span class="math">\[
w_i^{(0)}(\boldsymbol{A_ix^{(0)}}-b_i)=0,\quad i=1,2,\dotsb,m\tag{1.24}
\]&lt;/span> 由此可以看出&lt;span class="math">\(\boldsymbol{w^{(0)T}(A x^{(0)}-b)}=0\)&lt;/span>，即 &lt;span class="math">\[
\boldsymbol{w^{(0)T}A x^{(0)}}=\boldsymbol{w^{(0)T}b} \tag{1.25}
\]&lt;/span> 由&lt;span class="math">\((1.23)\)&lt;/span>和&lt;span class="math">\((1.25)\)&lt;/span>可以得到 &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}=\boldsymbol{w^{(0)T}b}
\]&lt;/span> 由&lt;code>定理1.1&lt;/code>的&lt;code>推论1&lt;/code>可知，&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解。&lt;/p>
&lt;h3 id="非对称形式">非对称形式&lt;/h3>
&lt;p>对于非对称形式的对偶规划，由于在原问题中约束条件是&lt;span class="math">\(\boldsymbol{Ax=b}\)&lt;/span>，而对偶变量无正负 限制。因此互补松弛性质叙述如下&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理1.4&lt;/strong>：设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.3)\)&lt;/span>和对偶问题&lt;span class="math">\((1.5)\)&lt;/span>的可行解，那么&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>都是最优解的充要条件是，对于所有&lt;span class="math">\(j\)&lt;/span>，下列关系成立：&lt;/p>
&lt;blockquote>
&lt;ol style="list-style-type: decimal">
&lt;li>如果&lt;span class="math">\(x^{(0)}_j&amp;gt;0\)&lt;/span>，就有&lt;span class="math">\(\boldsymbol{w^{(0)T}p_j}=c_j\)&lt;/span>;&lt;/li>
&lt;li>如果&lt;span class="math">\(\boldsymbol{w^{(0)T}p_j}&amp;lt;c_j\)&lt;/span>，就有&lt;span class="math">\(x^{(0)}_j=0\)&lt;/span>;&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="强互补松弛定理">强互补松弛定理&lt;/h2>
&lt;p>暂时略&lt;/p></description></item><item><title>优化理论之单纯形法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</guid><description>
&lt;h2 id="单纯行法">单纯行法&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#核心思想">核心思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单纯形方法原理">单纯形方法原理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一般计算步骤">一般计算步骤&lt;/a>&lt;/li>
&lt;li>&lt;a href="#收敛性">收敛性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#使用表格形式的单纯形方法">使用表格形式的单纯形方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#退化情形">退化情形&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="核心思想">核心思想&lt;/h2>
&lt;p>由优化理论可知，如果线性规划中的最优解，那么这个最优解必是最优基本可行解。因此线性规划的核心就变成了求&lt;strong>最优基本可行解&lt;/strong>。这构成了单纯形法的基础。&lt;/p>
&lt;p>单纯形法核心思路：&lt;/p>
&lt;ul>
&lt;li>找到一个基本可行解&lt;/li>
&lt;li>求一个使目标函数值有所改善的基本可行解&lt;/li>
&lt;li>不断改进的基本可行解直到最优解&lt;/li>
&lt;/ul>
&lt;p>因此，大方向可以分为两个：1.找一个初时基本可行；2.改进基本可行解。对于第一个问题，用二阶段法或者大M法，第二个问题则需要考虑退化情形。&lt;/p>
&lt;h2 id="单纯形方法原理">单纯形方法原理&lt;/h2>
&lt;p>考虑线性规划问题 &lt;span class="math">\[
\begin{aligned}
\min\ &amp;amp;f \overset{\mathop{def}}{=} \boldsymbol{c}^T\boldsymbol{x}\\
\mathop{s.t.}\ &amp;amp;\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b},\\
&amp;amp;\boldsymbol{x}\geq 0
\end{aligned}\tag{1.1}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>是&lt;span class="math">\(m×n\)&lt;/span>矩阵，秩为&lt;span class="math">\(m\)&lt;/span>。&lt;span class="math">\(\boldsymbol{c}^T\)&lt;/span>是n维行向量，&lt;span class="math">\(\boldsymbol{b}≥0\)&lt;/span>是m维列向量(每个分量都大于等于0)。我们以列向量来表示&lt;span class="math">\(\boldsymbol{A}\)&lt;/span> &lt;span class="math">\[
\boldsymbol{A}=(\boldsymbol{p_1},\boldsymbol{p_2},\dotsb,\boldsymbol{p_n})
\]&lt;/span> 显然&lt;span class="math">\(\boldsymbol{A}\boldsymbol{x}\)&lt;/span>可表示为： &lt;span class="math">\[
\boldsymbol{A}\boldsymbol{x}=\sum_{j=1}^n x_j\boldsymbol{p_j}
\]&lt;/span> &lt;span class="math">\(x_j\)&lt;/span>可以看成是列向量&lt;span class="math">\(\boldsymbol{p_j}\)&lt;/span>线性组合的系数。现将&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>分解成&lt;span class="math">\((\boldsymbol{B},\boldsymbol{N})\)&lt;/span>（可能经过列调换），使得其中&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>是基矩阵，&lt;span class="math">\(\boldsymbol{N}\)&lt;/span>是非基矩阵。在此分解下，我们假设能得到一个基本可行解： &lt;span class="math">\[
\boldsymbol{x^{(0)}}=\begin{bmatrix}
\boldsymbol{B^{-1}b}\\
\boldsymbol{0}\\
\end{bmatrix}
\]&lt;/span> 这个基本可行解对应了线性规划的某个极点，在点&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>处的目标函数值是 &lt;span class="math">\[
f_0=\boldsymbol{c}^T\boldsymbol{x^{(0)}}=(\boldsymbol{c_B^T,c_N^T})\begin{bmatrix}
\boldsymbol{B^{-1}b}\\
\boldsymbol{0}\\
\end{bmatrix}\\
=\boldsymbol{c_B^T B^{-1}b}\tag{1.2}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{c^T_B}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{c}^T\)&lt;/span>中与&lt;strong>基变量&lt;/strong>对应的分量组成的m维行向量，&lt;span class="math">\(\boldsymbol{c^T_N}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{c}^T\)&lt;/span>中与&lt;strong>非基变量&lt;/strong>对应的分量组成的n-m维行向量（因为为了让&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>的秩为m，可能对&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>中的列变量进行过变换，对应的&lt;span class="math">\(\boldsymbol{x,c}\)&lt;/span>都要变换）。&lt;/p>
&lt;p>现在我们从这个基本可行解&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>出发，利用非基变量得出一个改进的基本可行解。设 &lt;span class="math">\[
\boldsymbol{x}=\begin{bmatrix}
\boldsymbol{x_B}\\
\boldsymbol{x_N}
\end{bmatrix}
\]&lt;/span> 是任一个可行解，则由&lt;span class="math">\(\boldsymbol{Ax=b}\)&lt;/span>可得： &lt;span class="math">\[
\boldsymbol{x_B}=\boldsymbol{B^{-1}b-B^{-1}Nx_N}\tag{1.3}
\]&lt;/span> 在此点处的目标函数值是 &lt;span class="math">\[
\begin{aligned}
f&amp;amp;=\boldsymbol{c^Tx}=(\boldsymbol{c^T_B,c^T_N})\begin{bmatrix}\boldsymbol{x_B}\\\boldsymbol{x_N}\end{bmatrix}\\
&amp;amp;=\boldsymbol{c^T_B x_B}+\boldsymbol{c^T_N x_N}\\
&amp;amp;=\boldsymbol{c^T_B(B^{-1}b-B^{-1}Nx_N)}+\boldsymbol{c^T_N x_N}\\
&amp;amp;=\underbrace{\boldsymbol{c^T_B B^{-1}b}}_{基本可行解函数值}-\boldsymbol{(c^T_B B^{-1}N-c^T_N)x_N}\\
&amp;amp;=f_0 - \sum_{j\in R}(\underbrace{\boldsymbol{c^T_B B^{-1}p_j}}_{z_j}-c_j)x_j,\ R为非基变量下标集\\
&amp;amp;=f_0 - \sum_{j ∈ R}(z_j-c_j)x_j
\end{aligned}\tag{1.4}
\]&lt;/span> 由于&lt;span class="math">\(\boldsymbol{c^T_B}\)&lt;/span>是m维行向量，&lt;span class="math">\(\boldsymbol{B^{-1}}\)&lt;/span>是&lt;span class="math">\(m×m\)&lt;/span>维矩阵，&lt;span class="math">\(\boldsymbol{p_j}\)&lt;/span>是m维列向量，因此它们的积是一个数字。由&lt;span class="math">\((1.4)\)&lt;/span>可知，适当的选择非基变量的值&lt;span class="math">\(x_j\)&lt;/span>，可以使得 &lt;span class="math">\[
\sum_{j ∈ R}(z_j-c_j)x_j&amp;gt;0\tag{1.5}
\]&lt;/span> 例如，当&lt;span class="math">\(z_j-c_j&amp;lt;0\Rightarrow x_j=0\)&lt;/span>；当&lt;span class="math">\(z_j-c_j≥0\Rightarrow x_j&amp;gt;0\)&lt;/span>。&lt;strong>从而得到使目标函数的值减少的新的基本可行解&lt;/strong>。为了方便，我们不妨令&lt;span class="math">\(n-m-1\)&lt;/span>个非基变量取0，一个非基变量，比如&lt;span class="math">\(x_k\)&lt;/span>大于0。需要注意的是，这个&lt;span class="math">\(x_k\)&lt;/span>的系数&lt;span class="math">\(z_j-c_j\)&lt;/span>应该是大于0的。&lt;/p>
&lt;p>根据&lt;span class="math">\((1.4)\)&lt;/span>，&lt;strong>正&lt;/strong>系数&lt;span class="math">\(z_j-c_j\)&lt;/span>越大，目标函数下降越快，我们不妨用&lt;strong>贪心法&lt;/strong>，选择&lt;span class="math">\(x_k\)&lt;/span>，使 &lt;span class="math">\[
z_k-c_k=\max_{j\in R}\{z_j-c_j\}&amp;gt;0\tag{1.6}
\]&lt;/span> &lt;span class="math">\(x_k\)&lt;/span>由0变成正数后，得到方程组&lt;span class="math">\(\boldsymbol{Ax=b}\)&lt;/span>的另一个解： &lt;span class="math">\[
\begin{aligned}
\boldsymbol{x_B}&amp;amp;=\boldsymbol{B^{-1}b-B^{-1}Nx_N}\\
&amp;amp;=\boldsymbol{\underbrace{B^{-1}b}_{\bar b}-\underbrace{B^{-1}p_k}_{y_k}}x_k\\
&amp;amp;=\boldsymbol{\bar b}- \boldsymbol{y_k}x_k
\end{aligned}\tag{1.7}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{\bar b, y_k}\)&lt;/span>都是m维列向量，&lt;span class="math">\(x_k\)&lt;/span>是标量。我们知道标准形式下的可行解要求&lt;span class="math">\(\boldsymbol{x_B}≥0\)&lt;/span>，若我们把新得到的解&lt;span class="math">\(\boldsymbol{x_B}\)&lt;/span>按分量写出，即 &lt;span class="math">\[
\boldsymbol{x_B}=\begin{bmatrix}x_{B_1}\\x_{B_2}\\ \vdots \\x_{B_m}\end{bmatrix}=\begin{bmatrix}\bar{b}_1\\\bar{b}_2\\ \vdots \\\bar{b}_m\end{bmatrix}-\begin{bmatrix}y_{k_1}\\y_{k_2}\\ \vdots \\y_{k_m}\end{bmatrix}x_k ≥0 \tag{1.8}
\]&lt;/span> &lt;span class="math">\[
\boldsymbol{x_N}=(0,0,\dotsb,0,x_k,0,\dotsb,0)^T\tag{1.9}
\]&lt;/span> 在新得到的点，目标函数的函数值为 &lt;span class="math">\[
f=f_0-(z_k-c_k)x_k\tag{1.10}
\]&lt;/span> 再来，我们分析怎样确定&lt;span class="math">\(x_k\)&lt;/span>的取值。根据&lt;span class="math">\((1.8)\)&lt;/span>的可行性限制条件(非负)，我们不难得出，对&lt;span class="math">\(\forall i ∈ \{1,2,\dotsb,m\}且y_{k_i}&amp;gt;0\)&lt;/span>.： &lt;span class="math">\[
x_{B_i}=\bar b_i -y_{k_i}x_k≥0\Rightarrow x_k≤\frac{\bar b_i}{y_{k_i}}
\]&lt;/span> 因为若&lt;span class="math">\(y_{k_i}\)&lt;/span>为负，&lt;span class="math">\(x_k\)&lt;/span>为正，这一分量不会成为破坏非负限制条件的分量。为了保证非负限制条件，我们必须有 &lt;span class="math">\[
x_k≤\min\big\{\frac{\bar b_i}{y_{k_i}}\big |y_{k_i}&amp;gt;0\big\}
\]&lt;/span> 同时，为了让&lt;span class="math">\(f\)&lt;/span>尽可能减少，我们希望&lt;span class="math">\(x_k\)&lt;/span>尽量取大的值，所以有 &lt;span class="math">\[
x_k=\min\big\{\frac{\bar b_i}{y_{k_i}}\big |y_{k_i}&amp;gt;0\big\}=\frac{\bar b_r}{y_{k_r}}\tag{1.11}
\]&lt;/span> 认为第&lt;span class="math">\(r\)&lt;/span>项即为最小值。回顾一下刚刚的步骤，在目标函数中，因为&lt;span class="math">\(x_i\)&lt;/span>是未定变量，因此我们先简单的使用贪心法，选择正系数最大的&lt;span class="math">\((z_k-c_k)\)&lt;/span>；接下来，再判断&lt;span class="math">\((z_k-c_k)\)&lt;/span>对应的变量&lt;span class="math">\(x_k\)&lt;/span>的取值范围，得到其最大值。&lt;/p>
&lt;p>根据&lt;span class="math">\((1.8)\)&lt;/span>和&lt;span class="math">\((1.11)\)&lt;/span>，我们得到了一个改进可行解，其&lt;span class="math">\(x_r\rightarrow 0, x_k\rightarrow\frac{\bar b_r}{y_{k_r}}\)&lt;/span>，即 &lt;span class="math">\[
\boldsymbol{x_B&amp;#39;}=(x_{B_1},x_{B_2},\dotsb,x_{B_{r-1}},0,x_{B_{r+1}},\dotsb,x_{B_m},0,\dotsb,x_k,\dotsb,0)^T
\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>这个可行解一定是基本可行解&lt;/strong>。这是因为原来的基矩中， &lt;span class="math">\[
\boldsymbol{B}=(\boldsymbol{p_{B_1},p_{B_2},\dotsb,p_{B_m}})
\]&lt;/span> 中的m个列是线性无关的，其中不包含原属于&lt;span class="math">\(\boldsymbol{N}\)&lt;/span>中的列&lt;span class="math">\(\boldsymbol{p_{k}}\)&lt;/span>。根据&lt;span class="math">\((1.7)\)&lt;/span>，有&lt;span class="math">\(\boldsymbol{y_{k}}=\boldsymbol{B^{-1}p_k}\Rightarrow \boldsymbol{p_k}=\boldsymbol{By_k}=\sum_{i=1}^m y_{k_i}\boldsymbol{p_{B_i}}\)&lt;/span>，即&lt;span class="math">\(\boldsymbol{p_k}\)&lt;/span>是线性无关向量组&lt;span class="math">\((\boldsymbol{p_{B_1},p_{B_2},\dotsb,p_{B_m}})\)&lt;/span>的线性组合，且至少存在一个非0系数&lt;span class="math">\(y_{k_r}\)&lt;/span>.根据&lt;strong>替换定理&lt;/strong>，用&lt;span class="math">\(\boldsymbol{p_{k}}\)&lt;/span>来替代&lt;span class="math">\(\boldsymbol{p_{B_r}}\)&lt;/span>得到如下向量组依然是&lt;strong>线性无关的&lt;/strong>。 &lt;span class="math">\[
\boldsymbol{B}=(\boldsymbol{p_{B_1},p_{B_2},\dotsb,p_{B_r}\rightarrow p_{k},\dotsb,p_{B_m}})
\]&lt;/span> 这样可以组成新的基矩阵&lt;span class="math">\(\boldsymbol{B&amp;#39;}\)&lt;/span>，并得到新的基解&lt;span class="math">\(\boldsymbol{x_B&amp;#39;}\)&lt;/span>，而从&lt;span class="math">\((1.8)和(1.11)\)&lt;/span>，我们确定&lt;span class="math">\(\boldsymbol{x_B&amp;#39;}\)&lt;/span>满足每一个分量非负的条件，因此其必然是一个基本可行解。&lt;/p>
&lt;p>经过上述转换，&lt;span class="math">\(x_k\)&lt;/span>由原来的非基变量变成了基变量，而原来的基变量&lt;span class="math">\(x_{B_r}\)&lt;/span>变成了非基变量，同时目标函数值比原来减少了&lt;span class="math">\((z_k-c_k)x_k&amp;gt;0\)&lt;/span>。重复以上过程，进一步改进基本可行解，直到式&lt;span class="math">\((1.6)\)&lt;/span>的结果只能为非正数，以致任何一个非基变量取正值都无法再使得目标函数值降低，此时即为最优解。&lt;/p>
&lt;blockquote>
&lt;p>总结定理1：若在极小化问题中，对于某个基本可行解，所有&lt;span class="math">\((z_j-c_j)≤0\)&lt;/span>，则这个这个基本可行解是最优解；&lt;/p>
&lt;p>若在极大化问题中，对于某个基本可行解，所有&lt;span class="math">\((z_j-c_j)≥0\)&lt;/span>，则这个这个基本可行解是最优解；&lt;/p>
&lt;p>其中，&lt;span class="math">\(z_j-c_j=\boldsymbol{c_B^T B^{-1} p_j}-c_j,\ j=1,\dotsb,n\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>在线性规划中，通常称&lt;span class="math">\(z_j-c_j\)&lt;/span>为&lt;strong>判别数或检验数&lt;/strong>。&lt;/p>
&lt;h2 id="一般计算步骤">一般计算步骤&lt;/h2>
&lt;ol start="0" style="list-style-type: decimal">
&lt;li>首先，我们要获得一个初始基本可行解（可通过直接计算，大M法，两阶段法获得），设初始基矩阵为&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>&lt;/li>
&lt;li>在限制条件&lt;span class="math">\(\boldsymbol{Ax}=\bigl[\boldsymbol{B}\ \boldsymbol{N}\bigr] \bigl[ \begin{smallmatrix} \boldsymbol{x_B} \\ \boldsymbol{x_N} \end{smallmatrix} \bigr]=\boldsymbol{b}\)&lt;/span>中，令非基变量&lt;span class="math">\(\boldsymbol{x_N=0}\)&lt;/span>，则&lt;span class="math">\(\boldsymbol{Bx_B=b}\Rightarrow \boldsymbol{x_B}=\boldsymbol{B^{-1}b}=\boldsymbol{\bar b}\)&lt;/span>，计算目标函数值&lt;span class="math">\(f=\boldsymbol{c^T_Bx_B}\)&lt;/span>&lt;/li>
&lt;li>对于此基矩阵的解集，求&lt;strong>单纯形乘子&lt;/strong>&lt;span class="math">\(\boldsymbol{w^T=c_B^T B^{-1}}\)&lt;/span>（m维行向量）。对于所有非基变量，计算判别数&lt;span class="math">\(\boldsymbol{c_B^T B^{-1}p_j}-c_j=\boldsymbol{w^T p_j}-c_j=z_j-c_j\)&lt;/span>，求得&lt;span class="math">\(z_k-c_k=\max\limits_{j\in R}\{z_j-c_j\}\)&lt;/span>。若&lt;span class="math">\(z_k-c_k≤0\)&lt;/span>，停止计算，目前基本可行解是最优解。否则，进行步骤（3）&lt;/li>
&lt;li>&lt;span class="math">\(\boldsymbol{x_B&amp;#39;}=\boldsymbol{\underbrace{B^{-1}b}_{\bar b}-\underbrace{B^{-1}p_k}_{y_k}}x_k\)&lt;/span>。若&lt;span class="math">\(\boldsymbol{y_k}≤0\)&lt;/span>，即每一个分量都不大于0，则停止计算，问题不存在有限最优解；否则，进行步骤（4）&lt;/li>
&lt;li>确定下标&lt;span class="math">\(r\)&lt;/span>，使得&lt;span class="math">\(x_k=\frac{\bar b_r}{y_{k_r}}=\min\big\{\frac{\bar b_i}{y_{k_i}}\big |y_{k_i}&amp;gt;0\big\}\)&lt;/span>。&lt;span class="math">\(x_{B_r}\)&lt;/span>为离基变量，&lt;span class="math">\(x_k\)&lt;/span>为进基变量，用&lt;span class="math">\(\boldsymbol{p_k}\)&lt;/span>替代变量&lt;span class="math">\(p_{B_r}\)&lt;/span>得到新的基矩阵&lt;span class="math">\(\boldsymbol{B&amp;#39;\rightarrow B}\)&lt;/span>，返回步骤（1）&lt;/li>
&lt;/ol>
&lt;h2 id="收敛性">收敛性&lt;/h2>
&lt;p>对于非退化情形，单纯形法有优秀的结论：&lt;/p>
&lt;blockquote>
&lt;p>定理2：对于非退化问题，单纯形方法经过有限次迭代后，&lt;strong>能达到最优解或得出无界的结论&lt;/strong>。在此条件下，单纯形法是收敛的。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/单纯形法收敛性非退化.png" alt="单纯形法收敛性非退化" />&lt;p class="caption">单纯形法收敛性非退化&lt;/p>
&lt;/div>
&lt;p>对于非退化情形，每次迭代都有 &lt;span class="math">\[
\boldsymbol{x_B=B^{-1}b=\bar b}&amp;gt;0
\]&lt;/span> 此时能保证&lt;span class="math">\(x_r=\frac{b_r}{y_{k_r}}&amp;gt;0\)&lt;/span>。因此经迭代，目标函数值减小，并且由此可知，各次迭代得到的基本可行解互不相同。由于基本可行解的个数有限，因此经有限次迭代必能达到最优解。&lt;strong>对于退化情形，我们在后面将要证明，如果最优解存在，只要采取一定的措施，也能做到有限步收敛&lt;/strong>。&lt;/p>
&lt;h2 id="使用表格形式的单纯形方法">使用表格形式的单纯形方法&lt;/h2>
&lt;p>用单纯形方法求解线性规划问题的过程，实际上就是解线性方程组。只是在每次迭代中，要按一定规则选择自由未知量，以便能够得到改进的基本可行解。这个求解过程可以通过变换单纯形表来实现。具体做法可见《最优化理论与算法（第2版）》第3.1.4节。&lt;/p>
&lt;h2 id="退化情形">退化情形&lt;/h2>
&lt;p>对于退化情形，即存在基变量为0的情形（基变量为0是否一定退化有待验证），经过单纯形法多次迭代后，可能出现循环解现象（退化常见而循环不常见）。因此，我们可以采用&lt;strong>摄动法&lt;/strong>、Bland法，字典序法。&lt;/p></description></item><item><title>优化理论之线性规划</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/</guid><description>
&lt;h2 id="线性规划的标准形式和基本性质">线性规划的标准形式和基本性质&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="标准形式">标准形式&lt;/h2>
&lt;p>&lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\sum_{j=1}^n c_jx_j\\
\mathop{s.t.}\; &amp;amp;\sum_{j=1}^n \alpha_{ij}x_j=b_i,\ &amp;amp;i=1,\dotsb,m\\
&amp;amp;x_j\geq 0,\ &amp;amp;j=1,\dotsb,n\\
\end{aligned}\tag{1.1}
\]&lt;/span> 用矩阵表示为： &lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\boldsymbol c^T \boldsymbol x\\
\mathop{s.t.}\; &amp;amp;\boldsymbol A \boldsymbol x = \boldsymbol b\\
&amp;amp;\boldsymbol x \geq \boldsymbol 0\\
\end{aligned}\tag{1.2}
\]&lt;/span> 其中&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>是&lt;span class="math">\(m\times n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol c^T\)&lt;/span>是n维行向量，&lt;span class="math">\(\boldsymbol b\)&lt;/span>是m维列向量，且&lt;span class="math">\(\boldsymbol b \geq 0\)&lt;/span>(每个分量都大于等于0)&lt;/p>
&lt;h2 id="从非标准形式转化为标准形式">从非标准形式转化为标准形式&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>如果存在&lt;span class="math">\(\boldsymbol b\)&lt;/span>的某一分量小于0，可以把方程两端都乘以-1&lt;/li>
&lt;li>在标准形式中，&lt;span class="math">\(x_j\)&lt;/span>都是非负的，这在实际生活中有着现实意义，但是处理数据时，难免有负数变量，因此可以令&lt;span class="math">\(x_j=x_j&amp;#39;-x_j&amp;#39;&amp;#39;\)&lt;/span>，其中&lt;span class="math">\(x_j&amp;#39;\geq 0,x_j&amp;#39;&amp;#39;\geq 0\)&lt;/span>。即用非负变量替换&lt;span class="math">\(x_j\)&lt;/span>&lt;/li>
&lt;li>当变量有上下界，不符合标准形式的要求时，也可做变量替换。当&lt;span class="math">\(x_j\geq l_j\)&lt;/span>,可令&lt;span class="math">\(x_j&amp;#39;=x_j-l_j\)&lt;/span>,则取&lt;span class="math">\(x_j&amp;#39;\geq 0\)&lt;/span>。当&lt;span class="math">\(x_j\leq u_j\)&lt;/span>时，可令&lt;span class="math">\(x_j&amp;#39;=u_j-x_j\)&lt;/span>，则取&lt;span class="math">\(x_j&amp;#39;\geq 0\)&lt;/span>&lt;/li>
&lt;li>当存在小于等于时，使用正松弛法；当存在大于等于号时用负松弛法。 &lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\sum_{j=1}^n c_jx_j\\
\mathop{s.t.}\; &amp;amp;\sum_{j=1}^n \alpha_{ij}x_j=b_i,\ &amp;amp;i=1,\dotsb,m\\
&amp;amp;x_j\geq 0,\ &amp;amp;j=1,\dotsb,n\\
&amp;amp; 小于等于限制转化为（松弛变量）\Rightarrow\\
&amp;amp;\sum_{j=1}^n \alpha_{ij}x_j + x_{n+i}= b_i,\ &amp;amp;i=1,\dotsb,m\\
&amp;amp; x_{n+1},x_{n+2},\dotsb,x_{n+m}\geq 0
\end{aligned}\tag{1.3}
\]&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>用单纯性方法计算线性规划时，必须要用标准形。&lt;/p>
&lt;h2 id="基本性质">基本性质&lt;/h2>
&lt;h3 id="可行域">可行域&lt;/h3>
&lt;p>线性规划中，所有约束条件均为线性等式及不等式，满足这些条件的点的集合时凸集。即&lt;strong>可行域是凸集&lt;/strong>。&lt;/p>
&lt;h3 id="最优极点">最优极点&lt;/h3>
&lt;p>线性规划如果存在最优解，那么最优解一定能够在某个&lt;strong>极点&lt;/strong>上达到。下面简要证明：根据表示定理（见附录），任何可行点&lt;span class="math">\(\boldsymbol{x}\)&lt;/span>可以表示为极点和极方向的组合： &lt;span class="math">\[
\begin{aligned}
&amp;amp;\boldsymbol x=\sum_{j=1}^k \lambda_j\boldsymbol x^j + \sum_{j=1}^l \mu_j \boldsymbol{d}^j.\\
&amp;amp;\sum_{j=1}^k \lambda_j = 1.\\
&amp;amp;\lambda_j \geq 0,\ j=1,2,\dotsb,k.\\
&amp;amp;\mu_j \geq 0,\ j=1,2,\dotsb,l.
\end{aligned}\tag{2.1}
\]&lt;/span> 把&lt;span class="math">\(\boldsymbol x\)&lt;/span>的表达式代入&lt;span class="math">\((1.2)\)&lt;/span>式，得到以&lt;span class="math">\(\lambda_j, \mu_j\)&lt;/span>为变量的等价的线性规划 &lt;span class="math">\[
\begin{aligned}
\min\ &amp;amp;\boldsymbol x=\sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^j + \sum_{j=1}^l \mu_j \boldsymbol c^T\boldsymbol{d}^j.\\
\mathop{s.t.}\ &amp;amp;\sum_{j=1}^k \lambda_j = 1.\\
&amp;amp;\lambda_j \geq 0,\ j=1,2,\dotsb,k.\\
&amp;amp;\mu_j \geq 0,\ j=1,2,\dotsb,l.
\end{aligned}\tag{2.2}
\]&lt;/span> 注意，我们之前设过&lt;span class="math">\(\boldsymbol c^T\)&lt;/span>是一个行向量。由于&lt;span class="math">\(\mu_j\)&lt;/span>没有上限，因此若对于某个&lt;span class="math">\(j\)&lt;/span>，有&lt;span class="math">\(\boldsymbol c^T \boldsymbol d^j&amp;lt;0\)&lt;/span>，则&lt;span class="math">\(\mu_j \boldsymbol c^T\boldsymbol{d}^j\)&lt;/span>可以随着&lt;span class="math">\(\mu_j\)&lt;/span>的无限增大而无限减小，从而使目标函数趋向&lt;span class="math">\(-\infty\)&lt;/span>。对于这种情形，我们称该问题无界，或不存在有限最优值。&lt;/p>
&lt;p>如果&lt;span class="math">\(\forall j, 有\boldsymbol c^T\boldsymbol{d}^j\geq 0\)&lt;/span>，这时为了极小化目标函数，需令 &lt;span class="math">\[\forall j∈\{1,2,\dotsb,l\},\ \mu_j=0, \tag{2.3}\]&lt;/span> 则&lt;span class="math">\((2.2)\)&lt;/span>可以简写为极点的凸组合： &lt;span class="math">\[
\begin{aligned}
\min \ &amp;amp; \sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^j\\
\mathop{s.t.}\ &amp;amp;\sum_{j=1}^k \lambda_j=1,\\
&amp;amp;\lambda_j\geq 0,\ j=1,2,\dotsb,k
\end{aligned}\tag{2.4}
\]&lt;/span> 在所有&lt;span class="math">\(\boldsymbol c^T\boldsymbol x^j\)&lt;/span>中，必然有一个最小的值，令其为 &lt;span class="math">\[
\boldsymbol c^T\boldsymbol x^p = \min_{1\leq j \leq k} \boldsymbol c^T\boldsymbol x^j \tag{2.5}
\]&lt;/span> 显然当 &lt;span class="math">\[
\lambda_p=1\ 且 \lambda_j = 0,\ j\neq p\tag{2.6}
\]&lt;/span> 时，目标函数取最小值，即&lt;span class="math">\((2.3)\)&lt;/span>和&lt;span class="math">\((2.6)\)&lt;/span>式组合到一起时线性规划&lt;span class="math">\((2.2)\)&lt;/span>的最优解。此时必有 &lt;span class="math">\[
\begin{aligned}
\boldsymbol c^T\boldsymbol x &amp;amp;= \sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^j + \sum_{j=1}^l \mu_j \boldsymbol c^T\boldsymbol{d}^j\\
&amp;amp;\geq \sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^j\\
&amp;amp;\geq \sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^p=\boldsymbol c^T\boldsymbol x^p
\end{aligned}
\]&lt;/span> 因此，极点&lt;span class="math">\(\boldsymbol x^p\)&lt;/span>是线性规划的解。&lt;/p>
&lt;blockquote>
&lt;p>定理1：设线性规划&lt;span class="math">\((1.2)\)&lt;/span>的可行域非空，则有下列结论：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>线性规划&lt;span class="math">\((1.2)\)&lt;/span>存在有限最优解的充要条件是所有&lt;span class="math">\(\boldsymbol c^T\boldsymbol d^j\)&lt;/span>为非负数。其中，&lt;span class="math">\(\boldsymbol d^j\)&lt;/span>是可行域的极方向。&lt;/li>
&lt;li>若线性规划&lt;span class="math">\((1.2)\)&lt;/span>存在优先最优解，则目标函数的最优值可在某个极点上达到。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>一般情况下，把无界问题也归为不存在最优解，&lt;strong>只讨论存在有限最优解的情形&lt;/strong>。&lt;/p>
&lt;h2 id="最优基本可行解">最优基本可行解&lt;/h2>
&lt;p>基矩阵、基（本）解（基矩阵的解）（基解vs可行解）--》&lt;/p>
&lt;p>基解由&lt;span class="math">\(\boldsymbol{x_b}\)&lt;/span>的基变量与非基变量&lt;span class="math">\(\boldsymbol{x_n}\)&lt;/span>组成--》&lt;/p>
&lt;p>不是所有基解都是满足非负条件。满足&lt;strong>非负条件&lt;/strong>的基解为基本可行解（=基解 &amp;amp;&amp;amp; 可行解）（解的所有分量大于等于0）--》&lt;/p>
&lt;p>退化（某个基变量取值为0）与非退化（基变量取值全部大于0）--》&lt;/p>
&lt;p>基本可行解与极点有着对应关系--》&lt;/p>
&lt;p>线性规化最优解为最优基本可行解&lt;/p>
&lt;blockquote>
&lt;p>定理2：令&lt;span class="math">\(K=\{\boldsymbol x|\boldsymbol A \boldsymbol x=\boldsymbol b,\boldsymbol x\geq 0 \}, \boldsymbol A 为 m\times n\)&lt;/span>矩阵。&lt;span class="math">\(\boldsymbol A\)&lt;/span>的秩为&lt;span class="math">\(m\)&lt;/span>，则&lt;span class="math">\(K\)&lt;/span>的极点集和&lt;span class="math">\(\boldsymbol A \boldsymbol x = \boldsymbol b, \boldsymbol x\geq 0\)&lt;/span>的基本可行解集&lt;strong>等价&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：可见最优化理论与算法（第2版）定理2.2.3的证明。&lt;/p>
&lt;p>根据定理1和定理2，我们可知&lt;strong>线性规划的求解问题归结为求最优基本可行解&lt;/strong>。&lt;/p>
&lt;h2 id="基本可行解的存在问题">基本可行解的存在问题&lt;/h2>
&lt;p>在什么条件下存在最优解呢？其实&lt;strong>表示定理的第一点&lt;/strong>已经明示了答案。&lt;/p>
&lt;blockquote>
&lt;p>若&lt;span class="math">\(S=\{\boldsymbol x \vert \boldsymbol A \boldsymbol x= \boldsymbol b, \boldsymbol x \geq 0\}\)&lt;/span>为非空多面集，则有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>极点集非空，且存在有限个极点&lt;span class="math">\(\boldsymbol x^1,\boldsymbol x^2,\dotsb,\boldsymbol x^k\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>而有限的极点集必存在一个使目标函数最小的极点。如果从方程求解的角度来看，可以等价为以下定理：&lt;/p>
&lt;blockquote>
&lt;p>定理3： 如果&lt;span class="math">\(\boldsymbol A \boldsymbol x= \boldsymbol b, \boldsymbol x \geq 0\)&lt;/span>有可行解，则一定存在基本可行解。其中&lt;span class="math">\(\boldsymbol A\)&lt;/span>是&lt;span class="math">\(m \times n\)&lt;/span>维矩阵，且&lt;span class="math">\(rank(\boldsymbol A)=m\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="附录表示定理">附录：表示定理&lt;/h2>
&lt;blockquote>
&lt;p>表示定理：设&lt;span class="math">\(S=\{\boldsymbol x \vert \boldsymbol A \boldsymbol x= \boldsymbol b, \boldsymbol x \geq 0\}\)&lt;/span>为非空多面集，则有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>极点集非空，且存在有限个极点&lt;span class="math">\(\boldsymbol x^1,\boldsymbol x^2,\dotsb,\boldsymbol x^k\)&lt;/span>&lt;/li>
&lt;li>极方向集合为空集的充要条件是&lt;span class="math">\(S\)&lt;/span>有界。若&lt;span class="math">\(S\)&lt;/span>无界，则存在有限个极方向&lt;span class="math">\(\boldsymbol d^1,\boldsymbol d^2,\dotsb,\boldsymbol d^l\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\boldsymbol x\in S\)&lt;/span>的充要条件是式&lt;span class="math">\((2.1)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>关于上述定理的证明可见文章&lt;/p>
&lt;p>[1] Bazaraa M S, Jarvis J J. Linear programming and Network flows， New York： Wiley 1977&lt;/p></description></item><item><title>优化理论之线性规划的初始基本可行解.md</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%88%9D%E5%A7%8B%E5%9F%BA%E6%9C%AC%E5%8F%AF%E8%A1%8C%E8%A7%A3.md/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%88%9D%E5%A7%8B%E5%9F%BA%E6%9C%AC%E5%8F%AF%E8%A1%8C%E8%A7%A3.md/</guid><description>
&lt;h2 id="线性规划的初始基本可行解">线性规划的初始基本可行解&lt;!-- omit in toc -->&lt;/h2>
&lt;p>我们使用单纯形法的时候，需要一个初始基本可行解来开始迭代优化，求出其他改进的基本可行解。下面我们介绍两个求初始基本可行解的方法。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#两阶段法">两阶段法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两阶段法核心思想">两阶段法核心思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大m法">大M法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大m法核心思想">大M法核心思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单个人工变量技巧">单个人工变量技巧&lt;/a>&lt;/li>
&lt;li>&lt;a href="#注意人工变量vs松弛变量">注意：人工变量vs松弛变量&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="两阶段法">两阶段法&lt;/h2>
&lt;h3 id="两阶段法核心思想">两阶段法核心思想&lt;/h3>
&lt;p>第一阶段：每个方程增加一个系数为1的&lt;strong>非负人工变量&lt;/strong>，这些人工变量组成单位矩阵，可求出一组拓展的基本可行解。接下来用消主元法消去所有添加的人工变量（使人工变量都是非基变量取0，不必遵守单纯形法的主元选择规则），剩下的就是一个初始基本可行解。&lt;/p>
&lt;p>第二阶段：根据第一阶段得到的初始基本可行解，用一般单纯形法求解问题。&lt;/p>
&lt;p>总的来说，第一阶段问题要让人工变量为0，第二阶段正常解原问题。&lt;/p>
&lt;h2 id="大m法">大M法&lt;/h2>
&lt;h3 id="大m法核心思想">大M法核心思想&lt;/h3>
&lt;p>类似于惩罚函数法。在约束中增加&lt;strong>非负人工变量&lt;/strong>,&lt;span class="math">\(\boldsymbol{x_a}\)&lt;/span>，同时修改目标函数，增加惩罚项&lt;span class="math">\(M\boldsymbol{e^Tx_a}\)&lt;/span>，其中&lt;span class="math">\(M\)&lt;/span>是一个很大的正数，这样由于极小化目标函数的过程中，必然要优化&lt;span class="math">\(M\)&lt;/span>，将迫使人工变量离基。&lt;/p>
&lt;p>具体方式：目标函数加上带&lt;span class="math">\(M\)&lt;/span>的人工变量，限制条件中增加非负人工变量，再用一般单纯形法求解。&lt;/p>
&lt;p>由于添加的非负人工变量的系数为1，能够组成单位阵，因此很容易获得初始基本可行解。&lt;/p>
&lt;h2 id="单个人工变量技巧">单个人工变量技巧&lt;/h2>
&lt;p>处理&lt;span class="math">\(\boldsymbol{\bar b=B^{-1}b}\)&lt;/span>存在负数项情形，引入一个标量&lt;span class="math">\(x_a\)&lt;/span>，转换为 &lt;span class="math">\[
\boldsymbol{x_B}+\boldsymbol{B^{-1}N x_N}-x_a \boldsymbol{e}=\boldsymbol{\bar b}\\
\boldsymbol{x}=\begin{bmatrix}\boldsymbol{x_B} \\ \boldsymbol{x_N}\end{bmatrix}≥\boldsymbol{0},x_a≥0
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{e}=\{1,1,\dotsb,1\}^T\)&lt;/span>是m维列向量。为了消除&lt;span class="math">\(\boldsymbol{]\bar b}\)&lt;/span>中的所有负数，可以借以将&lt;span class="math">\(x_a\)&lt;/span>引入基变量来实现。令 &lt;span class="math">\[
b_r = \min\{b_i\}&amp;lt;0
\]&lt;/span> 以&lt;span class="math">\(x_a\)&lt;/span>所在列为主列，第r行为主行进行主元消去（&lt;span class="math">\(x_a\rightarrow x_r\)&lt;/span>）。此时，约束方程右端变为 &lt;span class="math">\[
\begin{cases}
\bar b_r&amp;#39; = -\bar b_r &amp;gt;0 \\
\bar b_i&amp;#39; = \bar b_i - \bar b_r ≥0, i\neq r
\end{cases}
\]&lt;/span> 于是，我们得到了一个关于添加单个人工变量的一个基本可行解（&lt;span class="math">\(\boldsymbol{\bar b}≥0\)&lt;/span>）。再由此基本可行解出发，用两阶段法或大M法求解。&lt;/p>
&lt;h2 id="注意人工变量vs松弛变量">注意：人工变量vs松弛变量&lt;/h2>
&lt;p>人工变量与前面介绍过的松弛变量是两个不同的概念。松弛变量的作用是把不等式约束改写成等式约束，改写前后的两个问题是等价的。松弛变量的取值能够表达现行的可行点是在可行域的内部还是在其边界，也就是说，在此可行解处，原来的约束是成立严格不等式还是等式。因此，松弛变量是“合法”的变量。而人工变量的引入，改变了原来的约束条件，从这个意义上讲，它们是“不合法”的变量。这两种变量不可混为一谈。&lt;/p></description></item><item><title>优化理论-框架</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-%E6%A1%86%E6%9E%B6/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-%E6%A1%86%E6%9E%B6/</guid><description>
&lt;h2 id="优化理论框架">优化理论框架&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#优化理论解的存在性">优化理论解的存在性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束数值优化条件">无约束数值优化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束优化一阶条件">无约束优化一阶条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束优化二阶条件">无约束优化二阶条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束数值优化方法">无约束数值优化方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方向确定">方向确定&lt;/a>&lt;/li>
&lt;li>&lt;a href="#步长确定-精确搜索与不精确搜索">步长确定-精确搜索与不精确搜索&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#线性搜索法">线性搜索法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#信任法信赖域levenberg-marquardt方法">信任法（信赖域）——Levenberg-Marquardt方法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化条件">有约束数值优化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化一阶条件">有约束数值优化一阶条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kkt点">KKT点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化二阶条件">有约束数值优化二阶条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化方法">有约束数值优化方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#转换为无约束优化拉格朗日法">转换为无约束优化——拉格朗日法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#barrier-methods">Barrier methods&lt;/a>&lt;/li>
&lt;li>&lt;a href="#惩罚函数">惩罚函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#收敛速度">收敛速度&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="优化理论解的存在性">优化理论解的存在性&lt;/h2>
&lt;p>解的存在性：&lt;/p>
&lt;ul>
&lt;li>维尔斯特拉斯极值定理：若函数&lt;span class="math">\(f\)&lt;/span>在n维有界闭区域&lt;span class="math">\(S\)&lt;/span>上连续，那么&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(S\)&lt;/span>上一定有全局最小值；&lt;/li>
&lt;li>若函数&lt;span class="math">\(f\)&lt;/span>在n维闭区域&lt;span class="math">\(S\)&lt;/span>上连续并且向正无穷发散(coercive，即&lt;span class="math">\(lim||x||⇒∞,f(x)=+∞\)&lt;/span>)，那么&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(S\)&lt;/span>上一定有全局最小值。&lt;/li>
&lt;/ul>
&lt;p>以上定理只能保证最小值存在，没有建立最小值和极小值之间的关系。但是，对于一类特殊的函数，它在一定区域内的极小值一定是最小值，这类函数即&lt;strong>凸函数&lt;/strong>。凸函数为定义在凸区间上的一种函数，它满足任意两点的连线位于抽象的函数曲面之下；而凸区间则满足任意两点连线仍然在区间中。定义在凸区间内的严格凸函数有唯一的极小值，该极小值为该函数在该区间上的最小值。&lt;/p>
&lt;h2 id="无约束数值优化条件">无约束数值优化条件&lt;/h2>
&lt;p>参见笔记——《最优化条件：无约束问题最优化条件》&lt;/p>
&lt;h3 id="无约束优化一阶条件">无约束优化一阶条件&lt;/h3>
&lt;h3 id="无约束优化二阶条件">无约束优化二阶条件&lt;/h3>
&lt;h2 id="无约束数值优化方法">无约束数值优化方法&lt;/h2>
&lt;p>线搜索和信任域（信赖域），两者主要区别是计算步长和搜索方向的步骤相反。线搜索的思路是先选择搜索方向,然后才是长度。信任域的思路是先选择长度（信任域半径），然后选择方向及其长度。&lt;/p>
&lt;p>对于搜索方向的选择，我们又有最速下降、牛顿、拟牛顿、非线性共轭梯度等方法。主要基础理论就是泰勒定理和矩阵相关知识。&lt;/p>
&lt;h3 id="方向确定">方向确定&lt;/h3>
&lt;ul>
&lt;li>梯度法（最速下降法）&lt;/li>
&lt;li>牛顿法&lt;/li>
&lt;li>拟牛顿法&lt;/li>
&lt;li>DFP算法&lt;/li>
&lt;li>BFGS算法&lt;/li>
&lt;li>L-BFGS算法&lt;/li>
&lt;/ul>
&lt;h3 id="步长确定-精确搜索与不精确搜索">步长确定-精确搜索与不精确搜索&lt;/h3>
&lt;p>两个常用的步长控制算法是&lt;strong>线搜索&lt;/strong>和&lt;strong>信赖域&lt;/strong>方法。在一个线搜索方法中，模型函数给出一个步骤方向，然后沿着这个方向搜索以找到一个可以达到收敛的适当的点。在信赖域方法中，每一步要更新一个距离，而在这个距离内模型函数是被信任的。如果模型步骤位于该距离之内，则可以被使用；否则，模型函数在信赖域边界上的极小值将被使用。一般来说，信赖域方法更稳健，但是它们需要更多的数值线性代数运算。&lt;/p>
&lt;h4 id="线性搜索法">线性搜索法&lt;/h4>
&lt;p>线搜索技术是多变量函数优化的基础，它包括&lt;strong>精确线搜索技术&lt;/strong>和&lt;strong>非精确线搜索技术&lt;/strong>。常见的精确线搜索技术可分为&lt;strong>分割方法和插值方法&lt;/strong>两大类：分割方法有二分法、黄金分割法、斐波那契法等；插值方法有一点二次插值法（牛顿法）、二点二次插值法（包括割线法）、三点二次插值法、二点三次插值法等。非精确线搜索技术基于&lt;strong>非精确线搜索准则&lt;/strong>，常用的准则有 Armijo-Goldstein 准则、Wolfe-Powell 准则、强 Wolfe-Powell 准则和简单准则。这里特别指出精确与非精确是指对&lt;strong>步长搜索的精确性&lt;/strong>。&lt;/p>
&lt;p>什么是线搜索呢？我们知道最优化算法的基本框架就是依据&lt;strong>迭代公式&lt;span class="math">\(\vec x_{k+1}=\vec x_k+\alpha_k\vec d_k\)&lt;/span>不断产生下一个迭代点&lt;/strong>，直到满足某些终止条件。迭代公式的确定涉及搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>和搜索步长&lt;span class="math">\(\alpha_k\)&lt;/span>的确定，其中&lt;span class="math">\(\alpha_k\)&lt;/span>的确定过程就是所谓线搜索的过程。&lt;/p>
&lt;p>精确线搜索的精确有两种理解。第一，对于分割法，精确搜索每次使得搜索空间有固定比例的缩小。例如&lt;span class="math">\(l_k\)&lt;/span>是第&lt;span class="math">\(k\)&lt;/span>次搜索空间大小，则第&lt;span class="math">\(k+1\)&lt;/span>次搜索空间就是&lt;span class="math">\(\alpha l_k(\alpha ∈ (0,1))\)&lt;/span>，第二，对于插值法，求使得近似（多项式）函数&lt;span class="math">\(p(\vec x_k+\alpha_k\vec d_k)\)&lt;/span>取到最小值时对应的&lt;span class="math">\(\alpha\)&lt;/span>，换句话说：精确线搜索求使得目标函数沿搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>下降最多的搜索步长。其他精确线搜索还有共轭梯度法conjugate gradient method。&lt;/p>
&lt;p>非精确线搜索没有找出导数为零的点,而是使f(x)有一个充分的下降(sufficient descent)，即只求使得目标函数沿搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>有一定下降的搜索步长，这个步长选择不好甚至可能会导致偏离搜索目标（比如梯度下降法中学习率过大的情况）。非精确搜索方法的基础是backtracking，如果能够收敛，则需要考虑以下几种条件。&lt;/p>
&lt;ul>
&lt;li>wolfe conditions&lt;/li>
&lt;li>Curvature condition&lt;/li>
&lt;li>Armijo conditions&lt;/li>
&lt;li>goldstein conditions&lt;/li>
&lt;/ul>
&lt;p>需要指出的是，非精确搜索和精确搜索不是完全互斥的，一些精确搜索方法也满足非精确线搜索准则。&lt;/p>
&lt;p>实际计算中，&lt;strong>通常采用非精确线搜索技术&lt;/strong>，这是因为精确线搜索技术耗费大量的计算资源，而且对于多变量函数的优化，许多算法的收敛速度并不取决于是否采用精确线搜索技术。&lt;/p>
&lt;h4 id="信任法信赖域levenberg-marquardt方法">信任法（信赖域）——Levenberg-Marquardt方法&lt;/h4>
&lt;h2 id="有约束数值优化条件">有约束数值优化条件&lt;/h2>
&lt;p>参见笔记——《最优化条件：有约束问题最优化条件》&lt;/p>
&lt;h3 id="有约束数值优化一阶条件">有约束数值优化一阶条件&lt;/h3>
&lt;h3 id="kkt点">KKT点&lt;/h3>
&lt;h3 id="有约束数值优化二阶条件">有约束数值优化二阶条件&lt;/h3>
&lt;h2 id="有约束数值优化方法">有约束数值优化方法&lt;/h2>
&lt;h3 id="转换为无约束优化拉格朗日法">转换为无约束优化——拉格朗日法&lt;/h3>
&lt;h3 id="barrier-methods">Barrier methods&lt;/h3>
&lt;h3 id="惩罚函数">惩罚函数&lt;/h3>
&lt;h2 id="收敛速度">收敛速度&lt;/h2>
&lt;p>如果我们假设了算法产生的迭代点列&lt;span class="math">\(\{ {x_k}\}\)&lt;/span>是在某种范数下收敛的, 即&lt;span class="math">\(\lim\limits_{k→∞}||x_k–x^\ast||=0\)&lt;/span>, 那么考察收敛速度的话, &lt;strong>Q-收敛速度&lt;/strong>较为常用是这么定义的:&lt;/p>
&lt;p>若存在实数&lt;span class="math">\(\alpha &amp;gt; 0\)&lt;/span>及一个与迭代次数&lt;span class="math">\(k\)&lt;/span>无关的常数&lt;span class="math">\(q &amp;gt; 0\)&lt;/span>, 使得 &lt;span class="math">\[\lim_{k→∞}\frac{||x_{k+1}–x^\ast||}{||x_k–x^\ast||^\alpha} = q\]&lt;/span>&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>线性收敛速度: &lt;span class="math">\(\alpha = 1,q &amp;gt; 0\)&lt;/span>&lt;/li>
&lt;li>超线性收敛速度: &lt;span class="math">\(1 &amp;lt; \alpha &amp;lt; 2,q &amp;gt; 0\)&lt;/span>或者&lt;span class="math">\(\alpha = 1,q = 0\)&lt;/span>&lt;/li>
&lt;li>二阶收敛速度: &lt;span class="math">\(\alpha = 2\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>注: 还有其他的一些收敛速度的定义, 但并不常用。&lt;/p></description></item><item><title>优化理论-基础</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-%E5%9F%BA%E7%A1%80/</link><pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-%E5%9F%BA%E7%A1%80/</guid><description>
&lt;h2 id="优化理论基础">优化理论基础&lt;!-- omit in toc -->&lt;/h2>
&lt;p>证明链条：凸集与超平面-&amp;gt;凸集分离定理-&amp;gt;Farkas引理&amp;gt;Gordan定理和择一性定理-&amp;gt;Kuhn-Tucker条件&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#凸集的定义和性质">凸集的定义和性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#性质">性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#凸集的极点">凸集的极点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#凸集的方向">凸集的方向&lt;/a>&lt;/li>
&lt;li>&lt;a href="#射线">射线&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方向的定义">方向的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#极方向">极方向&lt;/a>&lt;/li>
&lt;li>&lt;a href="#凸集的极射线">凸集的极射线&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多面集的表示定理">多面集的表示定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多面集表示定理">多面集表示定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#锥与凸锥">锥与凸锥&lt;/a>&lt;/li>
&lt;li>&lt;a href="#凸集分离定理">凸集分离定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#farkas引理">Farkas引理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gordan定理">Gordan定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录">附录&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="凸集的定义和性质">凸集的定义和性质&lt;/h2>
&lt;p>设&lt;span class="math">\(S⊆R^n\)&lt;/span>，若对&lt;span class="math">\(∀x_1,x_2∈S\)&lt;/span>及&lt;span class="math">\(∀λ∈[0,1]\)&lt;/span>，都有&lt;span class="math">\(λx_1+(1−λ)x_2∈S\)&lt;/span>，则称&lt;span class="math">\(S\)&lt;/span>为凸集。&lt;/p>
&lt;h3 id="性质">性质&lt;/h3>
&lt;p>设&lt;span class="math">\(S_1和S_2\)&lt;/span>是两个凸集，&lt;span class="math">\(β\)&lt;/span>实数，则&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\beta S_1=\{\beta x|x \in S_1\}\)&lt;/span>是凸集&lt;/li>
&lt;li>&lt;span class="math">\(S_1+S_2= \{x_1+x_2|x_1 \in S_1, x_2 \in S_2\}\)&lt;/span>是凸集&lt;/li>
&lt;li>&lt;span class="math">\(S_1-S_2=\{x_1-x_2|x_1 \in S_1, x_2 \in S_2\}\)&lt;/span>是凸集&lt;/li>
&lt;li>&lt;span class="math">\(S_1 \cap S_2\)&lt;/span>是凸集&lt;/li>
&lt;/ul>
&lt;h2 id="凸集的极点">凸集的极点&lt;/h2>
&lt;p>设&lt;span class="math">\(S\)&lt;/span>是非空集合，&lt;span class="math">\(x∈S\)&lt;/span>，若&lt;span class="math">\(x\)&lt;/span>不能表示成&lt;span class="math">\(S\)&lt;/span>中两个不同点的凸组合，即若假设&lt;span class="math">\(x=λx_1+(1−λ)x_2\)&lt;/span>，必推出&lt;span class="math">\(x=x_1=x_2\)&lt;/span>，则称&lt;span class="math">\(x\)&lt;/span>是凸集&lt;span class="math">\(S\)&lt;/span>的极点。从图形的角度来看，&lt;span class="math">\(x\)&lt;/span>是多边形的顶点或曲边的点。&lt;/p>
&lt;h2 id="凸集的方向">凸集的方向&lt;/h2>
&lt;h3 id="射线">射线&lt;/h3>
&lt;p>&lt;span class="math">\(∀α_0,\vec{d}\in R^n,\vec{d}≠0\)&lt;/span>称集合 &lt;span class="math">\(S=\{α_0+k\vec{d}|k∈R,k≥0\}\)&lt;/span>为一条射线。称&lt;span class="math">\(α_0\)&lt;/span>为射线的顶点，&lt;span class="math">\(\vec{d}\)&lt;/span>为射线的方向。&lt;/p>
&lt;p>射线&lt;span class="math">\(S=\{\alpha_0+k\vec{d}|k \in R,k\geq 0\}\)&lt;/span>是凸集。&lt;/p>
&lt;p>&lt;strong>证明：&lt;/strong>&lt;span class="math">\(\forall \alpha,\beta \in S,\exist k_1 \in R,k_1\geq 0,\)&lt;/span>使得&lt;span class="math">\(\alpha = \alpha_0+k_1\vec{d};\exist k_2 \in R,k_2\geq 0,\)&lt;/span>使得&lt;span class="math">\(\beta = c\)&lt;/span>。对于&lt;span class="math">\(\forall \lambda \in [0,1],\lambda k_1+(1-\lambda)k_2 \geq 0\)&lt;/span>，于是&lt;span class="math">\(\lambda\alpha+(1-\lambda)\beta=\lambda(\alpha_0+k_1\vec{d})+(1-\lambda)(\alpha_0+k_2\vec{d})=\alpha_0+[\lambda k_1+(1-\lambda)k_2]\vec{d}\in S\)&lt;/span>。即射线&lt;span class="math">\(S\)&lt;/span>为凸集。&lt;/p>
&lt;h3 id="方向的定义">方向的定义&lt;/h3>
&lt;p>对于任意一个凸集&lt;span class="math">\(S\)&lt;/span>，&lt;span class="math">\(\forall \vec{d}\in R^n,d \neq 0,若\forall \alpha_0 \in S,\forall k \in R,k \geq 0,都有\alpha_0+k\vec{d} \in S\)&lt;/span>，则称&lt;span class="math">\(\vec{d}\)&lt;/span>为&lt;span class="math">\(S\)&lt;/span>的一个方向。&lt;/p>
&lt;p>&lt;strong>comment：&lt;/strong> 方向能够无限延伸，说明在此方向上所有的点都在凸集内。方向是射线，起点是任意的，在方向上无界。&lt;/p>
&lt;p>设&lt;span class="math">\(S=\{x∣Ax=b,x≥0\}≠∅\)&lt;/span>，&lt;span class="math">\(\vec{d}\)&lt;/span>是非零向量，则&lt;span class="math">\(\vec{d}\)&lt;/span>是&lt;span class="math">\(S\)&lt;/span>的方向 ⟺ &lt;span class="math">\(\vec{d}≥0且A\vec{d}=0\)&lt;/span>。&lt;/p>
&lt;h3 id="极方向">极方向&lt;/h3>
&lt;p>对于任意一个凸集&lt;span class="math">\(S\)&lt;/span>,对于 &lt;span class="math">\(S\)&lt;/span>中的任意两个方向&lt;span class="math">\(\vec{d}_1,\vec{d}_2\)&lt;/span>, 若&lt;span class="math">\(\forall k&amp;gt;0,\vec{d}_1\neq k \vec{d}_2\)&lt;/span>,则称这两个方向是不同的。&lt;/p>
&lt;p>若&lt;span class="math">\(\vec{d}\)&lt;/span>是S的一个方向，且不是S的任意两个&lt;strong>不同&lt;/strong>方向的&lt;strong>正线性组合&lt;/strong>。即：对于S的任意两个方向&lt;span class="math">\(\vec{d}_1,\vec{d}_2\)&lt;/span>，若存在&lt;span class="math">\(k_1&amp;gt;0,k_2&amp;gt;0\)&lt;/span>，使得&lt;span class="math">\(k_1\vec{d}_1+k_2\vec{d}_2=\vec{d}\)&lt;/span>，则&lt;span class="math">\(\vec{d}_1,\vec{d}_2\)&lt;/span>必是相同的方向，则称&lt;span class="math">\(\vec{d}\)&lt;/span>是S的一个极方向。&lt;/p>
&lt;h3 id="凸集的极射线">凸集的极射线&lt;/h3>
&lt;p>凸集S中的任意一条射线，若它的方向是极方向，则称这条射线为极射线。&lt;/p>
&lt;h2 id="多面集的表示定理">多面集的表示定理&lt;/h2>
&lt;p>&lt;strong>多面集&lt;/strong>：在空间&lt;span class="math">\(\mathbb{R}^n\)&lt;/span>中，由有限个闭半空间相交组成的集合： &lt;span class="math">\[S=\{x \in \mathbb{R}^n:P_i^T x \leq \alpha_i,i=1,2,\dotsb,m\}\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>多面锥&lt;/strong>：在空间&lt;span class="math">\(\mathbb{R}^n\)&lt;/span>中，由有限个&lt;strong>包含原点&lt;/strong>的半空间相交而成： &lt;span class="math">\[S=\{x \in \mathbb{R}^n:P_i^T x \leq 0,i=1,2,\dotsb,m\}\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>多面体&lt;/strong>：有界的多面集。&lt;/p>
&lt;ul>
&lt;li>多面体是有限个点集的凸包。&lt;/li>
&lt;li>凸锥可以有有限个向量生成（凸锥组合）&lt;/li>
&lt;li>多面集是闭集&lt;/li>
&lt;li>多面集是凸集&lt;/li>
&lt;/ul>
&lt;h3 id="多面集表示定理">多面集表示定理&lt;/h3>
&lt;p>设&lt;span class="math">\(S=\{X∈\mathbb{R}^n:AX≤\vec{b},X≥\vec{0}\}\)&lt;/span>为非空多面集，其中 &lt;span class="math">\[A=\begin{pmatrix}
a_1 \\
a_2 \\
\vdots\\
a_m
\end{pmatrix} \in \mathbb{R}^{m\times n},
b=\begin{pmatrix}
b_1 \\
b_2 \\
\vdots\\
b_m
\end{pmatrix} \in \mathbb{R}^{m}\]&lt;/span> 则有：&lt;/p>
&lt;ul>
&lt;li>极点集非空，且存在有限个极点&lt;span class="math">\(\{X(1),⋯,X(k)\}, k \in N,k \geq 1\)&lt;/span>&lt;/li>
&lt;li>极方向集合为空集 ⟺ &lt;span class="math">\(S\)&lt;/span>有界。若&lt;span class="math">\(S\)&lt;/span>无界，则存在有限个极方向&lt;span class="math">\(\{d(1),d(2),⋯,d(l)\},l \in N ,l \geq 0\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(∀X∈R_n,X∈S\)&lt;/span>当且仅当&lt;span class="math">\(X\)&lt;/span> 可以被表示成&lt;span class="math">\(X_1,⋯,X_k\)&lt;/span>的凸组合加上&lt;span class="math">\(d_1,⋯,d_l\)&lt;/span>的非负线性组合，即存在集合 &lt;span class="math">\[\{λ_i∈R:\sum_{i=1}^k λ_i=1,λ_i≥0,i∈N,1≤i≤k\}与 \\
\{ \mu_i \in \mathbb R: \mu_i \ge 0, i \in \mathbb N, 1 \le i \le l \} \\
使得：X = \sum\limits_{i = 1}^{k} \lambda_i X_i + \sum\limits_{i = 1}^{l} \mu_i d_i\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;h2 id="锥与凸锥">锥与凸锥&lt;/h2>
&lt;p>&lt;strong>锥(cone)&lt;/strong> 定义：对于集合&lt;span class="math">\(C⊆R_n,∀x∈C,θ≥0\)&lt;/span>，有&lt;span class="math">\(θx⊆C\)&lt;/span>则&lt;span class="math">\(x\)&lt;/span>构成的集合称为锥。说明一下，锥不一定是连续的（可以是数条过原点的射线的集合）。&lt;/p>
&lt;p>&lt;strong>凸锥&lt;/strong>（convex cone）定义：凸锥包含了集合内点的所有凸锥组合。若锥&lt;span class="math">\(C⊆R_n,x_1,x_2...x_n∈C,θ_i≥0\)&lt;/span>，则凸锥组合&lt;span class="math">\(θ_1 x_1+θ_2 x_2+...+θ_n x_n\)&lt;/span>也属于凸锥集合&lt;span class="math">\(C\)&lt;/span>。这里说明一下，就是说一个集合&lt;strong>既是凸集又是锥&lt;/strong>，那么就是凸锥。&lt;/p>
&lt;p>&lt;strong>凸锥包&lt;/strong>（convex cone hull）定义：凸锥包是包含集合&lt;span class="math">\(C\)&lt;/span>的最小的凸锥，假设&lt;span class="math">\(x_1,x2_...x_n∈C\)&lt;/span>，凸锥包表示为： &lt;span class="math">\[\{θ_1x_1+θ_2x_2+...+θ_nx_n|x_1,x_2...x_n∈C，θ_i≥0\}\]&lt;/span>&lt;/p>
&lt;p>另见常见的凸集，凸锥，仿射集:&lt;a href="https://www.cnblogs.com/saysei/p/10124314.html">https://www.cnblogs.com/saysei/p/10124314.html&lt;/a>&lt;/p>
&lt;h2 id="凸集分离定理">凸集分离定理&lt;/h2>
&lt;p>&lt;strong>凸集分离定理&lt;/strong>（超平面分离定理）是应用凸集到最优化理论中的重要结果，这个结果在最优化理论中有重要的位置。所谓&lt;strong>两个凸集分离&lt;/strong>，直观地看是指两个凸集合没有交叉和重合的部分，因此可以用&lt;strong>一张超平面&lt;/strong>将两者隔在两边。&lt;strong>需要注意的是，凸集分离定理研究n维欧式空间，所以“紧”的概念等同于闭合+有界&lt;/strong>。&lt;/p>
&lt;p>设&lt;span class="math">\(S_1,S_2\subseteq R^n\)&lt;/span>为两个非空集合,且&lt;span class="math">\(S_1\cap S_2=\emptyset\)&lt;/span>，如果存在非零向量&lt;span class="math">\(\vec{p}\in R^n\)&lt;/span>及&lt;span class="math">\(\alpha \in R\)&lt;/span>使得 &lt;span class="math">\[S_1 \subseteq H^-=\{x\in R^n|\vec{p}^Tx\leq \alpha\}\]&lt;/span> &lt;span class="math">\[S_2 \subseteq H^+=\{x\in R^n|\vec{p}^Tx\geq \alpha\}\]&lt;/span> 则称超平面&lt;span class="math">\(H=\{x \in R^n |\vec{p}^T x=\alpha\}\)&lt;/span>分离了集合&lt;span class="math">\(S_1与S_2\)&lt;/span>。&lt;/p>
&lt;p>&lt;strong>证明：&lt;/strong> 首先我们需要证明一个引理。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>引理&lt;/strong> &lt;span class="math">\(K\subset R^n\)&lt;/span>，且K是非空闭凸集。那么在K种&lt;strong>存在唯一一个向量&lt;/strong>具有最小范数（长度）。&lt;/p>
&lt;p>&lt;strong>引理证明&lt;/strong>：令&lt;span class="math">\(\delta=\inf\{|x|\mid x\in K\}\)&lt;/span>。令&lt;span class="math">\(x_j\)&lt;/span>为K中的一个序列，且&lt;span class="math">\(|x_j|\to\delta\)&lt;/span>。注意到K是一个凸集，且&lt;span class="math">\((x_i+x_j)/2\)&lt;/span>也会在K中。所以&lt;span class="math">\((x_i+x_j)/2\)&lt;/span>的范数大于&lt;span class="math">\(\delta\)&lt;/span>。 &lt;span class="math">\[|(x_i+x_j)/2|\geq|\delta|\Rightarrow |x_i+x_j|^2 \geq 4\delta^2 \\
|x_i-x_j|^2=2|x_i|^2+2|x_j|^2-|x_i+x_j|^2\\
\leq 2|x_i|^2+2|x_j|^2-4\delta^2 \to 0\]&lt;/span> 由于&lt;span class="math">\(i,j\to\infty\)&lt;/span>，并且&lt;span class="math">\(x_i\)&lt;/span>是柯西序列，因此x的极限在K中。唯一性是由于y也在K中有范数，而&lt;span class="math">\(|x-y|^2 \leq 2|x_i|^2+2|x_j|^2-4\delta^2=0\)&lt;/span>，根据范数定义&lt;span class="math">\(x=y\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>接下来继续超平面分离定理证明。考虑两个不相交的分控凸集&lt;span class="math">\(A，B\)&lt;/span>，令&lt;span class="math">\(K=A+(-B)=\{x-y|x\in A,y\in B\}\)&lt;/span>。由于&lt;span class="math">\(-B\)&lt;/span>也是凸集，所以&lt;span class="math">\(K\)&lt;/span>也是凸集。由于K可能是开集，我们考虑K的闭包&lt;span class="math">\(\overline{K}\)&lt;/span>（闭集），有一个最小范数的向量&lt;span class="math">\(v\)&lt;/span>。由于&lt;span class="math">\(\overline{K}\)&lt;/span>是凸的，所以对于&lt;span class="math">\(\forall n \in K\)&lt;/span>，线段 &lt;span class="math">\[v+t(n-v),0\leq t \leq 1\]&lt;/span> 也在&lt;span class="math">\(\overline{K}\)&lt;/span>中，所以 &lt;span class="math">\[|v|^2\leq |v+t(n-v)^2|=|v|^2+2t&amp;lt;v,n-v&amp;gt;+t^2|n-v|^2\]&lt;/span> 上式是因为&lt;span class="math">\(v\)&lt;/span>是唯一最小范数。对于&lt;span class="math">\(0&amp;lt;t \leq 1\)&lt;/span>，我们有 &lt;span class="math">\[0\leq 2&amp;lt;v,n&amp;gt;-2|v|^2+t|n-v|^2\\
|v|^2-0.5t|n-v|^2\leq &amp;lt;v,n&amp;gt;\]&lt;/span> 当&lt;span class="math">\(t\to 0\)&lt;/span>时，&lt;span class="math">\(&amp;lt;n,v&amp;gt;\geq |v|^2\)&lt;/span>。因此对于任意&lt;span class="math">\(x \in A\)&lt;/span>和&lt;span class="math">\(y \in B\)&lt;/span>,有&lt;span class="math">\(&amp;lt;x-y,v&amp;gt; \geq |v|^2\)&lt;/span>。所以，如果&lt;span class="math">\(v\)&lt;/span>时非零的。那么： &lt;span class="math">\[\inf_{x \in A}\langle x,v \rangle \geq |v|^2+\sup_{y \in B}\langle y,v \rangle\]&lt;/span> 即存在一个平面分隔了凸集&lt;span class="math">\(A,B\)&lt;/span>。更广泛的&lt;span class="math">\(v=0\)&lt;/span>说明可见维基百科。&lt;/p>
&lt;p>另一个证明：&lt;a href="https://www.cnblogs.com/szqfreiburger/p/11573936.html">https://www.cnblogs.com/szqfreiburger/p/11573936.html&lt;/a>&lt;/p>
&lt;h2 id="farkas引理">Farkas引理&lt;/h2>
&lt;blockquote>
&lt;p>令&lt;span class="math">\(A\in \mathbb{R}^{m\times n},\mathbf{b}\in \mathbb{R}^m\)&lt;/span>，那么下面两个陈述只有一个成立：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>存在&lt;span class="math">\(\mathbf{x}\in \mathbb{R}^n，使得Ax=b,且x \geq 0\)&lt;/span>&lt;/li>
&lt;li>存在&lt;span class="math">\(y \in \mathbb{R}^m,使得A^Ty\geq 0，且b^Ty&amp;lt;0\)&lt;/span>.&lt;/li>
&lt;/ol>
&lt;p>需要注意的时，&lt;span class="math">\(\mathbf{x}\geq 0\)&lt;/span>是指向量&lt;span class="math">\(\mathbf{x}\)&lt;/span>的所有组成元素都非负。&lt;/p>
&lt;/blockquote>
&lt;p>先从几何角度了解一下这个引理的含义。我们认为矩阵&lt;span class="math">\(A\)&lt;/span>是由&lt;span class="math">\(n\)&lt;/span>个&lt;span class="math">\(m\)&lt;/span>维的向量组成的向量组，由于在(1)中&lt;span class="math">\(x\)&lt;/span>的每一个元素大于0，所以&lt;span class="math">\(Ax\)&lt;/span>为&lt;span class="math">\(n\)&lt;/span>个向量的非负线性组合，即凸锥组合。它们张成的是凸锥（或者说包含这&lt;span class="math">\(n\)&lt;/span>个向量的凸锥包），长成下图所示。&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/凸锥包.jpg" alt="凸锥包" />&lt;p class="caption">凸锥包&lt;/p>
&lt;/div>
&lt;p>每一条棱表示&lt;span class="math">\(A\)&lt;/span>中的一个向量。而（1）结果&lt;span class="math">\(Ax=b\)&lt;/span>，表示&lt;span class="math">\(b\)&lt;/span>在凸锥中，如下左图。&lt;/p>
&lt;img src="../../images/Farkas引理.jpg" alt="Farkas引理.jpg" />
&lt;center>
左为b在凸锥内的情况，右图为b在凸锥外的情况，总能找到过原点的超平面（二维情况下为直线，法向量为y），把b和凸锥分开。
&lt;/center>
&lt;p>对于情形（2），则是凸集分离定理的推广。单点也是凸集。特别的，如果&lt;span class="math">\(C\)&lt;/span>不仅是凸集，还是事实一中所提到的&lt;strong>凸锥&lt;/strong>，我们可以找到一个&lt;strong>过原点&lt;/strong>的平面，分开凸锥和它外面的一个点，也就是说如果&lt;span class="math">\(C\)&lt;/span>是一个凸锥，&lt;span class="math">\(x \notin C\)&lt;/span>，存在非零 &lt;span class="math">\(d\in R^n\)&lt;/span>满足对于所有的 &lt;span class="math">\(y in C\)&lt;/span>，有&lt;span class="math">\(d^Tx&amp;lt;0\)&lt;/span>，且&lt;span class="math">\(d^Ty&amp;gt;0\)&lt;/span>。上面右图所示。&lt;/p>
&lt;p>严格证明可以参考:&lt;a href="https://lijiancheng0614.github.io/2015/09/17/2015_09_17_Optimization%20Methods/">https://lijiancheng0614.github.io/2015/09/17/2015_09_17_Optimization%20Methods/&lt;/a>&lt;/p>
&lt;h2 id="gordan定理">Gordan定理&lt;/h2>
&lt;p>设&lt;span class="math">\(A\)&lt;/span>为&lt;span class="math">\(m×n\)&lt;/span>矩阵，则&lt;span class="math">\(Ax&amp;lt;0\)&lt;/span>有解，⟺ &lt;span class="math">\(A^Ty=0,y≥0(y≠0)\)&lt;/span>无解。&lt;/p>
&lt;h2 id="附录">附录&lt;/h2>
&lt;p>在&lt;span class="math">\(R^n\)&lt;/span>中，下面三个条件等价：&lt;/p>
&lt;ul>
&lt;li>“有界闭”（bounded and closed）；&lt;/li>
&lt;li>“紧”（compact）；&lt;/li>
&lt;li>“列紧”（sequentially compact）。&lt;/li>
&lt;/ul>
&lt;div class="figure">
&lt;embed src="../../images/凸集凸函数凸优化.webp" />&lt;p class="caption">凸集凸函数凸优化.webp&lt;/p>
&lt;/div></description></item><item><title>优化理论之一维优化-步长控制-线搜索与置信域</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96-%E6%AD%A5%E9%95%BF%E6%8E%A7%E5%88%B6-%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E7%BD%AE%E4%BF%A1%E5%9F%9F/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96-%E6%AD%A5%E9%95%BF%E6%8E%A7%E5%88%B6-%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E7%BD%AE%E4%BF%A1%E5%9F%9F/</guid><description>
&lt;h2 id="步长控制-线搜索与置信域">步长控制-线搜索与置信域&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;p>在最优化(optimization)问题中，线搜索(line search)和置信域(trust region)方法是寻找局部最小值(local minimum)基本迭代方法(iterative approach)。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#线搜索line-search">线搜索(Line search)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#精确线搜索步长">精确线搜索步长&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非精确线搜索步长">非精确线搜索步长&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#wolfe-conditions">Wolfe conditions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#armijo-conditions">Armijo conditions&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;h3 id="线搜索line-search">线搜索(Line search)&lt;/h3>
&lt;p>以f(x)为例，线搜索会先找一个使f(x)下降的方向，接着计算一个步长，步长决定了x改变的大小。&lt;/p>
&lt;p>下降方向:可以通过梯度下降，牛顿法，拟牛顿法等计算。&lt;/p>
&lt;p>步长:有精确(exact)和非精确(inexact)两种，精确方法就是找出导数为零的极值点，例如共轭梯度法conjugate gradient method;非精确方法没有找出导数为零的点，而是使f(x)有一个充分的下降(sufficient descent)，例如backtracking，wolfe conditions，goldstein conditions&lt;/p>
&lt;p>线搜索流程:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>计算线搜索方向&lt;span class="math">\(\vec{p}_k\)&lt;/span>，使得该方向满足&lt;span class="math">\(\nabla f^T_k\vec{p}_k&amp;lt;0，\nabla f_k\ne\vec{0}\)&lt;/span>。（满足上述不等式则该方向为下降法向，因为&lt;span class="math">\(\nabla f_k\)&lt;/span>是方向上升方向，其夹角只有大于180度才会为负）&lt;/li>
&lt;li>计算步长&lt;span class="math">\(\alpha_k&amp;gt;0\)&lt;/span>，使得&lt;span class="math">\(f(x_k+\alpha_k*p_k)&amp;lt;f(x_k)\)&lt;/span>&lt;/li>
&lt;li>更新&lt;span class="math">\(x:x_{k+1}=x_k+\alpha_k*p_k\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>(完全囊括了梯度算法和牛顿类算法的框架，如图1所示。)&lt;/p>
&lt;img src="../../images/Line_search_alogorithm_chart.png" alt="Line_search_alogorithm_chart.png" />
&lt;center>
图1 线搜索算法流程图
&lt;/center>
&lt;h4 id="精确线搜索步长">精确线搜索步长&lt;/h4>
&lt;p>如何确定合适的步长呢？如果我们从表达式上来看，令步长&lt;span class="math">\(\alpha\)&lt;/span>为自变量： &lt;span class="math">\[\phi(\alpha)=f(x_k+\alpha*p_k),\alpha&amp;gt;0\]&lt;/span> 在方向&lt;span class="math">\(\vec{p}_k\)&lt;/span>已经确定的情况下，&lt;span class="math">\(\alpha\)&lt;/span>的最佳选择无疑是使&lt;span class="math">\(\phi(\alpha)\)&lt;/span>的值最小，即使得在给定方向上下降最大的距离。但是，这计算起来有些不切实际，因为计算&lt;span class="math">\(\phi\)&lt;/span>函数的最小值增加了大量计算量，尤其是&lt;span class="math">\(\phi\)&lt;/span>函数有不少局部最小值和稳定点的时候，如图2所示：&lt;/p>
&lt;img src="../../images/Step_length_image.png" alt="发现理想步长的复杂度" />
&lt;center>
图2 发现理想步长的复杂度 (Nocedal &amp;amp; Wright)
&lt;/center>
&lt;p>常见精确搜索方法（在导数不可求得情况下用的）：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>二分搜索&lt;/li>
&lt;li>斐波那契搜索&lt;/li>
&lt;li>黄金分割搜索&lt;/li>
&lt;li>二次插值法&lt;/li>
&lt;li>三次插值法&lt;/li>
&lt;li>D.S.C.法&lt;/li>
&lt;/ol>
&lt;p>精确搜索法的核心是&lt;strong>找到到达函数最小值的步长&lt;/strong>。但是一个实用的正常寻找合适步长的法子比找到&lt;span class="math">\(\phi\)&lt;/span>函数最小值要容易的多，只要使目标函数的值下降即可（不会震荡到函数值更高的点），用表达式来说就是： &lt;span class="math">\[f(x_k+\alpha*p_k)&amp;lt;f(x_k)\]&lt;/span> 该方法不能保证收敛到函数的最小值，因此我们可以添加两个&lt;strong>条件&lt;/strong>，来要求在每次迭代过程中都要显著地减小目标函数。&lt;/p>
&lt;h4 id="非精确线搜索步长">非精确线搜索步长&lt;/h4>
&lt;h5 id="wolfe-conditions">Wolfe conditions&lt;/h5>
&lt;p>Wolfe conditions 由 Armijo conditions和Curvature conditions构成,先分别介绍Armijo conditions和Curvature conditions&lt;/p>
&lt;h5 id="armijo-conditions">Armijo conditions&lt;/h5></description></item><item><title>优化理论之一维精确优化（下）</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96%E4%B8%8B/</link><pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96%E4%B8%8B/</guid><description>
&lt;h2 id="一维精确优化">一维精确优化&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#一维优化方法">一维优化方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#搜索法">搜索法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二分搜索">二分搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#黄金分割搜索">黄金分割搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#斐波那契搜索">斐波那契搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#近似法">近似法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二次插值法">二次插值法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#一点二次插值即牛顿法">一点二次插值（即牛顿法）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二点二次插值">二点二次插值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三点二次插值">三点二次插值&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#三次插值法">三次插值法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二点三次插值法cubic-interpolation-method-with-two-points">二点三次插值法（Cubic Interpolation Method with Two-Points）&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#综合方法">综合方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dsc法">D.S.C.法&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="一维优化方法">一维优化方法&lt;/h2>
&lt;h2 id="搜索法">搜索法&lt;/h2>
&lt;h3 id="二分搜索">二分搜索&lt;/h3>
&lt;h3 id="黄金分割搜索">黄金分割搜索&lt;/h3>
&lt;h3 id="斐波那契搜索">斐波那契搜索&lt;/h3>
&lt;h2 id="近似法">近似法&lt;/h2>
&lt;p>插值法是一类重要的一维搜索方法。其基本思想是在搜索区间中不断用低次多项式来近似目标函数，并逐步用插值多项式的极小点来逼近一维搜索问题的极小点。当函数具有较好的&lt;strong>解析性质时&lt;/strong>，插值法比搜索法效果更好。伟大的泰勒！泰勒展开，多项式！&lt;/p>
&lt;p>常见的插值方法有一点二次插值法、二点二次插值法、三点二次插值法和二点三次插值法。其中一点二次插值法就是牛顿法，二点二次插值法中的一种就是割线法。&lt;/p>
&lt;p>几次插值法就是插值多项式的最高阶数，几点就是选用几个点。比如二次插值法，插值多项式为&lt;span class="math">\(q(x)=ax^2+bx+c\)&lt;/span>有3个待定系数，可以从两个方面来确定系数，一是用点的值，二是用点的导数。&lt;/p>
&lt;h3 id="二次插值法">二次插值法&lt;/h3>
&lt;p>二次插值法就是不断构造二次插值多项式&lt;span class="math">\(q(x)=ax^2+bx+c\)&lt;/span>来近似目标函数&lt;span class="math">\(f(x)\)&lt;/span>并将最后一步构造的二次插值多项式的极小值点作为目标函数极小值点的方法，不同二次插值法的区别在于构造二次插值多项式时使用的信息不同。（注：以下介绍各方法基本原理时，&lt;span class="math">\(x_k\)&lt;/span>均指第&lt;span class="math">\(k\)&lt;/span>步迭代中构造的插值多项式的极小值点）&lt;/p>
&lt;h4 id="一点二次插值即牛顿法">一点二次插值（即牛顿法）&lt;/h4>
&lt;p>一点二次插值在第&lt;span class="math">\((k+1)\)&lt;/span>步构造二次插值多项式时使用 1 个函数、1 个一阶导数和 1 个二阶导数信息。一点，最高阶为2次，名字由来。 &lt;span class="math">\[\begin{aligned}
q(x_k)=&amp;amp;ax_k^2+bx_k+c&amp;amp;=f(x_k)\\
q&amp;#39;(x_k)=&amp;amp;2ax_k+b&amp;amp;=f&amp;#39;(x_k)\\
q&amp;#39;&amp;#39;(x_k)=&amp;amp;2a&amp;amp;=f&amp;#39;&amp;#39;(x_k)
\end{aligned}\]&lt;/span> 求关于&lt;span class="math">\(a,b,c\)&lt;/span>的解方程组： &lt;span class="math">\[a=\frac{\begin{vmatrix}f(x_k)&amp;amp;x_k&amp;amp;1\\f&amp;#39;(x_k)&amp;amp;1&amp;amp;0\\f&amp;#39;&amp;#39;(x_k)&amp;amp;0&amp;amp;0\end{vmatrix}}
{\begin{vmatrix}x_k^2&amp;amp;x_k&amp;amp;1\\2x_k&amp;amp;1&amp;amp;0\\2&amp;amp;0&amp;amp;0\end{vmatrix}}=\frac{f&amp;#39;&amp;#39;(x_k)}{2}\]&lt;/span> &lt;span class="math">\[b=\frac{\begin{vmatrix}x_k^2&amp;amp;f(x_k)&amp;amp;1\\2x_k&amp;amp;f&amp;#39;(x_k)&amp;amp;0\\2&amp;amp;f&amp;#39;&amp;#39;(x_k)&amp;amp;0\end{vmatrix}}
{\begin{vmatrix}x_k^2&amp;amp;x_k&amp;amp;1\\2x_k&amp;amp;1&amp;amp;0\\2&amp;amp;0&amp;amp;0\end{vmatrix}}=f&amp;#39;(x_k)-x_kf&amp;#39;&amp;#39;(x_k)\]&lt;/span> 要求&lt;span class="math">\(q&amp;#39;(x)=f&amp;#39;(x)=0\)&lt;/span>，所以此时&lt;span class="math">\(x_{k+1}=x_k-\frac{f&amp;#39;(x)}{f&amp;#39;&amp;#39;(x)}\)&lt;/span>。从原理上来看，牛顿法只是求导数为0的点，这个点可能是极大值也可能是极小值。&lt;/p>
&lt;h4 id="二点二次插值">二点二次插值&lt;/h4>
&lt;p>二点二次插值法（割线法, Secant Method）在第&lt;span class="math">\(\left(k+1 \right )\)&lt;/span>步构造二次插值多项式时使用 1 个函数和 2 个一阶导数信息 &lt;span class="math">\[q\left(x_{k-1} \right ) = f \left(x_{k-1} \right ),\ q&amp;#39;\left(x_{k-1} \right ) = f&amp;#39; \left(x_{k-1} \right ),\ q&amp;#39;\left(x_k \right ) = f&amp;#39; \left(x_k \right )\]&lt;/span> 由此导出二点二次插值法（割线法）的迭代公式为： &lt;span class="math">\[x_{k+1} = x_k - \frac{f&amp;#39; \left(x_k \right )}{\frac{f&amp;#39; \left(x_k \right ) - f&amp;#39; \left(x_{k-1} \right )}{x_k - x_{k-1}}} = x_k - \frac{x_k - x_{k-1}}{f&amp;#39;\left(x_k \right )-f&amp;#39;\left(x_{k-1} \right )}f&amp;#39;\left(x_k \right )\]&lt;/span> 而二点二次插值法（非割线法, Quadratic Interpolation Method with Two-Points）则使用 2 个函数和 1 个一阶导数信息 &lt;span class="math">\[q\left(x_{k-1} \right ) = f \left(x_{k-1} \right ),\ q&amp;#39;\left(x_{k-1} \right ) = f&amp;#39; \left(x_{k-1} \right ),\ q\left(x_k \right ) = f \left(x_k \right )\]&lt;/span> 由此导出二点二次插值法（非割线法）的迭代公式为： &lt;span class="math">\[x_{k+1} = x_k + \frac{1}{2}\frac{x_k - x_{k-1}}{\frac{f \left(x_k \right ) - f \left(x_{k-1} \right )}{f&amp;#39;\left(x_k \right )\left(x_k - x_{k-1} \right )} - 1} = x_k - \frac{1}{2}\frac{x_k-x_{k-1}}{f&amp;#39;\left(x_k \right )-\frac{f\left(x_k \right )-f\left(x_{k-1} \right )}{x_k - x_{k-1}}}f&amp;#39;\left(x_k \right )\]&lt;/span>&lt;/p>
&lt;p>二点二次插值法具有1.618斐波那契的超线性收敛速度。&lt;/p>
&lt;h4 id="三点二次插值">三点二次插值&lt;/h4>
&lt;h3 id="三次插值法">三次插值法&lt;/h3>
&lt;p>类似地，三次插值法就是不断构造三次插值多项式&lt;span class="math">\(p \left(x \right ) = ax^3 + bx^2 + cx + d\)&lt;/span>来近似目标函数 &lt;span class="math">\(f \left(x \right )\)&lt;/span> 并将最后构造的三次插值多项式的极小值点作为目标函数极小值点的方法。&lt;/p>
&lt;h4 id="二点三次插值法cubic-interpolation-method-with-two-points">二点三次插值法（Cubic Interpolation Method with Two-Points）&lt;/h4>
&lt;p>在第 &lt;span class="math">\(k\)&lt;/span> 步构造三次插值多项式时使用 2 个函数和 2 个一阶导数信息 &lt;span class="math">\[p \left(a_k \right ) = f \left(a_k \right ), p&amp;#39;\left(a_k \right ) = f&amp;#39;\left(a_k \right ), p \left(b_k \right ) = f \left(b_k \right ), p&amp;#39;\left(b_k \right ) = f&amp;#39;\left(b_k \right )\]&lt;/span> &lt;span class="math">\(a_k &amp;lt; b_k\)&lt;/span> 的值根据 &lt;span class="math">\(f&amp;#39; \left(x_k \right )\)&lt;/span> 与 &lt;span class="math">\(0\)&lt;/span> 的大小关系，从 &lt;span class="math">\(a_{k-1},\ b_{k-1},\ x_k\)&lt;/span> 中选取。由上述信息得到的第 &lt;span class="math">\(\left(k+1 \right )\)&lt;/span> 步迭代中构造的三次插值多项式的极小值点公式为： &lt;span class="math">\[\begin{aligned} &amp;amp;x_{k+1} = a_k + \frac{\eta_k-f&amp;#39;\left(a_k \right )-\omega_k}{2\eta_k-f&amp;#39;\left(a_k \right )+f&amp;#39;\left(b_k \right )}\left(b_k-a_k \right ) \\ &amp;amp;\eta_k = \sqrt{\omega_k^2-f&amp;#39;\left(a_k \right )f&amp;#39;\left(b_k \right )} \\ &amp;amp;\omega_k = \frac{3\left[f\left(b_k \right )-f\left(a_k \right ) \right ]}{b_k-a_k}-f&amp;#39;\left(a_k \right )f&amp;#39;\left(b_k \right ) \end{aligned}\]&lt;/span>&lt;/p>
&lt;p>二点三次插值法的收敛速度为2阶。&lt;/p>
&lt;h2 id="综合方法">综合方法&lt;/h2>
&lt;h3 id="d.s.c.法">D.S.C.法&lt;/h3></description></item><item><title>优化理论之一维精确优化（上）</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96%E4%B8%8A/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96%E4%B8%8A/</guid><description>
&lt;h2 id="一维精确优化">一维精确优化&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#一维优化方法">一维优化方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#搜索法">搜索法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二分搜索">二分搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#黄金分割搜索">黄金分割搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#斐波那契搜索">斐波那契搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#近似法">近似法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二次插值法">二次插值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三次插值法">三次插值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#综合方法">综合方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dsc法">D.S.C.法&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="一维优化方法">一维优化方法&lt;/h2>
&lt;p>三种常见的非线性优化问题为：&lt;/p>
&lt;ul>
&lt;li>一维无约束问题&lt;/li>
&lt;li>多维无约束问题&lt;/li>
&lt;li>多维约束问题&lt;/li>
&lt;/ul>
&lt;p>第一种问题是最容易求解的，而第三种是最困难的。在实际应用中，我们通常将多维约束问题简化为多维无约束问题，进而简化为一维无约束问题。事实上，大部分非线性规划问题是基于单变量函数的最小化，并且是无约束的，因此如果我们想构造出有效的多维无约束或约束算法，就需要有效的一维优化算法。&lt;/p>
&lt;p>如果不考虑导数，一维优化方法是&lt;strong>搜索法与近似法&lt;/strong>。&lt;/p>
&lt;p>在&lt;strong>搜索法&lt;/strong>中，首先构建包含&lt;span class="math">\(x^\ast\)&lt;/span>的区间&lt;span class="math">\([x_L,x_U]\)&lt;/span>，然后根据函数不断的减小区间，直到&lt;span class="math">\([x_{L,k},x_{U,k}]\)&lt;/span>充分小，区间&lt;span class="math">\([x_{L,k},x_{U,k}]\)&lt;/span>的中点做为最小值。这种方法可以用于任何函数，函数不一定要可导。&lt;/p>
&lt;p>在&lt;strong>近似法&lt;/strong>中，函数用低阶的多项式来近似（泰勒展开），通常选择二阶或三阶，然后用初等微积分分析，推断出&lt;span class="math">\(x^∗\)&lt;/span>的近似值，这样就减小了区间，然后重复这个过程直到&lt;span class="math">\(x^∗\)&lt;/span>值充分精确。这种方法要求函数&lt;span class="math">\(f(x)\)&lt;/span>是连续可导的。&lt;/p>
&lt;p>接下来会介绍一些一维优化方法，如下所示：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>二分搜索&lt;/li>
&lt;li>斐波那契搜索&lt;/li>
&lt;li>黄金分割搜索&lt;/li>
&lt;li>二次插值法&lt;/li>
&lt;li>三次插值法&lt;/li>
&lt;li>D.S.C. 法&lt;/li>
&lt;/ol>
&lt;p>前三种是搜索法，第四与第五种是近似法，第六种是一种实用的方法，它结合了搜索法与近似法。&lt;/p>
&lt;h2 id="搜索法">搜索法&lt;/h2>
&lt;p>考虑一个&lt;strong>单峰&lt;/strong>函数，在区间&lt;span class="math">\([x_L,x_U]\)&lt;/span>内有最小值，这个区间称为不确定范围，通过不断缩小这个不确定范围可以得出&lt;span class="math">\(f(x)\)&lt;/span>的最小值&lt;span class="math">\(x^∗\)&lt;/span>。在搜索方法中，使用&lt;span class="math">\(f(x)\)&lt;/span>在合适点处的值就能确定出来。讨论的函数形如图1.&lt;/p>
&lt;img src="../../images/convex_func.gif" alt="函数图像" />
&lt;center>
图1 函数图像
&lt;/center>
&lt;h3 id="二分搜索">二分搜索&lt;/h3>
&lt;p>如果&lt;span class="math">\(f(x)\)&lt;/span>在点&lt;span class="math">\(x_a\)&lt;/span>处的值是已知的，其中&lt;span class="math">\(x_L&amp;lt;x_a&amp;lt;x_U\)&lt;/span>，那么点&lt;span class="math">\(x^∗\)&lt;/span>可能在&lt;span class="math">\(x_L\)&lt;/span>与&lt;span class="math">\(x_a\)&lt;/span>之间，或者&lt;span class="math">\(x_a\)&lt;/span>与&lt;span class="math">\(x_U\)&lt;/span>之间，如图2所示，因此获得信息不足以进一步缩小不确定范围。然而，如果我们知道&lt;span class="math">\(f(x)\)&lt;/span>在两个点&lt;span class="math">\(x_a,x_b\)&lt;/span> 处的值，那就可以缩小了，这时候会有三种情况：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(f(x_a)&amp;lt;f(x_b)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x_a)&amp;gt;f(x_b)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x_a)=f(x_ b)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>对于第一种情况，&lt;span class="math">\(x^∗\)&lt;/span>的范围可能是&lt;span class="math">\(x_L&amp;lt;x^∗&amp;lt;x_a\)&lt;/span>或者&lt;span class="math">\(x_a&amp;lt;x^∗&amp;lt;x_b\)&lt;/span>，即&lt;span class="math">\(x_L&amp;lt;x^∗&amp;lt;x_b\)&lt;/span>，如图2所示。&lt;span class="math">\(x_b&amp;lt;x^∗&amp;lt;x_U\)&lt;/span>的情况被排除了，否则的话&lt;span class="math">\(f(x)\)&lt;/span>会有两个极小值：一个在&lt;span class="math">\(x_b\)&lt;/span>的左边，一个在&lt;span class="math">\(x_b\)&lt;/span>的右边。同样的，对于第二种情况，我们肯定有&lt;span class="math">\(x_a&amp;lt;x^∗&amp;lt;x_U\)&lt;/span>，如图3所示。对于第三种情况，我们有&lt;span class="math">\(x_a&amp;lt;x^∗&amp;lt;x_b\)&lt;/span>，即不等式&lt;span class="math">\(x_L&amp;lt;x^∗&amp;lt;x_b\)&lt;/span>与&lt;span class="math">\(x_a&amp;lt;x^∗&amp;lt;x_U\)&lt;/span>都满足，如图4所示。&lt;/p>
&lt;img src="../../images/binary_search1.png" alt="binary_search_1" />
&lt;center>
图2 二分搜索情况1
&lt;/center>
&lt;img src="../../images/binary_search2.png" alt="binary_search_2" />
&lt;center>
图3 二分搜索情况2
&lt;/center>
&lt;img src="../../images/binary_search3.png" alt="binary_search_3" />
&lt;center>
图4 二分搜索情况3
&lt;/center>
&lt;p>知道了这三种关系之后，我们需要逐步缩小&lt;span class="math">\(x_a,x_b\)&lt;/span>的范围。如果缩小的尺度没有掌握好，会使得最优值&lt;span class="math">\(x^\ast\)&lt;/span>被排除到&lt;span class="math">\(x_a,x_b\)&lt;/span>之外。一种缩小不确定范围的基本策略是二分搜索。对于这个方法，我们取初始值&lt;span class="math">\(x_1\)&lt;/span>为&lt;span class="math">\(x_L,x_U\)&lt;/span>的中点，即&lt;span class="math">\(x_1=\frac{x_L+x_U}{2}\)&lt;/span>，并计算&lt;span class="math">\(f(x)\)&lt;/span> 在两点&lt;span class="math">\(x_a=x_1−ε/2\)&lt;/span>与&lt;span class="math">\(x_b=x1+ε/2\)&lt;/span>的值，其中ε是很小的正数，然后根据&lt;span class="math">\(f(x_a)&amp;lt;f(x_b)\)&lt;/span>还是&lt;span class="math">\(f(x_a)&amp;gt;f(x_b)\)&lt;/span>，判断范围是&lt;span class="math">\(x_L\)&lt;/span>到&lt;span class="math">\(x_1+ε/2\)&lt;/span>还是&lt;span class="math">\(x_1−ε/2\)&lt;/span>到&lt;span class="math">\(x_U\)&lt;/span>，如果&lt;span class="math">\(f(x_a)=f(x_b)\)&lt;/span>，那么两者都可以。这样范围立刻可以缩小到&lt;span class="math">\(x_1\pm ε/2\)&lt;/span>附近，即&lt;span class="math">\(x_L,x_U\)&lt;/span>的中点附近，探索范围缩小一半，不断重复这个过程直到满足精度要求。例如，如果二分查找应用到图5所示的函数上，那么不确定范围在四次迭代后从&lt;span class="math">\(0&amp;lt;x^∗&amp;lt;1\)&lt;/span>减小到&lt;span class="math">\(9/16+ε/2&amp;lt;x^∗&amp;lt;5/8−ε/2\)&lt;/span>(误差范围&lt;span class="math">\(ε\)&lt;/span>)。&lt;/p>
&lt;img src="../../images/binary_search_steps.png" alt="binary_search_steps" />
&lt;center>
图4 二分搜索逐步缩小范围
&lt;/center>
&lt;h3 id="黄金分割搜索">黄金分割搜索&lt;/h3>
&lt;p>我们在使用二分法是，每次切割区间需要计算两次函数值（&lt;span class="math">\(f(x-ε/2),f(x+ε/2)\)&lt;/span>）。其中一个值是用来舍弃的。在单峰函数求极值的过程中，始终需要两个试探值，这都是拜这个原则所赐，再来复习下：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(x^\ast,x_a,x_b \in D=[x_L,x_U]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x_a) \le f(x_b),x^\ast\in[x_L,x_b]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x_a) \ge f(x_b),x^\ast\in[x_a,x_U]\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>如果我们能够合理利用试探点信息，就可以每个迭代周期只用计算一次函数值。我们做出如下两个简单要求：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>两个试探点&lt;span class="math">\(\lambda_k,\mu_k\)&lt;/span>距离边界&lt;span class="math">\([a_k,b_k]\)&lt;/span>距离相等（&lt;span class="math">\(a_0=x_L,b_0=x_U\)&lt;/span>）。&lt;span class="math">\(\lambda_k-a_k=b_k-\mu_k\)&lt;/span>&lt;/li>
&lt;li>每次迭代，搜索空间缩小的比例为常数&lt;span class="math">\(\tau\)&lt;/span>。&lt;span class="math">\(b_{k+1}-a_{k+1}=\tau*(b_k-a_k)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>我们假设试探点&lt;span class="math">\(f(\lambda_k) \le f(\mu_k)\)&lt;/span>（在另一种场景&lt;span class="math">\(f(\lambda_k) \ge f(\mu_k)\)&lt;/span>可以得到一样的结果），则&lt;span class="math">\(a_{k+1}=a_k,b_{k+1}=\mu_k\)&lt;/span>。&lt;/p>
&lt;p>根据要求2，我们带入&lt;span class="math">\(a_{k+1}=a_k,b_{k+1}=\mu_k\)&lt;/span>，可得： &lt;span class="math">\[b_{k+1}-a_{k+1}=\mu_k-a_k=\tau(b_k-a_k)\\
\Rightarrow \mu_k=a_k+\tau(b_k-a_k) \tag{2.1}\]&lt;/span> 将&lt;span class="math">\((2.1)\)&lt;/span>代入要求1可得： &lt;span class="math">\[\lambda_k-a_k=b_k-\mu_k=b_k-(a_k+\tau(b_k-a_k))\\
\Rightarrow \lambda_k=a_k+(1-\tau)(b_k-a_k)\tag{2.2}\]&lt;/span> 根据&lt;span class="math">\((2.1)\)&lt;/span>和&lt;span class="math">\(a_{k+1}=a_k,b_{k+1}=\mu_k\)&lt;/span>，进一步推导&lt;span class="math">\(u_{k+1}\)&lt;/span>： &lt;span class="math">\[\begin{aligned}
u_{k+1}&amp;amp;=a_{k+1}+\tau(b_{k+1}-a_{k+1 })\\
&amp;amp;=a_k+\tau(u_k-a_k)\\
&amp;amp;=a_k+\tau(a_k+\tau(b_k-a_k)-a_k)\\
&amp;amp;=a_k+\tau^2(b_k-a_k) \tag{2.3}
\end{aligned}\]&lt;/span> 由于搜索模式限制，我们必须通过计算两个函数点，才能判断出区间。如果能够合理设置数值，我们可以交替的使用上次没用到的试探点作为下一次迭代的试探点。&lt;/p>
&lt;p>在情形&lt;span class="math">\(f(\lambda_k) \le f(\mu_k)\)&lt;/span>中，&lt;span class="math">\(b_{k+1}=\mu_k，\lambda_k\)&lt;/span>没有用到，因此我们依旧把&lt;span class="math">\(\lambda_k\)&lt;/span>作为k+1次迭代的试探点。令： &lt;span class="math">\[\begin{aligned}
\mu_{k+1}&amp;amp;=\lambda_k\\
a_k+\tau^2(b_k-a_k)&amp;amp;=a_k+(1-\tau)(b_k-a_k)\\
\tau^2&amp;amp;=1-\tau(\tau&amp;gt;0)\\
\tau&amp;amp;=\frac{\sqrt{5}-1}{2}\approx0.618
\end{aligned}\]&lt;/span> 也就是说，当&lt;span class="math">\(\tau=\frac{\sqrt{5}-1}{2}\approx0.618\)&lt;/span>的时候，&lt;span class="math">\(\mu_{k+1}=\lambda_k,f(\mu_{k+1})=f(\lambda_k)\)&lt;/span>，每次迭代只需要引入一个新值，计算一次函数值。&lt;/p>
&lt;p>同理，在情形&lt;span class="math">\(f(\lambda_k) \ge f(\mu_k)\)&lt;/span>中，&lt;span class="math">\(a_{k+1}=\lambda_k,b_{k+1}=b_k,\lambda_{k+1}=\mu_k,f(\lambda_{k+1})=f(\mu_k)\)&lt;/span>。&lt;/p>
&lt;p>黄金分割法的搜索效率不如二分法，缩短率&lt;span class="math">\(\tau=0.618\)&lt;/span>，属于线性收敛。但是除了第一次迭代，每次只需要在计算一次函数值，这里效率高一些。&lt;/p>
&lt;h3 id="斐波那契搜索">斐波那契搜索&lt;/h3>
&lt;p>对于斐波那契搜索，我们把黄金分割搜索的条件放宽，只要求1个：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>考虑第k次迭代的搜索空间： 两个试探点&lt;span class="math">\(\lambda_k,\mu_k\)&lt;/span>距离边界&lt;span class="math">\([a_k,b_k]\)&lt;/span>距离相等（&lt;span class="math">\(a_0=x_L,b_0=x_U\)&lt;/span>）。&lt;span class="math">\(\lambda_k-a_k=b_k-\mu_k\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;p>令第k次迭代的搜索空间为&lt;span class="math">\(I_k=[a_k,b_k]\)&lt;/span>。同样根据试探节点的函数值大小，我们有，如果&lt;span class="math">\(f(\lambda_k)&amp;lt;f(\mu_k)\)&lt;/span>，则选择左区间： &lt;span class="math">\[I_{k+1}^L=[a_k,\mu_k]\]&lt;/span> 如果&lt;span class="math">\(f(\lambda_k)&amp;gt;f(\mu_k)\)&lt;/span>，则选择右区间： &lt;span class="math">\[I_{k+1}^R=[\lambda_k,b_k]\]&lt;/span>&lt;/p>
如果右区间&lt;span class="math">\(I_{k+1}^R\)&lt;/span>被选中，那么它将包含最小值，另外还知道点&lt;span class="math">\(\mu_k \rightarrow \lambda_{k+1}\)&lt;/span>处的函数值。如果我们知道点&lt;span class="math">\(\mu_{k+1}\)&lt;/span>处的值，那么我们就有充分的信息来进一步减小不确定区域，然后不断重复此过程。这种方法每次迭代&lt;strong>只需要估计一个函数值&lt;/strong>，计算量相对于二分搜索要小。如图5所示。 &lt;img src="../../images/斐波那契搜索区间.png" alt="斐波那契搜索空间" />
&lt;center>
斐波那契搜索空间
&lt;/center>
&lt;p>由图5可知 &lt;span class="math">\[I_k=I_{k+1}^L+I_{k+2}^R\]&lt;/span> 根据要求1，我们知道左右两个区间大小相等。那么&lt;span class="math">\(I_{k+1}^L=I_{k+1}^R=I_{k+1},I_{k+2}^L=I_{k+2}^R=I_{k+2}\)&lt;/span>。由此可得递推公式： &lt;span class="math">\[I_k=I_{k+1}+I_{k+2}\]&lt;/span> 如果上面的过程重复多次，那么我们会得到如下的区间序列&lt;span class="math">\({I_1,I_2,…,I_n}\)&lt;/span>: &lt;span class="math">\[I_1=I_2+I_3\\
I_2=I_3+I_4\\
\dotsb\\
I_n=I_{n+1}+I_{n+2}\\
\]&lt;/span> 假设&lt;span class="math">\(n+2\)&lt;/span>次迭代后区间消失，即&lt;span class="math">\(I_{n+2}=0\)&lt;/span>，那么我们就得到斐波那契序列，如果我们令&lt;span class="math">\(k=n\)&lt;/span>，可以得到: &lt;span class="math">\[\begin{aligned}
I_{n+1}&amp;amp;=I_n-I_{n+2}=I_n=F_0 I_n\\
I_{n}&amp;amp;=I_{n+1}+I_{n+2}=I_n=F_1 I_n\\
I_{n-1}&amp;amp;=I_{n}+I_{n+1}=2I_n=F_2 I_n\\
I_{n-2}&amp;amp;=I_{n-1}+I_{n}=3I_n=F_3 I_n\\
I_{n-3}&amp;amp;=I_{n-2}+I_{n-1}=5I_n=F_4 I_n\\
\dotsb\\
I_{1}&amp;amp;=I_2+I_3=F_n I_n\\
\end{aligned}\]&lt;/span> 所生成的序列&lt;span class="math">\(\{1,1,2,3,5,8,13,…\}=\{F_0,F_1,F_2,F_3,F_4,F_5,F_6,…\}\)&lt;/span>&lt;/p>
&lt;p>如果迭代的总数为&lt;span class="math">\(n\)&lt;/span>，那么斐波那契搜索将不确定区间缩小到 &lt;span class="math">\[I_n=\frac{I_1}{F_n}\]&lt;/span> 例如如果&lt;span class="math">\(n=11\)&lt;/span>，那么&lt;span class="math">\(F_n=144\)&lt;/span>，这样的话&lt;span class="math">\(I_n\)&lt;/span>将缩小到不足&lt;span class="math">\(I_1\)&lt;/span>的1%，其中会有11次迭代。因为每次迭代只需要一个函数估计值，所以一共需要11个函数估计值，如果二分搜索要达到同样的精度需要14个函数估计值，故斐波那契搜索比二分搜索效率更高。事实上，相对于其他几个搜索方法，从计算效率上看它是最高效的。&lt;/p>
&lt;p>斐波那契算法中试探点为： &lt;span class="math">\[\lambda_k=a_k+(1-\frac{F_{n-k}}{F_{n-k+1}})(b_k-a_k)\\
\mu_k=a_k+\frac{F_{n-k}}{F_{n-k+1}}(b_k-a_k)\\\]&lt;/span> 不难看出&lt;span class="math">\(\frac{F_{n-k}}{F_{n-k+1}}\)&lt;/span>相当于黄金分割算法中&lt;span class="math">\(\tau\)&lt;/span>，为缩短率。缩短率一开始为0.5，后来逐渐趋近于0.618。需要注意的时，斐波那契搜索需要提前知道迭代的次数。如果最后要求的区间不大于&lt;span class="math">\(\sigma\)&lt;/span>，则 &lt;span class="math">\[F_n\ge \frac{b_1-a_1}{\sigma}\\
F_n=\frac{1}{\sqrt{5}}[(\frac{1+\sqrt{5}}{2})^{n+1}-(\frac{1-\sqrt{5}}{2})^{n+1}]\]&lt;/span> 可推得n。可以证明斐波那契搜索时分割方法求一维最优化问题的&lt;strong>最优策略&lt;/strong>。&lt;/p>
&lt;h2 id="近似法">近似法&lt;/h2>
&lt;h3 id="二次插值法">二次插值法&lt;/h3>
&lt;h3 id="三次插值法">三次插值法&lt;/h3>
&lt;h2 id="综合方法">综合方法&lt;/h2>
&lt;h3 id="d.s.c.法">D.S.C.法&lt;/h3></description></item><item><title>优化理论之一维不精确优化</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%B8%8D%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96/</link><pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%B8%8D%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96/</guid><description>
&lt;h2 id="一维不精确优化">一维不精确优化&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;p>精确一维搜索方法往往需要花费很大的时间，特别当迭代点远离问题的解时，精确求解通常不十分有效。另外，实际上很多最优化方法，如牛顿法和拟牛顿法，其收敛速度并不依赖于精确一维搜索过程。因此，只要保证目标函数&lt;span class="math">\(f(x)\)&lt;/span>在每一次迭代有满意的下降，可大大节省计算量。&lt;/p>
&lt;p>我们在这里再次说明，不做特殊说明下，我们的目标是求函数的极小值。&lt;/p>
&lt;p>&lt;span class="math">\[\min_{x ∈ D} f(x)\\
D为n维约束集\]&lt;/span>&lt;/p>
&lt;p>不精确搜索一般而言主要有两种准则&lt;strong>Armijo-Goldstein 准则、Wolfe-Powell 准则&lt;/strong>，还有基于两大准则的推广，例如强 Wolfe-Powell 准则和简单准则。它们的出发点都是保证搜索步长不太大或不太小。不精确一维搜索方法总体希望收敛快，每一步不要求达到精确最小，速度快，虽然步数增加，则整个收敛过程更快速。&lt;/p>
&lt;p>为什么要遵循这些准则？&lt;/p>
&lt;p>由于采用了不精确的一维搜索，所以，为了能让算法收敛（即：求得极小值），人们逐渐发现、证明了一些规律，当你遵循这些规律的时候，算法就&lt;strong>很有可能&lt;/strong>收敛。因此，为了达到&lt;strong>让算法收敛&lt;/strong>的目的，我们就要遵循这些准则。如果你不愿意遵循这些已经公认有效的准则，而是要按自己的准则来设计算法，那么恭喜你，如果你能证明你的做法是有效的，未来若干年后，书本里可能也会出现你的名字。&lt;/p>
&lt;h2 id="不精确步长搜索基础算法">不精确步长搜索基础算法&lt;/h2>
&lt;h3 id="试探法backtracking-approach">试探法（Backtracking Approach）&lt;/h3>
&lt;p>如果备选的步长合适，那么通过试探法，我们可以只用充分下降条件来做终止条件，同时选出一个不错的步长。（因为这时候 α 是从大往小变化，所以不可能变得太小）。其算法的具体步骤如下：&lt;/p>
&lt;blockquote>
&lt;p>初始化：一般选择&lt;span class="math">\(\gamma∈[0.5,0.8] ,c∈(0,1)\)&lt;/span>&lt;br />依据以下步骤从&lt;span class="math">\(\vec x_{old}\)&lt;/span>更新到&lt;span class="math">\(\vec x_{new}\)&lt;/span>:&lt;br />Step1：计算后向跟踪步长 &lt;span class="math">\[\begin{aligned}\alpha^\ast&amp;amp;:= \max \gamma^v\\
s.t. \quad &amp;amp; v ∈\{0,1,2,\dots\}\\
&amp;amp;f(\vec x_{old}+\gamma^v\vec d)≤f(x_{old})+c\gamma^v f&amp;#39;(\vec x_{old},\vec d)\end{aligned}\]&lt;/span> 其中&lt;span class="math">\(\gamma^v\)&lt;/span>是几何序列（等比序列）。&lt;br />Step2： &lt;span class="math">\(\vec x_{new}=\vec x_{old}+\alpha^\ast\vec d\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>试探法是大多数线性搜索方法的基础。基于这个算法，我们来详细谈谈它的拓展。&lt;/p>
&lt;h2 id="充分下降准则简单准则">充分下降准则(简单准则)&lt;/h2>
&lt;p>我们要求目标函数的最小值，最重要的就是要让函数有&lt;strong>足够&lt;/strong>的下降。足够这个词，是希望一种发散的下降，而不是会收敛到某一点的下降，举个反例：&lt;/p>
&lt;p>&lt;span class="math">\[\min f(x)=(x+1)^2\\
x(k)=5/k,k=1,2,3,\dotsb\]&lt;/span> 这个&lt;span class="math">\(x\)&lt;/span>序列确实一直让函数值在下降，但是不够充分，每次下降的越来越小，直到收敛到点&lt;span class="math">\((0,1)\)&lt;/span>。无穷的收敛性有时候很好，也有时候令人恼怒。因此，我们需要指定一个充分下降条件。当迭代到第&lt;span class="math">\(k\)&lt;/span>次时：&lt;/p>
&lt;p>&lt;span class="math">\[f(\vec x_k+\alpha\vec d_k)≤f(\vec x_k)+\alpha c\nabla f(\vec x_k)\vec d_k\quad PS:f&amp;#39;(\vec x_k;\vec d_k)=\nabla f(\vec x_k)\vec d_k\]&lt;/span>&lt;/p>
&lt;p>这个不等式如何保证呢？首先我们需要找到一个严格下降方向&lt;span class="math">\(\vec d\)&lt;/span>，其方向导数是负数，即 &lt;span class="math">\[\lim_{\alpha→0}\frac{f(\vec x_{old}+\alpha\vec d)-f({x_{old})}}{\alpha}=f&amp;#39;(x_{old};\vec d)&amp;lt;0\quad \alpha\in(0,\bar \alpha)\]&lt;/span> 取一个&lt;span class="math">\(c ∈(0,1)\)&lt;/span>，那么 &lt;span class="math">\[\lim_{\alpha→0}\frac{f(\vec x_{old}+\alpha\vec d)-f({x_{old})}}{\alpha}=f&amp;#39;(x_{old};\vec d)&amp;lt;cf&amp;#39;(x_{old};\vec d)&amp;lt;0\\
\Rightarrow f(\vec x_{old}+\alpha\vec d)≤f(x_{old})+\alpha cf&amp;#39;(x_{old};\vec d)\]&lt;/span> 一般而言，&lt;span class="math">\(c\)&lt;/span>是一个小量&lt;span class="math">\([0.0001,0.1]\)&lt;/span>，其作用是放缓曲线，下图举了个例子&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/充分下降准则.png" alt="充分下降准则.png" />&lt;p class="caption">充分下降准则.png&lt;/p>
&lt;/div>
&lt;p>如果直接使用&lt;span class="math">\(\nabla f(x)^Td\)&lt;/span>则有可能导致没有可行的步长，因此使用一个平缓参数&lt;span class="math">\(c\)&lt;/span>。&lt;span class="math">\(c\)&lt;/span>一般是一个小数，把直线图像抬起来，有的时候&lt;span class="math">\(c=0.0001\)&lt;/span>，那么&lt;span class="math">\(f(x_{old})+\alpha cf&amp;#39;(x_{old};\vec d)\)&lt;/span>基本差不多是一条水平线了。但是&lt;span class="math">\(c\)&lt;/span>是一个定值，保证了下降过程不会收敛。&lt;/p>
&lt;p>此外，如果只满足充分下降准则，也可以称之满足于简单准则。&lt;/p>
&lt;h2 id="wolfe-powell准则">Wolfe-Powell准则&lt;/h2>
&lt;p>仅靠充分下降条件并不能保证算法每一步都能有很好的进步（make reasonable progress）。事实上，我们可以发现，当&lt;span class="math">\(α\)&lt;/span>充分小时，充分下降条件总是满足的。所以，算法可能会常常产生令人无法接受的小步长。为了避免这种情况，我们引入另一个条件，称为曲率条件，即需要&lt;span class="math">\(α_k\)&lt;/span>满足 &lt;span class="math">\[\nabla f(x_k+\alpha_k d_k)^T p_k\ge c_2\nabla f_k^T d_k\]&lt;/span> 其中&lt;span class="math">\(c_2∈(0,1)\)&lt;/span>为一常数。&lt;/p>
&lt;p>因此， Wolfe-Powell准则归结为&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>充分下降准则&lt;/li>
&lt;li>曲率条件&lt;/li>
&lt;/ol>
&lt;div class="figure">
&lt;img src="../../images/Wolfe条件.png" alt="Wolfe条件" />&lt;p class="caption">Wolfe条件&lt;/p>
&lt;/div>
&lt;h2 id="armijo-goldstein准则">Armijo-Goldstein准则&lt;/h2>
&lt;p>Armijo-Goldstein准则的核心内容有两条：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>充分下降准则&lt;/li>
&lt;li>搜索的步长不应该太小&lt;/li>
&lt;/ol>
&lt;p>&lt;span class="math">\[1:f(\vec x_k+\alpha\vec d_k)≤f(\vec x_k)+\alpha c\nabla f(\vec x_k)\vec d_k\]&lt;/span> &lt;span class="math">\[2:f(\vec x_k+\alpha\vec d_k)≥f(\vec x_k)+\alpha (1-c)\nabla f(\vec x_k)\vec d_k\]&lt;/span> 其中，&lt;span class="math">\(c ∈ (0,0.5)\)&lt;/span>&lt;/p>
&lt;div class="figure">
&lt;img src="../../images/Armijo-Goldstein.png" alt="Armijo-Goldstein" />&lt;p class="caption">Armijo-Goldstein&lt;/p>
&lt;/div>
&lt;h2 id="推广强wolfe-powell准则">推广：强Wolfe-Powell准则&lt;/h2>
&lt;p>强Wolfe条件对&lt;span class="math">\(α_k\)&lt;/span>的限制为 &lt;span class="math">\[\begin{array}{c}f(x_k+\alpha_k p_k)\le f(x_k)+c_1\alpha_k\nabla f_k^T p_k \\ |\nabla f(x_k+\alpha_k p_k)^T p_k|\le c_2|\nabla f_k^T p_k| \end{array}\]&lt;/span> 注意到，这个条件比Wolfe条件只多了绝对值符号。强Wolfe条件实际上限制了&lt;span class="math">\(ϕ′(α_k)\)&lt;/span>不能为一个很大的正数，即可行的&lt;span class="math">\(α\)&lt;/span>不能离最低点太远。&lt;/p>
&lt;p>这个笔记并不完整，一些内容可见&lt;http://keson96.github.io/2016/12/08/2016-12-08-Line-Search/>，我在这上面花了不少时间，但是不知道是否值得，现在先暂停下。&lt;/p></description></item><item><title>优化理论之一维优化综述</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96%E7%BB%BC%E8%BF%B0/</link><pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96%E7%BB%BC%E8%BF%B0/</guid><description>
&lt;h2 id="一维优化综述">一维优化综述&lt;!-- omit in toc -->&lt;/h2>
&lt;p>摘自：&lt;a href="https://www.longzf.com/optimization/2/line_search/">https://www.longzf.com/optimization/2/line_search/&lt;/a>&lt;/p>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;p>一维优化技术又称为&lt;strong>线性搜索技术&lt;/strong>。因为一维空间直观的表现就是一条直线，而在这条线上找最优值，就是线性搜索。&lt;/p>
&lt;p>线搜索技术是多变量函数优化的基础，它包括&lt;strong>精确线搜索技术&lt;/strong>和&lt;strong>非精确线搜索技术&lt;/strong>。常见的精确线搜索技术可分为&lt;strong>分割方法和插值方法&lt;/strong>两大类：分割方法有二分法、黄金分割法、斐波那契法等；插值方法有一点二次插值法（牛顿法）、二点二次插值法（包括割线法）、三点二次插值法、二点三次插值法等。非精确线搜索技术基于&lt;strong>非精确线搜索准则&lt;/strong>，常用的准则有 Armijo-Goldstein 准则、Wolfe-Powell 准则、强 Wolfe-Powell 准则和简单准则。这里特别指出精确与非精确是指对&lt;strong>步长搜索的精确性&lt;/strong>。&lt;/p>
&lt;p>什么是线搜索呢？我们知道最优化算法的基本框架就是依据&lt;strong>迭代公式&lt;span class="math">\(\vec x_{k+1}=\vec x_k+\alpha_k\vec d_k\)&lt;/span>不断产生下一个迭代点&lt;/strong>，直到满足某些终止条件。迭代公式的确定涉及搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>和搜索步长&lt;span class="math">\(\alpha_k\)&lt;/span>的确定，其中&lt;span class="math">\(\alpha_k\)&lt;/span>的确定过程就是所谓线搜索的过程。&lt;/p>
&lt;p>精确线搜索的精确有两种理解。第一，对于分割法，精确搜索每次使得搜索空间有固定比例的缩小。例如&lt;span class="math">\(l_k\)&lt;/span>是第&lt;span class="math">\(k\)&lt;/span>次搜索空间大小，则第&lt;span class="math">\(k+1\)&lt;/span>次搜索空间就是&lt;span class="math">\(\alpha l_k(\alpha ∈ (0,1))\)&lt;/span>，第二，对于插值法，求使得近似（多项式）函数&lt;span class="math">\(p(\vec x_k+\alpha_k\vec d_k)\)&lt;/span>取到最小值时对应的&lt;span class="math">\(\alpha\)&lt;/span>，换句话说：精确线搜索求使得目标函数沿搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>下降最多的搜索步长。&lt;/p>
&lt;p>非精确线搜索则只求使得目标函数沿搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>有一定下降的搜索步长，这个步长甚至可能会导致偏离搜索目标（比如梯度下降法中学习率过大的情况）。&lt;/p>
&lt;p>需要指出的是，非精确搜索和精确搜索不是完全互斥的，一些精确搜索方法也满足非精确线搜索准则。&lt;/p>
&lt;p>实际计算中，&lt;strong>通常采用非精确线搜索技术&lt;/strong>，这是因为精确线搜索技术耗费大量的计算资源，而且对于多变量函数的优化，许多算法的收敛速度并不取决于是否采用精确线搜索技术。&lt;/p></description></item></channel></rss>