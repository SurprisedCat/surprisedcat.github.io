<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Studynotes on SurprisedCat</title><link>https://surprisedcat.github.io/studynotes/</link><description>Recent content in Studynotes on SurprisedCat</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2020–2021, SurprisedCat; all rights reserved.</copyright><lastBuildDate>Wed, 01 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://surprisedcat.github.io/studynotes/index.xml" rel="self" type="application/rss+xml"/><item><title>复变函数之复变函数微积分（微分篇）</title><link>https://surprisedcat.github.io/studynotes/%E5%A4%8D%E5%8F%98%E5%87%BD%E6%95%B0%E4%B9%8B%E5%A4%8D%E5%8F%98%E5%87%BD%E6%95%B0%E5%BE%AE%E7%A7%AF%E5%88%86%E5%BE%AE%E5%88%86%E7%AF%87/</link><pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%A4%8D%E5%8F%98%E5%87%BD%E6%95%B0%E4%B9%8B%E5%A4%8D%E5%8F%98%E5%87%BD%E6%95%B0%E5%BE%AE%E7%A7%AF%E5%88%86%E5%BE%AE%E5%88%86%E7%AF%87/</guid><description>
&lt;h2 id="复变函数微积分微分篇">复变函数微积分（微分篇） &lt;!-- omit in toc -->&lt;/h2>
&lt;p>复变函数的形式很特殊，定义的形式、性质都接近于一元实函数，但是函数的实部、虚部又可以分成两个二元实函数，因此一些二元函数的处理方式也可以应用到复函数中，一元复函数的许多性质证明也是利用二元实函数。同时，在复函数组成实部、虚部的两个二元实函数又不是独立的，尤其在复函数可微、可积时，它们又有着密切联系，如C-R方程、拉普拉斯方程等，我们利用这些关系可以得出一般二元实函数所没有的结论。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#复变函数定义">复变函数定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#复函数的基本性质">复函数的基本性质&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#复函数极限">复函数极限&lt;/a>&lt;/li>
&lt;li>&lt;a href="#复函数连续性">复函数连续性&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#复变函数的导数与微分">复变函数的导数与微分&lt;/a>&lt;/li>
&lt;li>&lt;a href="#再看全纯函数解析函数">再看全纯函数（解析函数）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#柯西-黎曼方程c-r方程">柯西-黎曼方程（C-R方程）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#全纯函数解析函数与调和函数">全纯函数（解析函数）与调和函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#初等复函数">初等复函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基础指数函数">基础：指数函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对数函数">对数函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#幂函数">幂函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三角函数">三角函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#双曲函数">双曲函数&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="复变函数定义">复变函数定义&lt;/h2>
&lt;blockquote>
&lt;p>复变函数： 设在复平面&lt;span class="math">\(\mathbb{C}\)&lt;/span>上有一点集&lt;span class="math">\(E\)&lt;/span>，如果对于&lt;span class="math">\(E\)&lt;/span>内每一点&lt;span class="math">\(z\)&lt;/span>值，都有一个或&lt;strong>多个&lt;/strong>复数值&lt;span class="math">\(w\)&lt;/span>与之对应，则称&lt;span class="math">\(w\)&lt;/span>为&lt;span class="math">\(z\)&lt;/span>的&lt;strong>复函数&lt;/strong>，记为&lt;span class="math">\(w=f(z)\)&lt;/span>，&lt;span class="math">\(E\)&lt;/span>为定义域，可表示为： &lt;span class="math">\[\forall z\in E, \exist w = f(z)\tag{1}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>和过去实函数的定义的有个区别，实函数中要求函数值只有一个，即单值函数，而复函数可以让一个自变量对应多个因变量，即&lt;strong>多值函数&lt;/strong>。多值函数除了应用在在复变函数以外，还广泛应用于隐函数，特别地，不定积分也是常见的多值函数（后面会跟一个任意常数&lt;span class="math">\(C\)&lt;/span>）。&lt;/p>
&lt;p>根据复函数可分为实部和虚部的特点，我们可以将其拆分为两个实二元函数，即 &lt;span class="math">\[
\begin{aligned}
z&amp;amp; = x + i y\\
w &amp;amp;= u + i v\\
w &amp;amp;= f(z) = f(x,y)\\
&amp;amp;=u(x,y)+iv(x,y)
\end{aligned}\tag{2}
\]&lt;/span> 这样一元复函数就变成了两个二元实函数的&lt;strong>有序&lt;/strong>组合。因此，我们可以同时用一元函数和二元函数的方式去研究复函数。由于复函数的这种复杂性，也很难用一幅函数图像完整地描述复函数。&lt;/p>
&lt;p>目前大多数基础数学都是研究单复变函数，因为多复变函数的复杂性远远大于单复变函数，并且是20世纪重要的数学成就之一。&lt;strong>本文默认所讨论的复变函数当时单复变函数&lt;/strong>。&lt;/p>
&lt;h3 id="复函数的基本性质">复函数的基本性质&lt;/h3>
&lt;p>如果只看一元复函数，其性质和一元实函数很多是一样的，也有地方会有稍许区别。同时，由于一个复函数对应两个二元实函数的有序组合，因此他们之间也是有联系的。&lt;/p>
&lt;h4 id="复函数极限">复函数极限&lt;/h4>
&lt;blockquote>
&lt;p>极限： 设函数&lt;span class="math">\(w=f(z)\)&lt;/span>在&lt;span class="math">\(z_0\)&lt;/span>的去心邻域&lt;span class="math">\(|z-z_0|&amp;lt;\rho\)&lt;/span>内有定义，如存在复数&lt;span class="math">\(A\neq \infty, \forall \varepsilon &amp;gt;0, \exist \delta&amp;gt;0\)&lt;/span>使得当&lt;span class="math">\(0&amp;lt;|z-z_0|&amp;lt;\delta\)&lt;/span>，有&lt;span class="math">\(|f(z)-A|&amp;lt;\varepsilon\)&lt;/span>，则称&lt;span class="math">\(A\)&lt;/span>为函数&lt;span class="math">\(w=f(z)\)&lt;/span>当&lt;span class="math">\(z\)&lt;/span>趋向于&lt;span class="math">\(z_0\)&lt;/span>时的极限，记作&lt;span class="math">\(\lim\limits_{z\rightarrow z_0} f(z)=A\)&lt;/span>（极限存在不一定需要函数值在&lt;span class="math">\(z_0\)&lt;/span>有定义）。&lt;/p>
&lt;/blockquote>
&lt;p>从定义来看和一元实函数基本一样，不过需要强调，在一元实函数中，&lt;span class="math">\(x\)&lt;/span>只会从&lt;span class="math">\(x_0\)&lt;/span>正负两个方向趋近，&lt;strong>在复函数中&lt;span class="math">\(z\)&lt;/span>趋向于&lt;span class="math">\(z_0\)&lt;/span>的方向是任意的&lt;/strong>。复函数极限的几何意义是当自变量&lt;span class="math">\(z\)&lt;/span>一旦进入&lt;span class="math">\(z_0\)&lt;/span>充分小的&lt;span class="math">\(\delta\)&lt;/span>邻域时，它的像点&lt;span class="math">\(w=f(z)\)&lt;/span>就落在&lt;span class="math">\(A\)&lt;/span>预先给带的&lt;span class="math">\(\varepsilon\)&lt;/span>邻域内。&lt;/p>
&lt;p>&lt;img src="../images/复函数极限.png" alt="复函数极限" />)&lt;/p>
&lt;p>复函数极限的运算性质和一元实函数是一样的：当&lt;span class="math">\(\lim\limits_{z\rightarrow z_0} f(z)=A, \lim\limits_{z\rightarrow z_0} g(z)=B\)&lt;/span>时&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\lim\limits_{z\rightarrow z_0}[f(z)\plusmn g(z)]=A\plusmn B\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\lim\limits_{z\rightarrow z_0}[f(z)\cdot g(z)]=A\cdot B\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\lim\limits_{z\rightarrow z_0}[f(z)/g(z)]=A/B, (B\neq 0)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>由于一元复函数和两个二元实函数的对应关系，我们可以得到如下结论：&lt;/p>
&lt;blockquote>
&lt;p>由于&lt;span class="math">\(f(z)=u(x,y)+iv(x,y)\)&lt;/span>，设&lt;span class="math">\(A=u_0+iv_0,\ z_0=x_0+iy_0\)&lt;/span>，则 &lt;span class="math">\[\lim\limits_{z\rightarrow z_0} f(z)=A \Leftrightarrow \lim\limits_{x\rightarrow x_0\atop y\rightarrow y_0} u(x,y)=u_0,\ \lim\limits_{x\rightarrow x_0\atop y\rightarrow y_0} v(x,y)=v_0\tag{3}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>从多元函数角度，也说明我们需要考虑所有趋近方向来证明极限的存在性。&lt;/p>
&lt;h4 id="复函数连续性">复函数连续性&lt;/h4>
&lt;blockquote>
&lt;p>若&lt;span class="math">\(\lim\limits_{z\rightarrow z_0} f(z)=f(z_0)\)&lt;/span>，则称&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(z_0\)&lt;/span>点连续。若&lt;span class="math">\(f(z)\)&lt;/span>在区域&lt;span class="math">\(D\)&lt;/span>内处处连续，则称函数&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(D\)&lt;/span>内连续。&lt;/p>
&lt;/blockquote>
&lt;p>连续三要素：&lt;span class="math">\(f(z_0)\)&lt;/span>存在、&lt;span class="math">\(\lim\limits_{z\rightarrow z_0} f(z)\)&lt;/span>存在、二者相等。就是通常所说的当自变量充分靠近时，函数值充分靠近。从定义来看，复函数的连续也和一元实函数定义是一样的。&lt;/p>
&lt;p>复函数连续的运算性质也和一元实函数一致：&lt;/p>
&lt;ul>
&lt;li>在&lt;span class="math">\(z_0\)&lt;/span>连续的两个函数&lt;span class="math">\(f(z),\ g(z)\)&lt;/span>的和、差、积、商在&lt;span class="math">\(z_0\)&lt;/span>处连续。&lt;/li>
&lt;li>如果函数&lt;span class="math">\(\xi=g(z)\)&lt;/span>在点&lt;span class="math">\(z_0\)&lt;/span>处连续，函数&lt;span class="math">\(f(\xi)\)&lt;/span>在&lt;span class="math">\(\xi_0=g(z_0)\)&lt;/span>处连续，则复合函数&lt;span class="math">\(w=f(g(z))\)&lt;/span>在&lt;span class="math">\(z_0\)&lt;/span>处连续。&lt;/li>
&lt;li>如果函数&lt;span class="math">\(f(z)\)&lt;/span>在有界闭区域&lt;span class="math">\(\bar{D}\)&lt;/span>上连续，则：&lt;/li>
&lt;li>&lt;span class="math">\(|f(z)|\)&lt;/span>在&lt;span class="math">\(\bar{D}\)&lt;/span>上必有界。&lt;/li>
&lt;li>&lt;span class="math">\(|f(z)|\)&lt;/span>在&lt;span class="math">\(\bar{D}\)&lt;/span>上必能取到最大、最小值。&lt;/li>
&lt;li>&lt;span class="math">\(|f(z)|\)&lt;/span>在&lt;span class="math">\(\bar{D}\)&lt;/span>上必一致连续。&lt;/li>
&lt;/ul>
&lt;p>当考虑到复函数与两个二元函数的关系，我们还有以下性质：&lt;/p>
&lt;blockquote>
&lt;p>复函数&lt;span class="math">\(f(z)=u(x,y)+iv(x,y)\)&lt;/span>在&lt;span class="math">\(z_0=x_0+iy_0\)&lt;/span>点连续&lt;span class="math">\(\Leftrightarrow u(x,y),\ v(x,y)\)&lt;/span>在点&lt;span class="math">\((x_0,y_0)\)&lt;/span>点连续。&lt;/p>
&lt;/blockquote>
&lt;h2 id="复变函数的导数与微分">复变函数的导数与微分&lt;/h2>
&lt;p>如果要问复分析研究的核心问题，&lt;strong>纯函数和亚纯函数&lt;/strong>应该算是其中之一。维基百科上说：“复分析（英语：Complex analysis）是研究复变的函数，特别是亚纯函数和复变解析函数（全纯函数）的数学理论。”而这两种函数都需要一个概念就是&lt;strong>复函数的微分&lt;/strong>。接下来我们也会说明，当复函数可微时，其实部函数&lt;span class="math">\(u(x,y)\)&lt;/span>和虚部函数&lt;span class="math">\(v(x,y)\)&lt;/span>也会有很强的关联性。&lt;/p>
&lt;p>我们先来看看什么是全纯函数、半纯函数。&lt;/p>
&lt;blockquote>
&lt;p>全纯函数（holomorphic function）:定义在复平面&lt;span class="math">\(\mathbb{C}\)&lt;/span>的开子集上的，在复平面&lt;span class="math">\(\mathbb{C}\)&lt;/span>中取值的，在&lt;strong>每点上皆可微的函数&lt;/strong>。复变函数中全纯函数也叫&lt;strong>解析函数&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>下文中默认解析和全纯是同义词，不过对于函数的某点&lt;span class="math">\(z_0\)&lt;/span>，我们习惯用“解析性”一词来讨论点的微分性质。&lt;/p>
&lt;blockquote>
&lt;p>亚纯函数（meromorphic function）:一个复平面的开子集&lt;span class="math">\(D\)&lt;/span>上的亚纯函数是一个在&lt;span class="math">\(D\)&lt;/span>上&lt;strong>除一个或若干个孤点集合之外的区域全纯的函数&lt;/strong>，那些孤立点称为该函数的&lt;strong>极点或奇点&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>从定义中可以看出，这个&lt;strong>函数“纯不纯”得看它可不可微&lt;/strong>。而复函数的可微就很微妙。&lt;/p>
&lt;p>首先，一元复函数和一元实函数一样，可微&lt;span class="math">\(\Leftrightarrow\)&lt;/span>可导。不过，复函数可微，可导不像一元实函数可微那么容易，需要从二元函数的角度考虑极限趋近的方向，此外实部和虚部两个二元函数间还必须满足特定关系，复函数才能可微、可导。因此，&lt;strong>一元复函数可微、可导是比一元实函数可微，甚至多元实函数可微强得多的条件&lt;/strong>。&lt;/p>
&lt;p>一元复函数导数的定义和一元实函数是类似的：&lt;/p>
&lt;blockquote>
&lt;p>设复函数&lt;span class="math">\(w=f(z)\)&lt;/span>定义于区域&lt;span class="math">\(D\)&lt;/span>，在&lt;span class="math">\(z_0\in D\)&lt;/span>的某邻域内&lt;span class="math">\(z_0+\Delta z\)&lt;/span>有定义，如果 &lt;span class="math">\[\lim_{\Delta z\rightarrow 0}\frac{\Delta w}{\Delta z}=\lim_{\Delta z\rightarrow 0}\frac{f(z_0+\Delta z)-f(z_0)}{\Delta z}\tag{4}\]&lt;/span> 存在，则称&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(z_0\)&lt;/span>处可导，该极限值为点&lt;span class="math">\(z_0\)&lt;/span>处的导数，记为&lt;span class="math">\(f&amp;#39;(z_0)\)&lt;/span>或&lt;span class="math">\(\frac{\mathrm{d}w}{\mathrm{d}z}|_{z=z_0}\)&lt;/span>。如果函数&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(D\)&lt;/span>内的每一点都可导，则称&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(D\)&lt;/span>内可导，此时即得&lt;span class="math">\(f(z)\)&lt;/span>的导函数&lt;span class="math">\(f&amp;#39;(z)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>可见一元复函数导数的定义和一元实函数是类似，只不过求极限时方向不止正负方向，而是任意方向。类似地，我们可以通过和一元实函数相同的方法得到复函数微分。&lt;/p>
&lt;p>根据极限定义，当导数存在时，式（4）可以改写成： &lt;span class="math">\[
\lim_{\Delta{z}\rightarrow 0} \Delta w = f&amp;#39;(z_0)\Delta z + o(\Delta z)\tag{5}
\]&lt;/span> 这样函数的增量&lt;span class="math">\(\Delta w\)&lt;/span>就表示成了&lt;strong>自变量线性增量和自变量的高阶无穷小两部分，而这也恰恰是微分的定义&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>设函数&lt;span class="math">\(w=f(z)\)&lt;/span>定义于区域&lt;span class="math">\(D\)&lt;/span>，在&lt;span class="math">\(z_0\in D\)&lt;/span>的某邻域内&lt;span class="math">\(z_0+\Delta z\)&lt;/span>有定义，对于邻域内任一点，如果存在&lt;span class="math">\(A\)&lt;/span>，使得 &lt;span class="math">\[\Delta w = f(z+\Delta z)-f(z)=A\Delta(z)+o(\Delta z)\tag{6}\]&lt;/span> 则称&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(z_0\)&lt;/span>处可微，&lt;span class="math">\(A\Delta z\)&lt;/span>为微分，记作&lt;span class="math">\(\mathrm{d}w = A\mathrm{d}z\)&lt;/span>。如果函数&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(D\)&lt;/span>内的每一点都可微，则称&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(D\)&lt;/span>内可微。&lt;/p>
&lt;/blockquote>
&lt;p>复函数导数侧重反映函数的“变化率”；而微分更能体现“以直代曲”的逼近思想。当&lt;span class="math">\(\Delta z\)&lt;/span>充分小时，两种思想是共同的。从上面也不难发现，对于一元复函数，可导可微互为充要条件。&lt;/p>
&lt;blockquote>
&lt;p>可导&lt;span class="math">\(\Leftrightarrow\)&lt;/span> 可微，即&lt;span class="math">\(\mathrm{d}w=f&amp;#39;(z)\mathrm{d}z,\ f&amp;#39;(z)=\frac{\mathrm{d}w}{\mathrm{d}z}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>简要证明如下：&lt;/p>
&lt;p>可导&lt;span class="math">\(\Rightarrow\)&lt;/span> 可微。可导&lt;span class="math">\(\Rightarrow \lim\limits_{\Delta z\rightarrow 0}\frac{\Delta w}{\Delta z}=f&amp;#39;(z) \Rightarrow \Delta w = f&amp;#39;(z_0)\Delta z + o(\Delta z)\Rightarrow\)&lt;/span>可微&lt;/p>
&lt;p>可导&lt;span class="math">\(\Leftarrow\)&lt;/span> 可微。可微&lt;span class="math">\(\Rightarrow \Delta w = A\Delta z + o(\Delta z) \Rightarrow \frac{\Delta w}{\Delta z}=A+\frac{o(\Delta z)}{\Delta z} \Rightarrow \lim\limits_{\Delta z\rightarrow 0}\frac{\Delta w}{\Delta z}=A=f&amp;#39;(z)\Rightarrow\)&lt;/span>可导。&lt;/p>
&lt;p>综合前面函数连续的内容，我们可以得出一个和一元实函数一样的关系：&lt;/p>
&lt;blockquote>
&lt;p>可微&lt;span class="math">\(\Leftrightarrow\)&lt;/span>可导&lt;span class="math">\(\Rightarrow\)&lt;/span>连续&lt;span class="math">\(\Rightarrow\)&lt;/span>有极限&lt;/p>
&lt;/blockquote>
&lt;p>此外，通过定义可证明复函数求导、微分的法则也和一元实函数一样：&lt;/p>
&lt;ul>
&lt;li>四则运算法则。&lt;/li>
&lt;li>&lt;span class="math">\([f(z)\plusmn g(z)]&amp;#39;=f&amp;#39;(z)\plusmn g&amp;#39;(z)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\([f(z)g(z)]&amp;#39;=f&amp;#39;(z)g(z)+f(z)g&amp;#39;(z)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\([\frac{f(z)}{g(z)}]&amp;#39;=\frac{f&amp;#39;(z)g(z)-f(z)g&amp;#39;(z)}{g^2(z)},g(z)\neq 0\)&lt;/span>&lt;/li>
&lt;li>复合函数求导法则：&lt;span class="math">\([f(g(z))]&amp;#39;=f&amp;#39;(g(z))g&amp;#39;(z)\)&lt;/span>&lt;/li>
&lt;li>反函数求导法则（注意只针对单值函数）：&lt;span class="math">\(\varphi&amp;#39;(w)=\frac{1}{f&amp;#39;(z)},\ z=\varphi(w),\ w=f(z)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="再看全纯函数解析函数">再看全纯函数（解析函数）&lt;/h3>
&lt;p>我们再回看全纯函数和可导的关系，对于某一点&lt;span class="math">\(z_0\)&lt;/span>而言，全纯要求函数&lt;span class="math">\(f(z)\)&lt;/span>在&lt;span class="math">\(z_0\)&lt;/span>及其邻域内都可微，不仅仅是那一个点，因此对于某个点而言，点解析的要求比点可导要更强： &lt;span class="math">\[
点解析\Rightarrow 点可导/可微
\]&lt;/span> 举个例子：&lt;span class="math">\(f(z)=|z|^2\)&lt;/span>，该函数仅在&lt;span class="math">\(z=0\)&lt;/span>处可导（其他位置都不可导），但是不解析。&lt;/p>
&lt;p>当我们考虑一个区域&lt;span class="math">\(D\)&lt;/span>时，点和周围的邻域一直都是被一起考虑的，因此对区域&lt;span class="math">\(D\)&lt;/span>来说，解析和可导/可微是等价的。 &lt;span class="math">\[
区域解析\Leftrightarrow 区域可导/可微
\]&lt;/span>&lt;/p>
&lt;p>解析函数的性质和导数性质也是类似的：&lt;/p>
&lt;ul>
&lt;li>在区域&lt;span class="math">\(D\)&lt;/span>内解析的两个函数&lt;span class="math">\(f(z),\ g(z)\)&lt;/span>的和、差、积、商（除去分母为0的点）在&lt;span class="math">\(D\)&lt;/span>内解析。&lt;/li>
&lt;li>推论：多项式复函数都是解析的（全纯函数）；有理式复函数在分母不为0的点也是解析的（半纯函数）；&lt;/li>
&lt;li>复合函数在对应的解析区域内也解析。&lt;/li>
&lt;/ul>
&lt;p>现在这么一看，一元复函数和一元实函数的微分性质似乎是完全一样，那么就没有必要特地研究复函数了。实际上，复函数可微的要求是比一元函数严格的多的，这个严格的体现就是：&lt;strong>全纯函数从各个方向求极限得到的导数都一致&lt;/strong>。只有在这个大前提下，复函数才能有类似于一元实函数的简单性质。&lt;/p>
&lt;h3 id="柯西-黎曼方程c-r方程">柯西-黎曼方程（C-R方程）&lt;/h3>
&lt;p>前文说过，复函数解析要求点及邻域从各个方向求极限得到的导数都一致，这很容易让我们想到多元函数导数中的方向导数。不过，相较于一个二元实函数，我们已经指出复函数对应的是两个二元实函数的有序对。因此，复函数可导不仅要求实部、虚部两个二元函数各自从各个方向的方向导数一致，还要求两个函数之间满足特殊的关系，这就是&lt;strong>柯西-黎曼方程&lt;/strong>。&lt;/p>
&lt;p>柯西-黎曼方程体现的是复函数&lt;span class="math">\(f(z)\)&lt;/span>的实部二元函数&lt;span class="math">\(u(x,y)\)&lt;/span>与虚部二元函数&lt;span class="math">\(v(x,y)\)&lt;/span>之间的关系。从复函数可导推出柯西-黎曼方程是很容易的。由于复函数可导，可知： &lt;span class="math">\[
f&amp;#39;(z)=\lim_{\Delta x\rightarrow 0\atop \Delta y\rightarrow 0}\frac{\Delta u +i\Delta v}{\Delta x +i\Delta y}\tag{7}
\]&lt;/span> 函数在&lt;span class="math">\(z\)&lt;/span>点可导，就意味着&lt;span class="math">\(∆z = ∆x + i∆y\)&lt;/span>以任意方式趋向于零，上式右边的极限都趋于同样的有限值，即该点导数值&lt;span class="math">\(f&amp;#39;(z)\)&lt;/span>。在&lt;span class="math">\(z\)&lt;/span>平面上有无限条线路使&lt;span class="math">\(∆z\rightarrow 0\)&lt;/span>, 我们选取如下两条路线：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/柯西-黎曼方程推导.png" alt="柯西-黎曼方程推导" />&lt;p class="caption">柯西-黎曼方程推导&lt;/p>
&lt;/div>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(∆x\rightarrow 0\)&lt;/span>但&lt;span class="math">\(∆y = 0\)&lt;/span>，&lt;span class="math">\(f&amp;#39;(z)=\lim\limits_{∆x\rightarrow 0\atop ∆y = 0}\frac{∆u+i∆v}{∆x} = \frac{\partial u}{\partial x}+i\frac{\partial v}{\partial x}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(∆y\rightarrow 0\)&lt;/span>但&lt;span class="math">\(∆x = 0\)&lt;/span>，&lt;span class="math">\(f&amp;#39;(z)=\lim\limits_{∆y\rightarrow 0\atop ∆x = 0}\frac{∆u+i∆v}{i∆y} = \frac{\partial v}{\partial y}-i\frac{\partial u}{\partial y}\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>联立上方两个石子，且要求实部与虚部相等，有：&lt;/p>
&lt;p>&lt;span class="math">\[
\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y},\\
\frac{\partial u}{\partial y}=-\frac{\partial v}{\partial x} \tag{8}
\]&lt;/span> 即为柯西-黎曼方程，简称C-R方程。可见，并不是任意两个二元实函数组成一对的复函数都是可导的，他们之间至少得满足C-R方程（必要条件）。根据C-R方程，我们也能得出，&lt;strong>一旦复函数是个解析函数，其实部（虚部）一旦给定，则虚部（实部）也基本确定&lt;/strong>。为什么说是基本确定呢？后面我们谈到调和函数的时候在细说。 &lt;span class="math">\[
u(x,y) = \int \frac{\partial v}{\partial y} \mathrm{d}x = \int -\frac{\partial v}{\partial x} \mathrm{d}y\\
v(x,y) = \int \frac{\partial u}{\partial x} \mathrm{d}y = \int -\frac{\partial u}{\partial y} \mathrm{d}x\tag{9}
\]&lt;/span>&lt;/p>
&lt;p>如果想要让C-R方程变成可导的充分条件，还需要加上什么要求呢？很简单，再要求&lt;span class="math">\(u(x,y),\ v(x,y)\)&lt;/span>可微即可。&lt;/p>
&lt;blockquote>
&lt;p>点可导充要条件：函数&lt;span class="math">\(w=f(z)=u(x,y)+iv(x,y)\)&lt;/span>在点&lt;span class="math">\(z=x+iy\)&lt;/span>处可导的充要条件是：&lt;span class="math">\(u(x,y)\)&lt;/span>和&lt;span class="math">\(v(x,y)\)&lt;/span>在点&lt;span class="math">\((x,y)\)&lt;/span>处&lt;strong>可微且满足柯西-黎曼方程&lt;/strong>。&lt;/p>
&lt;p>区域解析（可导）的充要条件：函数&lt;span class="math">\(w=f(z)=u(x,y)+iv(x,y)\)&lt;/span>在区域&lt;span class="math">\(D\)&lt;/span>内解析的充要条件是：&lt;span class="math">\(u(x,y)\)&lt;/span>和&lt;span class="math">\(v(x,y)\)&lt;/span>在&lt;strong>区域&lt;span class="math">\(D\)&lt;/span>内可微且满足柯西-黎曼方程&lt;/strong>。区域可导和区域解析互为充要条件。&lt;/p>
&lt;/blockquote>
&lt;p>上面定理的必要性是显然的。简要证明如下。&lt;/p>
&lt;p>证明：首先C-R方程就是从可微复函数两个方向求方向导数得出来的，其次，复函数可微说明它可以写成&lt;span class="math">\(\Delta w = A \Delta z + o(\Delta z)\)&lt;/span>的形式，其中自变量和因变量的增量分别可以表示成实部与虚部的组合&lt;span class="math">\(\Delta w = \Delta u + i \Delta v, \Delta z = \Delta x + i\Delta y\)&lt;/span>。重要的是系数&lt;span class="math">\(A=f&amp;#39;(z)\)&lt;/span>是个固定的复数，即&lt;span class="math">\(A=a+ib\)&lt;/span>。将它们分别带入就可得： &lt;span class="math">\[
\Delta u + i \Delta v = (a+ib)(\Delta x + i\Delta y) + o(\Delta z)\\
\begin{cases}
\Delta u = a\Delta x - b \Delta y + o(\Delta z)\\
\Delta v = a\Delta y + b \Delta x + o(\Delta z)
\end{cases}\tag{10}
\]&lt;/span> 式(10)正是二元函数&lt;span class="math">\(u(x,y),v(x,y)\)&lt;/span>的微分形式。必要性得证。&lt;/p>
&lt;p>我们下面只要证明充分性，即C-R方程+两个部分的二元函数可微&lt;span class="math">\(\Rightarrow\)&lt;/span> 复函数可导。我们以点可导为例，区域可导的思路是一样的。&lt;/p>
&lt;p>证明：&lt;span class="math">\(u(x,y),\ v(x,y)\)&lt;/span>在&lt;span class="math">\((x,y)\)&lt;/span>可微则有： &lt;span class="math">\[
\begin{cases}
\Delta u = \frac{\partial u}{\partial x} \Delta x + \frac{\partial u}{\partial y} \Delta y + o(\Delta z)\\
\Delta v = \frac{\partial v}{\partial x} \Delta x + \frac{\partial v}{\partial y} \Delta y + o(\Delta z)
\end{cases}
\]&lt;/span> 根据C-R方程，我们可以将上面的&lt;span class="math">\(\frac{\partial u}{\partial y}\)&lt;/span>替换成&lt;span class="math">\(-\frac{\partial v}{\partial x}\)&lt;/span>，&lt;span class="math">\(\frac{\partial v}{\partial y}\)&lt;/span>替换成&lt;span class="math">\(\frac{\partial v}{\partial u}\)&lt;/span>，则有 &lt;span class="math">\[
\begin{cases}
\Delta u = \frac{\partial u}{\partial x} \Delta x {\color{Red}{- \frac{\partial v}{\partial x}}} \Delta y + o(\Delta z)\\
\Delta v = \frac{\partial v}{\partial x} \Delta x + {\color{Red}{\frac{\partial u}{\partial x}}} \Delta y + o(\Delta z)
\end{cases}\\
\Rightarrow \Delta u + i\Delta v = \frac{\partial u}{\partial x} \Delta x - \frac{\partial v}{\partial x} \Delta y + i\frac{\partial v}{\partial x} \Delta x+ i\frac{\partial u}{\partial x} \Delta y + o(\Delta z)\\
=\underbrace{(\frac{\partial u}{\partial x}+ i\frac{\partial v}{\partial x})}_{A}\underbrace{(\Delta x + i\Delta y)}_{\Delta z} + o(\Delta z)\\
=A\Delta z + o(\Delta z)
\]&lt;/span> 上式即为原复函数的微分形式。因此复函数在点&lt;span class="math">\(z\)&lt;/span>处可导。&lt;/p>
&lt;p>在多元函数微分中，还有这样一个关系：&lt;strong>一阶偏导数存在且连续则函数可微&lt;/strong>，我们同样可以将其应用到复函数可微中，替换实部、虚部的可微要求。（注意一阶偏导数连续是个更强的条件，不是充要条件）。从而有如下推论：&lt;/p>
&lt;blockquote>
&lt;p>推论：若函数&lt;span class="math">\(f(z) = u(x, y) + iv(x, y)\)&lt;/span>在&lt;span class="math">\(z\)&lt;/span>点满足C-R方程，且函数的四个一阶偏导数&lt;span class="math">\(\frac{\partial u}{\partial x},\frac{\partial u}{\partial y},\frac{\partial v}{\partial x},\frac{\partial v}{\partial y}\)&lt;/span>在&lt;span class="math">\(z\)&lt;/span>点连续，则函数在&lt;span class="math">\(z\)&lt;/span>点可导。&lt;/p>
&lt;/blockquote>
&lt;p>反之并不成立，原因在于可微不能推出偏导数连续。&lt;/p>
&lt;h3 id="全纯函数解析函数与调和函数">全纯函数（解析函数）与调和函数&lt;/h3>
&lt;p>解析函数实部和虚部的还不仅是C-R方程的关系，其自身也得满足&lt;strong>拉普拉斯方程&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>拉普拉斯方程：若二元实函数&lt;span class="math">\(\varphi(x,y)\)&lt;/span>在区域&lt;span class="math">\(D\)&lt;/span>内可微且有二阶偏导数，且有 &lt;span class="math">\[ \frac{\partial^2 \varphi}{\partial x^2} + \frac{\partial^2 \varphi}{\partial y^2} =0\tag{11}\]&lt;/span> 则称&lt;span class="math">\(\varphi(x,y)\)&lt;/span>满足拉普拉斯方程。&lt;/p>
&lt;/blockquote>
&lt;p>满足拉普拉斯方程的二元函数&lt;span class="math">\(\varphi(x,y)\)&lt;/span>为区域&lt;span class="math">\(D\)&lt;/span>内的&lt;strong>调和函数&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>若二阶偏导数不为0，而是&lt;span class="math">\(f(x,y)\)&lt;/span>，即 &lt;span class="math">\[ \frac{\partial^2 \varphi}{\partial x^2} + \frac{\partial^2 \varphi}{\partial y^2} = f(x,y)\tag{12} \]&lt;/span> 则被称为泊松方程。&lt;/p>
&lt;/blockquote>
&lt;p>那么解析函数和调和函数有什么关系呢？调和函数研究的二元实函数，我们可以尝试对复函数的实部、虚部求二阶偏导数。&lt;/p>
&lt;p>由C-R方程可知，其一阶偏导数有式（8）的关系，那么我们对其再求偏导有： &lt;span class="math">\[
\left .
\begin{aligned}\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}\overset{\frac{\partial}{\partial x}}{\Rightarrow}\frac{\partial^2 u}{\partial x^2} = \frac{\partial^2 v}{\partial y\partial x} \\
\frac{\partial u}{\partial y}=-\frac{\partial v}{\partial x}\overset{\frac{\partial}{\partial y}}{\Rightarrow} \frac{\partial^2 u}{\partial y^2} = -\frac{\partial^2 v}{\partial x\partial y}\end{aligned} \right\}\overset{+}{\Rightarrow} \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0
\]&lt;/span> 同理，有&lt;span class="math">\(\frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2} = 0\)&lt;/span>。由上我们可以得出以下定理：&lt;/p>
&lt;blockquote>
&lt;p>若函数&lt;span class="math">\(f(z)=u(x,y)+iv(x,y)\)&lt;/span>在区域&lt;span class="math">\(D\)&lt;/span>内解析，则&lt;span class="math">\(u(x,y),v(x,y)\)&lt;/span>在区域&lt;span class="math">\(D\)&lt;/span>内都是调和函数。&lt;/p>
&lt;/blockquote>
&lt;p>对于同属于一个解析复函数的实部和虚部都是调和函数，我们将虚部的调和函数&lt;span class="math">\(v\)&lt;/span>称为实部&lt;span class="math">\(u\)&lt;/span>的&lt;strong>共轭调和函数&lt;/strong>。对应的，一对共轭调和函数可以确定一解析复函数&lt;span class="math">\(f(z)\)&lt;/span>，我们可以将其写成如下定理：&lt;/p>
&lt;blockquote>
&lt;p>复变函数&lt;span class="math">\(f(z)=u(x,y)+iv(x,y)\)&lt;/span>在区域&lt;span class="math">\(D\)&lt;/span>内解析的充要条件是：在区域&lt;span class="math">\(D\)&lt;/span>内，&lt;span class="math">\(f(z)\)&lt;/span>的虚部&lt;span class="math">\(v(x,y)\)&lt;/span>是实部&lt;span class="math">\(u(x,y)\)&lt;/span>的共轭调和函数，&lt;/p>
&lt;/blockquote>
&lt;p>需要注意的是，&lt;span class="math">\(v\)&lt;/span>是&lt;span class="math">\(u\)&lt;/span>的共轭调和函数，并不意味着&lt;span class="math">\(u\)&lt;/span>也是&lt;span class="math">\(v\)&lt;/span>的共轭调和函数！不具有对称性。&lt;/p>
&lt;p>还记得我们在&lt;a href="#柯西-黎曼方程c-r方程">柯西-黎曼方程（C-R方程）&lt;/a>那一节说过：“一旦复函数是个解析函数，其实部（虚部）一旦给定，则虚部（实部）也基本确定”。这种确定性，就是C-R方程和调和函数所共同确定的。此时，对于一个解析复函数，我们已知实部&lt;span class="math">\(u\)&lt;/span>，能求虚部&lt;span class="math">\(v\)&lt;/span>( 或者已知虚部&lt;span class="math">\(v\)&lt;/span>，求实部&lt;span class="math">\(u\)&lt;/span>)。起主要作用的是C-R方程，调和函数要求起辅助作用，具体方法主流的有两种：偏积分法和全微分法。不过本文在这里不对这些方法做具体说明了。&lt;/p>
&lt;h2 id="初等复函数">初等复函数&lt;/h2>
&lt;p>复变函数中的初等函数是实数域中初等函数的推广（复平面的解析延拓），它们的定义方式尽可能保持一致，特别是当自变量取实值时，两者是一样的。同时实函数过去一些无法进行的操作，比如对负数求对数，在复数域也是可以的。&lt;/p>
&lt;h3 id="基础指数函数">基础：指数函数&lt;/h3>
&lt;blockquote>
&lt;p>对于复数&lt;span class="math">\(z=x+iy\)&lt;/span>，其以&lt;span class="math">\(e\)&lt;/span>为底的指数函数为&lt;span class="math">\(w=e^z=\exp z = e^x(\cos y + i\sin y)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>从现在的角度来看，将复指数函数&lt;span class="math">\(f(z)=e^z,\ z\in\mathbb{C}\)&lt;/span>是实函数&lt;span class="math">\(f(x)=e^x,\ x\in\R\)&lt;/span>的简单延拓是很正常的，不过在复数函数发展过程中并非如此，开始时，复指数函数是通过&lt;span class="math">\(w=e^x(\cos y + i\sin y)\)&lt;/span>来定义的。因为虚数诞生的时候，数学家还是倾向是将实部和虚部分开考虑，后来欧拉在棣莫佛的研究上通过对比函数的无穷级数，发现了欧拉公式&lt;span class="math">\(e^{iy}=\cos y + i\sin y\)&lt;/span>才将&lt;span class="math">\(e^x(\cos y + i\sin y)\)&lt;/span>统合成了&lt;span class="math">\(e^{x+iy}=e^z\)&lt;/span>。&lt;strong>指数函数是初等函数中最重要的函数，其余的初等函数都通过指数函数来定义&lt;/strong>。&lt;/p>
&lt;p>由定义我们可以获得以下结论（下面的结论不是平凡的，需要证明，虽然不难）：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(|e^z|=e^x|e^{iy}|=e^x|\cos y + i\sin y|=e^x\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(Arg e^z = y+2k\pi\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(e^z\)&lt;/span>为&lt;strong>单值函数&lt;/strong>，区别于后面的复对数函数&lt;/li>
&lt;li>&lt;span class="math">\(e^z \neq 0\)&lt;/span>，因为&lt;span class="math">\(e^x&amp;gt;0,\ \cos y +i\sin y \neq 0\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(e^z\)&lt;/span>在复平面上处处解析，且导数&lt;span class="math">\((e^z)&amp;#39;=e^z\)&lt;/span>。&lt;/li>
&lt;li>&lt;span class="math">\(e^{z_1}e^{z_2}=e^{z_1+z_2}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(e^z\)&lt;/span>是以&lt;span class="math">\(2k\pi i\)&lt;/span>为周期的周期函数。&lt;/li>
&lt;/ul>
&lt;h3 id="对数函数">对数函数&lt;/h3>
&lt;blockquote>
&lt;p>&lt;span class="math">\(e^w = z\)&lt;/span>则称&lt;span class="math">\(w\)&lt;/span>为&lt;span class="math">\(z\)&lt;/span>的对数函数，记为&lt;span class="math">\(w=Ln(z)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>复对数函数是用复指数函数来定义的（注意&lt;span class="math">\(Ln\)&lt;/span>首字母是大写的用来区别于实函数&lt;span class="math">\(\ln\)&lt;/span>，此外&lt;span class="math">\(Ln\)&lt;/span>并不是以&lt;span class="math">\(e\)&lt;/span>为底的意思，只是一个记号），与实函数一样，复对数函数也是复指数函数的反函数。&lt;/p>
&lt;p>那我们如何根据这个定义实际计算复对数函数呢？同样也是利用指数函数。设&lt;span class="math">\(z=re^{i\theta},\ w=u_iv\)&lt;/span>，那么根据复对数函数定义有&lt;span class="math">\(e^w=e^u e^{iv}=re^{i\theta+2k\pi}\)&lt;/span>。由此，我们可以算出： &lt;span class="math">\[
u=\ln r\\
v = \theta+2k\pi
\]&lt;/span> 因为指数函数具有周期性，导致辐角的值不止一个，所以复对数函数是一个&lt;strong>多值函数&lt;/strong>，即 &lt;span class="math">\[
w=Ln(z)=\ln r+i(\theta+2k\pi)\\
w=Ln(z)=\ln(z)+i(Arg(z)+2k\pi)\tag{13}
\]&lt;/span> 复对数函数的多值性显然是由虚部（辐角的周期性）引起的，对于每一个给定的&lt;span class="math">\(k\)&lt;/span>，&lt;span class="math">\(w_k\)&lt;/span>就成了单值函数，称为&lt;span class="math">\(Ln(z)\)&lt;/span>的一个分支，其中&lt;strong>辐角取主值(k=0)的那一支称为主枝&lt;/strong>，也记为&lt;span class="math">\(\ln z\)&lt;/span>。&lt;/p>
&lt;p>例如：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(Ln(1+i)=\ln|(1+i)|+iArg(1+i)+2k\pi i=\ln \sqrt{2}+i(\arctan 1 + 2k\pi)=\ln \sqrt{2}+i(\pi/4 + 2k\pi)\)&lt;/span>，主值为&lt;span class="math">\(\ln \sqrt{2}+i\pi/4\)&lt;/span>。&lt;/li>
&lt;li>&lt;span class="math">\(Ln(-1)=\ln |-1| +iArg(-1)+2k{\pi}i= 0 + i(\arctan -1+ 2k\pi)=i(\pi+2k\pi)\)&lt;/span>，主值为&lt;span class="math">\(i\pi\)&lt;/span>。可见，在复数域内，负实数是可以求对数的。&lt;/li>
&lt;/ul>
&lt;p>复对数函数的性质有以下几个主要点：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>由于&lt;span class="math">\(Arg(z)\)&lt;/span>在原点无定义（且&lt;span class="math">\(e^w\neq 0\)&lt;/span>），因此&lt;span class="math">\(Ln\ z\)&lt;/span>也在原点无定义。&lt;/li>
&lt;li>由于复数定义问题，我们要求辐角&lt;span class="math">\(-\pi&amp;lt;0&amp;lt;\pi\)&lt;/span>，因此&lt;span class="math">\(Arg(z)\)&lt;/span>在负实数轴是不连续的，即从顺时针方向趋近于负实轴为&lt;span class="math">\(-\pi\)&lt;/span>，从逆时针方向趋近于负实轴为&lt;span class="math">\(+\pi\)&lt;/span>，中间有&lt;span class="math">\(2\pi\)&lt;/span>的跳跃，这导致&lt;span class="math">\(Ln\ z\)&lt;/span>的各分支在负实轴(以及原点)也是不连续的，其他位置连续。&lt;img src="../images/The-branch-cut-for-the-determination-of-the-complex-square-root.png" alt="The-branch-cut-for-the-determination-of-the-complex-square-root.png" />&lt;/li>
&lt;li>算是性质2的推论：&lt;span class="math">\(Ln\ z\)&lt;/span>的各分支在除去原点及负实轴的复平面内解析。其导数为&lt;span class="math">\((Ln\ z)&amp;#39;=1/z\)&lt;/span>，和实函数一样。&lt;/li>
&lt;li>在集合意义上：&lt;span class="math">\(Ln(z_1z_2)=Ln(z_1)+Ln(z_2),\ Ln(z_1/z_2)=Ln(z_1)-Ln(z_2)\)&lt;/span>。但是&lt;span class="math">\(Ln(z^n)\neq nLn\ z\)&lt;/span>，&lt;span class="math">\(Ln(z^n)\)&lt;/span>的周期已经和&lt;span class="math">\(Ln(z)\)&lt;/span>不同了！&lt;/li>
&lt;/ol>
&lt;p>最后我们了解下指数函数与对数函数的关系，在实函数中，它们互为反函数，但是在； 复数域，一个是单值函数，一个是多值函数。对数函数会将自变量映射到各个分支，每个分支周期性地相等。如果我们将对数函数的值域限定在一个分支以内，二者还是反函数关系的。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/复指数函数与复对数函数关系.png" alt="复指数函数与复对数函数关系.png" />&lt;p class="caption">复指数函数与复对数函数关系.png&lt;/p>
&lt;/div>
&lt;h3 id="幂函数">幂函数&lt;/h3>
&lt;p>幂函数可以通过指数函数与对数函数的复合来定义。&lt;/p>
&lt;blockquote>
&lt;p>函数&lt;span class="math">\(w=z^\alpha\)&lt;/span>，规定&lt;span class="math">\(z^\alpha = e^{\alpha Ln(z)},\ \alpha\in\mathbb{C},\ z\neq 0\)&lt;/span>，称为复变量&lt;span class="math">\(z\)&lt;/span>的幂函数。此外，我们规定&lt;span class="math">\(z^0=1\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>由于&lt;span class="math">\(Ln\)&lt;/span>的存在，幂函数也是多值函数。幂函数特性与&lt;span class="math">\(\alpha\)&lt;/span>的取值相关。&lt;/p>
&lt;p>当&lt;span class="math">\(\alpha\)&lt;/span>为正整数&lt;span class="math">\(n\)&lt;/span>时，&lt;span class="math">\(w= z^n = e^{n Ln z}=e^{n(\ln |z|+iArg(z)+i2k\pi)}\)&lt;/span>由于复指数函数以&lt;span class="math">\(2k\pi\)&lt;/span>为周期，当&lt;span class="math">\(n\)&lt;/span>为正整数时有&lt;span class="math">\(e^{n(\ln |z|+iArg(z)+i2k\pi)}=e^{n(\ln |z|+iArg(z))}=e^{n\ln z}\)&lt;/span>。此时多值性消除，幂函数为单值函数，且处处解析，导数为&lt;span class="math">\((z^n)&amp;#39;=nz^{n-1}\)&lt;/span>。&lt;/p>
&lt;p>当&lt;span class="math">\(\alpha\)&lt;/span>为负整数&lt;span class="math">\(-n\)&lt;/span>时，同理可得其也是单值函数，且在原点外处处解析，且导数为&lt;span class="math">\((z^-n)&amp;#39;=-nz^{-n-1}\)&lt;/span>&lt;/p>
&lt;p>当&lt;span class="math">\(\alpha\)&lt;/span>为有理数时，即&lt;span class="math">\(\alpha = p/q, \ p,q\)&lt;/span>互质。由于&lt;span class="math">\(Ln\)&lt;/span>的存在，幂函数是多值函数，有&lt;span class="math">\(q\)&lt;/span>个值（&lt;span class="math">\(q\)&lt;/span>个分支）。解析域受到&lt;span class="math">\(Ln\)&lt;/span>函数限制，在除原点和负实轴外处处解析，导数为&lt;span class="math">\((z^\alpha)&amp;#39;=\alpha z^{\alpha-1}\)&lt;/span>&lt;/p>
&lt;p>当&lt;span class="math">\(为无理数或复数时，一般为无穷多值，解析域受到\)&lt;/span>Ln$函数限制，在除原点和负实轴外处处解析。&lt;/p>
&lt;p>举几个例子：&lt;/p>
&lt;p>&lt;span class="math">\[2^i=e^{i Ln(2)}=e^{i(\ln 2 + 2k\pi i)}=e^{-2k\pi}e^{i\ln 2}\\
=e^{-2k\pi}(\cos\ln 2+i\sin\ln 2),\ k\in Z\]&lt;/span> 显然上式有无数个值。 &lt;span class="math">\[
i^i = e^{i Ln i}=e^{i\times i(\pi/2+2k\pi)}=e^{-\pi/2-2k\pi},\ k\in Z
\]&lt;/span> 可见&lt;span class="math">\(i^i\)&lt;/span>是正实数，主值为&lt;span class="math">\(e^{-\pi/2}\)&lt;/span>。 &lt;span class="math">\[
1^{\sqrt{2}}=e^{\sqrt{2}Ln(1)}=e^{\sqrt{2}\times 2k\pi i}=e^{2\sqrt{2}k\pi i}
\]&lt;/span> 可见，&lt;strong>不要想当然地认为1的任意次幂都是1&lt;/strong>，仅限于有理数次幂才成立。&lt;/p>
&lt;h3 id="三角函数">三角函数&lt;/h3>
&lt;p>三角函数和欧拉公式密不可分，我们可以通过欧拉公式使用指数函数的形式来定义三角函数。&lt;/p>
&lt;blockquote>
&lt;p>根据欧拉公式有&lt;span class="math">\(e^{i\theta}=\cos\theta+i\sin\theta,\ e^{-i\theta}=\cos\theta-i\sin\theta\)&lt;/span>可推得： &lt;span class="math">\[ \cos z = \frac{e^{iz}+e^{-iz}}{2}\\ \sin z = \frac{e^{iz}-e^{-iz}}{2i}\]&lt;/span> 由此&lt;span class="math">\(\cos z\)&lt;/span>和&lt;span class="math">\(\sin z\)&lt;/span>我们可以定义出其他三角函数 &lt;span class="math">\[ \tan z =\frac{\sin z}{\cos z},\ \cot z = \frac{\cos z}{\sin z} \\ \sec z = {1\over\cos z},\ \csc z={1\over \sin z}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>三角函数的性质和实函数许多是一样的，但是有一点区别很大。&lt;/p>
&lt;ul>
&lt;li>根据欧拉公式中的指数函数可知，复三角函数不再是有界函数，即&lt;span class="math">\(|\sin x|≤1,\ |\cos x|≤1,x\in\R\)&lt;/span>，三角函数值可以随之实部变得无穷大。&lt;/li>
&lt;li>&lt;span class="math">\(\sin z,\ \cos z\)&lt;/span>在复平面处处解析。&lt;span class="math">\((\sin z)&amp;#39;=\cos z;\ (\cos z)&amp;#39;=-\sin z\)&lt;/span>。&lt;/li>
&lt;li>三角函数周期性、可导性、奇偶性、零点与实函数一样。&lt;/li>
&lt;li>各种三角公式可以照搬。&lt;/li>
&lt;li>反三角函数可以通过欧拉公式推导出，也是多值函数。&lt;/li>
&lt;/ul>
&lt;h3 id="双曲函数">双曲函数&lt;/h3>
&lt;p>和三角函数一样，也可通过欧拉公式推的。&lt;/p>
&lt;blockquote>
&lt;p>双曲正弦函数&lt;span class="math">\(\sh z =\frac{1}{2}(e^z-e^{-z})\)&lt;/span>；反双曲正弦函数&lt;span class="math">\(\mathrm{Arsh} z=\mathrm{Ln}(z+\sqrt{z^2+1})\)&lt;/span>&lt;/p>
&lt;p>双曲余弦函数&lt;span class="math">\(\ch z =\frac{1}{2}(e^z+e^{-z})\)&lt;/span>；反双曲余弦函数&lt;span class="math">\(\mathrm{Arch} z=\mathrm{Ln}(z+\sqrt{z^2-1})\)&lt;/span>&lt;/p>
&lt;p>双曲正切函数&lt;span class="math">\(\th z = \frac{\sh z}{\ch z}\)&lt;/span>；反双曲正切函数&lt;span class="math">\(\mathrm{Arth} z=\frac{1}{2}\mathrm{Ln}\frac{1+z}{1-z}\)&lt;/span>&lt;/p>
&lt;p>双曲余切函数&lt;span class="math">\(\coth z = \frac{\ch z}{\sh z}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>双曲正弦、双曲余弦在复平面显然处处解析，曲正切函数在除去&lt;span class="math">\(z=i(k\pi+\frac{\pi}{2})\)&lt;/span>外处处解析。&lt;/p>
&lt;p>在复分析中，双曲函数对复数映射是非常常用的，需要的时候可以在查询。&lt;/p></description></item><item><title>数学分析-导数、偏导与微分</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E5%AF%BC%E6%95%B0%E5%81%8F%E5%AF%BC%E4%B8%8E%E5%BE%AE%E5%88%86/</link><pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E5%AF%BC%E6%95%B0%E5%81%8F%E5%AF%BC%E4%B8%8E%E5%BE%AE%E5%88%86/</guid><description>
&lt;h2 id="导数偏导与微分">导数、偏导与微分&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#多元函数与一元函数差别的根源">多元函数与一元函数差别的根源&lt;/a>&lt;/li>
&lt;li>&lt;a href="#导数的延拓">导数的延拓&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从增量到微分">从增量到微分&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从多元函数导数到微分">从多元函数导数到微分&lt;/a>&lt;/li>
&lt;li>&lt;a href="#偏导与可微">偏导与可微&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>一元函数与多元函数的导数（偏导数）、微分以及函数的连续性之间关系密切，总体而言它们的关系如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/极限连续可导可微关系.png" alt="极限连续可导可微关系.png" />&lt;p class="caption">极限连续可导可微关系.png&lt;/p>
&lt;/div>
&lt;p>一元函数的内容在数学分析中较为容易理解，本文将着重介绍多元函数的偏导数、微分以及其他衍生概念和它们之间的关系。&lt;/p>
&lt;h2 id="多元函数与一元函数差别的根源">多元函数与一元函数差别的根源&lt;/h2>
&lt;p>在一元函数中，可微和可导是等价的，但是在多元函数中并不如此，并且导数在多元函数中推广并不是自然的拓展，而是衍生出偏导数、方向导数、全导数等额外概念。此外，多元函数连续性的概念也不仅仅是从负方向和正方向两边连续这么简单了。造成这一切的根源是&lt;strong>自变量维度的变化&lt;/strong>。&lt;/p>
&lt;p>我们先看一元函数场景下：&lt;/p>
&lt;ul>
&lt;li>函数&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>处极限：&lt;span class="math">\(\lim\limits_{x\rightarrow x_0} f(x) =A\)&lt;/span>；&lt;/li>
&lt;li>函数&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>处连续：&lt;span class="math">\(\lim\limits_{x\rightarrow x_0}f(x) = f(x_0)\)&lt;/span>(函数在&lt;span class="math">\(x_0\)&lt;/span>出有定义);&lt;/li>
&lt;li>函数&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>处导数：&lt;span class="math">\(\lim\limits_{x\rightarrow x_0} \frac{f(x)-f(x_0)}{x-x_0}=f&amp;#39;(x_0)\)&lt;/span>(函数在&lt;span class="math">\(x_0\)&lt;/span>&lt;strong>邻域&lt;/strong>有定义);&lt;/li>
&lt;/ul>
&lt;p>以上三者都谈到了&lt;span class="math">\(x\rightarrow x_0\)&lt;/span>的极限场景，而这三类极限存在的&lt;strong>充要&lt;/strong>条件都是存在左极限&lt;span class="math">\(x\rightarrow x_0^-\)&lt;/span>和右极限&lt;span class="math">\(x\rightarrow x_0^+\)&lt;/span>且相等。也就是说，&lt;strong>一元函数这些定义中，自变量是一维的，只要研究极限从点&lt;span class="math">\(x_0\)&lt;/span>正负两个方向趋近即可&lt;/strong>。如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/左右极限.png" alt="左右极限.png" />&lt;p class="caption">左右极限.png&lt;/p>
&lt;/div>
&lt;p>从上图中可知，在以为场景下，趋近于某一点只需要从两个方向分别趋近即可，但是当维度大于一维时，“趋近”的概念有了本质的变化。以二元空间的自变量&lt;span class="math">\((x,y)\)&lt;/span>组成的二维空间为例。如果我们按照惯有思路推导，会认为二维空间的极限是需要从四个方向趋近点&lt;span class="math">\((x_0^-,0),(x_0^+,0),(0,y_0^-),(0,y_0^+)\)&lt;/span>，如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/二维空间极限轴方向.png" alt="二维空间极限轴方向" />&lt;p class="caption">二维空间极限轴方向&lt;/p>
&lt;/div>
&lt;p>这显然考虑的不够周全，比如考虑二维连续函数，在&lt;span class="math">\(x，y\)&lt;/span>两个轴是1，其他位置都是0，它在原点&lt;span class="math">\((0,0)\)&lt;/span>处，&lt;span class="math">\((x_0^-,0),(x_0^+,0),(0,y_0^-),(0,y_0^+)\)&lt;/span>四个方向都是连续的，但是从沿着向量&lt;span class="math">\((1,1)\)&lt;/span>的方向就不连续了，函数值从0跳跃到了1。所以，当自变量大于一维时，我们考虑&lt;strong>点所在空间的所有方向&lt;/strong>，而不限于几个分量的方向：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/二维空间极限全方向.png" alt="二维空间极限全方向.png" />&lt;p class="caption">二维空间极限全方向.png&lt;/p>
&lt;/div>
&lt;p>当我们研究的问题从两个方向一下子扩展到无穷个方向后，传统的一些做法和结论就不适用了。&lt;/p>
&lt;p>换个严谨点的说法：多维极限的存在，是指点&lt;span class="math">\(\vec{p}\)&lt;/span>以任何方式趋于点&lt;span class="math">\(\vec{p}_0\)&lt;/span>时，函数值&lt;span class="math">\(f(\vec{p})\)&lt;/span>都无限接近于&lt;span class="math">\(f(\vec{p}_0)\)&lt;/span>。因此，如果点&lt;span class="math">\(\vec{p}\)&lt;/span>只是以特殊方式，例如沿着一条定直线或定曲线趋于$ &lt;em>0&lt;span class="math">\(，即使函数值同样趋近于\)&lt;/span>f(&lt;/em>0)&lt;span class="math">\(，我们还不能由此断定函数的极限存在。但是反过来，如果当\)&lt;/span>&lt;span class="math">\(以不同方式趋近于\)&lt;/span>_0&lt;span class="math">\(时，函数\)&lt;/span>f()$趋于不同的值，那么就可以断定这函数的极限不存在。&lt;/p>
&lt;p>一句话总结：极限趋近方向从两个变成了无穷个。&lt;/p>
&lt;h2 id="导数的延拓">导数的延拓&lt;/h2>
&lt;p>一元函数的导数是直观的，因为自变量只有一个，导数的定义就是函数值（因变量）随着自变量变化的变化率。但是在多元函数中，自变量不止一个，如何定义导数就出现了不同方式。&lt;/p>
&lt;p>首先，如果只考虑其中一个自变量的变化，比如&lt;span class="math">\(x\)&lt;/span>，而其他自变量（&lt;span class="math">\(y, z, \dotsb\)&lt;/span>）不变，那么此时多元函数就可以当成&lt;span class="math">\(x\)&lt;/span>的一元函数。这时仅对&lt;span class="math">\(x\)&lt;/span>求的导数就成为多元函数&lt;span class="math">\(f\)&lt;/span>对于&lt;span class="math">\(x\)&lt;/span>的偏导数，记为：&lt;span class="math">\(\frac{\partial f}{\partial x}\)&lt;/span>。同理，我们可以求出关于其他变量的偏导数&lt;span class="math">\(\frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}, \dotsb\)&lt;/span>&lt;/p>
&lt;p>偏导数只能反映出轴方向的变化性，因此偏导数存在可能说明函数延轴方向是连续的，对于其他方向爱莫能助，因此偏导数存在不能说明函数整体的连续性。&lt;/p>
&lt;p>下图展示了二元函数的偏导数。固定&lt;span class="math">\(y=y_0, x\)&lt;/span>方向的偏导数（红色为x轴）；固定&lt;span class="math">\(x=x_0, y\)&lt;/span>方向的偏导数（绿色为y轴）&lt;/p>
&lt;div class="figure">
&lt;embed src="../images/x轴偏导数.jfif" />&lt;p class="caption">x轴偏导数.jfif&lt;/p>
&lt;/div>
&lt;div class="figure">
&lt;embed src="../images/y轴偏导数.jfif" />&lt;p class="caption">y轴偏导数.jfif&lt;/p>
&lt;/div>
&lt;p>其次，偏导数只反应了函数沿各平行于坐标轴的方向的变化率，对于其他的方向还需要定义新的导数。设函数&lt;span class="math">\(f(\vec{x})\)&lt;/span>是一个&lt;span class="math">\(n\)&lt;/span>维自变量的函数，&lt;span class="math">\(\vec{x}=\{x_1,x_2,\dotsb,x_n\}\)&lt;/span>，若函数&lt;span class="math">\(f\)&lt;/span>在某点&lt;span class="math">\(\vec{x}\)&lt;/span>邻域内有定义，那么给定一个非零向量&lt;span class="math">\(\vec{v}\)&lt;/span>，函数沿向量&lt;span class="math">\(\vec{v}\)&lt;/span>的方向导数为： &lt;span class="math">\[
\nabla_{\vec{v}} f(\vec{x}) = \lim_{t\rightarrow 0}\frac{f(\vec{x}+t\vec
{v})-f(\vec{x})}{t}
\]&lt;/span> 也可记为&lt;span class="math">\(\frac{\mathrm{d}f_{\vec{v}}}{\mathrm{d}t}|_{t\rightarrow 0}\)&lt;/span>。有些书籍中会较为严格地定义方向导数为函数在某一点沿&lt;strong>单位长度向量&lt;/strong>的方向导数，在这样的上下文中，“函数在某点沿向量&lt;span class="math">\(\vec{v}\)&lt;/span>方向上的导数”指的是函数在这一点沿着&lt;span class="math">\(\vec{v}\)&lt;/span>对应的单位向量&lt;span class="math">\(\vec{v_{unit}}=\frac{\vec{v}}{||\vec{v}||}\)&lt;/span>的方向导数。不过，只要&lt;span class="math">\(t\rightarrow 0\)&lt;/span>，是否指定向量&lt;span class="math">\(\vec{v}\)&lt;/span>为单位向量并不重要。下图展示了一个二维的方向导数。&lt;/p>
&lt;div class="figure">
&lt;embed src="../images/方向导数.jfif" />&lt;p class="caption">方向导数.jfif&lt;/p>
&lt;/div>
&lt;p>方向导数可以看成是偏导数的推广，当方向导数的方向正好沿着坐标轴时，方向导数就退化成了偏导数。例如，方向导数为&lt;span class="math">\(\vec{v}=\{1,0,0,\dotsb\}\)&lt;/span>平行于&lt;span class="math">\(x_1\)&lt;/span>轴，那么 &lt;span class="math">\[
\begin{aligned}
\nabla_{\vec{v}} f(\vec{x}) &amp;amp;= \lim_{t\rightarrow 0}\frac{f(\vec{x}+t\vec
{v})-f(\vec{x})}{t}\\
&amp;amp;=\lim_{t\rightarrow 0}\frac{f(x_1+t,x_2,\dotsb,x_n)-f(x_1,x_2,\dotsb,x_n)}{t}
\end{aligned}
\]&lt;/span> 我们将&lt;span class="math">\(t\)&lt;/span>改写成&lt;span class="math">\(\Delta x_1\)&lt;/span>，则上式为： &lt;span class="math">\[
\lim_{\Delta x_1\rightarrow 0}\frac{f(x_1+\Delta x_1,x_2,\dotsb,x_n)-f(x_1,x_2,\dotsb,x_n)}{\Delta x_1}=\frac{\partial f(\vec
{x})}{\partial x_1}
\]&lt;/span> 此时，方向导数就是&lt;span class="math">\(x_1\)&lt;/span>分量的偏导数。&lt;/p>
&lt;p>我们假设点&lt;span class="math">\(\vec{x}\)&lt;/span>邻域内各方向的方向导数都存在，那么是否有一个方向导数的最大值呢？该最大值又是什么方向呢？这就引出了梯度的概念。&lt;strong>即梯度是多元函数在点&lt;span class="math">\(\vec{x}\)&lt;/span>处上升最快的方向&lt;/strong>。&lt;/p>
&lt;p>我们在这里直接给出梯度的表达式为： &lt;span class="math">\[
\nabla f = \frac{\partial f}{\partial x_1}\vec{a}_1+\frac{\partial f}{\partial x_2}\vec{a}_2+\dotsb+\frac{\partial f}{\partial x_n}\vec{a}_n
\]&lt;/span> 其中，&lt;span class="math">\(\frac{\partial f}{\partial x_i}\)&lt;/span>是&lt;span class="math">\(\vec{x}_i\)&lt;/span>分量的偏导数，&lt;span class="math">\(\vec{a}_i\)&lt;/span>是&lt;span class="math">\(\vec{x}_i\)&lt;/span>方向的单位向量。 至于证明读者可以从各种教材上寻找到。&lt;/p>
&lt;p>另外，还有一个叫做全导数的概念，不过它和偏导数、方向导数、梯度讨论的场景并不一样。全导数是在&lt;strong>多元复合函数&lt;/strong>场景下，所有自变量都受到&lt;strong>唯一&lt;/strong>参数&lt;span class="math">\(t\)&lt;/span>控制的，关于&lt;span class="math">\(t\)&lt;/span>的导数。即对于多元函数&lt;span class="math">\(f(\vec{x})\)&lt;/span>，向量&lt;span class="math">\(\vec{x}\)&lt;/span>的每一个分量&lt;span class="math">\(x_i\)&lt;/span>都是&lt;span class="math">\(t\)&lt;/span>的函数&lt;span class="math">\(x_i(t)\)&lt;/span>，那么全导数&lt;span class="math">\(\frac{\mathrm{d}f}{\mathrm{d}t}\)&lt;/span>为： &lt;span class="math">\[
\frac{\mathrm{d}f}{\mathrm{d}t}=\frac{\partial f}{\partial x_1}\frac{\partial x_1}{\partial t}+\frac{\partial f}{\partial x_2}\frac{\partial x_2}{\partial t}+\dotsb+\frac{\partial f}{\partial x_n}\frac{\partial x_n}{\partial t}
\]&lt;/span> 全导数&lt;strong>本质上是一个一元函数导数&lt;/strong>，只不过这个自变量&lt;span class="math">\(t\)&lt;/span>隐含在相关变量&lt;span class="math">\(x_i(t)\)&lt;/span>中，实际上等同于&lt;span class="math">\(f(t)\)&lt;/span>对于变量&lt;span class="math">\(t\)&lt;/span>的导数，反映了整体函数值关于&lt;span class="math">\(t\)&lt;/span>的变化率。&lt;/p>
&lt;h2 id="从增量到微分">从增量到微分&lt;/h2>
&lt;p>在说明微分之前，我们需要介绍一个更直观的概念：增量&lt;span class="math">\(\Delta\)&lt;/span>。我们在上文中已经使用过这个概念了，例如&lt;span class="math">\(\Delta x\)&lt;/span>，不过我们都只是在多元函数自变量的某个分量中使用增量，表示某一分量的变化值。&lt;/p>
&lt;p>增量即自变量或因变量的变化值（不一定非要是正值），在一元函数中，自变量&lt;span class="math">\(x\)&lt;/span>的增量就是沿着&lt;span class="math">\(x\)&lt;/span>轴左右增减的量，因此，&lt;span class="math">\(\Delta x\)&lt;/span>是线性变化的量。因变量&lt;span class="math">\(\Delta y\)&lt;/span>是两个函数值的差&lt;span class="math">\(\Delta y = f(x+\Delta x)-f(x)\)&lt;/span>，若函数&lt;span class="math">\(f\)&lt;/span>不是线性函数，增量通常也不是线性的。&lt;/p>
&lt;p>在实际使用中，&lt;span class="math">\(\Delta y\)&lt;/span>计算会涉及函数&lt;span class="math">\(f\)&lt;/span>，因此十分不可控，比如当&lt;span class="math">\(f\)&lt;/span>非常复杂时，&lt;span class="math">\(\Delta y\)&lt;/span>将难以计算，同时&lt;span class="math">\(\Delta y\)&lt;/span>没有特定的数学性质，会随着映射&lt;span class="math">\(f\)&lt;/span>的改变而改变。因此，在微积分中，借助其核心思想之一：“&lt;strong>以直代曲&lt;/strong>”，对增量&lt;span class="math">\(\Delta y\)&lt;/span>进行简化，诞生了微分&lt;span class="math">\(\mathrm{d}y\)&lt;/span>。&lt;/p>
&lt;p>对于曲线&lt;span class="math">\(y\)&lt;/span>的研究通常不是平凡的，微积分的一大创举就是证明了以直代曲的广泛可行性。如果一条曲线不好研究，那么我们就将其分割成一个个小段，每一小段上用一条直线段取代，当分割的间隔非常非常小时，就可以用这一系列小折线段组取代原来的曲线。如下图所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/以直代曲.png" alt="以直代曲.png" />&lt;p class="caption">以直代曲.png&lt;/p>
&lt;/div>
&lt;p>那么如果确定每个小区间里的直线方程呢？一个广泛的做法是统一小区间内某一点，然后做过该点的切线作为代替曲线的直线，比如统一取区间左端点做切线（或中点、右端点都行，当区间足够小时，别没有差别。如果严格地来说，采用闭区间套定理不断缩小区间，最终每个区间只会剩下一个点^_^，所以在无穷小的情况下也不会考虑用割线，只有切线。）而选中点的切线的斜率就是我们之前所说的该点的导数，因此，已知直线斜率和过直线一点&lt;span class="math">\((x_0,f(x_0))\)&lt;/span>，可以写出直线的方程&lt;span class="math">\(g(x)\)&lt;/span>： &lt;span class="math">\[
g(x)=f&amp;#39;(x_0)(x-x_0)+f(x_0)
\]&lt;/span> 我们令&lt;span class="math">\(\mathrm{d}y = g(x)-f(x_0)\)&lt;/span>，由于函数&lt;span class="math">\(g(x)\)&lt;/span>是一个线性函数，因此&lt;span class="math">\(\mathrm{d}y\)&lt;/span>随着变化量&lt;span class="math">\(\Delta x\)&lt;/span>线性变化，至于&lt;span class="math">\(\Delta x = x-x_0\)&lt;/span>，前面已经说过本来就是沿着&lt;span class="math">\(x\)&lt;/span>轴的线性变化，所以有&lt;span class="math">\(\Delta x =\mathrm{d}x\)&lt;/span>。因此，上式关系可改写为： &lt;span class="math">\[
\mathrm{d}y = f&amp;#39;(x_0)\mathrm{d}x
\]&lt;/span> 即为函数&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>处相应于自变量&lt;span class="math">\(\Delta x\)&lt;/span>的微分。注意，&lt;span class="math">\(\mathrm{d}x=\Delta x\)&lt;/span>。显然，&lt;span class="math">\(\mathrm{d}y\)&lt;/span>是在&lt;span class="math">\(x_0\)&lt;/span>邻域内，随自变量&lt;span class="math">\(\mathrm{d}x\)&lt;/span>，以&lt;span class="math">\(f&amp;#39;(x_0)\)&lt;/span>为斜率的线性变化量。&lt;/p>
&lt;p>那么，&lt;span class="math">\(\mathrm{d}y\)&lt;/span>和&lt;span class="math">\(\Delta y\)&lt;/span>有什么关系呢？我们还是需要导数这个中间桥梁。&lt;/p>
&lt;ul>
&lt;li>导数：&lt;span class="math">\(f&amp;#39;(x_0)=\lim\limits_{x\rightarrow x_0} \frac{f(x)-f(x_0)}{x-x_0}= \lim\limits_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x}\)&lt;/span>&lt;/li>
&lt;li>微分：&lt;span class="math">\(f&amp;#39;(x_0)=\frac{\mathrm{d}y}{\mathrm{d}x}\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>上面二者结果都是&lt;span class="math">\(x_0\)&lt;/span>处的导数值！即&lt;span class="math">\(\frac{\mathrm{d}y}{\mathrm{d}x}=\lim\limits_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x}\)&lt;/span>。后面的极限在&lt;span class="math">\(\Delta x\rightarrow 0\)&lt;/span>时只剩下了一项，说明&lt;span class="math">\(\Delta y\)&lt;/span>包含的其他项至少是&lt;span class="math">\(\Delta x\)&lt;/span>的高阶无穷小（如果&lt;span class="math">\(f(x)\)&lt;/span>是线性函数，剩余项就是0），所在才会在&lt;span class="math">\(\Delta x\)&lt;/span>趋于0的过程中消去，即&lt;span class="math">\(\Delta y =f&amp;#39;(x_0)\Delta x+o(\Delta x), o(\Delta x)\)&lt;/span>表示&lt;span class="math">\(\Delta x\)&lt;/span>的高阶无穷小。我们已知&lt;span class="math">\(\Delta x=\mathrm{d}x\)&lt;/span>，那么&lt;span class="math">\(\Delta y =f&amp;#39;(x_0)\mathrm{d}x+o(\Delta x)\Rightarrow \Delta y = \mathrm{d}y + o(\Delta x)\)&lt;/span>。其含义是&lt;span class="math">\(\Delta y\)&lt;/span>增量是由线性增长部分&lt;span class="math">\(\mathrm{d}y\)&lt;/span>和高阶增长量&lt;span class="math">\(o(\Delta x)\)&lt;/span>组成，当&lt;span class="math">\(\Delta x\)&lt;/span>很小很小，以0为极限时有：&lt;span class="math">\(\lim\limits_{\Delta x\rightarrow 0} \Delta y/\Delta x = \mathrm{d}y/\mathrm{d}x\)&lt;/span>。（上面的过程也暗含了一元函数可微与可导的等价性）&lt;/p>
&lt;p>他们之间的关系如下图所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/增量与微分.png" alt="增量与微分.png" />&lt;p class="caption">增量与微分.png&lt;/p>
&lt;/div>
&lt;h2 id="从多元函数导数到微分">从多元函数导数到微分&lt;/h2>
&lt;p>在一元函数中，可导和可微是互为充要条件。但是由于在多元函数中自变量维度的提升，可导变成了偏导，被限定了方向，甚至连多元函数的连续性都无法确定，而&lt;strong>微分在向多元函数延时拓没有受到方向限制&lt;/strong>，这种充要关系也被破坏了。&lt;/p>
&lt;p>&lt;strong>具体谈到多元函数微分的话，通常指的是全微分&lt;/strong>。不过微分中是存在偏微分的，而偏微分确实也和偏导数是充要关系。例如： &lt;span class="math">\[
f(x+\Delta x, y)-f(x,y)\approx \frac{\partial f}{\partial x}\Delta x\\
f(x, y+\Delta y)-f(x,y)\approx \frac{\partial f}{\partial y}\Delta y
\]&lt;/span> 当我们只考虑偏微分时，和一元函数是类似的，然而偏微分单独使用场景较少，因此谈到微分大家&lt;strong>默认都是说全微分&lt;/strong>。&lt;/p>
&lt;p>多元函数全微分的引入和一元函数类似相同，都是通过增量概念，多元函数的总体增量称为&lt;strong>全增量&lt;/strong>，记作&lt;span class="math">\(\Delta f\)&lt;/span>: &lt;span class="math">\[
\Delta f(\vec{x})=f(\vec{x}+\Delta \vec{x})-f(\vec{x})
\]&lt;/span> 如果增量&lt;span class="math">\(\Delta f\)&lt;/span>可以表示成&lt;strong>每个分量的增量的线性组合&lt;/strong>加上一个增量距离的高阶无穷小，那么则称函数&lt;span class="math">\(f\)&lt;/span>在点&lt;span class="math">\(\vec{x}\)&lt;/span>处可微： &lt;span class="math">\[
\Delta f(\vec{x}) = \vec{A}^T\Delta\vec{x}+o(\rho)
\]&lt;/span> 其中，&lt;span class="math">\(\vec{A}, \Delta\vec{x}\)&lt;/span>是n维列向量，&lt;span class="math">\(\rho=||\Delta\vec{x}||_2\)&lt;/span>。同样的，在每个分量的增量都是沿坐标轴线性增长的，所以&lt;span class="math">\(\Delta\vec{x}=\mathrm{d}\vec{x}\)&lt;/span>。当&lt;span class="math">\(\Delta\vec{x}\)&lt;/span>很小很小时，&lt;span class="math">\(o(\rho)\)&lt;/span>忽略，剩余增量&lt;span class="math">\(\Delta f\)&lt;/span>记为&lt;span class="math">\(\mathrm{d}f\)&lt;/span>，称为&lt;span class="math">\(f\)&lt;/span>在点&lt;span class="math">\(\vec{x}\)&lt;/span>处的全微分： &lt;span class="math">\[
\mathrm{d}f=\vec{A}^T\mathrm{d}\vec{x}=a_1x_1+\dotsb+a_nx_n
\]&lt;/span> 由上可知，多元函数微分也是“以直代曲”思想的实践，只不过从用直线代替曲线变成了（高维）平面代替（高维）曲面。那每一个系数&lt;span class="math">\(a_i\)&lt;/span>到底等于多少呢？这就是从一般到特殊。&lt;/p>
&lt;p>我们知道全微分是对点邻域任意方向都成立的，因此我们取平行于&lt;span class="math">\(x_i\)&lt;/span>的方向趋近，此时&lt;span class="math">\(\Delta\vec{x}=\{0,\dotsb,\Delta x_i,\dotsb,0\}\)&lt;/span>，全增量在该方向的表达式为： &lt;span class="math">\[
f(\vec{x}+\Delta \vec{x})-f(\vec{x})=a_i\Delta x_i\\
\Rightarrow \lim_{\Delta x_i\rightarrow 0} a_i=\frac{f(\vec{x}+\Delta \vec{x})-f(\vec{x})}{\Delta x_i}
\]&lt;/span> 熟悉的读者立刻就能发现，上式正式多元函数在&lt;span class="math">\(x_i\)&lt;/span>分量上的偏导数！即&lt;span class="math">\(a_i=\frac{\partial f}{\partial x_i}\)&lt;/span>。因此，全微分&lt;span class="math">\(\mathrm{d}f\)&lt;/span>等于： &lt;span class="math">\[
\mathrm{d}f=\sum_{i=1}^n\frac{\partial f}{\partial x_i}\mathrm{d}x_i
\]&lt;/span> 从上面的过程我们也可以得出：&lt;strong>当全微分存在时，其平行于各分量的偏导数也存在&lt;/strong>（可微&lt;span class="math">\(\Rightarrow\)&lt;/span>有偏导）。&lt;/p>
&lt;p>反过来，如果有偏导，那么只能保证在轴方向的偏微分存在，全微分不一定存在。典型例子如下： &lt;span class="math">\[
f(x,y)=\begin{cases}
\frac{xy}{\sqrt{x^2+y^2}},(x,y)\neq (0,0)\\
0,(x,y)=(0,0).
\end{cases}
\]&lt;/span> 这个函数在&lt;span class="math">\((0,0)\)&lt;/span>处连续且偏导存在，但是不可微。 其在点&lt;span class="math">\((0,0)\)&lt;/span>全增量为: &lt;span class="math">\[
\Delta f=f(0+\Delta x,0+\Delta y)-f(0,0)=\frac{\Delta x\Delta y}{\sqrt{\Delta x^2+\Delta y^2}}
\]&lt;/span> 当我们选用从x轴或y轴方向趋近于&lt;span class="math">\((0,0)\)&lt;/span>，显然有： &lt;span class="math">\[
\text{方向}_{\Delta x\rightarrow 0,\Delta y=0}: \Delta f = \mathrm{d}f = 0 \mathrm{d}x+0 \mathrm{d}y + 0= 0\\
\text{方向}_{\Delta y\rightarrow 0,\Delta x=0}: \Delta f = \mathrm{d}f = 0 \mathrm{d}x+0 \mathrm{d}y +0 = 0
\]&lt;/span> 注意，上式表示时我们只谈从&lt;span class="math">\({\Delta x\rightarrow 0,\Delta y=0}\)&lt;/span>或&lt;span class="math">\({\Delta y\rightarrow 0,\Delta x=0}\)&lt;/span>的方向，而非取极限&lt;/p>
&lt;p>但是我们选用从方向&lt;span class="math">\((1,1)\)&lt;/span>，即&lt;span class="math">\(\Delta x = \Delta y\)&lt;/span>趋近于&lt;span class="math">\((0,0)\)&lt;/span>时： &lt;span class="math">\[
\text{方向}_{\Delta x\rightarrow 0,\Delta y=\Delta x}: \Delta f = \mathrm{d}f = \frac{\Delta x^2}{\sqrt{\Delta x^2+\Delta x^2}}=\frac{\sqrt{2}}{2}\mathrm{d} x+ 0\mathrm{d} y + 0
\]&lt;/span> 显然点&lt;span class="math">\((0,0)\)&lt;/span>处从&lt;span class="math">\((1,1)\)&lt;/span>方向的微分和&lt;span class="math">\(x,y\)&lt;/span>轴方向的微分不相等，因此全微分并不存在。需要指出的是上面三个式子中&lt;span class="math">\(o(\rho)=0\)&lt;/span>，因此我们可以直接写&lt;span class="math">\(\Delta f = \mathrm{d}f\)&lt;/span>。&lt;/p>
&lt;p>从上面可以看出可微的要求还是挺高的，需要从各个方向考虑。那么有没有一个简单的方法判断可微呢？有的，那就是一阶偏导数连续。&lt;/p>
&lt;h3 id="偏导与可微">偏导与可微&lt;/h3>
&lt;p>其实偏导数连续是一个非常强的条件，一般函数都不会有这么好的性质。甚至偏导数连续是可微的充分条件，而非充要条件。可微并不能推导出偏导数连续！&lt;/p></description></item><item><title>数学分析之无穷级数</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E6%97%A0%E7%A9%B7%E7%BA%A7%E6%95%B0/</link><pubDate>Fri, 06 Jan 2023 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E6%97%A0%E7%A9%B7%E7%BA%A7%E6%95%B0/</guid><description>
&lt;h2 id="无穷级数">无穷级数&lt;!-- omit in toc -->&lt;/h2>
&lt;p>级数理论是数学分析的一个分支，它与另一个分支微积分学一起作为基础知识和工具出现在其余数学分支中。实际上，不同于现在数学学习中对微积分的过度侧重，级数的地位和微积分相当，二者共同以极限为基本工具，分别从离散与连续两个方面，结合起来研究分析学的对象，即变量之间的依赖关系──函数。&lt;/p>
&lt;p>无穷级数思想——逼近——微积分 数学分析词源 分析学&lt;/p>
&lt;p>欧拉、高斯都是使用级数的高手 计算使用级数&lt;/p>
&lt;p>无穷级数与差分&lt;/p>
&lt;p>&lt;span class="math">\[
\bvec
\]&lt;/span>&lt;/p></description></item><item><title>概率统计随机过程之相关分析与因果推断</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/</link><pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/</guid><description>
&lt;h2 id="概率统计随机过程之相关分析与因果推断">概率统计随机过程之相关分析与因果推断&lt;!-- omit in toc -->&lt;/h2>
&lt;p>因果关系是人类不断探寻的深刻议题，不过想要探究因果联系并不是那么容易的，因此很多学者都会退一步从更弱的关联性分析入手，尤其在大数据时代，关联性的作用也不容小觑。因此，统计学上的关系分析是非常重要的一个环节。本文主要讲解分类数据和数值数据的列联分析和方差分析等内容。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#因果分析的复杂性">因果分析的复杂性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分类数据的chi2拟合优度检验">分类数据的&lt;span class="math">\(\\chi^2\)&lt;/span>拟合优度检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#列联分析">列联分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析analysis-of-variance-anova">方差分析（Analysis of variance, ANOVA）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单因素方差分析">单因素方差分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#双因素方差分析">双因素方差分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态性检验">正态性检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#w检验">W检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差齐性检验">方差齐性检验&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="因果分析的复杂性">因果分析的复杂性&lt;/h2>
&lt;p>统计学上如果想要进行因果分析，通常会有如下图的阶段：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/统计学探索变量间关系.png" alt="统计学探索变量间关系" />&lt;p class="caption">统计学探索变量间关系&lt;/p>
&lt;/div>
&lt;p>首先，查看变量间是否具有关联性，没有关联性的就是相互独立的变量，一个变量的变化并不会对另一个变量产生影响；当发现两个变量具备关联性时，我们还得查看关联性的强弱，是强相关还是弱相关；在之后，需要检查这种相关性是不是有什么其他隐含的因素，比如二者都是同一个原因的结果，二者本身不具备因果性；最后才能进一步确认因果性。在确认因果性时，一般通过以下模型实现：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/因果关系理论抽象.drawio.svg" alt="因果关系理论抽象.drawio.svg" />&lt;p class="caption">因果关系理论抽象.drawio.svg&lt;/p>
&lt;/div>
&lt;p>在复杂的统计模型中，其中上边的每一步也需要仔细、系统的研究。当然上述只是因果分析的简要流程。&lt;/p>
&lt;p>在因果分析中，对于自变量、因变量不同的类型，也有着不同的分析方法，对于本科生水平大概如下：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/统计分析一般方法.png" alt="统计分析一般方法" />&lt;p class="caption">统计分析一般方法&lt;/p>
&lt;/div>
&lt;p>其中，数据类型一般可以分成定性的分类数据（品质数据）、顺序数据和定量的数值数据，数值数据还可分为离散数据和连续数据。这些数据的级别是由低到高的，高阶数据可以转换为低阶数据，例如连续数据可以归并成离散数据，数值数据可以按照大小排成顺序数据，顺序数据也可以分成几类形成分类数据。但是低阶数据无法转换成高阶数据。&lt;/p>
&lt;p>对于硕士研究生可能需要掌握到下面的成程度：&lt;/p>
&lt;p>分类数据统计分析方法：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/分类数据统计分析方法.png" alt="分类数据统计分析方法" />&lt;p class="caption">分类数据统计分析方法&lt;/p>
&lt;/div>
&lt;p>数值数据统计分析方法：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/数值数据统计分析方法.png" alt="数值数据统计分析方法" />&lt;p class="caption">数值数据统计分析方法&lt;/p>
&lt;/div>
&lt;p>对于博士生大概是这样的：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/组间差异检验.png" alt="组间差异检验.png" />&lt;p class="caption">组间差异检验.png&lt;/p>
&lt;/div>
&lt;p>接下来我将有选择的挑几个阐明。首先，先从本科阶段的内容说起吧。&amp;lt;( ￣^￣)&amp;gt;&lt;/p>
&lt;h2 id="分类数据的chi2拟合优度检验">分类数据的&lt;span class="math">\(\chi^2\)&lt;/span>拟合优度检验&lt;/h2>
&lt;p>对于离散分布，比如对于二项分布&lt;span class="math">\(B(n,p)\)&lt;/span>，我们希望验证其服从二项分布，进行了&lt;span class="math">\(M\)&lt;/span>次&lt;span class="math">\(n\)&lt;/span>重伯努利实验得到&lt;span class="math">\(M\)&lt;/span>个值，其中实验成功次数为&lt;span class="math">\(0,1,2,3,\dotsb,n\)&lt;/span>的频数分别为&lt;span class="math">\(m_0,m_1,m_2,\dotsb,m_n,M=\sum\limits_{i=0}^n m_i\)&lt;/span>。那么如果想要验证该n重伯努利分布得到的随机变量&lt;span class="math">\(X\)&lt;/span>是服从二项分布&lt;span class="math">\(B(n,p)\)&lt;/span>的，需要怎么做呢？一个简单可用的方法是&lt;span class="math">\(\chi^2\)&lt;/span>拟合优度检验。&lt;/p>
&lt;p>分布的拟合检验是在随机变量&lt;span class="math">\(X\)&lt;/span>分布未知时的检验（因为我们要验证的即是其分布类型），因此不同于参数的假设检验问题，属于非参数检验。一般而言分类数据的结果是频数，&lt;span class="math">\(\chi^2\)&lt;/span>检验是对分类数据的频数进行分析的统计方法。&lt;/p>
&lt;p>之所以叫&lt;span class="math">\(\chi^2\)&lt;/span>拟合优度检验，是因为在1900年，统计学四大天王之一卡尔-皮尔逊证明提出对于实验统计出来的频数&lt;span class="math">\(f_i,i\in \mathrel{\Theta}\)&lt;/span>，它和理论期望的频数&lt;span class="math">\(e_i=M×p_i\)&lt;/span>，（&lt;span class="math">\(M\)&lt;/span>为总数，&lt;span class="math">\(p_i\)&lt;/span>为对应概率）存在以下关系： &lt;span class="math">\[
X^2=\sum_{i\in\mathrel{\Theta}} \frac{(f_i-e_i)^2}{e_i}\sim \chi^2(|\mathrel{\Theta}|-1)\tag{1}
\]&lt;/span> 即构造的统计量&lt;span class="math">\(X^2=\sum\limits_{i\in\mathrel{\Theta}} \frac{(f_i-e_i)^2}{e_i}\)&lt;/span>应该服从自由度为&lt;span class="math">\(|\mathrel{\Theta}|-1\)&lt;/span>的卡方分布，&lt;span class="math">\(|\mathrel{\Theta}|\)&lt;/span>为实验可能出现结果的样数。并且期望频数越大，该分布与卡方分布越接近。当期望频数大于5时，与卡方分布符合比较好。此外，卡方分布只适用于观测数均不小于5的大样本场合。&lt;/p>
&lt;p>我们从数学角度单纯地看式（1）,&lt;span class="math">\((f_i-e_i)^2\)&lt;/span>实际是指实验做出的实际结果与理论分布差值的平方，体现的是实际与理论的差异，这个值越大，说明二者越不相符，分母的&lt;span class="math">\(e_i\)&lt;/span>更像是正则系数，降低绝对差值的比例影响。所以，从式（1）也可以看出随机变量&lt;span class="math">\(X^2\)&lt;/span>越大，我们越倾向于不认同实验分布服从理论分布，从数学角度讲是拒绝域在右侧的&lt;span class="math">\(\chi^2\)&lt;/span>检验。这种方式最早由卡尔-皮尔逊提出，通常也称之为皮尔逊&lt;span class="math">\(\chi^2\)&lt;/span>拟合优度检验。&lt;/p>
&lt;p>下面我们用泰坦尼克号的存活率与性别是否相关举一个简单的例子：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/卡方优度检验.png" alt="卡方优度检验" />&lt;p class="caption">卡方优度检验&lt;/p>
&lt;/div>
&lt;p>拟合优度检验只针对一个分类变量进行检验，如果需要对两个或多个分类变量进行出里就需要列联分析。&lt;/p>
&lt;h2 id="列联分析">列联分析&lt;/h2>
&lt;p>如果我们希望&lt;strong>分析两个或多个分类变量之间的是否独立，可以使用列联表&lt;/strong>。列联分析是一种&lt;strong>独立性检验&lt;/strong>，通常列联表常用于分类数据的两两分析，多维数据的多维列联表不太直观，一般用的较少。&lt;/p>
&lt;p>原理也是使用卡方统计量。&lt;/p>
&lt;p>列联表中，若两个分类变量&lt;span class="math">\(A,B\)&lt;/span>，其中&lt;span class="math">\(A\)&lt;/span>有&lt;span class="math">\(r\)&lt;/span>个可取值，记为&lt;span class="math">\(A_1,\dotsb,A_r\)&lt;/span>，&lt;span class="math">\(B\)&lt;/span>有&lt;span class="math">\(c\)&lt;/span>个可取值，记为&lt;span class="math">\(B_1,\dotsb,B_c\)&lt;/span>，从总体中抽取样本容量为&lt;span class="math">\(n\)&lt;/span>的样本，设其中有&lt;span class="math">\(n_{ij}\)&lt;/span>个个体，其属性为&lt;span class="math">\(A_i,B_j\)&lt;/span>，称其为频数，我们根据上述信息可制作频数列联表：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;span class="math">\(A\setminus B\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_1\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_j\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_c\)&lt;/span>&lt;/th>
&lt;th align="center">行和&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{11}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{1j}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{1c}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{1\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_i\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{i1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{ij}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{ic}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{i\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_r\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{r1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{rj}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{rc}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{r\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">列和&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{\cdot 1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{\cdot j}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n_{\cdot c}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(n\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>以上列联表是会根据实验或数据集给出的数据制作而成，都是已知的数据。我们将上述表中数据都除以总数&lt;span class="math">\(n\)&lt;/span>得到频率/概率表：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;span class="math">\(A\setminus B\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_1\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_j\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(B_c\)&lt;/span>&lt;/th>
&lt;th align="center">行和&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{11}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{1j}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{1c}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{1\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_i\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{i1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{ij}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{ic}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{i\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(A_r\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{r1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{rj}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{rc}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{r\cdot}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">列和&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{\cdot 1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{\cdot j}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p_{\cdot c}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(1\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>根据频率/概率关系有&lt;span class="math">\(\sum_i\sum_j p_{ij}=1, \sum_j p_{\cdot j}=1, \sum_i p_{i\cdot}=1\)&lt;/span>。这个表格的目的就是计算&lt;span class="math">\(p_{i\cdot}\)&lt;/span>和&lt;span class="math">\(p_{\cdot j}\)&lt;/span>。如果变量&lt;span class="math">\(A,B\)&lt;/span>是独立的，那么会有&lt;span class="math">\(p_{ij}=p_{i\cdot}p_{\cdot j}\)&lt;/span>，但是实际统计频率&lt;span class="math">\(p_{ij}\)&lt;/span>必然和理论值有所偏差，我们用计算得到的&lt;span class="math">\(p_{i\cdot}\)&lt;/span>和&lt;span class="math">\(p_{\cdot j}\)&lt;/span>相乘，得到独立假设下的理论概率&lt;span class="math">\(\hat{p}_{ij}=p_{i\cdot}×p_{\cdot j}\)&lt;/span>，再乘以总数&lt;span class="math">\(n\)&lt;/span>得到期望频数&lt;span class="math">\(n\hat{p}_{ij}\)&lt;/span>，那么这就可以看成有&lt;span class="math">\(r×c\)&lt;/span>个可选值的卡方拟合优度检验，其自由度为&lt;span class="math">\((r-1)×(c-1)\)&lt;/span>。据此，检验统计量为： &lt;span class="math">\[
X^2=\sum_{i=1}^r\sum_{j=1}^c \frac{(n_{ij}-n\hat{p}_{ij})^2}{n\hat{p}_{ij}}\tag{2}
\]&lt;/span> 其中，&lt;span class="math">\(\hat{p}_{ij}=p_{i\cdot}×p_{\cdot j}=\frac{n_{i\cdot}}{n}×\frac{n_{\cdot j}}{n}\)&lt;/span>，同样的&lt;span class="math">\(n_{ij}\)&lt;/span>和&lt;span class="math">\(n\hat{p}_{ij}\)&lt;/span>差别越大，统计量值越大，概率分布服从性也越差。又因为理论概率为独立假设下的概率分布，概率服从性差意味着两个分类变量独立性也越差。&lt;/p>
&lt;p>下面举一个例子：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/列联表分析.png" alt="列联表分析.png" />&lt;p class="caption">列联表分析.png&lt;/p>
&lt;/div>
&lt;p>此外，对于数值型数据，我们也可以通过将其分割归类成几段，降维成分类数据，从而使用列联表分析。不过，我们还有一种对其有更佳的处理方式，即方差分析。&lt;/p>
&lt;h2 id="方差分析analysis-of-variance-anova">方差分析（Analysis of variance, ANOVA）&lt;/h2>
&lt;p>&lt;strong>当自变量是分类变量，因变量是数值变量时的相关性分析，可以使用方差分析&lt;/strong>。比如探究不同教学方式是否对成绩有影响、不同专业毕业之后的薪资是否有区别等等。区别于列联分析，方差分析的因变量都是数值。&lt;/p>
&lt;p>如果只是想知道是否数值型因变量是否受到分类型自变量影响，那么使用假设检验也是可以的。但是需要研究的目标变多时，例如设4个总体的均值分别为&lt;span class="math">\(\mu_1,\mu_2,\mu_3,\mu_4\)&lt;/span>，如果用一般假设检验方法，如t检验，一次只能研究两个样本，要检验4个均值是否相等，就需要检验6次：&lt;/p>
&lt;ul>
&lt;li>检验1：&lt;span class="math">\(H_0:\mu_1=\mu_2\)&lt;/span>；&lt;/li>
&lt;li>检验2：&lt;span class="math">\(H_0:\mu_1=\mu_3\)&lt;/span>；&lt;/li>
&lt;li>检验3：&lt;span class="math">\(H_0:\mu_1=\mu_4\)&lt;/span>；&lt;/li>
&lt;li>检验4：&lt;span class="math">\(H_0:\mu_2=\mu_3\)&lt;/span>；&lt;/li>
&lt;li>检验5：&lt;span class="math">\(H_0:\mu_2=\mu_4\)&lt;/span>；&lt;/li>
&lt;li>检验6：&lt;span class="math">\(H_0:\mu_3=\mu_4\)&lt;/span>；&lt;/li>
&lt;/ul>
&lt;p>很显然，这样做十分的繁琐，并且多次检验会导致出错概率增加，如果设拒绝域&lt;span class="math">\(\alpha=0.05\)&lt;/span>，即每次检验犯第一类错误的概率为0.05，做6次检验会使犯第一类（至少一次）的概率变成&lt;span class="math">\(1-(1-\alpha)^6\approx 0.265\)&lt;/span>，相应置信水平会降低到0.735。因此使用方差分析一是可以提升检验的效率，二是可以增加分析的可靠性，避免多次检验造成的误差累积。&lt;/p>
&lt;blockquote>
&lt;p>方差分析：在共同的显著性水平&lt;span class="math">\(\alpha\)&lt;/span>下，同时考虑多个平均值的差异。通常以F分布来进行检验，称为方差分析。&lt;/p>
&lt;/blockquote>
&lt;p>方差分析由统计学四天王之一的Fisher于1923年提出。我们在进行方差分析之前还要注意其需要满足以下三个条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>正太总体。每个组的总体应服从正态分布，对于因素的每一个水平，其观测值是来自正太分布总体的简单随机样本。&lt;/li>
&lt;li>方差齐性。各个总体的方差&lt;span class="math">\(\sigma^2\)&lt;/span>必须相同。&lt;/li>
&lt;li>独立性。每个观测值必须是独立的。&lt;/li>
&lt;/ol>
&lt;p>在上述假设成立的前提下，&lt;strong>要分析自变量对因变量是否有影响，在形式上也就转化成为检验自变量的各个水平（总体）的均值是否相等&lt;/strong>。而以上三种假设都有对应的检验方法。如正态性检验、方差齐性检验以及独立性检验。&lt;/p>
&lt;p>方差分析的原理是&lt;strong>对数据误差的来源进行分类分析&lt;/strong>。对于同一因素的不同处理水平，产生的结果可能有不同。根据误差来源，结果的不同可能确实是不同处理水平导致的，也有可能仅仅是因为随机误差。但是这种不同到底是不是不同因素水平导致的呢？方差分析就是通过将&lt;strong>误差分解成组间误差和组内误差，用二者的比值的偏离程度&lt;/strong>来进行分析。而数据中，误差的体现可以由方差透露，因此对误差的分析就能够变成对方差的分析。&lt;/p>
&lt;ul>
&lt;li>组内误差：在因素&lt;strong>同一&lt;/strong>水平处理下，数据的差异，这种差异只可能是随机性导致的。&lt;/li>
&lt;li>组间误差：在因素&lt;strong>不同&lt;/strong>水平处理下，数据的差异，这种差异虽然也包含随机性，但也可能是不同处理水平造成的系统性差异。&lt;/li>
&lt;/ul>
&lt;h3 id="单因素方差分析">单因素方差分析&lt;/h3>
&lt;p>如果只考虑一个因素（自变量）对结果（因变量）的影响，那么只需要单因素方差分析。我们将数据集用&lt;span class="math">\(X\)&lt;/span>表示，每一个样品&lt;span class="math">\(x_{ij}\)&lt;/span>表示在自变量第&lt;span class="math">\(i,i\in\{1,2,\dotsb,r\}\)&lt;/span>个处理水平下，获取到的第&lt;span class="math">\(j,j\in\{1,2,\dotsb,m\}\)&lt;/span>个结果。那么可以用&lt;span class="math">\(\overline{\overline{X}}\)&lt;/span>表示数据集整体的均值，&lt;span class="math">\(\overline{X}_i\)&lt;/span>表示第&lt;span class="math">\(i\)&lt;/span>个因素水平的组内均值。&lt;span class="math">\(x_{ij}\)&lt;/span>还可进行如下分解： &lt;span class="math">\[
\begin{aligned}
x_{ij}-\overline{\overline{X}}&amp;amp;=(x_{ij}-\overline{X}_i)+(\overline{X}_i-\overline{\overline{X}})\\
x_{ij}&amp;amp;=\overline{\overline{X}}+\underbrace{(x_{ij}-\overline{X}_i)}_{\text{组内误差}}+\underbrace{(\overline{X}_i-\overline{\overline{X}})}_{组间误差}
\end{aligned}\tag{3}
\]&lt;/span> 为了进一步分析误差来源，我们假设&lt;span class="math">\(x_{ij}\)&lt;/span>所在的处理组其服从分布为&lt;span class="math">\(N(\mu_i,\sigma^2)\)&lt;/span>的正态分布（正态总体假设，且由于方差齐性，各个处理组方差都是&lt;span class="math">\(\sigma^2\)&lt;/span>），那么&lt;span class="math">\(x_{ij}\)&lt;/span>也可以表示成&lt;span class="math">\(x_{ij}=\mu_i+\varepsilon_{ij}\)&lt;/span>，其中&lt;span class="math">\(\varepsilon_{ij}\)&lt;/span>是数据&lt;span class="math">\(x_{ij}\)&lt;/span>与组内真实均值&lt;span class="math">\(\mu_i\)&lt;/span>的离差，该离差的来源就是随机性。&lt;/p>
&lt;p>而根据中心极限定理，组内数据平均值&lt;span class="math">\(\overline{X}_i\)&lt;/span>应服从&lt;span class="math">\(N(\mu_i,\frac{\sigma^2}{m})\)&lt;/span>的正态分布，&lt;span class="math">\(m\)&lt;/span>为组内数据数量，那么&lt;span class="math">\(\overline{X}_i\)&lt;/span>可以表示成&lt;span class="math">\(\overline{X}_i=\mu_i+\varepsilon_i\)&lt;/span>，其中&lt;span class="math">\(\varepsilon_i\)&lt;/span>是数据的组内平均值&lt;span class="math">\(\overline{X}_i\)&lt;/span>与组内真实均值&lt;span class="math">\(\mu_i\)&lt;/span>的离差，不难证明&lt;span class="math">\(\varepsilon_i=\frac{1}{m}\sum_{j=1}^m \varepsilon_{ij}\)&lt;/span>，即组内均值的离差等于组内数据离差的均值。式（3）中的组内误差等价于： &lt;span class="math">\[
x_{ij}-\overline{X}_i=(\mu_i+\varepsilon_{ij})-(\mu_i+\varepsilon_i)=\varepsilon_{ij}-\varepsilon_i\tag{4}
\]&lt;/span> 所以&lt;strong>组内误差来源是随机性&lt;/strong>。&lt;/p>
&lt;p>同理，若我们将总体平均表示成&lt;span class="math">\(\overline{\overline{X}}=\mu+\varepsilon\)&lt;/span>的形式，其中&lt;span class="math">\(\mu\)&lt;/span>是所有分布真实的均值（各组都服从正态分布，和也服从正态分布），&lt;span class="math">\(\varepsilon\)&lt;/span>是数据均值与真正均值的离差，式(3)中的组间误差等价于 &lt;span class="math">\[
\overline{X}_i-\overline{\overline{X}}=(\mu_i+\varepsilon_i)-(\mu+\varepsilon)\tag{5}
\]&lt;/span> 如果组间没有系统性差异，那么组内真实均值&lt;span class="math">\(\mu_i\)&lt;/span>应该和总体均值&lt;span class="math">\(\mu\)&lt;/span>相同&lt;span class="math">\(\mu_i=\mu\)&lt;/span>，此时造成差异的只有离差&lt;span class="math">\(\varepsilon_i-\varepsilon\)&lt;/span>，就只有随机性造成的偏差。但当不同处理水平确实有影响时，那么某些组的均值就不会等同于整体均值&lt;span class="math">\(\mu_i\neq \mu\)&lt;/span>，此时式（5）中就会存在&lt;strong>系统误差&lt;/strong>项&lt;span class="math">\(\mu_i-\mu\)&lt;/span>和&lt;strong>随机误差&lt;/strong>项&lt;span class="math">\(\varepsilon_i-\varepsilon\)&lt;/span>。系统误差越大，组间误差就会越大。&lt;/p>
&lt;p>为了去除误差正负号的影响以及方便计算，我们将总误差进行平方求和&lt;span class="math">\(S_T=\sum_{i=1}^r\sum_{j=1}^m(x_{ij}-\overline{\overline{X}})^2\)&lt;/span>，结合式（3）有： &lt;span class="math">\[
\begin{aligned}
S_T&amp;amp;=\sum_{i=1}^r\sum_{j=1}^m(x_{ij}-\overline{\overline{X}})^2=\sum_{i=1}^r\sum_{j=1}^m[(x_{ij}-\overline{X}_i)+(\overline{X}_i-\overline{\overline{X}})]^2\\
S_T&amp;amp;=\sum_{i=1}^r\sum_{j=1}^m[(x_{ij}-\overline{X}_i)^2+(\overline{X}_i-\overline{\overline{X}})^2]+\underbrace{\sum_{i=1}^r\sum_{j=1}^m 2(x_{ij}-\overline{X}_i)(\overline{X}_i-\overline{\overline{X}})}_{=0}\\
S_T&amp;amp;=\underbrace{\sum_{i=1}^r\sum_{j=1}^m (x_{ij}-\overline{X}_i)^2}_{\text{组内误差平方和}}+\underbrace{\sum_{i=1}^r\sum_{j=1}^m (\overline{X}_i-\overline{\overline{X}})^2}_{组间误差平方和}
\end{aligned}
\]&lt;/span> 我们令组内误差平方和&lt;span class="math">\(S_e=\sum_{i=1}^r\sum_{j=1}^m (x_{ij}-\overline{X}_i)^2\)&lt;/span>，组间误差平方和&lt;span class="math">\(S_A=\sum_{i=1}^r\sum_{j=1}^m (\overline{X}_i-\overline{\overline{X}})^2=\sum_{i=1}^r m (\overline{X}_i-\overline{\overline{X}})^2\)&lt;/span>，所以有 &lt;span class="math">\[
S_T=S_E+S_A\tag{6}
\]&lt;/span> 这就是误差的平方和分解。下面我们不加证明地给出： &lt;span class="math">\[
\frac{S_e}{\sigma^2}\sim \chi^2(n-r)\tag{7}
\]&lt;/span> 当组间误差没有系统误差，即&lt;span class="math">\(\mu_i=\mu\)&lt;/span>时： &lt;span class="math">\[
\frac{S_A}{\sigma^2}\sim \chi^2(r-1)\tag{8}
\]&lt;/span> 且&lt;span class="math">\(S_e, S_A\)&lt;/span>二者独立。&lt;/p>
&lt;p>因此，当不同处理组没有区别，即不存在系统误差时，式(7)(8)都服从卡方分布，那么他们的商（还需除以自由度）就应该服从F分布： &lt;span class="math">\[
\frac{\frac{S_A}{\sigma^2×(r-1)}}{\frac{S_e}{\sigma^2×(n-r)}}=\frac{S_A/(r-1)}{S_e/(n-r)}\sim F(r-1,n-r)\tag{9}
\]&lt;/span> 考虑到系统误差&lt;span class="math">\(\mu_i-\mu\)&lt;/span>越大，&lt;span class="math">\(S_A\)&lt;/span>也就越大，那么式（9）的F统计量就越大。因此，该检验的拒绝域应为： &lt;span class="math">\[
W=\{F≥F_{1-\alpha}(r-1,n-r)\}\tag{10}
\]&lt;/span> 通常我们会将上述计算过程的结果汇总成&lt;strong>单因子方差分析表&lt;/strong>：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">来源&lt;/th>
&lt;th align="center">平方和&lt;/th>
&lt;th align="center">自由度&lt;/th>
&lt;th align="center">均方&lt;/th>
&lt;th align="center">F比&lt;/th>
&lt;th align="center">p值&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">因子&lt;/td>
&lt;td align="center">&lt;span class="math">\(S_A\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(f_A=r-1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(MS_A=S_A/f_A\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(F=MS_A/MS_e\)&lt;/span>&lt;/td>
&lt;td align="center">p&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">误差&lt;/td>
&lt;td align="center">&lt;span class="math">\(S_e\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(f_e=n-r\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(MS_e=S_E/f_e\)&lt;/span>&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">总和&lt;/td>
&lt;td align="center">&lt;span class="math">\(S_T\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(f_t=n-1\)&lt;/span>&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;td align="center">-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对于给定的&lt;span class="math">\(\alpha\)&lt;/span>，若&lt;span class="math">\(F≥F_{1-\alpha}(f_A,f_e)\)&lt;/span>，则认为因子显著，自变量的不同水平会对因变量有影响。&lt;/p>
&lt;p>如果认为因子的影响是显著的，想要进一步检验到底是哪个水平影响比较显著，还可继续进行多重检验（也叫事后检验），常用的一种多重检验是利用了t分布的LSD-t检验，其他还有SNK-q检验、Turkey、Duncan、Scheffe检验等。&lt;/p>
&lt;p>此外，还可以用组间方差占总误差的比例来衡量关系的强度，记为&lt;span class="math">\(R^2=SSA/SST\Rightarrow R=\sqrt{SSA/SST}\)&lt;/span>，当&lt;span class="math">\(R&amp;gt;0.5\)&lt;/span>，可认为是中等相关，若&lt;span class="math">\(R&amp;gt;0.8\)&lt;/span>可认为是强相关。&lt;/p>
&lt;h3 id="双因素方差分析">双因素方差分析&lt;/h3>
&lt;p>根据名字我们就知道，双因素方差分析是分析两个分类变量（常称为行因素和列因素）对试验结果的影响。根据两个因素对试验结果的影响是否独立，还可以分为&lt;strong>无交互作用的（无重复）双因素方差分析和有交互作用的（可重复）双因素方差分析&lt;/strong>。&lt;/p>
&lt;p>双因素方差分析也需要满足方差分析的三个假设：正态性、方差齐性、独立性。&lt;/p>
&lt;p>双因素方差分析的基本方法和单因素类似，区别是：&lt;/p>
&lt;p>无交互作用时：分为行因素误差平方和、列因素误差平方和、随机误差平方和三类。可以将行因素、列因素当成两个单因素误差分析来看。&lt;/p>
&lt;p>有交互作用时：还要添加第四类交互效应误差平方和。有交互作用时数据量会比较大。&lt;/p>
&lt;h2 id="正态性检验">正态性检验&lt;/h2>
&lt;p>正态分布时统计学中最重要的分布之一，判断一组数据是否来自正态总体是很多分析步骤的前置要求，甚至还有国标GB/T4882-2001专门设计了正态概率图来辅助我们判断数据是否服从正态分布。&lt;/p>
&lt;blockquote>
&lt;p>正态概率图是一种特殊绘制的坐标图，我们将所给数据绘制在坐标图上，如果这些数据大概处于一条直线上，那么可以认为服从正态分布。&lt;/p>
&lt;/blockquote>
&lt;p>当然，数理统计学还有更技术化的方法，比例最常用的W检验和EP检验。&lt;/p>
&lt;h3 id="w检验">W检验&lt;/h3>
&lt;p>W检验全称sharpiro-wilk检验，是由二人于1965年提出的正态检验方法。其适用范围为样本容量&lt;span class="math">\(8\leq n \leq 2000\)&lt;/span>，非常适合小样本的正态性检验，但是当样本容量小于7时，对偏离正态分布的检验不太有效，同时当数据量大于5000时也不适用。&lt;/p>
&lt;p>正态性检验方法总结图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/正态性检验.png" alt="正态性检验.png" />&lt;p class="caption">正态性检验.png&lt;/p>
&lt;/div>
&lt;p>贝叶斯检验主要利用了KL散度，这是衡量两种分布偏差程度的一种度量，也叫相对熵。&lt;/p>
&lt;p>正态性检验的计算往往十分复杂且常常需要查表，最好使用计算机程序辅助计算。&lt;/p>
&lt;h2 id="方差齐性检验">方差齐性检验&lt;/h2>
&lt;p>总结下这几种方法的利弊及适用条件：方差比、Hartley检验、Bartlett检验都需要原始数据是正态分布，Levene检验和BF法对正态分布不是很依赖。比较常用的是Levene检验，适用于多组方差的比较，且对正态性没要求。 &lt;a href="https://zhuanlan.zhihu.com/p/313397172">https://zhuanlan.zhihu.com/p/313397172&lt;/a>&lt;/p></description></item><item><title>数学分析之两类欧拉积分(Gamma、Beta函数与积分)</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E4%B8%A4%E7%B1%BB%E6%AC%A7%E6%8B%89%E7%A7%AF%E5%88%86gammabeta%E5%87%BD%E6%95%B0%E4%B8%8E%E7%A7%AF%E5%88%86/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E4%B8%A4%E7%B1%BB%E6%AC%A7%E6%8B%89%E7%A7%AF%E5%88%86gammabeta%E5%87%BD%E6%95%B0%E4%B8%8E%E7%A7%AF%E5%88%86/</guid><description>
&lt;ul>
&lt;li>&lt;a href="#哥德巴赫的疑问插值与阶乘研究">哥德巴赫的疑问——插值与阶乘研究&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三封信中诞生的gamma函数">三封信中诞生的Gamma函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#第一封丹尼尔-伯努利的解答">第一封：丹尼尔-伯努利的解答&lt;/a>&lt;/li>
&lt;li>&lt;a href="#第二封欧拉的无穷级数型解答">第二封：欧拉的无穷级数型解答&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无穷乘积中欧拉的发现">无穷乘积中欧拉的发现&lt;/a>&lt;/li>
&lt;li>&lt;a href="#第三封欧拉的积分函数型解答">第三封：欧拉的积分函数型解答&lt;/a>&lt;/li>
&lt;li>&lt;a href="#欧拉积分的进一步探究">欧拉积分的进一步探究&lt;/a>&lt;/li>
&lt;li>&lt;a href="#beta和gamma函数的命名">Beta和Gamma函数的命名&lt;/a>&lt;/li>
&lt;li>&lt;a href="#积分函数的偏移">积分函数的偏移&lt;/a>&lt;/li>
&lt;li>&lt;a href="#第二类欧拉积分的负数复数延拓以及统一表达式">第二类欧拉积分的负数、复数延拓以及统一表达式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#第二类欧拉积分与阶乘延拓的唯一性">第二类欧拉积分与阶乘延拓的唯一性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#不完全gamma函数与不完全beta函数">不完全Gamma函数与不完全Beta函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两类欧拉积分在概率论中的应用">两类欧拉积分在概率论中的应用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gamma分布">Gamma分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#beta分布">Beta分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gamma分布与一些常见分布的联系">Gamma分布与一些常见分布的联系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附gamma函数beta函数关系以及其他函数">附：Gamma函数、Beta函数关系以及其他函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gamma函数与beta函数关系推导">Gamma函数与Beta函数关系推导&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gamma函数与psi函数">Gamma函数与Psi函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gamma函数与zeta函数">Gamma函数与zeta函数&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="数学分析之两类欧拉积分">数学分析之两类欧拉积分&lt;!-- omit in toc -->&lt;/h2>
&lt;p>在概率论、微积分等数学领域中，我们经常能见到Beta函数（分布）、Gamma函数这种十分奇特的函数形式，他们很难从直觉上理解形式、作用，但是在很多时候有发挥着基础性作用。这不禁让人们好奇人们是怎么发现这两个奇葩的存在。这就要提到一位传奇数学巨匠——欧拉，他一生数学贡献无数，在数学及许多分支中都可以见到很多以欧拉命名的常数、公式和定理，他的工作使得数学更接近于现代数学的形态。他不但为数学界作出贡献，更把数学推至几乎整个物理的领域。此外欧拉还涉及建筑学、弹道学、航海学等领域。法国大数学家拉普拉斯曾说过一句话——“读读欧拉，他是所有人的老师。”两类欧拉积分，无疑也为其光辉的数学生涯又增加了浓墨重彩的一笔。两类欧拉积分的具体形式如下：&lt;/p>
&lt;p>第一类欧拉积分(Beta函数)： &lt;span class="math">\[
\Beta(x,y)=\int_0^1 t^{x-1}(1-t)^{y-1}\mathrm{d}t=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}\tag{1}
\]&lt;/span>&lt;/p>
&lt;p>第二类欧拉积分(Gamma函数)： &lt;span class="math">\[
\Gamma(x)=\int_0^\infty t^{x-1}e^{-t}\mathrm{d}t\tag{2}
\]&lt;/span>&lt;/p>
&lt;p>那么如此不直观的两个积分是如何被引入数学领域并在很多方向的起到奠基作用呢？这就用从&lt;strong>哥德巴赫对插值与阶乘的研究&lt;/strong>说起。&lt;/p>
&lt;h2 id="哥德巴赫的疑问插值与阶乘研究">哥德巴赫的疑问——插值与阶乘研究&lt;/h2>
&lt;p>克里斯蒂安·哥德巴赫（1690-1764）是活跃于17-18世纪的普鲁士（现德国）数学家，现在稍微对数学有些了解的人肯定会听过著名的“哥德巴赫猜想”，这个猜想描述起来十分简单：&lt;/p>
&lt;blockquote>
&lt;p>任一大于2的偶数，都可表示成两个素数之和。&lt;/p>
&lt;/blockquote>
&lt;p>但是，却是数论中存在最久的未解问题之一。哥德巴赫就是有这种能在简单事物中寻到复杂原理的直觉。两类欧拉积分函数的诞生，也同样来源于他对一个简单插值问题的研究。&lt;/p>
&lt;p>十七世纪的欧洲已经对科学实验与观测有了较为健全的认识，尤其是天文、海航领域，大量的观测与测量积累了丰富的相关数据，比如天文学上形体位置数据。由于每次观测都只获得一个结果，因此这些数据基本都是离散的数据。直观上来看，浩如烟海且庞杂多样的离散数据点令人头晕眼花，为了能提取出这些大量数据之中的精髓，总结数据的规律，数学家寄希望于&lt;strong>用一个相对简单的数学公式来替代大量的观测数据&lt;/strong>，&lt;strong>插值法&lt;/strong>的研究就此火热起来。内插，或称插值（英语：Interpolation），是一种通过已知的、离散的数据点，在范围内推求新数据点的过程或方法。求解科学和工程的问题时，通常有许多数据点借由采样、实验等方法获得，这些数据可能代表了有限个数值函数，其中自变量的值。而根据这些数据，我们往往希望得到一个连续的函数（曲线）。举个简单的例子：&lt;/p>
&lt;blockquote>
&lt;p>对于离散序列&lt;span class="math">\(\{(1,1),(2,q^1),(3,q^2),\dotsb,(k,q^{k-1}),\dotsb\},q\neq 1\)&lt;/span>的前&lt;span class="math">\(n\)&lt;/span>项之和，是否可以找出一条平滑的曲线穿过序列前&lt;span class="math">\(n\)&lt;/span>项的和？即 &lt;span class="math">\[f(n)=\sum_{i=1}^{n} a(i),\]&lt;/span> 找出一条过所有&lt;span class="math">\((n,f(n))\)&lt;/span>点且尽量简单的平滑曲线。&lt;/p>
&lt;/blockquote>
&lt;p>学过高中数学的读者很快就能发现，这个例子就是找的等比数列求和公式，画成图像是（&lt;span class="math">\(q=1.1\)&lt;/span>）：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/插值等比序列.png" alt="插值等比序列" />&lt;p class="caption">插值等比序列&lt;/p>
&lt;/div>
&lt;p>当我们用将这些点用一条平滑的曲线连接起来之后，不禁会思考，可以不可以用一个简单的表达式（函数）来表示这条曲线呢？这也是插值研究的一个重要问题。对于上面这个例子，根据中学数学知识，我们不难给出这样一条非常符合要求的曲线，其函数表达式为： &lt;span class="math">\[
f(x)=\frac{1-q^x}{1-q}(q=1.1)\tag{3}
\]&lt;/span>&lt;/p>
&lt;p>显然所有离散点都在这条曲线上，并且得到这个十分简单的函数表达式意味着我们不用再记那些离散的点，即使序列有无限长也无所谓，只需要计算函数表达式就可以根据任意给出的&lt;span class="math">\(n\)&lt;/span>得到相应的结果&lt;span class="math">\(f(n)\)&lt;/span>。&lt;/p>
&lt;p>这里必须要指出，是不是我们只能使用这一条曲线穿过所有的点呢？当时不是。我们可以信手画很多种曲线，都穿过图中那些离散的点，但是为什么大家选择了图中那条曲线呢？一是因为在哥德巴赫和欧拉所处的时代，函数这一词不仅仅意味着定义域到值域的抽象映射，更需要有一个能写出来的解析表达式。如果无法写出来一个明确的表达式，这个函数通常是不会被考虑的。二是因为这条曲线要足够的“自然”，这意味这条曲线需要一定合理的意义。当然即使满足了以上两点，函数曲线也并不是唯一的，大多数时候我们只是采用了那条&lt;strong>最符合我们直观想法&lt;/strong>的函数曲线。&lt;/p>
&lt;p>此外，找到的这个函数还有一个非常重要的特点：&lt;strong>连续性：即使在非正整数的点，例如-1，0.4736，1.1,&lt;span class="math">\(\sqrt{2}\)&lt;/span>,100.12345，函数都是有值的&lt;/strong>。这意味着我们将表达式可用的范围（定义域）从正整数&lt;strong>解析延拓&lt;/strong>到了整个实数域（除了&lt;span class="math">\(q\neq 1\)&lt;/span>）。&lt;/p>
&lt;blockquote>
&lt;p>解析延拓（英语：Analytic continuation）是数学上将解析函数从较小定义域拓展到更大定义域的方法，延拓的基本原则是延拓后的函数在原来的区域上和原函数等值。&lt;/p>
&lt;/blockquote>
&lt;p>对离散数据的插值和延拓得到的解析表达式提升了问题处理的泛用性，算是提取了大量数据中的精髓。说句题外话，现在火热的神经网络监督学习方法本质上也是通过离散数据（训练数据）拟合和延拓最贴近现实的函数。&lt;/p>
&lt;p>哥德巴赫在处理一系列插值与延拓问题时，遇到了一个看似很简单，处理起来却很棘手的问题，即阶乘的插值。&lt;/p>
&lt;p>在一系列数列中，阶乘的表达方式并不复杂甚至说非常简单： &lt;span class="math">\[
1,1\times 2,1\times 2\times 3,\dotsb,1\times 2\times\dotsb\times n,\dotsb
\]&lt;/span> 哥德巴赫知道连加求和的通项公式非常好找，因此他希望找到一个优雅简洁的通项公式，即能准确地描述阶乘&lt;span class="math">\(n!\)&lt;/span>，又能够像求和公式&lt;span class="math">\(\frac{n(n+1)}{2}\)&lt;/span>那样可以推广到实数域。经过多次尝试，哥德巴赫并没有在这个问题上取得太大进展，因此，他决定向当时他所认识的多位著名数学家求助。&lt;/p>
&lt;h2 id="三封信中诞生的gamma函数">三封信中诞生的Gamma函数&lt;/h2>
&lt;h3 id="第一封丹尼尔-伯努利的解答">第一封：丹尼尔-伯努利的解答&lt;/h3>
&lt;p>1722年他找尼古拉斯-伯努利请教这个阶乘插值问题，不过没有取得任何进展。即便如此，哥德巴赫却多年来一直不忘思考这个问题，1729 年他又请教尼古拉斯-伯努利的弟弟丹尼尔-伯努利，正好当时丹尼尔-伯努利对无穷级数有着很深入的了解，他从中获取灵感，于当年 10 月给哥德巴赫的一封回信中以无穷级数的形式给出了漂亮的解答。 &lt;span class="math">\[
x!=\lim_{A\rightarrow\infty}(A+1+\frac{x}{2})^{x-1} \prod_{i=1}^A \frac{i+1}{i+x}\tag{4}
\]&lt;/span> 这个插值公式的效果随&lt;span class="math">\(A\)&lt;/span>的增大而收敛，我们给出了一些&lt;span class="math">\(A\)&lt;/span>值下的结果图： &lt;img src="../images/Bernoulli_interpolation.png" alt="bernoulli_interpolation" />&lt;/p>
&lt;p>丹尼尔-伯努利通过无穷级数首先给出了一个阶乘解析表达式，虽然看上去很不好用，但是收敛速度还是挺快的，并且给接下来欧拉的研究提供灵感。&lt;/p>
&lt;h3 id="第二封欧拉的无穷级数型解答">第二封：欧拉的无穷级数型解答&lt;/h3>
&lt;p>当哥德巴赫向丹尼尔-伯努利写信咨询时，他正在圣彼得堡担任科学院成员，而他此时身边有一位好朋友叫莱昂纳德-欧拉，对这个找阶乘函数的问题也表现出了兴趣。同样，借助于&lt;strong>无穷&lt;/strong>这个强大工具，他在论文&lt;a href="https://arxiv.org/abs/1201.5631">《De termino generali serium hypergeometricarum》&lt;/a>中给出了自己的思路。&lt;/p>
&lt;blockquote>
&lt;p>Tips: 无穷的运算法则：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\infty\plusmn a=\infty,\forall a\in R\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(-\infty\plusmn a=-\infty,\forall a\in R\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\infty\times a=\infty,\forall a&amp;gt;0\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\infty\times -a=-\infty,\forall a&amp;gt;0\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>对于一个无穷大数&lt;span class="math">\(n\rightarrow\infty\)&lt;/span>，任一有限数字&lt;span class="math">\(x\)&lt;/span>，有 &lt;span class="math">\[
(n+x)!=n!\times (n+1)\times(n+2)\times\dotsb\times(n+x)\tag{5}
\]&lt;/span> 显然，一般理解下式&lt;span class="math">\((5)\)&lt;/span>只有当&lt;span class="math">\(x\)&lt;/span>为正整数时才合理。为了方便讨论，欧拉假设已经找到了一个适用于整个正实数域的阶乘插值函数（负数和复数以后再讨论），不妨令其为&lt;span class="math">\(\Delta(x),x\in R^+\)&lt;/span>，即式(5)为： &lt;span class="math">\[
\Delta(n+x)=\Delta(n)\times(n+1)\times(n+2)\times\dotsb\times(n+x)\tag{5.1}
\]&lt;/span> 若&lt;span class="math">\(x\)&lt;/span>为正整数，则&lt;span class="math">\(\Delta(x)=x!\)&lt;/span>。而根据无穷大的计算法则，当&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>： &lt;span class="math">\[
n+1=n\quad n+2=n\quad \dotsb \quad n+x=n
\]&lt;/span> 因此，式&lt;span class="math">\((5.1)\)&lt;/span>可以写成 &lt;span class="math">\[
\Delta(n+x)=\Delta(n)\times n^x\tag{6}
\]&lt;/span> 此时，欧拉再利用无穷大的性质，当&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>时，&lt;span class="math">\(n+\alpha=n\)&lt;/span>，&lt;span class="math">\(\alpha\)&lt;/span>是任一有限数。欧拉再将式&lt;span class="math">\((6)\)&lt;/span>中的&lt;span class="math">\(n^x\)&lt;/span>代换为&lt;span class="math">\((n+\alpha)^x\)&lt;/span>，即 &lt;span class="math">\[
\Delta(n+x)=\Delta(n)\times (n+\alpha)^x\tag{7}
\]&lt;/span> 若取&lt;span class="math">\(\alpha=1\)&lt;/span>，则&lt;span class="math">\((7)\)&lt;/span>可写成： &lt;span class="math">\[
\Delta(x+n)=\Delta(n)\times (n+1)^x, n\rightarrow\infty\\
\Rightarrow \lim_{n\rightarrow\infty}\frac{\Delta(n)\times(n+1)^x}{\Delta(n+x)}=1\tag{8}
\]&lt;/span> 最终，两边同时乘以&lt;span class="math">\(x\)&lt;/span>的阶乘函数&lt;span class="math">\(\Delta(x)\)&lt;/span>，则有： &lt;span class="math">\[
\Delta(x)=\lim_{n\rightarrow\infty}\frac{\Delta(n)\times(n+1)^x}{\Delta(n+x)}\times\Delta(x)\\
=\lim_{n\rightarrow\infty}\frac{1\cdot 2\cdot\dotsb \cdot n\times(n+1)^x}{(x+1)(x+2)\dotsb(x+n)},x\in R^+\tag{9}
\]&lt;/span> 上式(9)就是欧拉得到的无穷乘积形式的阶乘函数。从欧拉给出的思路来看，他真是将无穷的性质玩出花来了，当&lt;span class="math">\(n\)&lt;/span>取值远大于&lt;span class="math">\(x\)&lt;/span>时，这些式子的精度都很高。反复利用无穷性质的缺点是欧拉所给出的无穷乘积（式(9)）表达式的收敛速度，并没有丹尼尔伯努利给出的无穷乘积表达式（式(4)）收敛速度快。同时，丹尼尔-伯努利和欧拉二者给出的不同结果也告诉我们：满足整数点阶乘计算结果的插值&lt;strong>阶乘函数并不唯一&lt;/strong>。&lt;/p>
&lt;h3 id="无穷乘积中欧拉的发现">无穷乘积中欧拉的发现&lt;/h3>
&lt;p>我们发现无论是式(4)还是式(9)，&lt;span class="math">\(x\)&lt;/span>的取值都可以是任意正实数，并非一定要是整数，不过欧拉的无穷乘积表达式却是一只会下金蛋的鸡。欧拉作为20岁前就熟读伐里农、牛顿、笛卡尔、伽利略、雅各布-伯努利、约翰-伯努利、泰勒、沃利斯著作的学神，发现自己的无穷乘积表达式（是(9)）和英国数学家沃利斯发现的沃利斯乘积具有相似性。沃里斯乘积： &lt;span class="math">\[
\prod_{n=1}^{\infty}{\frac{2n}{2n-1}}\cdot {\frac{2n}{2n+1}}={\frac{2}{1}}\cdot {\frac {2}{3}}\cdot {\frac {4}{3}}\cdot {\frac {4}{5}}\cdot {\frac {6}{5}}\cdot {\frac {6}{7}}\cdot {\frac {8}{7}}\cdot {\frac {8}{9}}\cdots ={\frac {\pi }{2}}\tag{10}
\]&lt;/span> 而自己的无穷乘积表达式在&lt;span class="math">\(x=\frac{1}{2}\)&lt;/span>的时候，有： &lt;span class="math">\[
\begin{aligned}
&amp;amp;\lim_{n\rightarrow\infty}\frac{1\cdot 2\cdot\dotsb \cdot n\times(n+1)^x}{(x+1)(x+2)\dotsb(x+n)}\bigg|_{x=\frac{1}{2}}\\
&amp;amp;=\lim_{n\rightarrow\infty}(n+1)^{\frac{1}{2}}\frac{1}{\frac{1}{2}+1}\cdot\frac{2}{\frac{1}{2}+2}\cdot\frac{3}{\frac{1}{2}+3}\cdot\dotsb\frac{n}{\frac{1}{2}+n}\\
&amp;amp;=\lim_{n\rightarrow\infty}\sqrt{n+1}\cdot \frac{2}{3}\cdot \frac{4}{5}\cdot \frac{6}{7}\dotsb \cdot \frac{2n}{2n+1}\\
&amp;amp;=\lim_{n\rightarrow\infty}\sqrt{n+1}\cdot \sqrt{(\frac{2}{3}\cdot \frac{4}{5}\cdot \frac{6}{7}\dotsb \cdot \frac{2n}{2n+1})\cdot (\frac{2}{3}\cdot \frac{4}{5}\cdot \frac{6}{7}\dotsb \cdot \frac{2n}{2n+1})}\\
&amp;amp;=\lim_{n\rightarrow\infty}\sqrt{n+1}\cdot \sqrt{\frac{2\cdot 2 \cdot 4 \cdot 4\cdot 6\cdot 6\dotsb 2n \cdot 2n}{3\cdot 3 \cdot 5 \cdot 5 \cdot 7 \dotsb \cdot(2n+1)\cdot(2n+1)}}\\
&amp;amp;=\lim_{n\rightarrow\infty}\sqrt{\frac{n+1}{2n+1}}\cdot \sqrt{\underbrace{\prod_{i=1}^{n}{\frac{2i}{2i-1}}\cdot {\frac{2i}{2i+1}}}_{沃利斯乘积=\frac{\pi}{2}}}\tag{11}
\end{aligned}
\]&lt;/span> 欧拉惊奇地发现，后面那一项正是沃里斯公式（式(10)）的表达式，由此，我们可以将上式简写成： &lt;span class="math">\[
\Delta(\frac{1}{2})=\frac{1}{2}!=\lim_{n\rightarrow\infty}\sqrt{\frac{n+1}{2n+1}}\cdot \sqrt{\prod_{i=1}^{n}{\frac{2i}{2i-1}}\cdot {\frac{2i}{2i+1}}}\\
=\sqrt{\frac{1}{2}}\sqrt{\frac{\pi}{2}}=\frac{\sqrt{\pi}}{2}\tag{12}
\]&lt;/span> 出乎意料，在阶乘的函数表达式中出现了&lt;span class="math">\(\pi\)&lt;/span>。此时，欧拉意识到这个阶乘表达式可能与圆或者求面积有着密切联系，而求面积是积分的本职工作。此外，沃利斯也是在研究曲线下方面积（用现在角度看就是积分）时得到的沃利斯乘积，因此欧拉闪过灵感，&lt;strong>能否用积分的形式来替代无穷乘积形式表示阶乘函数呢&lt;/strong>？&lt;/p>
&lt;h3 id="第三封欧拉的积分函数型解答">第三封：欧拉的积分函数型解答&lt;/h3>
&lt;p>参考了前辈沃利斯、牛顿和斯特林的积分研究成果，欧拉选取了以下积分形式： &lt;span class="math">\[
J(e,n)=\int_0^1 x^e(1-x)^n\mathrm{d}x\tag{13}
\]&lt;/span> 其中&lt;span class="math">\(e\)&lt;/span>是任意数，&lt;span class="math">\(n\)&lt;/span>是整数。这个式子细看其实已经是第一类欧拉积分的原型了，后来勒让德修改了式(13)的形式，得到式(1)，Beta函数。 &lt;span class="math">\[
B(x,y)=\int_0^1 t^{x-1}(1-t)^{y-1}\mathrm{d}t\tag{1}
\]&lt;/span> 相较于第二类欧拉积分，它出现的更早，并且由式(13)推导出了第二类欧拉积分（式(2)），因此我们反而把更复杂一些的式(13)的形式定义为“第一类”欧拉积 分。&lt;/p>
&lt;p>我们通过分部积分法来处理式(13): &lt;span class="math">\[
\begin{aligned}
J(e,n)&amp;amp;=\frac{1}{e+1}x^{e+1}(1-x)^n\bigg |_0^1-\int_0^1 \frac{-n}{e+1}x^{e+1}(1-x)^{n-1}\mathrm{d}x\\
&amp;amp;=0+\frac{n}{e+1}\int_0^1 x^{e+1}(1-x)^{n-1}\mathrm{d}x\\
&amp;amp;=\frac{n}{e+1}J(e+1,n-1)
\end{aligned}\tag{14}
\]&lt;/span> 每用一次分部积分，&lt;span class="math">\(e\)&lt;/span>加一，&lt;span class="math">\(n\)&lt;/span>减一。这样我们递归地使用分部积分法可得： &lt;span class="math">\[\begin{aligned}
J(e,n)&amp;amp;=\frac{n}{e+1}J(e+1,n-1)\\
&amp;amp;=\frac{n(n-1)}{(e+1)(e+2)}J(e+2,n-2)\\
&amp;amp;\qquad\qquad\vdots\\
&amp;amp;=\frac{n(n-1)\dotsb 2\cdot 1}{(e+1)(e+2)\dotsb(e+n)}J(e+n,0)\\
&amp;amp;=\frac{n(n-1)\dotsb 2\cdot 1}{(e+1)(e+2)\dotsb(e+n)}\int_0^1 x^{e+n}(1-x)^0\mathrm{d}x\\
&amp;amp;=\frac{n!}{(e+1)(e+2)\dotsb(e+n)(e+n+1)}
\end{aligned}\tag{15}
\]&lt;/span> 即 &lt;span class="math">\[
\int_0^1 x^{e}(1-x)^n\mathrm{d}x=\frac{n!}{(e+1)(e+2)\dotsb(e+n)(e+n+1)}\tag{15.1}
\]&lt;/span> 欧拉经过分部积分，已经将阶乘&lt;span class="math">\(n!\)&lt;/span>与积分&lt;span class="math">\(\int_0^1 x^{e}(1-x)^n\mathrm{d}x\)&lt;/span>联系了起来，此时如果能将分母中&lt;span class="math">\((e+1)(e+2)\dotsb(e+n)(e+n+1)\)&lt;/span>与&lt;span class="math">\(n\)&lt;/span>的相关内容分离出来，就能够得到&lt;span class="math">\(n!\)&lt;/span>的积分表达式了！&lt;/p>
&lt;p>欧拉使用了一个处理技巧，令&lt;span class="math">\(e=\frac{f}{g}\)&lt;/span>，代入得： &lt;span class="math">\[
\int_0^1 x^{\frac{f}{g}}(1-x)^n\mathrm{d}x=\frac{n!}{(\frac{f}{g}+1)(\frac{f}{g}+2)\dotsb(\frac{f}{g}+n)(\frac{f}{g} +n+1)}\\
=\frac{n!\cdot g^{n+1}}{(f+g)(f+2g)\dotsb(f+ng)(f+(n+1)g)}\tag{16}
\]&lt;/span> 将等式右侧的&lt;span class="math">\(g^{n+1}\)&lt;/span>转移到另一侧，可得 &lt;span class="math">\[
\frac{\int_0^1 x^{\frac{f}{g}}(1-x)^n\mathrm{d}x}{g^{n+1}}=\frac{n!}{(f+g)(f+2g)\dotsb(f+ng)(f+(n+1)g)}\tag{16.1}
\]&lt;/span> 如果我们取&lt;span class="math">\(f=1,g\rightarrow 0\)&lt;/span>，那么等式右侧(RHS)在取极限时就会只剩下&lt;span class="math">\(n!\)&lt;/span>，而等式左侧(LHS)则是一个奇怪的结构： &lt;span class="math">\[
\lim_{g\rightarrow 0},_{f=1}\frac{\int_0^1 x^{\frac{f}{g}}(1-x)^n\mathrm{d}x}{g^{n+1}}=\frac{\int_0^1 x^{\frac{1}{0}}(1-x)^n\mathrm{d}x}{0^{n+1}}\tag{17}
\]&lt;/span> 为了处理这个结构，欧拉对积分变量做了一个代换，令&lt;span class="math">\(y=x^{\frac{f+g}{g}}\)&lt;/span>，&lt;span class="math">\(y\)&lt;/span>的积分域显然也是&lt;span class="math">\([0,1]\)&lt;/span>，则&lt;span class="math">\(x=y^{\frac{g}{f+g}}\)&lt;/span>，那么式(16.1)左侧可改写成： &lt;span class="math">\[
\begin{aligned}
(16.1)LHS&amp;amp;=\frac{\int_0^1 (y^{\frac{g}{f+g}})^{f/g}(1-y^{\frac{g}{f+g}})^n\mathrm{d}(y^{\frac{g}{f+g}})}{g^{n+1}}\\
&amp;amp;=\frac{\int_0^1 y^{\frac{f}{f+g}}(1-y^{\frac{g}{f+g}})^n (\frac{g}{f+g}y^{\frac{-f}{f+g}})\mathrm{d}y}{g^{n+1}}\\
&amp;amp;=\frac{\int_0^1 (1-y^{\frac{g}{f+g}})^n \mathrm{d}y}{g^{n}(f+g)}\\
&amp;amp;=\frac{1}{(f+g)^{n+1}}\int_0^1(\frac{1-y^{\frac{g}{f+g}}}{\frac{g}{f+g}})^n\mathrm{d}y
\end{aligned}\tag{18}
\]&lt;/span> 此时，我们再看式(18)的结果，其关键结构就是&lt;span class="math">\(\frac{1-y^{\frac{g}{f+g}}}{\frac{g}{f+g}}\)&lt;/span>，为了方便我们令&lt;span class="math">\(z=\frac{g}{f+g}\)&lt;/span>，当取&lt;span class="math">\(f=1,g\rightarrow 0\)&lt;/span>时，显然有&lt;span class="math">\(z\rightarrow 0\)&lt;/span>，即 &lt;span class="math">\[
\lim_{z\rightarrow 0}\frac{1-y^z}{z}\xlongequal{洛必达法则}\frac{-y^z\ln y}{1}\\
=-\ln y\tag{19}
\]&lt;/span> 将式(19)的结果代入式(18)，并联立式(16)可得： &lt;span class="math">\[
\int_0^1 (-\ln y)^n\mathrm{d}y=n!\tag{20}
\]&lt;/span> 为了追寻最后结果好看，也可以把&lt;span class="math">\(y\)&lt;/span>写成&lt;span class="math">\(x\)&lt;/span>，但是本文为了标识清晰，就保留&lt;span class="math">\(y\)&lt;/span>来表示。式(20)就是欧拉在原式论文中给出的结果，不过和式(2)给出的第二类欧拉积分似乎还有点不同。其实式(2)和(20)式等价的，我们只要令&lt;span class="math">\(y=e^{-t}\)&lt;/span>，那么便有： &lt;span class="math">\[
n!=\int_0^1 (-\ln e^{-t})^n\mathrm{d}e^{-t}=\int_0^\infty t^n e^{-t}\mathrm{d}t\tag{21}
\]&lt;/span> 显然，其结果就是第二类欧拉积分，又称为&lt;span class="math">\(\Gamma\)&lt;/span>函数。其在正实数范围内的图像如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/gamma-function-posotive-real.png" alt="gamma-function-posotive-real" />&lt;p class="caption">gamma-function-posotive-real&lt;/p>
&lt;/div>
&lt;p>需要注意的是，上式的函数和真正的&lt;span class="math">\(\Gamma\)&lt;/span>函数有一位偏移。做出这个偏移决定的也是欧拉，这也使得&lt;span class="math">\(\Gamma(x)=(x-1)!\)&lt;/span>而非&lt;span class="math">\(x!\)&lt;/span>。&lt;/p>
&lt;p>哥德巴赫和欧拉在关于阶乘函数的通信中诞生了两类欧拉积分即Beta函数和Gamma函数，此时是1730年，欧拉只有23岁。从此数学家对他们的探索就从未停止过，并且得出了许多改变数学历史的结论。我们将这些细节在下一章中说明。&lt;/p>
&lt;h2 id="欧拉积分的进一步探究">欧拉积分的进一步探究&lt;/h2>
&lt;h3 id="beta和gamma函数的命名">Beta和Gamma函数的命名&lt;/h3>
&lt;p>当欧拉在给哥德巴赫的回信中第一次给出两类积分的时候，显然不会像后世有“后验”经验的数学家那样意识到这两个积分函数的将会带来多大的数学财富。由于欧拉在当时已经是名满欧洲的大数学家，并且涉猎的领域宽广，因此很多其他方面的问题让欧拉并不能专心深入研究两类积分。&lt;/p>
&lt;p>不过，当时很多著名的数学家都和欧拉有着密切的联系，比如法国18世纪末的三个著名数学家，即称为“法国3L”的拉格朗日（Lagrange）、拉普拉斯（Laplace）和勒让德（Legendre）。他们三人都是欧拉的后辈，且深受欧拉的影响，拉格朗日是欧拉的学生，拉普拉斯则是拉格朗日的同事，文章开头那句“读读欧拉，他是所有人的老师。”就是拉普拉斯说的。而勒让德是被拉格朗日发觉的数学家，其关于椭圆曲线和数论的研究很大程度上基于欧拉的研究，并且是欧拉时代到高斯时代的重要过渡者（虽然被夹在两个巨匠之间掩盖了勒让德本身的光芒，尤其是高斯&lt;sup>-&lt;/sup>）&lt;/p>
&lt;p>勒让德花了许多时间研究欧拉相关的积分，并发表了几篇文章，在&lt;strong>1792年的文章《Mémoires sur les transcendantes elliptiques》（关于超越椭圆函数的备忘录）中首次将式（1）的形式命名为第一类欧拉积分&lt;/strong>。在1809年的《Recherches sur diverses sortes d'intégrales définies》（各类定积分的研究）中，勒让德正式地命名和引入了第一类欧拉积分（式(1)）和第二类欧拉积分（式(2)），并引入了符号&lt;span class="math">\(\Gamma\)&lt;/span>来表示第二类欧拉积分，即 &lt;span class="math">\[
\Gamma(x)=\int_0^\infty t^{x-1}e^{-t}\mathrm{d}t \tag{2.1}
\]&lt;/span> 至于为什么勒让德只给第二类欧拉积分找了个代表符号容易理解，因为第二类欧拉积分正好是欧拉找到的表示阶乘的连续函数，得到了更多重视，而第一类欧拉积分则是在推导过程中的一个辅助积分。而勒让德选择符号&lt;span class="math">\(\Gamma\)&lt;/span>的原因，Julio González Cabillón猜测因为一是符号&lt;span class="math">\(\Gamma\)&lt;/span>是勒让德（LeGendre）中“L”倒过来写，二是“Gamma”中的“G”代表自己名字中的“Gendre”。&lt;/p>
&lt;p>不过，随着研究的深入，直到1839年Jacques P. M. Binet才引入了符号&lt;span class="math">\(\Beta\)&lt;/span>（Beta ）表示第一类欧拉积分，他选用&lt;span class="math">\(\Beta\)&lt;/span>的原因也很简单，一是因为他名字中有字母“B”，二是第二类欧拉积分是由第一类欧拉积分推导出来的，理应使用希腊字母表中“&lt;span class="math">\(\Gamma\)&lt;/span>”的前一位字母“&lt;span class="math">\(\Beta\)&lt;/span>”。&lt;/p>
&lt;h3 id="积分函数的偏移">积分函数的偏移&lt;/h3>
&lt;p>如果仔细看欧拉得出的结果式(21)然后对比勒让德定义的&lt;span class="math">\(\Gamma\)&lt;/span>函数的表达式(2.1) &lt;span class="math">\[
n!=\int_0^\infty t^n e^{-t}\mathrm{d}t=\Gamma(n+1)\tag{22}
\]&lt;/span> 可以发现，欧拉的得出表达式严格来说并不是现在使用的第二类欧拉积分的表达式，&lt;strong>他们之间有一位偏移&lt;/strong>，即&lt;span class="math">\(\Gamma(n)\neq n!\)&lt;/span>而是等于&lt;span class="math">\((n-1)!\)&lt;/span>。其实，如果我们将式(21)的形式定义成&lt;span class="math">\(\Gamma\)&lt;/span>函数也不是不可以，事实上欧拉最初研究这个问题就用的式(21)的结果，此外欧拉的后辈数学王子高斯在研究这个积分也使用的如下形式： &lt;span class="math">\[
\Pi(x)=\int_0^\infty t^x e^{-t}\mathrm{d}t\tag{23}
\]&lt;/span> 当时主推式(2)定义的人是勒让德，即上文定义的式(2.1) &lt;span class="math">\[
\Gamma(x)=\int_0^\infty t^{x-1}e^{-t}\mathrm{d}t \tag{2.1}
\]&lt;/span> 相应地，第一类欧拉积分在勒让德定义时也区别与刚被引入时的原始形式，也做了一位偏移，即 &lt;span class="math">\[
\int_0^1 t^x(1-t)^y\mathrm{d}t\rightarrow\int_0^1 t^{x-1}(1-t)^{y-1}\mathrm{d}t\tag{24}
\]&lt;/span> 从后世的角度来看，显然勒让德偏移一位的定义方式更被人普遍接受。实际上，勒让德在研究时发现，在1768年欧拉的著作《Institutiones calculi integralis》中，欧拉引入第一类积分时已经对积分参数做了-1的修改。欧拉在《Institutiones calculi integralis》中研究的积分形式如下： &lt;span class="math">\[
\int_0^1 \frac{x^{p-1}}{\sqrt[n]{1-x^n}^{n-q}}\mathrm{d}x\tag{25}
\]&lt;/span> 当&lt;span class="math">\(n=1\)&lt;/span>时，式(25)即为第一类欧拉积分（Beta函数）。勒让德依照欧拉的方式，同样对第二类欧拉积分做出了-1的偏移，可以说欧拉的研究影响了勒让德，让其决定采用了-1位的偏移。&lt;/p>
&lt;p>至于为什么要最终采用-1位偏移的定义，且更广为接受呢？一个可能的原因是Beta函数与Gamma函数关系。按照勒让德的定义方式（式(2.1)），那么两个函数之间的关系可以简洁地写成： &lt;span class="math">\[
\Beta(x,y)=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}\tag{26}
\]&lt;/span> 式(26)简洁、对称，优雅，很符合数学家们的审美。如果按照不偏移的定义，即高斯的定义（式(23)），那么他们之间的关系则是 &lt;span class="math">\[
\Beta&amp;#39;(x,y)=\frac{\Pi(x)\pi(y)}{\Pi(x+y+1)}\tag{27}
\]&lt;/span> 分母那个多余的1，不仅仅会破坏对称美感，而且在计算种种带来额外项。&lt;/p>
&lt;p>另一个可能的猜测则与抽象代数有关，按照勒让德的定义式(2.1)，&lt;span class="math">\(\Gamma\)&lt;/span>分布的概率密度函数和卷积可以组成一个半环，而不做偏移的定义形式则没有这个性质。虽然有关群论的内容需要等待欧拉死后大约半个世纪的阿贝尔和伽罗华才初步建立，但是欧拉未出版的一些手稿中已经体现了群论的初步思想。至于欧拉是否也发现式(2.1)的形式在群论计算中的便利性，我们就无从得知了。不过，正因为群论的发展，使得式(2.1)的定义方式更广泛地被接受。&lt;/p>
&lt;h3 id="第二类欧拉积分的负数复数延拓以及统一表达式">第二类欧拉积分的负数、复数延拓以及统一表达式&lt;/h3>
&lt;p>在欧拉进行阶乘延拓的时候，他考虑的是正实数场景，当&lt;span class="math">\(x\in R^+\)&lt;/span>时，第二类欧拉积分，即&lt;span class="math">\(\Gamma(x)\)&lt;/span>函数是有定义且连续的，具体证明可参考网页资料&lt;a href="../网页资料/数学分析-学习笔记-Γ函数和B函数.html">数学分析-学习笔记-Γ函数和B函数.html&lt;/a>。正如从整数向正实数的解析延拓促使了&lt;span class="math">\(\Gamma\)&lt;/span>函数的诞生，在此之后，数学家们开始希望将阶乘的概念拓展到负数与复数。&lt;/p>
&lt;p>对于负数而言，虽说式(2)的形式并不适用，但是阶乘的思想还是能够借鉴的，关键就是递推公式： &lt;span class="math">\[
\Gamma(x+1)=x\Gamma(x)\tag{28}
\]&lt;/span> 那么如果我们不停地给自变量&lt;span class="math">\(x\)&lt;/span>减1，那么&lt;span class="math">\(x\)&lt;/span>很快就会落入负数的范围，比如： &lt;span class="math">\[
\begin{aligned}
&amp;amp;{1\over 2}!=\Gamma({3\over 2})=({1\over 2})\times \Gamma({3\over 2}-1)=({1\over 2})\times \Gamma({1\over 2})\\
&amp;amp;{1\over 2}!=({1\over 2})\times(-{1\over 2})\times \Gamma({1\over 2}-1)=({1\over 2})\times (-{1\over 2})\times \Gamma(-{1\over 2})\\
&amp;amp;{1\over 2}!=({1\over 2})\times(-{1\over 2})\times(-{3\over 2})\times \Gamma(-{1\over 2}-1)\\
&amp;amp;\qquad=({1\over 2})\times(-{1\over 2})\times (-{3\over 2})\times \Gamma(-{3\over 2})=(-{5\over 2})!\\
\end{aligned}\tag{29}
\]&lt;/span> 我们将递推关系反转过来，我们能够很轻易地得到阶乘的负数延拓，比如： &lt;span class="math">\[
(-{5\over 2})!=\Gamma(-{3\over 2})=({2\over 1})\times (-{2\over 1})\times (-{2\over 3})\times \underbrace{\Gamma({3\over 2})}_{=\frac{1}{2}!=\frac{\sqrt{\pi}}{2}}=\frac{4\sqrt{\pi}}{3}\tag{30}
\]&lt;/span> 依据递推关系，我们只要计算出&lt;span class="math">\(\Gamma\)&lt;/span>函数在0-1范围内的值，就可以推得&lt;strong>几乎所有&lt;/strong>负数&lt;span class="math">\(\Gamma\)&lt;/span>函数的值。注意，我这里使用的是“几乎”所有。这是因为使用递推关系有一个巨大的问题，即无法处理负整数的情形。比如-2的阶乘通过递推关系应该是： &lt;span class="math">\[
\Gamma(-1)=(-2)! \\
\Gamma(-1)\times (-1) \times 0 \times 1 = 1! =\Gamma(2)\tag{31}
\]&lt;/span> 但是这个连乘式子中有一个&lt;span class="math">\(\times 0\)&lt;/span>，结果理应是0，而不是&lt;span class="math">\(1!\)&lt;/span>，相应地，&lt;span class="math">\(\Gamma\)&lt;/span>函数在所有负整数的延拓都没有良好的定义，负整数是函数的极点。延拓到负数部分的&lt;span class="math">\(\Gamma\)&lt;/span>函数如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Gamma-function-real.svg" alt="Gamma-function-real.svg" />&lt;p class="caption">Gamma-function-real.svg&lt;/p>
&lt;/div>
&lt;p>相比于负数的延拓，复数的延拓要容易的多，因为&lt;span class="math">\(\Gamma\)&lt;/span>函数在自变量是“兼容”复数的，对于实数部分为正的复数&lt;span class="math">\(z,Re(z)&amp;gt;0\)&lt;/span>，我们甚至不必修改欧拉给出的式子原型。对于实数部分为负的复数&lt;span class="math">\(z,Re(z)&amp;gt;0\)&lt;/span>，只要实部不是负整数，也可以通过递推关系式得到。而实部为负整数的点，就是函数在复平面的极点。&lt;/p>
&lt;blockquote>
&lt;p>TIPS：全纯函数（英语：Holomorphic function）是复分析研究的中心对象；它们是定义在复平面&lt;span class="math">\(\mathbb {C}\)&lt;/span>的开子集上的，在复平面&lt;span class="math">\(\mathbb {C}\)&lt;/span>中取值的，在每点上皆复可微的函数(处处解析)。&lt;/p>
&lt;p>在复变函数中，亚纯函数（meromorphic function）是在区域&lt;span class="math">\(D\)&lt;/span>上有定义，且除去极点之外处处解析的函数。具体来说就是在复分析中，一个复平面的开子集&lt;span class="math">\(D\)&lt;/span>上的亚纯函数是一个在&lt;span class="math">\(D\)&lt;/span>上除一个或若干个孤立点集合之外的区域全纯的函数，那些孤立点称为该函数的极点。&lt;/p>
&lt;p>显然&lt;span class="math">\(\Gamma(x)\)&lt;/span>函数由于在负整数为极点，其他位置处处解析，因此它是亚纯函数，而&lt;span class="math">\(\Gamma(x)\)&lt;/span>函数的倒数&lt;span class="math">\(\frac{1}{\Gamma(x)}\)&lt;/span>却是处处解析的全纯函数（&lt;span class="math">\(\frac{1}{\Gamma(x)}\)&lt;/span>实部为负整数的点函数值都为0），因此在处理&lt;span class="math">\(\Gamma(x)\)&lt;/span>函数，很多数学家会从&lt;span class="math">\(\frac{1}{\Gamma(x)}\)&lt;/span>着手。&lt;/p>
&lt;/blockquote>
&lt;p>在整个复平面，&lt;span class="math">\(\Gamma\)&lt;/span>函数的&lt;strong>绝对值&lt;/strong>图像如下（注：那些顶端平的部分是为了方便显示，函数值截断的结果）&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Gamma_abs_3D.png" alt="Gamma_abs_3D" />&lt;p class="caption">Gamma_abs_3D&lt;/p>
&lt;/div>
&lt;p>数学家F.W. Newman在1848年给了&lt;span class="math">\(\Gamma\)&lt;/span>函数在整个复平面一个通用的表达式(32)，利用了&lt;span class="math">\(1/\Gamma(z)\)&lt;/span>是全纯函数的特性。 &lt;span class="math">\[
1/\Gamma(z)=ze^{\gamma z}\prod_{k=1}^{\infty}[(1+\frac{z}{k})e^{-\frac{z}{k}}],\qquad \gamma=0.5772156649\dotsb\tag{32}
\]&lt;/span> 其中，&lt;span class="math">\(\gamma\)&lt;/span>为欧拉-马斯刻若尼常数，简称欧拉常数&lt;span class="math">\(\gamma = \lim\limits_{n \rightarrow \infty }[( \sum_{k=1}^n \frac{1}{k} - \ln(n)]\)&lt;/span>。&lt;/p>
&lt;h3 id="第二类欧拉积分与阶乘延拓的唯一性">第二类欧拉积分与阶乘延拓的唯一性&lt;/h3>
&lt;p>阶乘的解析延拓是唯一的吗？答案是显然的：No。我们回到开头插值的问题，最开始介绍&lt;a href="#哥德巴赫的疑问插值与阶乘研究">哥德巴赫的疑问——插值与阶乘研究&lt;/a>的时候就提到如果单纯地找插值函数，数学家们可以找出无数条符合要求的曲线。在丹尼尔-伯努利和欧拉分别写信告诉哥德巴赫他们的无穷乘积结果时，给出的也是两个不同的无穷乘积形式，并且都能够很好地做为阶乘的插值函数。&lt;/p>
&lt;p>后来的数学家也解析延拓出了其他的阶乘函数，比如在1894年，法国数学家Hadamard利用&lt;span class="math">\(\Gamma\)&lt;/span>函数给出了一个在整个复平面内都解析的全纯函数： &lt;span class="math">\[
y=\frac{1}{\Gamma(1-x)}\frac{\mathrm{d}}{\mathrm{d}x}\log\left[\Gamma(\frac{1-x}{2})/\Gamma(1-\frac{x}{2})\right],x\in \mathbb{C}\tag{33}
\]&lt;/span> 其函数图像如下：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/hadamard阶乘函数.png" alt="hadamard阶乘函数.png" />&lt;p class="caption">hadamard阶乘函数.png&lt;/p>
&lt;/div>
&lt;p>相较于&lt;span class="math">\(\Gamma(x)\)&lt;/span>，式(33)在整个复平面内没有奇异性，即使在负整数处也没有极点，从函数分析理论角度来看，它是一个更适合分析的函数。&lt;/p>
&lt;p>那么，为什么只有第二类欧拉积分，即&lt;span class="math">\(\Gamma(x)\)&lt;/span>函数得到了数学家们的广泛认可呢？&lt;/p>
&lt;p>从研究积分的角度看，这是显然的。因为欧拉积分在特殊函数中出现的频率非常高，第二类欧拉积分可以说是很多特殊积分的基础。而从插值的角度看，&lt;span class="math">\(\Gamma\)&lt;/span>函数的特殊性并不那么显然，虽然可以从美学上说，&lt;span class="math">\(\Gamma\)&lt;/span>函数简洁而优雅，但是数学要求我们找出更理性的证据。&lt;/p>
&lt;p>既然阶乘插值的结果千千万，那我们来考虑阶乘函数除插值外，还应当满足哪些必要的要求。首先，阶乘的核心是递推关系&lt;span class="math">\(n!=n(n-1)!\)&lt;/span>，我们先要把递推关系延拓到所有正实数&lt;span class="math">\(f(x+1)=xf(x)，x\in R^+\)&lt;/span>（注意我们这里仿照勒让德的定义方式做了一位偏移），这使得将阶乘的概念从正整数延拓到正实数。同时我们规定阶乘函数的起点值&lt;span class="math">\(f(x)|_{x=1}=1\)&lt;/span>，这二者共同保证了在所有正整数点的函数值与整数的阶乘值一致。综上总结出以下两个要求：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(f(1)=1\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x+1)=xf(x),\forall x\in R^+\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>毫无疑问，&lt;span class="math">\(\Gamma(x)\)&lt;/span>是满足上述两个条件的，但问题是满足上述两个条件的函数也是无穷多个，这里我们给出一个典型的构造方法。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>我们将数轴正半轴以1为单位划分成段，构造分段函数&lt;span class="math">\(g_0(x),x\in(0,1];g_1(x),x\in(1,2];g_2(x),x\in(2,3];\dotsb;g_n(x),x\in (n,n+1];\dotsb\)&lt;/span>&lt;/li>
&lt;li>在区间&lt;span class="math">\([1,2]\)&lt;/span>内任意找出一个连续函数&lt;span class="math">\(g_1\)&lt;/span>，使得&lt;span class="math">\(g_1(1)=g_1(2)=1\)&lt;/span>。因为根据条件1和2，&lt;span class="math">\(f(1)=1,f(2)=1\times f(1)=1\)&lt;/span>。&lt;/li>
&lt;li>根据条件2的递推关系，当&lt;span class="math">\(x\in (0,1]\)&lt;/span>，有&lt;span class="math">\(x\times g_0(x)=g_1(x)\Rightarrow g_0(x)=\frac{g_1(x)}{x}\)&lt;/span>。&lt;/li>
&lt;li>根据条件2的递推关系，当&lt;span class="math">\(x\in (n,n+1],n&amp;gt;1\)&lt;/span>时，有&lt;span class="math">\(g_n(x)=g_1(x)\prod_{k=1}^{n-1}(x-k)\)&lt;/span>&lt;/li>
&lt;li>将函数段&lt;span class="math">\(g_0,g_1,g_2,\dotsb\)&lt;/span>，组合起来，即构造为伪&lt;span class="math">\(\Gamma\)&lt;/span>函数同时满足条件1，2。&lt;/li>
&lt;/ol>
&lt;p>举个例子，令&lt;span class="math">\(g_1(x)=1\)&lt;/span>，其构造的函数分别为： &lt;span class="math">\[
\begin{aligned}
&amp;amp;g_0(x)=1/x,\qquad 0&amp;lt;x\le 1;\\
&amp;amp;g_1(x)=1,\qquad 1&amp;lt;x\le 2;\\
&amp;amp;g_2(x)=x-1,\qquad 2&amp;lt;x\le 3;\\
&amp;amp;g_3(x)=(x-1)(x-2),\qquad 3&amp;lt;x\le 4;\\
&amp;amp;g_4(x)=(x-1)(x-2)(x-3),\qquad 4&amp;lt;x\le 5;\\
&amp;amp;\qquad \vdots
\end{aligned}\tag{34}
\]&lt;/span> 函数图像为： &lt;img src="../images/伪gamma函数1.png" alt="伪gamma函数1.png" />&lt;/p>
&lt;p>有些人觉得分段函数在每段连接处是生硬地转折，不可微，看上去不够自然。那么加上&lt;strong>可微这个条件&lt;/strong>，&lt;span class="math">\(\Gamma\)&lt;/span>函数是否就是唯一解呢？依旧不是。我们可以利用&lt;span class="math">\(\Gamma\)&lt;/span>函数构造出其他满足条件的例子，我们任意找一个解析并且周期为1的周期函数&lt;span class="math">\(p(x)\)&lt;/span>，同时确保&lt;span class="math">\(p(1)=1\)&lt;/span>，比如&lt;span class="math">\(p(x)=1+\sin(2\pi x)\)&lt;/span>，那么函数 &lt;span class="math">\[g(x)=\Gamma(x)p(x)\tag{35}\]&lt;/span> 就是一个符合上述三个要求的函数，其函数图像如下：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/可微的伪Gamma函数.png" alt="可微的伪Gamma函数.png" />&lt;p class="caption">可微的伪Gamma函数.png&lt;/p>
&lt;/div>
&lt;p>既然，这样都不是充分条件，那么我们还要如何加强这个问题，使得&lt;span class="math">\(\Gamma\)&lt;/span>函数成为唯一解呢？在19世纪中叶，数学家们确实给出了能让&lt;span class="math">\(\Gamma\)&lt;/span>函数作为唯一满足条件的连续函数。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\Gamma(1)=1\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\Gamma(x+1)=x\Gamma(x),\forall x\in R^+\)&lt;/span>&lt;/li>
&lt;li>满足反射公式：&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;span class="math">\[\Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin(\pi z)}\tag{35}\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;ol start="4" style="list-style-type: decimal">
&lt;li>满足乘法公式：&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;span class="math">\[\Gamma(nz)=(2\pi)^{\frac{1-n}{2}}n^{nz-\frac{1}{2}}\Gamma(z)\Gamma(z+\frac{1}{n})\Gamma(z+\frac{2 }{n})\dotsb\Gamma(z+\frac{n-1}{n})\tag{36}\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>不过新加的两个条件很难说是“符合直觉的”，它们太复杂，而且与其说是条件，倒不如说是&lt;span class="math">\(\Gamma(x)\)&lt;/span>函数所特有的性质，有点先射箭，再画靶的嫌疑。&lt;/p>
&lt;p>传统的数学分析领域似乎已经无法为我们提供更多帮助了。很多时候，代数和分析是相辅相成的，在康托尔发展了集合论以及拓扑学的完善之后，在20世纪之交，一个很“视觉”的概念，“凸函数”被提出来。&lt;/p>
&lt;blockquote>
&lt;p>&lt;span class="math">\(f\)&lt;/span>称为凸函数，意思是对&lt;span class="math">\(0\leq t\leq 1\)&lt;/span>及任意&lt;span class="math">\(x_{1},x_{2}\in C\)&lt;/span>，皆有 &lt;span class="math">\[f\left(tx_{1}+(1-t)x_{2}\right)\leq tf\left(x_{1}\right)+(1-t)f\left(x_{2}\right)\tag{37}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>在代数上的描述并不直观，但是从几何上来说，凸函数就是函数上任意两点的连续都在函数上方，如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/凸函数定义.png" alt="凸函数定义.png" />&lt;p class="caption">凸函数定义.png&lt;/p>
&lt;/div>
&lt;p>凸函数不像三角函数&lt;span class="math">\(\sin,\cos\)&lt;/span>那样上下波动，形状相对固定，在研究中有非常方便的性质，比如局部最优就是全局最优，二阶导非负（多元情况下就是Hessian矩阵半正定），这些方便的性质迅速让凸函数在均值理论、拓扑学、博弈论、线性规划中得到了广泛应用，为此还发展出了专门的一个方向：凸优化。因此，如果研究者发现需要研究的函数是“凸”的，那么他会大松一口气，很多问题就能够轻易的解决。&lt;/p>
&lt;p>而欧拉给出的&lt;span class="math">\(\Gamma(x)\)&lt;/span>函数恰好在正实数范围内是“凸”的，同时在负数部分的每一小段也都是“凸”的，我们从&lt;span class="math">\(\Gamma\)&lt;/span>函数的图像中很容易看出这一点。而Hadamard延拓出的函数式(33)和我们构造的解析的伪&lt;span class="math">\(\Gamma\)&lt;/span>函数&lt;span class="math">\(g(x)=\Gamma(x)p(x)\)&lt;/span>（式(35)）显然不满足“凸”这个性质。那么，很多数学家自然而然的想到，&lt;span class="math">\(\Gamma\)&lt;/span>函数是不是唯一满足条件1，2的凸函数呢？&lt;/p>
&lt;p>答案显然也不是，其实我们在前面已经给出了反例，就是前面构造出来的分段函数式(34)。虽然它是一段段曲线拼接起来的，且在拼接处不解析，但它是货真价实的凸函数。&lt;/p>
&lt;p>不过，抓到“凸”这个特性，我们离真正的答案也就不远了。&lt;/p>
&lt;p>在1922年，丹麦数学家Harald Bohr和Johannes Mollerup发现，&lt;span class="math">\(\Gamma(x)\)&lt;/span>函数不仅仅是凸的，而且&lt;span class="math">\(\log\Gamma(x)\)&lt;/span>也是凸函数，即&lt;strong>对数凸函数&lt;/strong>！对数凸函数是一个比凸函数更强的条件，对数凸函数必然是凸函数，由于对数函数会大幅降低函数成长的速率，因此若取对数后仍为凸函数，表示函数上升的速度比凸函数还快，因此会称为超凸函数。两位数学家证明：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Bohr–Mollerup定理&lt;/strong>：在&lt;span class="math">\(x&amp;gt;0\)&lt;/span>的区间上，&lt;span class="math">\(Γ\)&lt;/span>函数 &lt;span class="math">\[\Gamma(x)=\int_0^\infty t^{x-1}e^{-t}\mathrm{d}t \tag{2.1}\]&lt;/span> 是&lt;strong>唯一&lt;/strong>同时满足以下3条性质的函数 f ：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(f(1)=1\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x+1)=xf(x),\forall x\in R^+\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f\)&lt;/span>是对数凸函数。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>这个定理最早是出现在一本复分析教科书中，并且证明过程并不复杂，以至于当时Bohr和Mollerup都以为这是一个人们肯定已经知道的结果。不过这个定理确实是第一个给出了阶乘函数的解析延拓在何种情形下一定是&lt;span class="math">\(Γ\)&lt;/span>函数的简明充要条件。后来，数学家Emil Artin简化了定理的证明，因此这个定理也被称为“Bohr–Mollerup-Artin定理”。&lt;/p>
&lt;p>这个定理让数学家非常满意，让&lt;span class="math">\(\Gamma\)&lt;/span>函数充满了优雅与简洁的独特美学，甚至让追求严格性著称的尼古拉·布尔巴基合作者协会都以此作为阐述&lt;span class="math">\(\Gamma\)&lt;/span>函数的切入点。&lt;/p>
&lt;h3 id="不完全gamma函数与不完全beta函数">不完全Gamma函数与不完全Beta函数&lt;/h3>
&lt;p>不完全&lt;span class="math">\(Γ\)&lt;/span>、&lt;span class="math">\(\Beta\)&lt;/span>函数是&lt;span class="math">\(Γ\)&lt;/span>函数与&lt;span class="math">\(\Beta\)&lt;/span>函数的不定积分形式。这里不打算介绍太多，只介绍基本思想。由于两类欧拉积分都是定积分，在给出参数后就是固定值，后来数学家为了将它们推广到不定积分，就分别给出了不完全&lt;span class="math">\(Γ\)&lt;/span>函数与不完全&lt;span class="math">\(\Beta\)&lt;/span>函数。&lt;/p>
&lt;p>&lt;span class="math">\(Γ\)&lt;/span>函数式(2.1)，根据将上限变成不定积分和下限变成不定积分又分成上不完全&lt;span class="math">\(Γ\)&lt;/span>函数和下不完全&lt;span class="math">\(Γ\)&lt;/span>函数，其解析表达式如下：&lt;/p>
&lt;p>上不完全&lt;span class="math">\(Γ\)&lt;/span>函数： &lt;span class="math">\[\Gamma(s,x) = \int_x^{\infty} t^{s-1}\,e^{-t}\,{\rm d}t\quad \Re(s)&amp;gt;0, x\in\mathbb R_0^+\tag{38}\]&lt;/span> 当&lt;span class="math">\(x=0\)&lt;/span>时，上不完全&lt;span class="math">\(Γ\)&lt;/span>函数就是&lt;span class="math">\(Γ\)&lt;/span>函数&lt;span class="math">\(\Gamma(s,0)=\Gamma(s)\)&lt;/span>。&lt;/p>
&lt;p>下不完全&lt;span class="math">\(Γ\)&lt;/span>函数： &lt;span class="math">\[\gamma(s,x) = \int_0^x t^{s-1}\,e^{-t}\,{\rm d}t \quad \Re(s)&amp;gt;0, x\in\mathbb R_0^+\tag{39}\]&lt;/span>&lt;/p>
&lt;p>不完全&lt;span class="math">\(\Beta\)&lt;/span>函数是&lt;span class="math">\(\Beta\)&lt;/span>函数的一个推广，把&lt;span class="math">\(\Beta\)&lt;/span>函数中的定积分用不定积分来代替，需要指出的是不同于不完全&lt;span class="math">\(\Gamma\)&lt;/span>函数，不完全&lt;span class="math">\(\Beta\)&lt;/span>函数只会将上限改为不定积分。&lt;/p>
&lt;p>不完全&lt;span class="math">\(\Beta\)&lt;/span>函数： &lt;span class="math">\[
\Beta(x;\,a,b)=\int _{0}^{x}t^{{a-1}}\,(1-t)^{{b-1}}\,dt \quad x\in [0,1],a&amp;gt;0,b&amp;gt;0\tag{40}
\]&lt;/span> 当&lt;span class="math">\(x = 1\)&lt;/span>，上式即化为&lt;span class="math">\(\Beta\)&lt;/span>函数。&lt;/p>
&lt;h2 id="两类欧拉积分在概率论中的应用">两类欧拉积分在概率论中的应用&lt;/h2>
&lt;p>第二类欧拉积分在概率统计中频繁现身，众多的高阶统计分布，包括常见的统计学三大分布 (&lt;span class="math">\(t\)&lt;/span>分布，&lt;span class="math">\(χ^2\)&lt;/span>分布，&lt;span class="math">\(F\)&lt;/span>分布)、&lt;span class="math">\(\Beta\)&lt;/span>分布、狄利克雷分布的密度公式中都有&lt;span class="math">\(\Gamma\)&lt;/span>函数的身影。而第一类欧拉积分则和二项分布、伯努利分布有着密切关系，常作为贝叶斯统计中的先验分布。&lt;/p>
&lt;h3 id="gamma分布">Gamma分布&lt;/h3>
&lt;p>Gamma分布概率密度函数： &lt;span class="math">\[
p_{_\Gamma}(x|\alpha)=\frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}\tag{41}
\]&lt;/span>&lt;/p>
&lt;p>与&lt;span class="math">\(\Gamma\)&lt;/span>函数有最直接联系的概率分布当然是直接由&lt;span class="math">\(\Gamma\)&lt;/span>函数变换得到的&lt;span class="math">\(\Gamma\)&lt;/span>分布，实际上&lt;span class="math">\(\Gamma\)&lt;/span>分布就是借&lt;span class="math">\(\Gamma\)&lt;/span>函数作为归一化分母而得到的概率分布。如果做一个变换&lt;span class="math">\(x = βt\)&lt;/span>，就得到伽玛分布的更一般的形式 &lt;span class="math">\[
p_{_\Gamma}(t|\alpha,\beta)=\frac{\beta^{\alpha
}t^{\alpha-1}e^{-\beta t}}{\Gamma(\alpha)}\tag{42}
\]&lt;/span> &lt;span class="math">\(α\)&lt;/span>称为 shape parameter，主要决定了分布曲线的形状；&lt;span class="math">\(β\)&lt;/span>称为rate parameter，主要决定曲线有多陡。&lt;span class="math">\(θ=1/β\)&lt;/span>称为scale parameter，同样决定曲线有多陡。&lt;/p>
&lt;p>固定&lt;span class="math">\(α=4\)&lt;/span>,随着&lt;span class="math">\(β\)&lt;/span>（或&lt;span class="math">\(θ=1/β\)&lt;/span>）参数的增加，x轴的scale在减小，其分布相对就越窄。若放在同一个x轴上， 增加&lt;span class="math">\(β\)&lt;/span>将得到更陡的曲线 （相应地，y轴的scale增加）。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/gamma分布rate参数.gif" alt="gamma分布rate参数.gif" />&lt;p class="caption">gamma分布rate参数.gif&lt;/p>
&lt;/div>
&lt;p>固定&lt;span class="math">\(β=1\)&lt;/span>，&lt;span class="math">\(α\)&lt;/span>改变，曲线的形态发生改变。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/gamma分布shape参数.gif" alt="gamma分布shape参数.gif" />&lt;p class="caption">gamma分布shape参数.gif&lt;/p>
&lt;/div>
&lt;p>在这个形式下，Gamma分布期望&lt;span class="math">\(E(T)=\frac{\alpha}{\beta}\)&lt;/span>，方差&lt;span class="math">\(Var(T)=\frac{\alpha}{\beta^2}\)&lt;/span>。&lt;/p>
&lt;h3 id="beta分布">Beta分布&lt;/h3>
&lt;p>Beta分布概率密度函数： &lt;span class="math">\[
p_{B}=\frac{x^{\alpha-1}(1-x)^{\beta-1}} {\Beta(\alpha,\beta)}\!\tag{43}
\]&lt;/span> 其中，参数&lt;span class="math">\(\alpha,\beta&amp;gt;0\)&lt;/span>，定义域&lt;span class="math">\(x\in(0,1)\)&lt;/span>,分母&lt;span class="math">\(\Beta(\alpha,\beta)\)&lt;/span>为Beta函数。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Beta_distribution.png" alt="Beta_distribution.png" />&lt;p class="caption">Beta_distribution.png&lt;/p>
&lt;/div>
&lt;p>期望值和方差分别是：&lt;span class="math">\(\mu =\operatorname {E}(X)={\frac{\alpha }{\alpha +\beta }}\)&lt;/span>，&lt;span class="math">\(\operatorname {Var}(X)=\operatorname {E}(X-\mu )^{2}={\frac {\alpha \beta }{(\alpha +\beta )^{2}(\alpha +\beta +1)}}\)&lt;/span>。&lt;/p>
&lt;h3 id="gamma分布与一些常见分布的联系">Gamma分布与一些常见分布的联系&lt;/h3>
&lt;p>为了理解Gamma分布的广泛一般性，我们从分布的&lt;strong>可加性&lt;/strong>入手，从最简单的分布开始逐渐深入到Gamma分布。&lt;/p>
&lt;p>回想我们在研究最基础的伯努利分布时，将N个伯努利分布叠加后，形成了二项分布&lt;span class="math">\(B(N,p)\)&lt;/span>，换句话说二项分布就是独立同分布(i.i.d)的伯努利分布加出来的，因此多几个伯努力分布相加还是二项分布，只是二项分布的参数有所改变。这是二项分布可加性的来源。而泊松分布是二项分布在&lt;span class="math">\(\lambda=np\)&lt;/span>为定值时, &lt;span class="math">\(p\rightarrow 0, n\rightarrow\infty\)&lt;/span>的极限，因此本质也是一种二项分布，不难理解为什么泊松分布也有可加性了。此外，作为二项分布&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>的极限的正态分布，自然也是有可加性的，但是这已经拓展到连续分布了。（&lt;span class="math">\(e\)&lt;/span>是联系离散和连续的桥梁之一，这也是从离散的二项分布到正态分布表达式突然多出自然常数的一个暗示）。而其它具有可加性的离散分布，比如负二项分布是由N个独立同分布的几何分布加出来的，带有可加性也算是自然而然了。&lt;/p>
&lt;p>在连续分布中，也有一些分布是“加”出来的。最常见、应用最广泛的是&lt;strong>指数分布&lt;span class="math">\(X\thicksim \exp(\lambda)\)&lt;/span>的和&lt;/strong>，k个指数分布&lt;span class="math">\(X_i\thicksim \exp(\lambda),i\in\{1,2,\dotsb,k\}\)&lt;/span>相加是Erlang分布 &lt;span class="math">\[
X_{i}\sim \exp (\lambda ),\\
\sum_{i=1}^{k}{X_{i}}\sim \operatorname {Erlang} (k,\lambda )\Rightarrow p(x;k,\lambda )=\sum_{i=1}^{k}{X_{i}}\sim \operatorname {Erlang} (k,\lambda )\\
p(x;k,\lambda )={\lambda ^{k}x^{{k-1}}e^{{-\lambda x}} \over (k-1)!}\quad {\text{for }}x,\lambda \geq 0,k\geq 1\tag{44}
\]&lt;/span> 它和指数分布的参数&lt;span class="math">\(\lambda\)&lt;/span>是一样的，参数&lt;span class="math">\(k\)&lt;/span>是指由&lt;span class="math">\(k\)&lt;/span>个指数分布相加。其实际含义可以指代&lt;span class="math">\(k\)&lt;/span>个用户/物件到达所用的时间间隔等。显然，有指数分布加出来的Erlang分布也有可加性，无非就是多几个指数分布的和。而将&lt;span class="math">\(k\rightarrow \alpha\)&lt;/span>延拓到正实数域，就是Gamma分布&lt;span class="math">\(X\sim \Gamma(\alpha,\lambda)\)&lt;/span>。 &lt;span class="math">\[p(x;\alpha ,\lambda )={\frac {\lambda ^{\alpha }x^{\alpha -1}e^{-\lambda x}}{\Gamma (\alpha )}}\quad {\text{for }}x&amp;gt;0\quad \alpha ,\lambda &amp;gt;0\tag{45}\]&lt;/span> 其中,&lt;span class="math">\(\Gamma(\alpha)\)&lt;/span>是Gamma函数而卡方分布又是Gamma分布的特例，即&lt;span class="math">\(X\sim \chi^2(n)=\Gamma(\frac{n}{2},\frac{1}{2})\)&lt;/span>， &lt;span class="math">\[
p(x;n)={\frac {1}{2^{\frac {n}{2}}\Gamma(\frac{n}{2})}}x^{\frac {n}{2}-1}e^{\frac {-x}{2}}\tag{46}
\]&lt;/span> 因此这两种分布(Gamma分布、卡方分布)也不出意外的有可加性。实际上，这些分布都可以看成是Gamma分布在某种情况下的特例，&lt;strong>虽然说历史进程上是从常见分布到Gamma分布，但是Gamma分布确是其他常见分布的根本&lt;/strong>。&lt;/p>
&lt;p>Gamma分布的可加性的直接证明一般有两种思路，一是使用两个独立随机变量和的概率密度函数等于其概率密度函数的卷积；二是使用特征函数，独立随机变量的和等于特征函数的积。假设两个独立的服从Gamma分布的随机变量&lt;span class="math">\(X\sim p_{_X}(x;\alpha_1,\lambda),Y\sim p_{_Y}(x;\alpha_2,\lambda)\)&lt;/span>，现需证明&lt;span class="math">\(Z=X+Y\sim \Gamma(\alpha_1+\alpha_2,\lambda)\)&lt;/span>。&lt;/p>
&lt;p>&lt;strong>证明方法1&lt;/strong>：独立随机变量和等于概率密度函数卷积。由独立随机变量和的关系可知: &lt;span class="math">\[
\begin{aligned}
p_{_Z}(x)&amp;amp;=p_{_X}(x)*p_{_Y}(x)\\
&amp;amp;=\int_{-\infty}^{+\infty} p_{_X}(x-\tau)p_{_Y}(\tau)\mathrm{d}\tau
\end{aligned}\tag{47}
\]&lt;/span> 由于Gamma分布的概率密度函数在负数区域都为0，即&lt;span class="math">\(\tau\leq 0\)&lt;/span>时，&lt;span class="math">\(p_{_Y}(\tau)=0\)&lt;/span>，因此积分限 可以简化为&lt;span class="math">\((0,\infty)\)&lt;/span>，式（47）带入具体表达式有： &lt;span class="math">\[
\begin{aligned}
p_{_Z}(x)&amp;amp;=\int_{-\infty}^{+\infty} p_{_X}(x-\tau)p_{_Y}(\tau)\mathrm{d}\tau\\
&amp;amp;= \int_{0}^{+\infty} {\frac {\lambda ^{\alpha_1}(x-\tau)^{\alpha_1-1}e^{-\lambda (x-\tau)}}{\Gamma (\alpha_1)}}\times{\frac {\lambda ^{\alpha_2}\tau^{\alpha_2-1}e^{-\lambda\tau}}{\Gamma (\alpha_2)}}\mathrm{d}\tau\\
&amp;amp;=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}e^{-\lambda x}\int_{0}^{+\infty}(x-\tau)^{\alpha_1-1}\tau^{\alpha_2-1}\mathrm{d}\tau\\
&amp;amp;\overset{提取x}{=}\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}e^{-\lambda x}x^{\alpha_1+\alpha_2-2}\int_{0}^{+\infty}(1-\frac{\tau}{x})^{\alpha_1-1}(\frac{\tau}{x})^{\alpha_2-1}\mathrm{d}\tau\\
\end{aligned}
\tag{48}
\]&lt;/span> 我们令&lt;span class="math">\(\tau/x=t\)&lt;/span>，则有&lt;span class="math">\(\mathrm{d}\tau=x\mathrm{d}t\)&lt;/span>。同时由于&lt;span class="math">\(x&amp;gt;\tau&amp;gt;0\)&lt;/span>，所以&lt;span class="math">\(t=\frac{\tau}{x}\in(0,1)\)&lt;/span>。式（48）可写为 &lt;span class="math">\[
\begin{aligned}
p_{_Z}(x)&amp;amp;=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}e^{-\lambda x}x^{\alpha_1+\alpha_2-2}\int_{0}^{+\infty}(1-t)^{\alpha_1-1}t^{\alpha_2-1}x\mathrm{d}t\\
&amp;amp;=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}e^{-\lambda x}x^{\alpha_1+\alpha_2-1}\Beta(\alpha_1,\alpha_2)\\
&amp;amp;=\frac{\lambda^{\alpha_1+\alpha_2}x^{\alpha_1+\alpha_2-1}e^{-\lambda x}}{\Gamma(\alpha_1+\alpha_2)}=p(x;\alpha_1+\alpha_2,\lambda)
\end{aligned}\tag{49}
\]&lt;/span> 即服从&lt;span class="math">\(p_{_\Gamma}(x|\alpha_1+\alpha_2,\lambda)\)&lt;/span>，得证。&lt;/p>
&lt;p>&lt;strong>证明方法2&lt;/strong>：使用特征函数。可知Gamma分布的特征函数为&lt;span class="math">\(\phi_{_\Gamma}(t)=(1-it/\beta)^{-\alpha}\)&lt;/span>，特征函数有一个性质：&lt;/p>
&lt;blockquote>
&lt;p>若 &lt;span class="math">\(X_1,\ldots,X_n\)&lt;/span> 相互独立， &lt;span class="math">\(X_k\)&lt;/span> 特征函数为 &lt;span class="math">\(\phi_k(t)\)&lt;/span>，则 &lt;span class="math">\(Y=X_1+\cdots+X_n\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi_Y(t)=\phi_1(t)\cdots\phi_{k}(t)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>那么&lt;span class="math">\(X+Y\)&lt;/span>的特征函数应为&lt;span class="math">\(\phi_{_{X+Y}}(t)=(1-it/\lambda)^{-\alpha_1}(1-it/\lambda)^{-\alpha_2}=(1-it/\lambda)^{-(\alpha_1+\alpha_2)}\)&lt;/span>。再由特征函数的反演唯一性可得&lt;span class="math">\((1-it/\lambda)^{-(\alpha_1+\alpha_2)}\)&lt;/span>对应的概率密度函数为&lt;span class="math">\(p_{_\Gamma}(x|\alpha_1+\alpha_2,\lambda)\)&lt;/span>，得证。&lt;/p>
&lt;h2 id="附gamma函数beta函数关系以及其他函数">附：Gamma函数、Beta函数关系以及其他函数&lt;/h2>
&lt;h3 id="gamma函数与beta函数关系推导">Gamma函数与Beta函数关系推导&lt;/h3>
&lt;p>从文章开头的式(1)我们就给出了&lt;span class="math">\(\Gamma\)&lt;/span>函数和&lt;span class="math">\(\Beta\)&lt;/span>函数二者的关系，其具体推导过程如下： &lt;span class="math">\[
\begin{aligned}
\Gamma(x) \Gamma(y) &amp;amp;= \int_0^{+\infty} e^{-t} t^{x-1} \mathrm{d}t \int_0^{+\infty} e^{-s} s^{y-1} \mathrm{d}s \\
&amp;amp; = \int_0^{+\infty} \int_0^{+\infty} e^{-(s+t)} t^{x-1} s^{y-1} \mathrm{d}s \mathrm{d}t \\
&amp;amp;=4 \int_0^{+\infty} \int_0^{+\infty} e^{-(u^2+v^2)} u^{2x-2} v^{2y-2} \cdot uv \mathrm{d}u \mathrm{d}v \quad (t=u^2,s=v^2)\\
&amp;amp;= 4 \int_0^{+\infty} \int_0^{+\infty} e^{-(u^2+v^2)} u^{2x-1} v^{2y-1} \mathrm{d}u \mathrm{d}v \\
&amp;amp;= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-(u^2+v^2)} |u|^{2x-1}|v|^{2y-1} \mathrm{d}u \mathrm{d}v \\
&amp;amp;= \int_0^{+\infty} \int_0^{2\pi} r e^{-r^2} r^{2x-1} |\cos \theta|^{2x-1} r^{2y-1} |\sin \theta|^{2y-1} \mathrm{d}r \mathrm{d}\theta \quad (u=r\cos \theta,v=r\sin \theta) \\
&amp;amp;= \int_0^{+\infty} r e^{-r^2} r^{2x+2y-2} \mathrm{d}r \int_0^{2\pi} |\cos \theta|^{2x-1} |\sin \theta|^{2y-1} \mathrm{d} \theta \\
&amp;amp;= \frac{1}{2} \int_0^{+\infty} e^{-r^2} r^{2(x+y-1)} \mathrm{d}r^2 \int_0^{2\pi} |\cos \theta|^{2x-1} |\sin \theta|^{2y-1} \mathrm{d} \theta \\
&amp;amp;= \frac{1}{2} \Gamma(x+y) \int_0^{2\pi} |\cos \theta|^{2x-1} |\sin \theta|^{2y-1} \mathrm{d} \theta \\
&amp;amp;= \Gamma(x+y) \cdot 2\int_0^{\frac{\pi}{2}} \cos^{2x-1} \theta \sin^{2y-1} \theta \mathrm{d} \theta \\
&amp;amp;= \Gamma(x+y) \cdot 2 \int_0^1 t^{x-\frac{1}{2}} (1-t)^{y-\frac{1}{2}} \frac{1}{2} t^{-\frac{1}{2}} (1-t)^{-\frac{1}{2}} \mathrm{d}t \quad (t=\cos^2 \theta,\sin \theta =(1-t)^\frac{1}{2}, \mathrm{d}t = -2 t^{\frac{1}{2}} (1-t)^{\frac{1}{2}} \mathrm{d} \theta) \\
&amp;amp;= \Gamma(x+y) \int_0^1 t^{x-1} (1-t)^{y-1} \mathrm{d}t \\
&amp;amp;= \Gamma(x+y) \Beta (x,y).\\
&amp;amp;\Rightarrow \Beta(x,y)=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
\end{aligned}\tag{A1}
\]&lt;/span>&lt;/p>
&lt;h3 id="gamma函数与psi函数">Gamma函数与Psi函数&lt;/h3>
&lt;p>&lt;span class="math">\(\psi\)&lt;/span>函数又称双伽玛函数，是伽玛函数的对数导数，即 &lt;span class="math">\[
\psi (x)={\frac {d}{dx}}\ln {\Gamma (x)}={\frac {\Gamma &amp;#39;(x)}{\Gamma (x)}}\tag{A2}
\]&lt;/span> 由&lt;span class="math">\(\psi\)&lt;/span>函数可以推导出Stirling级数。&lt;/p>
&lt;h3 id="gamma函数与zeta函数">Gamma函数与zeta函数&lt;/h3>
&lt;p>黎曼泽塔函数，写作&lt;span class="math">\(ζ(z)\)&lt;/span>的定义如下：设一复数&lt;span class="math">\(z\)&lt;/span>使得&lt;span class="math">\(Re(z)&amp;gt;1\)&lt;/span>，则定义： &lt;span class="math">\[
\zeta(z)=\sum_{n=1}^\infty \frac{1}{n^z}\tag{A3}
\]&lt;/span> 它亦可以用积分定义： &lt;span class="math">\[
\zeta(z)=\frac{1}{\Gamma(z)}\int_{0}^{\infty} \frac{x^ {z-1}}{e^x-1} \mathrm{d}x\tag{A4}
\]&lt;/span> 黎曼的&lt;span class="math">\(ζ\)&lt;/span>函数被数学家认为主要和“最纯”的数学领域数论相关，黎曼函数最著名的就是黎曼猜想：&lt;/p>
&lt;blockquote>
&lt;p>黎曼猜想：黎曼&lt;span class="math">\(ζ\)&lt;/span>函数的非平凡零点（在此情况下是指&lt;span class="math">\(z\)&lt;/span>不为-2，-4，-6，...，等点的值）的实数部分是&lt;span class="math">\(\frac{1}{2}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>而&lt;span class="math">\(\Gamma\)&lt;/span>函数与&lt;span class="math">\(ζ\)&lt;/span>函数有如下关系： &lt;span class="math">\[
\pi ^{-{\frac {z}{2}}}\;\Gamma \left({\frac {z}{2}}\right)\zeta (z)=\pi ^{-{\frac {1-z}{2}}}\;\Gamma \left({\frac {1-z}{2}}\right)\;\zeta (1-z)\tag{A5}
\]&lt;/span>&lt;/p></description></item><item><title>概率统计随机过程之极简贝叶斯统计总结</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9E%81%E7%AE%80%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</link><pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9E%81%E7%AE%80%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</guid><description>
&lt;h2 id="概率统计随机过程之极简贝叶斯统计总结">概率统计随机过程之极简贝叶斯统计总结&lt;!-- omit in toc -->&lt;/h2>
&lt;p>本篇笔记是小岛宽之的书《统计学关我什么事》的阅读笔记总结。本书大体分为两个部分，第一部分是对贝叶斯统计学本质的形象介绍，深入浅出，对我理解贝叶斯统计推断的帮助很大。第二部分，作者用较为简略的数学语言介绍贝叶斯统计中常用的数学工具如先验共轭分布Beta分布、正态分布、条件概率等，对有一定统计学基础的人而言意义不大。本篇笔记主要总结第一部分。&lt;/p>
&lt;h3 id="一">一&lt;/h3>
&lt;p>统计学分为两大分支，一个是以内曼-皮尔逊范式为主导的频率学派，关键人物有内曼、皮尔逊父子以及费希尔；另一个则是以贝叶斯范式为主的贝叶斯学派，关键人物有贝叶斯、拉普拉斯、林德利以及萨维奇。我们过去学习的统计学以频率学派的概率为主，比如点估计、区间估计、假设检验等，而贝叶斯统计则在近段时间随着机器学习，计算机领域的快速的发展得到越来越多的重视。&lt;/p>
&lt;p>贝叶斯统计的主观性和思想性才是贝叶斯统计的本质和便利性所在。&lt;/p>
&lt;p>信息增加导致概率（分布）变化是贝叶斯推理的基本方法。其步骤如下：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>通过经验设定先验概率。信息指附加的状况，无任何信息时的概率分布就是先验概率&lt;/li>
&lt;li>构建在某种情况下的条件概率，这需要额外的信息或数据来支撑构建。&lt;/li>
&lt;li>通过观察出现的行为或现象（即信息），排除“不可能的情况”&lt;/li>
&lt;li>归一化剩下的情况的概率，即逆概率——后验概率。&lt;/li>
&lt;/ol>
&lt;p>总结：先验概率+条件概率+信息=后验概率&lt;/p>
&lt;p>贝叶斯统计推断中的“逆”是指由结果&lt;span class="math">\(\rightarrow\)&lt;/span>原因。比如事件有一定出现概率，但是我们不确定到底是什么事件，我们能通过事件所引发的现象（果）来推测事件到底是什么（因）。虽然多个事件可能引起相同的现象，但不同事件引起同一现象的概率不同，因此引起的现象可以作为信息，来帮助我们推测。贝叶斯统计就是通过信息增加，不断改进推测的方法，也是其被称为执果索因的来历。&lt;/p>
&lt;h3 id="二">二&lt;/h3>
&lt;p>贝叶斯推理有时候反直觉，主要是因为先验概率和条件概率二者容易被混淆。典型的例子就是一个很少见的病，如果被医学检测出来是阳性，检测的正确性就算是99%，实际上患病的概率也不高（至少不会是99%这么高）。这是因为条件概率虽然很大，但是先验概率很小。不过这种检测并不是无用，即使真正患病的概率依旧不高，但是相比原来先验概率还是有很大变化，他们的变化体现在概率上，即先验概率和后验概率的变化。&lt;/p>
&lt;h3 id="三">三&lt;/h3>
&lt;p>贝叶斯统计也可以根据主观数字进行推理。主观概率：描述心理感受或信念程度的方式，区别于客观概率，主观概率多是心理的描述。&lt;/p>
&lt;blockquote>
&lt;p>理由不充分原理：没有任何先验信息时，先验概率假定为&lt;strong>均匀分布&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>概率的概率：概率模型参数的概率，就是说概率模型的参数也是一个概率，二者组合的一起会得到一些神奇的概率分布，比如beta分布等。&lt;/p>
&lt;h3 id="四">四&lt;/h3>
&lt;p>再回到频率学派的内曼-皮尔逊范式统计推理，其核心工具假设检验，有零假设、备择假设，显著性水平。零假设只能用来拒绝，不能用来肯定。我们只能说不能拒绝零假设，而不能说我们能证明零假设是对的。而当我们拒绝零假设时，只能接受互斥的备择假设，反而能证明备择假设是真的。其核心时小概率事件很少发生这一事实，小概率体现在显著性水平这一概念上，有设置为1%，5%的，其实这也是一种&lt;strong>主观设置&lt;/strong>，没有严肃的数学推理，这也暗示着具有主观性的先验概率和它并没有完全分离的界限。&lt;/p>
&lt;h3 id="五">五&lt;/h3>
&lt;p>如果从贝叶斯角度看假设检验，那么就是先验概率&lt;span class="math">\(\rightarrow\)&lt;/span>后验概率的变化，在选择时，贝叶斯推断并不给出最终结果，而是给出结果的概率，因此无论在何种条件下，贝叶斯推断都能得出一个暂时的结果。内曼-皮尔逊范式和贝叶斯范式区别在于前者风险（或错误率）体现在显著性水平，而且这个正确率效果时基于大量结果的统计均值，并不针对当下情况，是一概而论的感觉。而后者风险（错误率）则体现在后验概率中，因为每一个选择都是一定的概率（分布），这种概率（分布）会随着信息的累加而变化，并不是固定的数值。&lt;/p>
&lt;h3 id="六">六&lt;/h3>
&lt;p>贝叶斯范式和内曼-皮尔逊范式的衔接点是：&lt;strong>最大似然估计（又叫极大似然估计）&lt;/strong>。其思路选择的是使结果概率最大的模型（即原因）本质就是贝叶斯式的思维方式。而从频率学派角度来看，则是使似然函数最大的选值。&lt;/p>
&lt;h3 id="七">七&lt;/h3>
&lt;p>贝叶斯的悖论：猜汽车/蒙提霍尔问题。信息影响的是信息影响下事件的概率，而非所有的概率。这点很难理解，也是贝叶斯思维方式的难点，推荐看书籍原文。令人吃惊的，如果理解这个理论，你会发现这是从贝叶斯角度给“多听长辈话”这一古老谚语的严谨数学解释。&lt;/p>
&lt;h3 id="八">八&lt;/h3>
&lt;p>掌握多条信息是的推理，需要一个前提是独立实验，应用概率的乘法公式。比如垃圾邮件分类器。同时我们会发现多个独立的信息可以逐步进行贝叶斯推断结果和直接使用所有信息的效果是一样的，这就是贝叶斯推断的“序贯性”。这样我们就可以不用大量存储所有数据，而是小步迭代，逐步精确。能这么做的核心原因是：&lt;strong>之前的信息在先验概率更新为后验概率的步骤中，保存到了后验概率中&lt;/strong>。因此后验概率是先验概率和信息的集中体现。这在计算机理论尤其是机器学习中有着方便、广泛的应用。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/贝叶斯概率更新.jpg" alt="贝叶斯概率更新" />&lt;p class="caption">贝叶斯概率更新&lt;/p>
&lt;/div>
&lt;h3 id="九">九&lt;/h3>
&lt;p>Beta分布基本形式 &lt;span class="math">\[
Beta(x)=C\times x^{\alpha-1}\times(1-x)^{\beta-1},x\in [0,1],\alpha\geq 1,\beta\geq 1
\]&lt;/span> 其中，&lt;span class="math">\(C\)&lt;/span>是标准化（归一化）系数，&lt;span class="math">\(\alpha,\beta\)&lt;/span>分别是两种事件发生次数的相关量，其期望是： &lt;span class="math">\[
E(x)=\frac{\alpha}{\alpha+\beta}
\]&lt;/span>&lt;/p>
&lt;p>由于归一化时，对&lt;span class="math">\(x^{\alpha-1}\times(1-x)^{\beta-1}\)&lt;/span>的积分用到了Gamma函数来计算，因此常数&lt;span class="math">\(C\)&lt;/span>中会有Gamma函数。Beta分布是伯努利分布/二项分布的共轭先验分布，此外正态分布是正态分布的共轭先验分布。&lt;/p>
&lt;blockquote>
&lt;p>共轭分布：在贝叶斯统计中，如果后验分布与先验分布属于同类（分布形式相同），则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。&lt;/p>
&lt;/blockquote>
&lt;p>共轭先验的好处主要在于代数上的方便性，可以直接给出后验分布的封闭形式，否则的话只能数值计算。共轭先验也有助于获得关于似然函数如何更新先验分布的直观印象。一个比较好的结论是所有指数家族的分布都有共轭先验。&lt;/p></description></item><item><title>概率统计随机过程之统计实验设计</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/</link><pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/</guid><description>
&lt;h2 id="概率统计随机过程之统计实验设计">概率统计随机过程之统计实验设计&lt;!-- omit in toc -->&lt;/h2>
&lt;p>实验是现代科学中不可缺少的一环，随着科学的发展，科学实验的成本也越来越高，例如现在大型强子对撞机的建设成本需要几十亿，甚至上百亿美元。此外，合理的实验也能让我们减少实验误差，提高数据精度。因此每次我们进行实验前都需要合理的设计实验（Design of Experiments）。目前，设计实验是数理统计学的一个分支，科学探究的一部分，涉及“用何方法可更好的设计一个实验”，属于方法论的范畴。由于任何实验都会受到外来环境影响，如何设计实验，使外来环境的变化能够对实验造成最小的影响，就是实验规划的目的。实验设计法广泛用于自然科学、社会科学、医学等各学科的实验设计里。&lt;/p>
&lt;p>经常使用的实验设计方法有完全随机设计、随机区组设计、交叉设计、析因设计、拉丁方设计、正交设计、嵌套设计、重复测量设计、裂区设计以及均匀设计等。不同的实验设计方法适用不同的情况。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#实验设计总述">实验设计总述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单因素实验设计">单因素实验设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#完全随机设计">完全随机设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#配对实验设计">配对实验设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#随机区组设计">随机区组设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#双多因素实验设计">双/多因素实验设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#交叉设计">交叉设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉丁方设计">拉丁方设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#析因设计">析因设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正交设计">正交设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均匀设计">均匀设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特殊实验设计方法">特殊实验设计方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#嵌套设计">嵌套设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量设计">重复测量设计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#裂区设计">裂区设计&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="实验设计总述">实验设计总述&lt;/h2>
&lt;p>实验设计原则：随机、对照、可重复、区域化（即尽量保证其他影响因素一致）。&lt;/p>
&lt;p>因素：实验中会影响最终结果的变量。&lt;/p>
&lt;p>水平：实验中一个因素所能取的值。不一定是数字，也可包含符号、文本性描述。因为实验中我们不能无限地取值，只能选取有代表性的值，因此水平必须是有限个。&lt;/p>
&lt;p>处理组：接受因素不同水平处理的实验组。比如在医学实验中，处理组A使用新的药剂，处理组B接受安慰剂。因素为用药，该因素水平为：用新药和用安慰剂两种。&lt;/p>
&lt;p>根据考虑变化因素对实验结果的影响，可以分为单因素实验设计、多因素实验设计。&lt;/p>
&lt;h2 id="单因素实验设计">单因素实验设计&lt;/h2>
&lt;p>当我们只考虑单个因素的影响时，可采用的设计方式。&lt;/p>
&lt;h3 id="完全随机设计">完全随机设计&lt;/h3>
&lt;p>完全随机设计只涉及一个处理因素的两个或多个水平，所以也称单因素设计。它的核心就是将样本中的&lt;strong>样品随机地分配&lt;/strong>到各处理组，分别接受不同的处理，然后得出实验结论。&lt;/p>
&lt;p>按照各处理组样本容量是否大体相等（个别处理组多或少一个样品，问题也不大）可分为&lt;strong>平衡设计&lt;/strong>：各组样本含量相等 （样本含量相等时检验效率较高）和&lt;strong>非平衡设计&lt;/strong>：各组样本含量不相等。&lt;/p>
&lt;p>由于分的处理组数和因素的水平个数相同，当水平数为2时为&lt;strong>二组比较的完全随机设计&lt;/strong>，当水平数有三个及以上时为&lt;strong>多组比较的完全随机设计&lt;/strong>。通常多组比较的实验比两组比较的实验要复杂一些，数据使用的检验方法也不一样。&lt;/p>
&lt;p>完全随时实验是实验设计最基础的方式，也体现了实验设计的几大原则。不过其缺点也不少例如样品个体客观上是存在个体差异的，这会导致组间样品的不平衡，尤其是在小样本场景下；其次，这种完全随机设计效率不算高；并且只能分析单因素。&lt;/p>
&lt;h3 id="配对实验设计">配对实验设计&lt;/h3>
&lt;p>配对实验设计的核心思想是&lt;strong>控制变量&lt;/strong>。&lt;/p>
&lt;p>配对设计是将受试对象按配对条件配成对子，&lt;strong>每对中的个体接受不同的处理&lt;/strong>。&lt;strong>配对设计的最主要动因是排除非考察因素的干扰&lt;/strong>，因此我们需要将非常接近的两个样品配成一对，然后每一对的两个个体分别接受不同的处理。比如动物实验中，常将同性别、同窝别、体重相近的两个动物配成一对，然后将每一对的样品分别进行两种因素水平的处理。&lt;/p>
&lt;p>配对实验的优点就是组间均衡性好，由于人为的控制了非处理因素的干扰，组间误差小需要的例数少，提高了检验效率。但是欠妥的配对方案会导致实验出现难以察觉的错误。&lt;/p>
&lt;h3 id="随机区组设计">随机区组设计&lt;/h3>
&lt;p>随机区组设计是配对实验设计的推广，因为配对实验设计只能处理因素的两个水平，而区组设计是针对因素多个水平而提出的。&lt;/p>
&lt;p>其处理手法和配对实验设计是共通的，首先是将样品按照性质分成N组，组内的样品性质要求接近，组件样品是有性质差异的，例如如病人的性别、年龄、体重和病情等非实验因素差异分成N个区组。然后将组内样品随机分配到同一因素不同水平的处理组。&lt;/p>
&lt;p>其优点和配对实验一样：每个区组内的受试对象有较好的同质性，排除了非实验因素对分析结果的影响，提高了分析效率。但是，这样也要求每个处理组至少分到一个受试对象，实验结果中若有数据缺失，统计分析较麻烦。&lt;/p>
&lt;h2 id="双多因素实验设计">双/多因素实验设计&lt;/h2>
&lt;p>当我们考虑的因素增多时，不仅要考虑每个因素对实验结果的影响，还要考虑存在因素之间的交互作用。&lt;/p>
&lt;p>对于无相互影响的多因素实验，有交叉设计和拉丁方设计；适用于有交互作用的实验设计方法有析因设计、正交设计和均匀设计。&lt;/p>
&lt;h3 id="交叉设计">交叉设计&lt;/h3>
&lt;p>交叉设计是一种特殊的自身对照设计，常用在临床试验中，在同一病人身上观察两种或多种处理水平的效应，消除不同病人之间的差异，减少误差。我们以两个阶段、两种处理水平为例说明操作步骤。首先将条件相近的观察对象进行配对，随机分配到两个实验组中。第一组先用处理方法A处理，然后再用处理方法B处理，处理顺序是AB；另一组则相反，先用处理方法B处理，再用处理方法A处理，处理顺序是BA。两种处理水平在全部实验过程中“交叉”进行。&lt;/p>
&lt;p>使用注意：交叉设计的&lt;strong>两个实验阶段之间需要留出足够的“衰减时间”&lt;/strong>。为了减除前一实验阶段对后一实验阶段的影响，需要设置“衰减时间”。如果是新药实验，那么衰减时间可以根据药物半衰期确定。&lt;/p>
&lt;p>交叉设计实际上就是自身对照实验设计，通过“交叉”的方式将时间因素的影响分解出来，避免了时间因素对研究结果的干扰。因此该设计的最大优点是可控制时间因素及个体差异对处理方式的影响，故节约样本含量，效率较高。但是交叉设计一般只用于两个因素之间的比较，如果因素多的话，就会因时间太长产生更多不可控的变量。&lt;/p>
&lt;h3 id="拉丁方设计">拉丁方设计&lt;/h3>
&lt;p>拉丁方设计用于&lt;strong>研究三个因素，各因素间无交互作用且每个因素的水平数相同的情况&lt;/strong>。其中有一个最重要的因素称之为处理因素，另外两个是需要加以控制的因素。此外，拉丁方设计还要求处理具有方差齐性。&lt;/p>
&lt;p>因素之间没有交互作用的情况毕竟是少数，更多的情况是存在因素之间的交互作用。下面几种实验设计方法适用于有交互作用的情况。&lt;/p>
&lt;h3 id="析因设计">析因设计&lt;/h3>
&lt;p>析因设计是单因素完全随机实验在多因素场景下的推广，其需要将两个或两个以上因素及其各种水平进行排列组合、交叉分组的试验设计。它可以研究单个因素多个水平的效应，也可以研究因素之间是否有交互作用，同时找到最佳组合。&lt;/p>
&lt;p>例如，现在有两个处理因素，一个因素有2个水平，另一个因素有三个水平，那么就进行&lt;span class="math">\(2\times 3=6\)&lt;/span>次实验；如果有三个处理因素，每个因素都有5个处理水平，那么就进行&lt;span class="math">\(5×5×5=125\)&lt;/span>次实验。析因分析的原理就是对每个因素的每个水平都进行实验，这样能够照顾到所有的因素和水平。&lt;/p>
&lt;p>显然析因设计是非常消耗资源的设计，需要进行大量实验，当因素或水平增加时，实验次数需要几何倍数增长，这在很多实验中是难以实施的。&lt;/p>
&lt;h3 id="正交设计">正交设计&lt;/h3>
&lt;p>为了降低析因设计的试验次数，后人提出了正交设计。&lt;strong>正交设计是析因设计的高效化&lt;/strong>。当析因设计要求的实验次数太多时，一个非常自然的想法就是从析因设计的水平组合中，选择一部分有代表性水平组合进行试验，而正交设计就能满足这个要求。&lt;/p>
&lt;p>正交试验设计一般包括以下几步：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>确定研究因素和指标水平；&lt;/li>
&lt;li>制作成正交试验表格；&lt;/li>
&lt;li>实施试验；&lt;/li>
&lt;li>试验结果分析&lt;/li>
&lt;/ol>
&lt;p>其中制作正交试验表格是关键步骤，也是正交设计的核心所在，不过我看网上大多数教程都是用软件或者直接查的正交表。&lt;/p>
&lt;h3 id="均匀设计">均匀设计&lt;/h3>
&lt;p>均匀设计是一种多因素多水平的试验设计，它放弃了正交表的整齐可比性，是在正交设计的基础上进一步发展而成的。均匀设计进一步提高了试验点的“均匀分散性”。均匀设计的最大优点是可以使因素的水平数很大，而试验次数又最节省。与正交设计一样，可以通过均匀设计表设计实验。&lt;/p>
&lt;p>正交设计和均匀设计都是设计实验需要系统学习的方法，这里我们不再赘述。&lt;/p>
&lt;h2 id="特殊实验设计方法">特殊实验设计方法&lt;/h2>
&lt;h3 id="嵌套设计">嵌套设计&lt;/h3>
&lt;p>如果处理因素之间存在层次性结构，或处理因素之间有主次之分，这时就需要用到嵌套设计。例如，研究催化剂和温度两个处理因素对化学反应速度的影响就是典型的例子。&lt;/p>
&lt;h3 id="重复测量设计">重复测量设计&lt;/h3>
&lt;p>重复测量设计广泛应用于各种科学研究中，它的显著特点就是在&lt;strong>不同的实验条件下，从同一个受试对象身上采集到多个数据，也就是同一个受试者在不同实验条件下进行数次实验&lt;/strong>，以获得更多信息。这里的数次实验需要考虑的就是“时间因素”。最常见的重复测量设计是在药物的临床试验中，例如，比较两种不同药物的疗效，将病人随机分成两组，分别给予不同的药物，然后在不同时间作病人的动态观察。&lt;/p>
&lt;h3 id="裂区设计">裂区设计&lt;/h3>
&lt;p>裂区试验设计（split-plot experiment design）：又称为分割试验设计，把一个或多个完全随机设计、随机区组设计或拉丁方设计结合起来的试验方法。其原理为先将受试对象作一级实验单位，再分为二级实验单位，分别施以不同的处理。实验单位分级是指当实验单位具有隶属关系时，高级实验单位包含低级实验单位。如小鼠接种不同的瘤株后，观察不同浓度的某注射液的抑瘤效果，这时接种瘤株的小鼠为一级单位，相应因素为一级处理，注射浓度为二级单位，相应因素为二级处理。当试验单位不存在明显的隶属关系时，实验单位分级可按因素的主次确定。在裂区试验中一级处理与一级单位混杂，而二级处理与二级单位不混杂。因此，设计时将最感兴趣或最主要的因素，差异较小、要求精度较高、试验条件较少、工序较易改变的因素作为二级因素。&lt;/p></description></item><item><title>概率统计随机过程之贝叶斯统计推断</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD/</link><pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD/</guid><description>
&lt;h2 id="概率统计随机过程之贝叶斯统计推断">概率统计随机过程之贝叶斯统计推断&lt;!-- omit in toc -->&lt;/h2></description></item><item><title>概率统计随机过程之Fisher信息</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8Bfisher%E4%BF%A1%E6%81%AF/</link><pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8Bfisher%E4%BF%A1%E6%81%AF/</guid><description>
&lt;ul>
&lt;li>&lt;a href="#理解fisher信息">理解Fisher信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="#故事的开始从最大似然估计开始">故事的开始：从最大似然估计开始&lt;/a>&lt;/li>
&lt;li>&lt;a href="#纠结方差与fisher信息">纠结：方差与Fisher信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mle的一个准严格推导">MLE的一个准严格推导&lt;/a>&lt;/li>
&lt;li>&lt;a href="#fisher信息的应用">Fisher信息的应用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#频率学派角度应用fisher信息">频率学派角度应用Fisher信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯学派角度应用fisher信息">贝叶斯学派角度应用Fisher信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小描述长度角度应用fisher信息">最小描述长度角度应用Fisher信息&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概率统计随机过程之fisher信息">概率统计随机过程之Fisher信息&lt;!-- omit in toc -->&lt;/h2>
&lt;p>Fisher信息是参数估计中的一个重要概念，它揭示了样本能够提供多少信息来给我们估计参数，并决定参数估计的精度，Fisher信息大小和样本容量、总体的概率分布、采用的估计方式都有关系。我们通过Fisher信息和Cramer-Rao界也可以确定参数估计的理论极限。Fisher信息是由著名的&lt;strong>频率学派大佬，Ronald Fisher&lt;/strong>提出并推广，Fisher是一个统计学历史上非常重要的学者，几乎以一己之力构建了现代统计学框架，一些常见的统计学名词如统计显著性，P值，线性判别分析（LDA），最大似然估计，F分布，充分统计量，Fisher信息，方差分析（ANOVA）都是由Fisher发明或推广，同时也对贝叶斯学派的观点提出诸多批判。本篇笔记旨在系统的整理有关Fisher信息的相关内容，并结合文章《A Tutorial on Fisher Information》，总结Fisher信息在频率统计学派，贝叶斯学派以及最小描述长度模型中的应用。&lt;/p>
&lt;p>注：本文前面部分默认Fisher信息的参数估计都是一维参数估计。对于多维参数估计，我们专门设置了Fisher信息矩阵的章节。&lt;/p>
&lt;h2 id="理解fisher信息">理解Fisher信息&lt;/h2>
&lt;p>为了深刻理解Fisher信息的意义，我们设计了&lt;strong>一个多次反转的叙述结构来初步介绍Fisher信息&lt;/strong>。&lt;/p>
&lt;p>在参数估计中，我们要做的&lt;strong>核心任务是已知一个含有未知参数&lt;span class="math">\(\theta\)&lt;/span>的模型，我们通过实际观测到数据来推断这个未知的参数&lt;span class="math">\(\theta\)&lt;/span>&lt;/strong>。对于观测到数据集&lt;span class="math">\(X\)&lt;/span>，我们既可以直接使用整个观测集来做参数的估计，也可以从数据集中提取一些关键信息来进行参数估计。这些提取出来的信息，被称为统计量&lt;span class="math">\(T(X)\)&lt;/span>，关于统计量的具体介绍，可以参考笔记&lt;a href="概率统计随机过程之数理统计常用概念.md">概率统计随机过程之数理统计常用概念&lt;/a>中充分统计量章节的内容。&lt;/p>
&lt;p>简单地说，统计量是由观测数据构造出来的函数&lt;span class="math">\(T=f(X)\)&lt;/span>，其&lt;strong>包含并提纯&lt;/strong>了有关未知参数&lt;span class="math">\(\theta\)&lt;/span>的信息。注意该函数&lt;span class="math">\(f(X)\)&lt;/span>中并不能有其他未知的参数，也就是说只要给定观测集合&lt;span class="math">\(X\)&lt;/span>，那么统计量的值&lt;span class="math">\(T\)&lt;/span>就是一个定值。如果该统计量&lt;span class="math">\(T\)&lt;/span>完全包含了观测数据集&lt;span class="math">\(X\)&lt;/span>中所有有关未知参数&lt;span class="math">\(\theta\)&lt;/span>的信息，则称为完全统计量（Sufficient Statistics），即&lt;span class="math">\(I(T;\theta)=I(X;\theta)\)&lt;/span>。&lt;/p>
&lt;p>无论是原始的数据集&lt;span class="math">\(X\)&lt;/span>，还是有数据集构造的统计量&lt;span class="math">\(T(X)\)&lt;/span>，其进行参数估计的理论根源是一样的：抽样的数据中含有关于&lt;strong>未知参数&lt;span class="math">\(\theta\)&lt;/span>的信息&lt;/strong>，这是所有参数估计方法的基石。但是，原始数据集&lt;span class="math">\(X\)&lt;/span>和带估计参数&lt;span class="math">\(\theta\)&lt;/span>之间的关系是模糊的，需要我们逐渐求证的，因此能够建立数据集和带估计参数之间联系的&lt;strong>参数估计方法&lt;/strong>就是非常关键的环节。本质上，参数估计方法也是通过数据集合&lt;span class="math">\(X\)&lt;/span>来构建统计量&lt;span class="math">\(T\)&lt;/span>，只不过这个统计量需要与带估计参数&lt;span class="math">\(\theta\)&lt;/span>有更加直观的联系。目前使用的最广泛的估计方法——&lt;strong>极大似然估计&lt;/strong>当然也不例外。实际上，在极大似然估计的处理流程，如构造似然函数、取&lt;span class="math">\(ln\)&lt;/span>、求偏导的过程，就是构建最大似然统计量&lt;span class="math">\(T=f(X)\)&lt;/span>的过程。&lt;/p>
&lt;p>那么，我们应该如何确定数据集&lt;span class="math">\(X\)&lt;/span>或构造的统计量&lt;span class="math">\(T=f(X)\)&lt;/span>中含有多少关于带估计参数&lt;span class="math">\(\theta\)&lt;/span>的信息呢？这就是Fisher信息的由来的内生动力。&lt;/p>
&lt;h3 id="故事的开始从最大似然估计开始">故事的开始：从最大似然估计开始&lt;/h3>
&lt;p>在参数估计中，使用的最广泛的方式就是最大似然估计（maximum likelihood estimation, MLE），而Fisher正是MLE最有力的推动者，没有之一。在Fisher所在的年代，大家都想为模型的参数估计提出一套牢靠可行的方法，Fisher从1912年到1922年发表了多篇文章系统性的阐述并推广了最大似然估计方法，并对最大似然估计的性能进行分析，而&lt;strong>Fisher信息正可用来衡量最大似然估计&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>TIPS：有趣的是，Fisher一直尝试严谨地证明最大似然估计理论，但是一直没有成功，直到1938年统计学家 Samuel S. Wilks最终给出了最大似然估计的证明结果，被称为Wilks定理，大概内容是多个独立观测值的估计的对数似然比的误差是渐近卡方分布的。其中，唯一比较难的证明步骤取决于Fisher信息矩阵的期望值，但是这个值正好已经被Fisher的另一个定理证明了，Fisher就这样可惜地错过了严谨地证明最大似然估计理论。&lt;/p>
&lt;/blockquote>
&lt;p>最大似然估计的严谨证明费了一般波折，但是其核心思想非常的直观，&lt;strong>就是如果一组事件发生了，那么我们就找出最容易使这些事件发生的概率模型&lt;/strong>。比如，我们抛了一个未知的硬币（可能是不均匀的），如果抛了10次，7次正面，3次反面，那我们就直观地认为该硬币得到正面的概率是0.7，所以说极大似然估计可能是最符合直觉的估计方法之一(虽然不一定是正确的，但是在抽样次数足够大时，能基本收敛到正确结果)。具体关于极大似然估计的内容可以参考笔记&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>，其精简步骤总结如下：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>写出总体的概率质量/密度函数（PMF/PDF）&lt;span class="math">\(p(x;\theta)\)&lt;/span>。&lt;/li>
&lt;li>根据采样的数据&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>写出似然估计函数&lt;span class="math">\(L(\theta)=\prod_{i=1}^n p(x_i;\theta)\)&lt;/span>，其中&lt;span class="math">\(\theta\)&lt;/span>是待估计参数。&lt;/li>
&lt;li>两边取自然对数&lt;span class="math">\(\ln\)&lt;/span>，即为&lt;span class="math">\(l(\theta)=\ln(L(\theta))=\sum_{i=1}^n \ln p(x_i;\theta)\)&lt;/span>。(注意，对数似然函数是小写&lt;span class="math">\(l\)&lt;/span>)&lt;/li>
&lt;li>对&lt;span class="math">\(l(\theta)\)&lt;/span>的&lt;span class="math">\(\theta\)&lt;/span>求导（多参数估计就是偏导），&lt;span class="math">\(\frac{\partial l(\theta)}{\partial\theta}=\sum_{i=1}^n \frac{\partial \ln(p(x_i;\theta))}{\partial \theta}\)&lt;/span>，令其导数为0。&lt;/li>
&lt;li>求出使导数为0的&lt;span class="math">\(\theta\)&lt;/span>，即为最大似然估计参数&lt;span class="math">\(\hat\theta_{MLE}=\arg\limits_{\theta}\{\sum_{i=1}^n \frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=0\}\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>TIPS：对于有些分布的极大似然估计没法直接求，比如均匀分布。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>似然函数取对数的原因&lt;/strong>:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>减少计算量。乘法变成加法，从而减少了计算量；同时，如果概率中含有指数项，如高斯分布，能把指数项也化为求和形式，进一步减少计算量；另外，在对联合概率求导时，和的形式会比积的形式更方便。&lt;/li>
&lt;li>计算时更准确。为概率值都在[0,1]之间，因此，概率的连乘将会变成一个很小的值，可能会引起浮点数下溢，尤其是当数据集很大的时候，联合概率会趋向于0，非常不利于之后的计算。&lt;/li>
&lt;li>取对数后，可以是一个上凸函数，更有利于求取最大值。&lt;/li>
&lt;/ol>
&lt;p>那么最大似然估计的估计结果有多准确呢？我们可以借助“估计值&lt;span class="math">\(\plusmn\)&lt;/span>一个范围”来描述，比如，&lt;span class="math">\(\hat\theta_{MLE}\plusmn \sigma\)&lt;/span>，如果我们要求一个比较高的概率能用这个范围覆盖住真实值&lt;span class="math">\(\theta_0\)&lt;/span>，同时&lt;span class="math">\(\sigma\)&lt;/span>的比较小，我们就认为估计是比较准确的。&lt;strong>如果估计的准确，我们也有理由认为估计时候信息比较充分，信息量比较多&lt;/strong>。不难发现，这里其实使用了区间估计的思想。&lt;/p>
&lt;p>至此，我们确定一个目标：&lt;strong>将最大似然估计的准确性和信息的定义联系起来，信息含量越大，估计的准确性越高。&lt;/strong>&lt;/p>
&lt;p>但是，我们现在并不知道&lt;span class="math">\(\hat{\theta}_{MLE}\)&lt;/span>的具体分布是什么，不过我们通过采样的样本，我们可以用样本的均值和方差近似 总体的样本和方差。知道了样本、方差再通过某些概率不等式，我们能够计算出一些区间估计的界（bound），比如最常见的切比雪夫不等式，即我们可以借助切比雪夫不等式，只通过期望和方差就能确定一个大致的估计准确性的范围。&lt;/p>
&lt;blockquote>
&lt;p>切比雪夫不等式：对&lt;strong>任何分布形状的数据都适用&lt;/strong>。可表示为：对于任意随机变量&lt;span class="math">\(T\)&lt;/span>，当给定某一&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>，有： &lt;span class="math">\[P(|T-E(T)|\geqslant \varepsilon)\leq {\frac {Var(T)}{\varepsilon^{2}}}\tag{1}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>注意，这里的随机变量&lt;span class="math">\(T\)&lt;/span>可以是由样本集&lt;span class="math">\(X\)&lt;/span>构造出来的任意随机变量，即&lt;span class="math">\(T\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>的某种统计量。&lt;/p>
&lt;p>使用切比雪夫不等式的最大优势就是前置要求少，并且几乎对任何概率分布都适用。从切比雪夫不等式我们可以发现，如果MLE是无偏估计的话（&lt;span class="math">\(E(T)=\theta_0\)&lt;/span>），我们可以用方差将估计误差&lt;span class="math">\(\varepsilon\)&lt;/span>限定在&lt;span class="math">\(\frac {Var(T)}{\varepsilon^{2}}\)&lt;/span>范围内。特别地，我们如果人为地取&lt;span class="math">\(\varepsilon=k\sqrt{Var(T)}=k\sigma\)&lt;/span>，那么有 &lt;span class="math">\[
P(|T-E(T)|\geqslant k\sigma)\leq {\frac {1}{k^2}}\tag{2}
\]&lt;/span>&lt;/p>
&lt;h3 id="纠结方差与fisher信息">纠结：方差与Fisher信息&lt;/h3>
&lt;p>从上文可知，&lt;strong>直观地想，信息量越大，估计的越准确，这是大多数人的直觉，而且我可以告诉，这个直觉是对的&lt;/strong>。我们回看式(1)(2)，在无偏估计下，样本期望就是真实值，即&lt;span class="math">\(E(T)=\theta_0\)&lt;/span>，同时&lt;span class="math">\(k，\varepsilon\)&lt;/span>是与样本集无关我们可以人为选定的量，那么影响估计准确性的不可控因素就只剩下方差&lt;span class="math">\(Var(T)=\sigma^2\)&lt;/span>。这显然意味着，&lt;strong>统计量的方差和样本中含有的未知参数的&lt;span class="math">\(\theta\)&lt;/span>的信息量密切相关&lt;/strong>！&lt;/p>
&lt;p>由于Fisher当时非常推崇MLE作为参数估计方法，且实际应用中效果也很好，因此Fisher在此发现的基础上，最开始提出了一个很intuitive想法：使用最大似然估计作为构造统计量的方法，然后计算方差： &lt;span class="math">\[
T(X)=\theta_{MLE}=\arg\limits_{\theta} \{\sum_{i=1}^n \frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=0\}\tag{3}
\]&lt;/span> 但是，其中的&lt;span class="math">\(\arg\limits_\theta\)&lt;/span>这个运算符很让他犯难，这个操作符的代表的运算很抽象，不同的式子，实际的求0过程可能迥然不同，比如 &lt;span class="math">\[
\arg\limits_{x} \{ax-b=0\}\Rightarrow x=\frac{b}{a},(a\neq 0)\\
\arg\limits_{x} \{ax^2+bx+c=0\}\Rightarrow x=\frac{-b\plusmn\sqrt{b^2-4ac}}{2a},(a\neq 0)
\]&lt;/span> 同时，当时Fisher也并没有建立起MLE的严谨理论，因此退而求其次，Fisher构造重新了一个&lt;strong>不包含&lt;span class="math">\(\arg\)&lt;/span>运算符且最接近MLE&lt;/strong>的统计量，即： &lt;span class="math">\[
T(X)=\sum_{i=1}^n\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=\frac{\partial l(X;\theta)}{\partial \theta}\tag{4}
\]&lt;/span> 在构造这个统计量时，仅仅是不包含&lt;span class="math">\(\arg\)&lt;/span>运算符，其他MLE结构都保留了下来。&lt;/p>
&lt;p>当式(4)是以样本随机变量&lt;span class="math">\(X\)&lt;/span>为自变量的时候，我们叫他统计量&lt;span class="math">\(T(X)\)&lt;/span>，但是如果我们换个角度，当式(4)以参数&lt;span class="math">\(\theta\)&lt;/span>为自变量时，我们将式(4)重新起一个名字，叫做分数函数&lt;span class="math">\(S(\theta)\)&lt;/span>，即； &lt;span class="math">\[
S(\theta)=\sum_{i=1}^n\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=\frac{\partial l(X;\theta)}{\partial \theta}\tag{5}
\]&lt;/span> 为了方便，我们可以将&lt;span class="math">\(\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}\)&lt;/span>记为&lt;span class="math">\(S(\theta;x_i)\)&lt;/span>，则式(5)可写为： &lt;span class="math">\[
S(\theta)=\sum_{i=1}^n S(\theta;x_i)\tag{5.1}
\]&lt;/span>&lt;/p>
&lt;p>从实质内容来看，式(4)(5)没有差别，只不过一个以&lt;span class="math">\(X\)&lt;/span>为自变量，把&lt;span class="math">\(\theta\)&lt;/span>当成已知的；一个以&lt;span class="math">\(\theta\)&lt;/span>作为自变量，把&lt;span class="math">\(X\)&lt;/span>当成已知的。我们之所以能这么做的原因，是因为式子中本质上存在两个未知变量：随机变量&lt;span class="math">\(X\)&lt;/span>和未知参数&lt;span class="math">\(\theta\)&lt;/span>，式(4)(5)都只是其中一个角度来看待。&lt;/p>
&lt;p>既然，已经选择好了一个好用的统计量&lt;span class="math">\(T(X)\)&lt;/span>或者另一个角度叫分数函数&lt;span class="math">\(S(\theta)\)&lt;/span>，我们来计算下它的方差&lt;span class="math">\(Var(S)=E[(S-E(S))^2]=E(S^2)-(E(S))^2\)&lt;/span>吧。显然，求期望的操作消除了&lt;span class="math">\(S\)&lt;/span>中的随机变量&lt;span class="math">\(X\)&lt;/span>，使之结果只可能和&lt;span class="math">\(\theta\)&lt;/span>有关（甚至可以证明期望和&lt;span class="math">\(\theta\)&lt;/span>都无关）。首先，我们来求其中的&lt;span class="math">\(E(S)\)&lt;/span>。 &lt;span class="math">\[
\begin{aligned}
E(S(X;\theta))&amp;amp;=E\big [ \sum_{i=1}^n\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}\big]=\sum_{i=1}^n E\big [\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}\big]\\
&amp;amp;=\sum_{i=1}^n\int_{-\infty}^{+\infty}\frac{\partial\ln p(x_i;\theta)}{\partial\theta}p(x_i;\theta)\mathrm{d}x_i\\
&amp;amp;=\sum_{i=1}^n\int_{-\infty}^{+\infty} \frac{1}{p(x_i;\theta)}\frac{\partial p(x_i;\theta)}{\partial \theta} p(x_i;\theta)\mathrm{d}x_i\\
&amp;amp;=\sum_{i=1}^n\int_{-\infty}^{+\infty} \frac{\partial p(x_i;\theta)}{\partial \theta}\mathrm{d}x_i(\text{交换积分和偏导})\\
&amp;amp;=\sum_{i=1}^n \frac{\partial }{\partial\theta}\int_{-\infty}^{+\infty} p(x_i;\theta)\mathrm{d}x_i=\sum_{i=1}^n \frac{\partial }{\partial\theta} 1=0
\end{aligned}\tag{6}
\]&lt;/span> 式(6)给出了一个很好的结论：分数函数的期望&lt;span class="math">\(E(S)\)&lt;/span>必然为0，和&lt;span class="math">\(\theta\)&lt;/span>也没有关系。那么，方差就可以简化为：&lt;span class="math">\(Var(S)=E(S^2)\)&lt;/span>。接下来，为了方便计算方差，我们先对其进行简化。由于对数似然函数中的每一个&lt;span class="math">\(x_i\)&lt;/span>都是独立同分布的，那么随机变量&lt;span class="math">\(x_i\)&lt;/span>的函数&lt;span class="math">\(\frac{\partial \ln (p(x_i;\theta))}{\partial \theta}\)&lt;/span>也是独立同分布的，根据方差的性质，独立同分布随机变量的和（差）的方差，等于独立同分布随机变量方差的和，即 &lt;span class="math">\[
\begin{aligned}
Var(S(\theta;X))&amp;amp;=Var[\sum_{i=1}^n\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}]=n\times Var[\frac{\partial \ln(p(x;\theta))}{\partial \theta}]\\
&amp;amp;=nE[(\frac{\partial \ln(p(x;\theta))}{\partial \theta})^2]=E[(\frac{\partial l(X;\theta)}{\partial \theta})^2]
\end{aligned}\tag{7}
\]&lt;/span> 我们注意到，式(7)存在小写的随机变量&lt;span class="math">\(x\)&lt;/span>和大写的随机变量&lt;span class="math">\(X\)&lt;/span>，大写的&lt;span class="math">\(X\)&lt;/span>是样本容量为&lt;span class="math">\(n\)&lt;/span>的样本，小写的&lt;span class="math">\(x\)&lt;/span>是每一次抽样的样品，当使用简单随机抽样时，由于每一个&lt;span class="math">\(x_i\)&lt;/span>都是独立同分布的，即每一个样品从概率论的角度来讲都是等价的，其概率特征如期望、方差都是一样的，在此情况下使用样本&lt;span class="math">\(X\)&lt;/span>和样品&lt;span class="math">\(x\)&lt;/span>的期望和方差都是&lt;span class="math">\(n\)&lt;/span>倍的关系，即&lt;span class="math">\(Var(S(\theta;X))=nVar(S(\theta;x))\)&lt;/span>。&lt;/p>
&lt;p>那么使用哪一个方差更合理呢？注意我们之前说过，我们默认使用的采样方式都是简单随机抽样，但是实际中采样方式可能不同，因此，&lt;span class="math">\(X\)&lt;/span>中每一个样品&lt;span class="math">\(\{x_1,x_2,\dotsb,x_n\}\)&lt;/span>可能并不是等价的。我们追求更精细化的表达，希望方差针对每一个样品进行定义，从这个角度，更倾向于使用单个样品的方差，即&lt;span class="math">\(Var(S(\theta;x))\)&lt;/span>。此时，整体样品本的方差就是所有单个样品方差的和。&lt;/p>
&lt;p>最终千呼万唤始出来，历史上Fisher将每一个抽样样品随机变量&lt;span class="math">\(x\)&lt;/span>的方差定义成了&lt;strong>Fisher信息&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>Fisher信息： &lt;span class="math">\[I(\theta)=Var(S(\theta;x))=E[(\frac{\partial l(x;\theta)}{\partial \theta})^2]\tag{7.1}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>注意，式(7.1)中使用的是小写&lt;span class="math">\(x\)&lt;/span>，在简单随机抽样时，样本&lt;span class="math">\(X\)&lt;/span>中的各个样品&lt;span class="math">\(x_i\)&lt;/span>独立同分布，即都为等价的随机变量。&lt;/p>
&lt;p>如果我们再对式（6）中&lt;span class="math">\(\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x\)&lt;/span>再求&lt;span class="math">\(\theta\)&lt;/span>的偏导（二阶偏导）有（由于独立同分布，省略下标&lt;span class="math">\(i\)&lt;/span>）： &lt;span class="math">\[
\begin{aligned}
0&amp;amp;=\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x(交互积分微分顺序)\\
&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)+\frac{\partial\ln p(x;\theta)}{\partial\theta}\frac{\partial p(x;\theta)}{\partial\theta}]\mathrm{d}x\\
&amp;amp;\because \frac{\partial p(x;\theta)}{\partial\theta}=\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\\
0&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)+(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2p(x;\theta)]\mathrm{d}x\\
0&amp;amp;=\int_{-\infty}^{+\infty}\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x+\underbrace{\int_{-\infty}^{+\infty}(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2p(x;\theta)\mathrm{d}x}_{I(\theta)}\\
\end{aligned}
\]&lt;/span> 从而有： &lt;span class="math">\[
\begin{aligned}
0&amp;amp;=\int_{-\infty}^{+\infty}\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x+I(\theta)\\
I(\theta)&amp;amp;=-\int_{-\infty}^{+\infty}\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x\\
&amp;amp;=-E[\frac{\partial^2 l(x;\theta)}{\partial \theta^2}]
\end{aligned}\tag{7.2}
\]&lt;/span> 综上所述，我们得到了Fisher信息的三种等价表达形式： &lt;span class="math">\[I(\theta)=Var(S(\theta;x))\tag{8.1}\]&lt;/span> &lt;span class="math">\[I(\theta)=E[(\frac{\partial l(x;\theta)}{\partial \theta})^2]=E[S^2(\theta;x)]\tag{8.2}\]&lt;/span> &lt;span class="math">\[I(\theta)=-E[\frac{\partial^2 l(x;\theta)}{\partial \theta^2}]=-E[S&amp;#39;(\theta;x)]\tag{8.3}\]&lt;/span> 为了简便，下文中&lt;span class="math">\(S(\theta;x)\)&lt;/span>默认写为&lt;span class="math">\(S(\theta)\)&lt;/span>。&lt;/p>
&lt;p>那么，现在我们明确知道了，Fisher信息是分数函数的方差（式(8.1)），式(8.2,)(8.2)是化简计算的结果。那么通常情况下，我们总觉得&lt;strong>估计的方差应该是越小越好吧&lt;/strong>？！极端情况，当估计的方差为0时，我们就可以精确地得到结果。但是，从信息的直观意思来看，通常大家是认为信息量越大越有利于估计。这两种直觉貌似是针锋相对的。&lt;/p>
&lt;p>实际上，这涉及到最根本的一个问题：&lt;strong>分数函数的方差和待估计参数的方差是一回事吗&lt;/strong>？我们之前提到，Fisher在考虑利用方差设计Fisher信息时，认为第四步&lt;span class="math">\(\arg\limits_{\theta}\)&lt;/span>操作难以处理，而采取了近似操作，正是因为这步近似，使得分数函数的方差和待估计参数&lt;span class="math">\(\theta_{MLE}\)&lt;/span>的估计方差产生了区别！二者确实紧密相关，但是并不一样。硬要说的话，二者是整体与局部的关系，分数函数的方差是描述在整个参数空间内的整体的方差大小，而真正MLE估计的方差的是描述在&lt;span class="math">\(\theta_{MLE}\)&lt;/span>附近的性质，如果是无偏估计就是描述估计真值&lt;span class="math">\(\theta_0\)&lt;/span>附近的性质！那么，如何将描述整个参数空间性质的Fisher信息和真正MLE估计的能力联系起来呢？&lt;/p>
&lt;h3 id="mle的一个准严格推导">MLE的一个准严格推导&lt;/h3>
&lt;p>MLE的估计精确估计结果需要较为复杂的推导，这里我们利用&lt;strong>中心极限定理&lt;/strong>，给出一种在抽样规模较大的情形下的准严格推导。上一小节中，我们知道Fisher信息来自于分数函数的方差，算是对整体性质的描述，而实际上我们更需要知道，在MLE估计结果在真值点&lt;span class="math">\(\theta=\theta_0\)&lt;/span>邻域的效果如何。&lt;/p>
&lt;p>数学中，如果想看一个函数在某点附近的特性，有一个非常常用的工具，即&lt;strong>级数展开&lt;/strong>，比如泰勒级数，洛朗级数（泰勒级数在复变领域的扩展），傅里叶级数，狄利克雷级数等等。&lt;strong>在这里，我们使用泰勒级数，在MLE估计的结果&lt;span class="math">\(\theta=\theta_0\)&lt;/span>进行展开，用以探索其邻域的性质&lt;/strong>。&lt;/p>
&lt;p>首先，根据MLE的计算，我们可以得到最大似然估计的结果&lt;span class="math">\(\theta_{MLE}\)&lt;/span>，此时根据MLE的步骤5，显然有： &lt;span class="math">\[
\theta_{MLE}=\arg\limits_{\theta}\{\sum_{i=1}^n \frac{\partial \ln(p(x_i;\theta))}{\partial \theta}=0\}\\
\Rightarrow S(\theta_{MLE};X)=\sum_{i=1}^n S(\theta_{MLE};x_i)=0\tag{9}
\]&lt;/span> 注意，&lt;span class="math">\(\theta_{MLE}\)&lt;/span>是让整体样本&lt;span class="math">\(X\)&lt;/span>的分数函数&lt;span class="math">\(S(\theta_{MLE};X)\)&lt;/span>为0，并不保证每一个样品&lt;span class="math">\(x_i\)&lt;/span>的分数函数&lt;span class="math">\(S(\theta_{MLE};x_i)\)&lt;/span>为0。其中： &lt;span class="math">\[
S(\theta_{MLE};x_i)=[\frac{\partial \ln(p(x_i;\theta))}{\partial \theta}|\theta=\theta_{MLE}]
\]&lt;/span> 其次，我们将分数函数&lt;span class="math">\(S(\theta;X)\)&lt;/span>在真值点&lt;span class="math">\(\theta_0\)&lt;/span>处泰勒展开有： &lt;span class="math">\[
\begin{aligned}
S(\theta;X)
&amp;amp;=\sum_{i=1}^n[S(\theta_0;x_i)+\frac{1}{1!}S&amp;#39;(\theta_0;x_i)(\theta-\theta_0)+o(\theta-\theta_0)]\\
&amp;amp;=\sum_{i=1}^n S(\theta_0;x_i)+ S&amp;#39;(\theta_0;x_i)(\theta-\theta_0)+\sum_{i=1}^n o(\theta-\theta_0)
\end{aligned}
\]&lt;/span> 将式(9)结果代入上市可得： &lt;span class="math">\[
0=\sum_{i=1}^n S(\theta_0;x_i)+\sum_{i=1}^n S&amp;#39;(\theta_0;x_i)(\theta_{MLE}-\theta_0)+\sum_{i=1}^n o(\theta_{MLE}-\theta_0)\\
\text{两边同时除以}\frac{1}{n}\\
0=\frac{1}{n}\sum_{i=1}^n S(\theta_0;x_i)+\frac{1}{n}\sum_{i=1}^n S&amp;#39;(\theta_0;x_i)(\theta_{MLE}-\theta_0)+\frac{1}{n}\sum_{i=1}^n o(\theta_{MLE}-\theta_0)\tag{10}
\]&lt;/span> 看似两边同时除以&lt;span class="math">\(\frac{1}{n}\)&lt;/span>是一个很普通的操作，但是，我们可以从概率论角度分别赋予等式右边各项实际含义。&lt;/p>
&lt;p>首先，我们看等式右边第一项&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n S(\theta_0;x_i)\)&lt;/span>，我们之前提到过分数函数和统计量是一体两面，实际上分数函数可以换成统计量的写法，即&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n T(x_i;\theta)\)&lt;/span>，其中&lt;span class="math">\(x_i\)&lt;/span>都是由随机变量&lt;span class="math">\(x\)&lt;/span>抽样出来的样品，所以等式左右可以看成是&lt;strong>随机变量抽取&lt;span class="math">\(n\)&lt;/span>次后求平均数&lt;/strong>，这恰好和&lt;strong>中心极限定理&lt;/strong>的应用场景匹配！&lt;/p>
&lt;blockquote>
&lt;p>林德伯格－列维中心极限定理：&lt;strong>独立同分布&lt;/strong>(iid)、且&lt;strong>数学期望和方差有限&lt;/strong>的随机变量序列均值的标准化和以标准正态分布为极限。用数学语言描述为：&lt;/p>
&lt;p>设随机变量&lt;span class="math">\(X_{1},X_{2},\cdots ,X_{n}\)&lt;/span>独立同分布，且具有&lt;strong>有限的数学期望和方差&lt;/strong>&lt;span class="math">\(E(X_{i})=\mu\)&lt;/span>，&lt;span class="math">\(D(X_{i})=\sigma ^{2}\neq 0(i=1,2,\cdots ,n)\)&lt;/span>。记 &lt;span class="math">\[{\bar{X}}={\frac {1}{n}}\sum_{i=1}^{n}X_{i}，\zeta_{n}={\frac {{\bar {X}}-\mu }{\sigma /{\sqrt {n}}}}，\]&lt;/span> 则 &lt;span class="math">\[\lim_{n\rightarrow \infty }P\left(\zeta_{n}\leq z\right)=\Phi \left(z\right)\]&lt;/span> 其中&lt;span class="math">\(\Phi (z)\)&lt;/span>是标准正态分布的分布函数。&lt;/p>
&lt;/blockquote>
&lt;p>因此当&lt;span class="math">\(n\)&lt;/span>较大时，式(10)的右侧第一项将会逐渐收敛于正态分布，且其期望、方差我们之前已经求出来：&lt;span class="math">\(E[S(\theta_0;x)]=0,Var[S(\theta_0;x)]=I(\theta_0)\)&lt;/span>，即 &lt;span class="math">\[
\frac{1}{n}\sum_{i=1}^n S(\theta_0;x_i)\sim N(0,\frac{1}{n}I(\theta_0))\tag{11}
\]&lt;/span> 式(11)是一个非常良好的结论，利用了中心极限定理得到了我们所期望的正态分布，也是后续步骤的根本。&lt;/p>
&lt;p>式(10)的第二项中&lt;span class="math">\(\theta_{MLE}-\theta_0\)&lt;/span>与求和项&lt;span class="math">\(i\)&lt;/span>无关，可以当成系数，而&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n S&amp;#39;(\theta_0;x_i)\)&lt;/span>求均值的操作，在当&lt;span class="math">\(n\)&lt;/span>较大时，根据大数定律可以看成是&lt;span class="math">\(E[S&amp;#39;(\theta_0;x)]\)&lt;/span>。而根据式(8.3)可知，&lt;span class="math">\(-E[S&amp;#39;(\theta_0;x)]=I(\theta_0)\)&lt;/span>，即 &lt;span class="math">\[
\frac{1}{n}\sum_{i=1}^n S&amp;#39;(\theta_0;x_i)\approx -I(\theta_0)\tag{12}
\]&lt;/span> 当&lt;span class="math">\(n\)&lt;/span>较大时成立。&lt;/p>
&lt;p>而式(10)的第三项，&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n o(\theta_{MLE}-\theta_0)\)&lt;/span>是&lt;span class="math">\((\theta_{MLE}-\theta_0)\)&lt;/span>的一序列高阶项的均值，其结果依然是&lt;span class="math">\((\theta_{MLE}-\theta_0)\)&lt;/span>的高阶项。当&lt;span class="math">\(n\)&lt;/span>较大时，&lt;span class="math">\(\theta_{MLE}\)&lt;/span>趋近于&lt;span class="math">\(\theta_0\)&lt;/span>，此时&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n o(\theta_{MLE}-\theta_0)\)&lt;/span>为高阶无穷小，可以近似忽略，那么，综合式(11,12)，式(10)的最终结果可以简化为： &lt;span class="math">\[
I(\theta_0)(\theta_{MLE}-\theta_0)\sim N(0,\frac{1}{n}I(\theta_0))\\
\Rightarrow \theta_{MLE}\sim N(\theta_0,\frac{1}{nI(\theta_0)})\tag{13}
\]&lt;/span> 即最大似然估计结果&lt;span class="math">\(\theta_{MLE}\)&lt;/span>渐进服从于以真值&lt;span class="math">\(\theta_0\)&lt;/span>为均值，&lt;span class="math">\(\frac{1}{nI(\theta_0)})\)&lt;/span>为方差的正态分布。&lt;/p>
&lt;p>式(13)给出的结论就很显然了，为了让&lt;span class="math">\(\theta_{MLE}\)&lt;/span>更贴近&lt;span class="math">\(\theta_0\)&lt;/span>，方差显然是越小越好。而且式(13)也给出了缩小方差的两个途径：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>增加&lt;span class="math">\(n\)&lt;/span>，即增加样本容量。这个很好理解，统计学一般都是抽样越多，效果越好。而且，在独立同分布场景下，样本总的Fisher信息量&lt;span class="math">\(I(\theta_0;X)\)&lt;/span>和样本容量&lt;span class="math">\(n\)&lt;/span>是线性关系，是单位样品Fisheries信息量&lt;span class="math">\(I(\theta_0;x)\)&lt;/span>的&lt;span class="math">\(n\)&lt;/span>倍。&lt;/li>
&lt;li>增加Fisher信息量&lt;span class="math">\(I(\theta)\)&lt;/span>。这里面门道就很多了，最重要的就是&lt;strong>设计合理的估计方法&lt;/strong>。同时也告诉我们，当估计方法确定时，估计的精确性是有上限的！这个理论上限和Fisher信息量直接相关，学术界把它命名为&lt;strong>Cramer-Rao界&lt;/strong>，和香农信息量规定的香农界一样，是统计学理论中最重要的理论边界之一。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>TIPS：需要指出的是，Fisher信息和Cramer-Rao界和香农定理一样，只是告诉了我们这个理论上界，但是并没有告诉我们如何达到这个理论界限的方法。因此，如何设计达到或接近理论界限是学术研究中乐此不疲的领域。&lt;/p>
&lt;/blockquote>
&lt;h2 id="fisher信息的应用">Fisher信息的应用&lt;/h2>
&lt;p>Fisher信息应用从三个方面来看（主要参考文章&lt;a href="https://arxiv.org/abs/1705.01064">A Tutorial on Fisher Information&lt;/a>），分别是频率学派角度，贝叶斯学派角度和最小描述长度角度。&lt;/p>
&lt;h3 id="频率学派角度应用fisher信息">频率学派角度应用Fisher信息&lt;/h3>
&lt;p>由于Fisher信息本身就是由频率学派大佬Fisher提出来的，因此Fisher信息在频率学派统计方法中也应用最早。除了为了最原始的功能，即提供参数估计的性能边界，之外还可以用于试验精度要求设计、假设检验和置信区间构造与估计。其利用Fisher信息的核心都是MLE估计结果的&lt;strong>概率分布或近似场景下的渐近正态性&lt;/strong>。&lt;/p>
&lt;p>感觉对我而言比较容易理解，同时用的不太多，需要用到是可以参考&lt;a href="https://arxiv.org/abs/1705.01064">A Tutorial on Fisher Information&lt;/a>中第二节内容，&lt;/p>
&lt;h3 id="贝叶斯学派角度应用fisher信息">贝叶斯学派角度应用Fisher信息&lt;/h3>
&lt;h3 id="最小描述长度角度应用fisher信息">最小描述长度角度应用Fisher信息&lt;/h3>
&lt;p>最小描述长度（Minimum Description Length， MDL）原则是将奥卡姆剃刀形式化后的一种结果。其想法是，在给予假说的集合的情况下，能产生最多资料压缩效果的那个假说是最好的，即该原则寻求最简单、最不复杂的模型。它是在1978年由Jorma Rissanen所引入的。其原理是对于一组给定的实例数据&lt;span class="math">\(D\)&lt;/span>，如果要对其进行保存，为了节省存储空间，&lt;strong>一般采用某种模型&lt;span class="math">\(H\)&lt;/span>对其进行编码压缩，然后再保存压缩后的数据&lt;/strong>。同时，为了以后正确恢复这些实例数据，将所用的模型&lt;span class="math">\(H\)&lt;/span>也保存起来。所以需要保存的总数据长度(比如比特数) 等于这些实例数据&lt;span class="math">\(D\)&lt;/span>进行编码压缩后的长度加上保存模型&lt;span class="math">\(H\)&lt;/span>所需的长度，将该数据长度称为总描述长度，即 &lt;span class="math">\[
L_{total}=L(H)+L(D|H)
\]&lt;/span> MDL原则就是就是要求&lt;strong>选择对此数据集&lt;span class="math">\(D\)&lt;/span>总描述长度最小的模型&lt;span class="math">\(H\)&lt;/span>&lt;/strong>。MDL计算与BIC（Bayesian Information Criterion, 贝叶斯信息准则）非常相似，在某些情况下可以证明是等效的。&lt;/p></description></item><item><title>读书笔记之协和八公众号文章总结笔记</title><link>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%8F%E5%92%8C%E5%85%AB%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%8F%E5%92%8C%E5%85%AB%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/</guid><description>
&lt;h2 id="概率统计随机过程之协和八公众号文章总结笔记">概率统计随机过程之协和八公众号文章总结笔记&lt;!-- omit in toc -->&lt;/h2>
&lt;p>在公众号【协和八】读到了一个将统计学的专栏，虽然是医学系研究者写的，但是他山之石可以攻玉，其中讲解了很多很有用的统计学知识和难解的点。本篇笔记是该专栏的阅读笔记。&lt;/p>
&lt;p>专栏链接：&lt;a href="https://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&amp;amp;mid=2652555217&amp;amp;idx=1&amp;amp;sn=da5332753856efb4a86aa695839523e6&amp;amp;chksm=80bbd60cb7cc5f1a3571f2f6389b3e0331fdf2612db62ae95de172c0d49b172efd8e4d236772&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0826Jegh5eGekDeo3gagkNmi&amp;amp;sharer_sharetime=1661503954453&amp;amp;sharer_shareid=2ac9f69d9b7a82eb74ce119fa6cee32f&amp;amp;key=38beb1eea6d155d34b2c714171a9bbdb1023b5929c6a8d4959ab5030989d46cf92a10bac409bfa0cd4d56ee20f279321f8723c97faa6694ae7d7835c23e88d84b080cf6e011938d7a86e0e3f88c01a06158aa31a07f803d47f5ec841f6d9e05a6f6c34694e85cfe4a434adb30c7f79de0d3f1f539bcca43bc11dbadcd1cb40ce&amp;amp;ascene=1&amp;amp;uin=MTcxMzg4NjU4MQ%3D%3D&amp;amp;devicetype=Windows+10+x64&amp;amp;version=62090538&amp;amp;lang=zh_CN&amp;amp;exportkey=AVUF2MlVFJXP3eTrXT%2FKrac%3D&amp;amp;acctmode=0&amp;amp;pass_ticket=BjR6ClCbka%2Fh7sRrwT4qhuK4uUG4PH9gs7DDKMgyVwMixFT6vexfMRG4H8Sf%2Fq8J&amp;amp;wx_header=0">说人话的统计学——终点·起点 | 协和八&lt;/a>&lt;/p>
&lt;h2 id="第1章高屋建瓴看统计">第1章：高屋建瓴看统计&lt;/h2>
&lt;h3 id="你真的懂p值吗">你真的懂p值吗？&lt;/h3>
&lt;h3 id="做统计多少数据才算够上">做统计，多少数据才算够？（上）&lt;/h3>
&lt;h3 id="做统计多少数据才算够下">做统计，多少数据才算够？（下）&lt;/h3>
&lt;h3 id="提升统计功效让评审心服口服">提升统计功效，让评审心服口服&lt;/h3>
&lt;h3 id="你的科研成果都是真的吗">你的科研成果都是真的吗？&lt;/h3>
&lt;h3 id="见识数据分析的独孤九剑">见识数据分析的「独孤九剑」&lt;/h3>
&lt;h3 id="贝叶斯vs频率派武功到底哪家强">贝叶斯vs频率派：武功到底哪家强？&lt;/h3>
&lt;h2 id="第2章算术平均数与正态分布">第2章：算术平均数与正态分布&lt;/h2>
&lt;h3 id="数据到手了第一件事先干啥">数据到手了，第一件事先干啥？&lt;/h3>
&lt;h3 id="算术平均数简单背后有乾坤">算术平均数：简单背后有乾坤&lt;/h3>
&lt;h3 id="正态分布到底是怎么来的">正态分布到底是怎么来的？&lt;/h3>
&lt;h2 id="第3章t检验两组平均数的比较">第3章：t检验：两组平均数的比较&lt;/h2>
&lt;h3 id="想玩转t检验你得从这一篇看起">想玩转t检验？你得从这一篇看起&lt;/h3>
&lt;h3 id="就是要实用t检验的七十二变">就是要实用！t检验的七十二变&lt;/h3>
&lt;h3 id="不是正态分布t检验还能用吗">不是正态分布，t检验还能用吗？&lt;/h3>
&lt;h3 id="只有-15-个标本也能指望t检验吗">只有 15 个标本，也能指望t检验吗？&lt;/h3>
&lt;h3 id="样本分布不正态数据变换来救场">样本分布不正态？数据变换来救场&lt;/h3>
&lt;h3 id="数据变换的万能钥匙box-cox变换">数据变换的万能钥匙：Box-Cox变换&lt;/h3>
&lt;h3 id="t检验用不了别慌还有神奇的非参数检验">t检验用不了？别慌，还有神奇的非参数检验&lt;/h3>
&lt;h3 id="只讲p值不讲效应大小都是耍流氓">只讲p值，不讲效应大小，都是耍流氓&lt;/h3>
&lt;h3 id="找出t检验的效应大小对耍流氓-say-no">找出t检验的效应大小，对耍流氓 say no&lt;/h3>
&lt;h3 id="用置信区间就是这么不自信">用置信区间，就是这么（不）自信&lt;/h3>
&lt;h3 id="如何确定t检验的置信区间">如何确定t检验的置信区间&lt;/h3>
&lt;h3 id="优雅秀出你的t检验提升paper逼格">优雅秀出你的t检验，提升Paper逼格&lt;/h3>
&lt;h3 id="要做t检验这两口毒奶可喝不得">要做t检验，这两口毒奶可喝不得&lt;/h3>
&lt;h2 id="第4章方差分析anova多组平均数的比较">第4章：方差分析（ANOVA）：多组平均数的比较&lt;/h2>
&lt;h3 id="要比较三组数据t检验还能用吗">要比较三组数据，t检验还能用吗？&lt;/h3>
&lt;h3 id="anova在手多组比较不犯愁">ANOVA在手，多组比较不犯愁&lt;/h3>
&lt;h3 id="anova的基本招式你掌握了吗">ANOVA的基本招式你掌握了吗？&lt;/h3>
&lt;h3 id="anova做出了显著性事儿还没完呢">ANOVA做出了显著性？事儿还没完呢&lt;/h3>
&lt;h3 id="听说成对t检验还有-anova进阶版">听说，成对t检验还有 ANOVA进阶版？&lt;/h3>
&lt;h3 id="重复测量-anova你要知道的事儿都在这里啦">重复测量 ANOVA：你要知道的事儿都在这里啦&lt;/h3>
&lt;h3 id="没听说过多因素-anova那你就可就-out了">没听说过多因素 ANOVA？那你就可就 OUT了&lt;/h3>
&lt;h3 id="多因素-anova好几个单因素-anova可没这么简单">多因素 ANOVA＝好几个单因素 ANOVA？可没这么简单&lt;/h3>
&lt;h3 id="两个因素相互影响anova结果该如何判读">两个因素相互影响，ANOVA结果该如何判读？&lt;/h3>
&lt;h3 id="anova还能搞三四五因素等等我头有点儿晕">ANOVA还能搞三四五因素？等等，我头有点儿晕&lt;/h3>
&lt;h3 id="要做-anova样本量多大才够用">要做 ANOVA，样本量多大才够用&lt;/h3>
&lt;h2 id="第5章线性回归统计建模初步">第5章：线性回归：统计建模初步&lt;/h2>
&lt;h3 id="车模航模你玩过统计学模型你会玩吗">车模航模你玩过，统计学模型你会玩吗？&lt;/h3>
&lt;h3 id="如果只能学习一种统计方法我选择线性回归">如果只能学习一种统计方法，我选择线性回归&lt;/h3>
&lt;h3 id="回归线三千我只取这一条">回归线三千，我只取这一条&lt;/h3>
&lt;h3 id="三千回归线里选中了你你靠谱吗">三千回归线里选中了你，你靠谱吗？&lt;/h3>
&lt;h3 id="自变量不止一个线性回归该怎么做">自变量不止一个，线性回归该怎么做？&lt;/h3>
&lt;h3 id="找出交互效应让线性模型更万能">找出「交互效应」，让线性模型更万能&lt;/h3>
&lt;h3 id="天啦噜没考虑到混杂因素后果会这么严重">天啦噜！没考虑到混杂因素，后果会这么严重？&lt;/h3>
&lt;h3 id="回归系数不显著也许是打开方式不对">回归系数不显著？也许是打开方式不对&lt;/h3>
&lt;h3 id="评价线性模型r平方是个好裁判吗">评价线性模型，R平方是个好裁判吗？&lt;/h3>
&lt;h3 id="如果r平方是砒霜本文教你三种解药">如果R平方是砒霜，本文教你三种解药&lt;/h3>
&lt;h3 id="线性模型生病了你懂得怎样诊断吗">线性模型生病了，你懂得怎样诊断吗？&lt;/h3>
&lt;h3 id="脱离群众的数据点是春风化雨还是秋风扫落叶">「脱离群众」的数据点，是「春风化雨」还是「秋风扫落叶」&lt;/h3>
&lt;h2 id="第6章广义线性模型统计建模进阶">第6章：广义线性模型：统计建模进阶&lt;/h2>
&lt;h3 id="你在-或者不在-需要逻辑回归来算">你在 或者不在 需要逻辑回归来算&lt;/h3>
&lt;h3 id="逻辑回归的袅娜曲线你是否会过目难忘">逻辑回归的袅娜曲线，你是否会过目难忘？&lt;/h3>
&lt;h3 id="逻辑回归的统计检验原来招数辣么多">逻辑回归的统计检验，原来招数辣么多？&lt;/h3>
&lt;h3 id="线性回归能玩多变量逻辑回归当然也能">线性回归能玩多变量，逻辑回归当然也能&lt;/h3>
&lt;h3 id="喂你的逻辑回归模型该做个体检啦">喂，你的逻辑回归模型该做个体检啦&lt;/h3>
&lt;h3 id="逻辑回归能摆平二分类因变量那不止二分类呢">逻辑回归能摆平二分类因变量，那……不止二分类呢？&lt;/h3>
&lt;h3 id="让人眼花缭乱的多项逻辑回归原来是这么用的">让人眼花缭乱的多项逻辑回归，原来是这么用的&lt;/h3>
&lt;h3 id="只问方向无问远近定序回归的执念你懂吗">只问方向，无问远近，定序回归的执念你懂吗？&lt;/h3>
&lt;h3 id="包教包会定序回归实战">包教包会：定序回归实战&lt;/h3>
&lt;h3 id="数风流人物还靠泊松回归">「数」风流人物，还靠泊松回归&lt;/h3>
&lt;h3 id="广义线性模型到底是个什么鬼">广义线性模型到底是个什么鬼？&lt;/h3>
&lt;h2 id="自检">自检&lt;/h2>
&lt;h3 id="妈妈说答对的童鞋才能中奖">妈妈说答对的童鞋才能中奖&lt;/h3>
&lt;h3 id="统计学的十个误区你答对了吗">统计学的十个误区，你答对了吗？&lt;/h3></description></item><item><title>概率统计随机过程之时间序列分析</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/</link><pubDate>Sat, 11 Jun 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/</guid><description>
&lt;h2 id="概率统计随机过程之时间序列分析">概率统计随机过程之时间序列分析&lt;!-- omit in toc -->&lt;/h2>
&lt;p>时间序列分析（Time-Series Analysis）是统计与随机过程中常用的场景，在机器学习、强化学习的预测中也有重要地位，本笔记介绍时间序列分析的基本概念和指标，记录对时序分析的基本理解。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#时间序列的基本概念">时间序列的基本概念&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时间序列的类型">时间序列的类型&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时间序列的因素分解">时间序列的因素分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时间序列分析基础">时间序列分析基础&lt;/a>&lt;/li>
&lt;li>&lt;a href="#发展水平与平均发展水平">发展水平与平均发展水平&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#绝对数时间序列的序时平均数">绝对数时间序列的序时平均数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相对数时间序列的序时平均数">相对数时间序列的序时平均数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#增长量">增长量&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#时间序列的速度分析">时间序列的速度分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#发展速度">发展速度&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#平均发展速度">平均发展速度&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#增长速度">增长速度&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#平均增长速度">平均增长速度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#年化增长速度">年化增长速度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#增长1绝对值">增长1%绝对值&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#时间序列的预测">时间序列的预测&lt;/a>&lt;/li>
&lt;li>&lt;a href="#平稳序列预测">平稳序列预测&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#简单平均法">简单平均法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#移动平均法">移动平均法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数平滑法">指数平滑法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#趋势序列的预测">趋势序列的预测&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#线性趋势预测">线性趋势预测&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非线性趋势预测">非线性趋势预测&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#季节因素考虑">季节因素考虑&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="时间序列的基本概念">时间序列的基本概念&lt;/h2>
&lt;blockquote>
&lt;p>时间序列：又叫动态序列，是指将同一对象不同时间的观测数据按期发生的先后顺序排列而成的序列。&lt;/p>
&lt;/blockquote>
&lt;p>这就是我们常说的时域表示，自变量（横轴）是时间，因变量（纵轴）是观察值。常见的时间序列如股市K线图，各年经济统计图表，Gartner技术成熟曲线等。&lt;/p>
&lt;p>时间序列是一种动态的数列分析。时间序列中&lt;strong>每一时期的数值都是由许多不同的因素同时作用的综合结果&lt;/strong>。&lt;/p>
&lt;p>时间序列的基本要素：&lt;/p>
&lt;ul>
&lt;li>所属的时间范围：可以是年份，季度，月份或者其他任何形式的时间&lt;/li>
&lt;li>不同时间上的观察值&lt;/li>
&lt;/ul>
&lt;h3 id="时间序列的类型">时间序列的类型&lt;/h3>
&lt;p>时间序列的类型从平稳性来看分为平稳序列和非平稳序列。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>平稳序列。基本不存在趋势的序列，各观察值基本在某个固定的水平上波动，或虽存在波动程度不同，但并不存在某种规律。我觉得和随机过程中的平稳概念类似，各态历经性，每一个时刻的值域是一样的，值域不会随着时间的变化而变化。&lt;/li>
&lt;li>非平稳序列。&lt;/li>
&lt;li>趋势型，随着时间增长，序列观察值存在与时间轴线性或非线性相关的趋势。&lt;/li>
&lt;li>复合型，趋势、季节性、周期性的复合序列。&lt;/li>
&lt;/ol>
&lt;div class="figure">
&lt;img src="../images/时间序列稳态与非稳态.png" alt="时间序列稳态与非稳态.png" />&lt;p class="caption">时间序列稳态与非稳态.png&lt;/p>
&lt;/div>
&lt;h3 id="时间序列的因素分解">时间序列的因素分解&lt;/h3>
&lt;p>时间序列分析的任务就是要正确地确定时间序列的性质，对影响时间序列的各种因素加以分解和测定，以便对未来的状况作出判断和预测。这些因素按照性质可以划分为：长期趋势、季节变动、循环变动、不规则变动。&lt;/p>
&lt;p>时间序列因素四部分组成：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>趋势（T，trend），线性趋势、非线性趋势&lt;/li>
&lt;li>季节性（S，seasonal fluctuation）&lt;/li>
&lt;li>周期性（C，cyclical fluctuation）&lt;/li>
&lt;li>随机性（I，irregular variations）&lt;/li>
&lt;/ol>
&lt;p>根据因素的关系，时间序列可以通过以下两种方式分解：&lt;/p>
&lt;p>&lt;strong>加法分解&lt;/strong>。 &lt;span class="math">\[Y_i=T_i+S_i+C_i+I_i\]&lt;/span> 加法分解中，各影响因素是相互独立的，且均与&lt;span class="math">\(Y\)&lt;/span>有着相同的计量单位。季节性变动、周期性变动在各自周期内总会应为0，随机波动长期来看，期望也是0。在加法模型中，各因素的分解是通过减法实现，如 &lt;span class="math">\[
Y-T=S+C+I
\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>乘法分解&lt;/strong>。 &lt;span class="math">\[Y_i=T_i\times S_i\times C_i\times I_i\]&lt;/span> 乘法分解中，只有趋势与与&lt;span class="math">\(Y\)&lt;/span>有着相同的计量单位，其他因素都是以&lt;strong>比率&lt;/strong>的形式出现，季节性变动、周期性变动在各自周期内几何平均会应为1（100%），随机波动长期来看，几何平均也是1（100%）。在乘法模型中，各因素的分解是通过除法实现，如 &lt;span class="math">\[
Y\div T=S×C×I
\]&lt;/span>&lt;/p>
&lt;h2 id="时间序列分析基础">时间序列分析基础&lt;/h2>
&lt;p>时间序列分析的基本目的是描述动态变化（分析过去），揭示变化规律（认识规律）以至获悉未来数量的趋势（预测未来）。最直观的时序分析方法是图像描述，在难道初始数据后，先画出一幅图，能有助于我们更快地了解大体情形。&lt;/p>
&lt;h3 id="发展水平与平均发展水平">发展水平与平均发展水平&lt;/h3>
&lt;blockquote>
&lt;p>发展水平：是指现象在不同的时间上的观察值，说明现象在某一时间上所达到的水平。表示为&lt;span class="math">\(Y_0,Y_1,\dotsb,Y_n\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>说白了，发展水平就是观察值的好听点说法。那么所谓&lt;strong>平均发展水平&lt;/strong>，就是一段时间内观察值的均值。&lt;/p>
&lt;blockquote>
&lt;p>平均发展水平:现象在不同时间上取值的平均数，又称序时平均数或动态平均数，说明现象在一段时期内所达到的一般水平。&lt;/p>
&lt;/blockquote>
&lt;h4 id="绝对数时间序列的序时平均数">绝对数时间序列的序时平均数&lt;/h4>
&lt;p>绝对数平均数就是我们常说的平均值，对于等分的&lt;strong>时期序列&lt;/strong>，其计算公式是平凡的： &lt;span class="math">\[
\bar{Y}=\frac{Y_1+Y_2+\dotsb+Y_n}{n}=\frac{\sum_{i=1}^n Y_i}{n}
\]&lt;/span> &lt;img src="../images/时期序列时点序列.png" alt="时期序列时点序列.png" />&lt;/p>
&lt;p>所谓时期序列，就是指观察值代表了某一时段的值。对应的是时点序列，含义是指观察值只代表当前时间点的值。对于时点序列，我们可以用两侧端点的平均数作为某个时段的代表值，即&lt;span class="math">\(\bar{Y_i}=\frac{1}{2}(Y_i+Y_{i+1})\)&lt;/span>，因此时点序列的平均发展水平就是： &lt;span class="math">\[
\bar{Y}=\frac{{(Y_1+Y_2)\over 2}+{(Y_2+Y_3)\over 2}+\dotsb+{(Y_{n-1}+Y_n)\over 2}}{n-1}=\frac{{Y_1\over 2}+Y_2+\dotsb+Y_{n-1}+{Y_n\over 2}}{n-1}
\]&lt;/span> 注意，只有&lt;span class="math">\(n-1\)&lt;/span>个时段，而非像时期序列有&lt;span class="math">\(n\)&lt;/span>个时段。&lt;/p>
&lt;p>有时，时间序列并不是均等的，如下图所示： &lt;img src="../images/间隔不相等的时间序列.png" alt="间隔不相等的时间序列" />&lt;/p>
&lt;p>我们可以用两侧端点的平均数作为某个时段的代表值，然后再乘以时间间隔的长度，最后除以总的时间，这里使用了加权平均的思想。 &lt;span class="math">\[
\bar{Y}=\frac{({Y_1+Y_2\over 2})T_1+({Y_2+Y_3\over 2})T_2+\dotsb+({Y_{n-1}+Y_n\over 2})T_{n-1}+}{\sum_{i=1}^{n-1} T_i}
\]&lt;/span>&lt;/p>
&lt;h4 id="相对数时间序列的序时平均数">相对数时间序列的序时平均数&lt;/h4>
&lt;p>首先，分别求出构成相对数的分子指标&lt;span class="math">\(a_i\)&lt;/span>和分母指标&lt;span class="math">\(b_i\)&lt;/span>的序时平均数；其次，在进行对比，即得相对数时间序列的序时平均数： &lt;span class="math">\[
\bar{Y}=\frac{\bar{a}}{\bar{b}}
\]&lt;/span>&lt;/p>
&lt;h4 id="增长量">增长量&lt;/h4>
&lt;ul>
&lt;li>逐期增长量：报告期水平与前一期水平之差，&lt;span class="math">\(\Delta_i=Y_i-Y_{i-1}\)&lt;/span>&lt;/li>
&lt;li>累计增长量：报告期水平与某一固定时期水平之差，&lt;span class="math">\(\Delta_i=Y_i-Y_{0}\)&lt;/span>。显然，累计增长量等于逐期增长量之和。&lt;/li>
&lt;li>年距增长量，为了消除季节变动影响，本期发展水平-去年同期发展水平&lt;/li>
&lt;li>平均增长量 &lt;span class="math">\[
平均增长量=\frac{累计增长量}{间隔期数}=\frac{累计增长量}{观察值个数-1}
\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;h2 id="时间序列的速度分析">时间序列的速度分析&lt;/h2>
&lt;p>时间序列的速度主要分发展速度和增长速度两个方面，发展速度是“连本带利”的速度，而增长速度主要看“利息”的增长，二者都反应了时间序列变化的快慢。&lt;/p>
&lt;h3 id="发展速度">发展速度&lt;/h3>
&lt;blockquote>
&lt;p>发展速度：报告期水平和基期水平之比，公式表达为： &lt;span class="math">\[发展速度=\frac{报告期水平}{基期水平}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>常用的发展速度有环比发展速度、同比发展速度（年距发展速度）、定基发展速度。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>环比发展速度：报告期水平与前一期水平之比&lt;span class="math">\(R_i=\frac{Y_i}{Y_{i-1}}\)&lt;/span>;&lt;/li>
&lt;li>同比发展速度：本期发展水平与去年同期发展水平之比；&lt;/li>
&lt;li>定基发展速度：报告期水平与某一固定时期水平之比&lt;span class="math">\(R_i=\frac{Y_i}{Y_0}\)&lt;/span>。如果是计算长时间内总的发展速度，称为总速度。&lt;/li>
&lt;/ol>
&lt;p>显然，观察期内各环比发展速度的乘积等于最末期的定基发展速度，即 &lt;span class="math">\[
\prod_{i=1}^n \frac{Y_i}{Y_{i-1}}=\frac{Y_n}{Y_0}
\]&lt;/span>&lt;/p>
&lt;h4 id="平均发展速度">平均发展速度&lt;/h4>
&lt;p>平均发展速度研究的是多个时期的发展速度均值，这里的均值计算方式需要仔细考虑下，并不能采用简单的算术平均，因为&lt;span class="math">\(n\)&lt;/span>个时期发展水平累积起来是乘积关系，所以求其平均数我们自然会想到将其开&lt;span class="math">\(n\)&lt;/span>次根号，即使用&lt;strong>几何平均数&lt;/strong>。 &lt;span class="math">\[
\bar{R}=\sqrt[n]{\frac{Y_1}{Y_0}\times\frac{Y_2}{Y_1}\times\dotsb\times\frac{Y_n}{Y_{n-1}}}=\sqrt[n]{\prod_{i=1}^n \frac{Y_i}{Y_{i-1}}}=\sqrt[n]{\frac{Y_n}{Y_{0}}}\\
反过来：Y_n=Y_0\bar{R}^n
\]&lt;/span>&lt;/p>
&lt;h3 id="增长速度">增长速度&lt;/h3>
&lt;blockquote>
&lt;p>增长速度：又称增长率，是增长量与基期发展水平之比，或报告期发展水平和基期发展水平之比&lt;strong>减1&lt;/strong>。 &lt;span class="math">\[\begin{aligned}
增长速度(增长率)&amp;amp;=\frac{增长量}{基期水平}=\frac{报告期水平-基期水平}{基期水平}\\&amp;amp;=发展速度-1
\end{aligned}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>增长速度表明了相对增加的程度，注意增长速度&lt;strong>可正可负可0&lt;/strong>，但是在负增长时有些概念需要谨慎使用。同样，增长速度也可分为环比增长速度、同比增长速度（年距增长速度）、定基增长速度。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>环比增长速度（率）：报告期水平与前一期水平之比减1，&lt;span class="math">\(G_i=\frac{Y_i-Y_{i-1}}{Y_{i-1}}=\frac{Y_i}{Y_{i-1}}-1\)&lt;/span>&lt;/li>
&lt;li>同比增长速度（率）:总增长率与去年同期发展水平之比，或者本期发展水平与去年同期发展水平之比减1；&lt;/li>
&lt;li>定基增长速度（率）：报告期水平与某一固定时期水平之比减1，&lt;span class="math">\(G_i=\frac{Y_i-Y_0}{Y_0}=\frac{Y_i}{Y_0}-1\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;h4 id="平均增长速度">平均增长速度&lt;/h4>
&lt;p>平均增长速度，又称平均增长率，是用来描述现象在整体观察期内平均每期增长变化的程度。它与平均发展速度有着密切关系，两者仅相差一个基数，即 &lt;span class="math">\[
平均增长速度=平均发展速度-1\\
\bar{G}=\sqrt[n]{\frac{Y_n}{Y_{0}}}-1=\bar{R}-1
\]&lt;/span>&lt;/p>
&lt;h4 id="年化增长速度">年化增长速度&lt;/h4>
&lt;p>如果我们计算不等长时期的增长率，比如要比较一个月的增长率和一个季度的增长率，由于其基准时期不同，其结果往往不客观。为了统一基准，我们可以采用年化增长率（又叫年率）来表示。我们借用平均增长速度的计算方式，先计算出观察时期的平均发展速度，然后再通过多次幂的方式求出年化发展速度。计算公式为： &lt;span class="math">\[
G_A=(\frac{Y_i}{Y_0})^{\frac{m}{n}}-1
\]&lt;/span> 其中，&lt;span class="math">\(m\)&lt;/span>为一年中的时期个数，比如按月算&lt;span class="math">\(m=12\)&lt;/span>，按季度算&lt;span class="math">\(m=4\)&lt;/span>;&lt;span class="math">\(n\)&lt;/span>表示当期和某一固定时期所跨的时期总数。&lt;/p>
&lt;h4 id="增长1绝对值">增长1%绝对值&lt;/h4>
&lt;p>在有些情况下，增长率会起到误导的作用，比如小基数场景下，不多的绝对值增长就会导致增长率的大幅上升。因此，不能单纯就增长率论增长率，需要与绝对值水平结合。为此，提出了增长1%绝对值的概念，即增长率每增长一个百分点而增加的绝对量。 &lt;span class="math">\[
增长1\%绝对值=\frac{前期水平}{100}
\]&lt;/span> 用于弥补增长率分析中的局限性。&lt;/p>
&lt;h2 id="时间序列的预测">时间序列的预测&lt;/h2>
&lt;p>我们对时间序列的研究往往是需要通过历史数据来预测未来的数据，其分析步骤如下：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>确定时间序列所包含的成分，即趋势、季节、周期、随机性的类型判断&lt;/li>
&lt;li>选择适合此类时间序列的预测方法&lt;/li>
&lt;li>对可能的预测方法进行评估，以确定最佳方案&lt;/li>
&lt;li>利用最佳方案进行预测&lt;/li>
&lt;/ol>
&lt;div class="figure">
&lt;img src="../images/时间序列预测方法.png" alt="时间序列预测方法" />&lt;p class="caption">时间序列预测方法&lt;/p>
&lt;/div>
&lt;div class="figure">
&lt;img src="../images/时间序列分析评估.png" alt="时间序列分析评估" />&lt;p class="caption">时间序列分析评估&lt;/p>
&lt;/div>
&lt;h3 id="平稳序列预测">平稳序列预测&lt;/h3>
&lt;p>对于既没有趋势性，也没有季节性周期性的纯随机时间序列，我们可以采用平稳序列预测方法。主要包括简单平均法、移动平均法、指数平均法三种。&lt;/p>
&lt;h4 id="简单平均法">简单平均法&lt;/h4>
&lt;p>顾名思义，就是取过去所有值的算术平均数，表达为： &lt;span class="math">\[
F_{t+1}=\frac{1}{t}(Y_1+Y_2+\dotsb+Y_t)=\frac{1}{t}\sum_{i=1}^t Y_i
\]&lt;/span> 简单平均法将所有观察值看作同等重要，不区分远期数据和近期数据的影响，适合对较为稳定的时间序列进行预测。其误差为： &lt;span class="math">\[
e_{t+1}=Y_{t+1}-F_{t+1}
\]&lt;/span>&lt;/p>
&lt;h4 id="移动平均法">移动平均法&lt;/h4>
&lt;p>一种对简单平均法做出改进的方法是移动平均法，通过对时间序列的&lt;strong>逐期递移&lt;/strong>，球的一系列平均数作为预测值。移动平均法本质上是构建了一个&lt;strong>滑动窗口&lt;/strong>，只计算窗口内的观察值的平均数。根据数据是否平等可分为简单移动平均和加权移动平均两种。&lt;/p>
&lt;p>简单移动平均：假设滑动窗口的大小为&lt;span class="math">\(k\)&lt;/span>，即计算最近的&lt;span class="math">\(k\)&lt;/span>个观察值的平均数，其预测值为： &lt;span class="math">\[
F_{t+1}=\bar{Y}_t=\frac{Y_{t-k+1}+Y_{t-k+2}+\dotsb+Y_{t-1}+Y_t}{k}
\]&lt;/span> 窗口大小&lt;span class="math">\(k\)&lt;/span>是一个超参数，需要根据实验或经验获得。&lt;span class="math">\(k\)&lt;/span>越小，越能跟上变化，&lt;span class="math">\(k\)&lt;/span>越大，预测曲线越稳定。&lt;/p>
&lt;p>对于&lt;span class="math">\(n\)&lt;/span>个预测值，我们使用均方误差（MSE）衡量其误差： &lt;span class="math">\[
MSE=\frac{\sum_{i=1}^n (Y_i-F_i)^2}{n}
\]&lt;/span>&lt;/p>
&lt;p>加权移动平均：假设滑动窗口的大小为&lt;span class="math">\(k\)&lt;/span>，这些观察值并不平等，有些观察值（比如近期的观察值）比较重要，需要加上一个权重&lt;span class="math">\(\alpha\)&lt;/span>，体现不同观察值的重要性，所有的权重值之和为1。 &lt;span class="math">\[
F_{t+1}=\bar{Y}_t=\frac{\alpha_1Y_{t-k+1}+\alpha_2Y_{t-k+2}+\dotsb+\alpha_{k-1}Y_{t-1}+\alpha_kY_t}{k}\\
\alpha_1+\alpha_2+\dotsb+\alpha_k=1
\]&lt;/span> 加权移动平均可以更加灵活的反映不同时期数据对均值的影响，但是除了超参数&lt;span class="math">\(k\)&lt;/span>，还引入了另一组超参数&lt;span class="math">\(\alpha_i,i=\{1,2,\dotsb,k\}\)&lt;/span>。我们可以通过合理地调整这些超参数，来最小化MSE。&lt;/p>
&lt;h4 id="指数平滑法">指数平滑法&lt;/h4>
&lt;p>指数平均法算是加权平均的一种特例（没有窗口限制，且权重呈等比数列）。对过去的观察值加权平均进行预测的一种方法。观察值时间越远，其权重也跟着呈现指数的下降，这也是指数平滑法的名称由来。&lt;/p>
&lt;p>指数平滑法会将时间序列进行修匀，消除随机波动，找出序列变化趋势。其计算公式为： &lt;span class="math">\[
F_{t}=\alpha Y_{t-1}+(1-\alpha)F_{t-1}
\]&lt;/span> 其中，&lt;span class="math">\(Y_{t-1}\)&lt;/span>为&lt;span class="math">\(t-1\)&lt;/span>期实际观察值，&lt;span class="math">\(F_{t-1}\)&lt;/span>为&lt;span class="math">\(t-1\)&lt;/span>期预测值，&lt;span class="math">\(\alpha\)&lt;/span>为平滑系数（&lt;span class="math">\(0&amp;lt;\alpha&amp;lt;1\)&lt;/span>），对于起点我们设&lt;span class="math">\(F_1=Y_1\)&lt;/span>。如果我们将式子展开，最终预测值会是前面各期的加权平均。显然，&lt;span class="math">\(\alpha\)&lt;/span>越大，新的观察值&lt;span class="math">\(Y_{t-1}\)&lt;/span>比例越大，预测对变动越敏感，适用于随机波动比较大的序列；反则反之。&lt;/p>
&lt;h3 id="趋势序列的预测">趋势序列的预测&lt;/h3>
&lt;p>所谓趋势，就是持续向上或持续向下的状态或规律，可分为线性趋势和非线性趋势。常用的趋势预测方法有线性趋势预测、非线性趋势预测和自回归模型预测。&lt;/p>
&lt;h4 id="线性趋势预测">线性趋势预测&lt;/h4>
&lt;p>本质上是线性回归，用观察值做线性回归。&lt;/p>
&lt;h4 id="非线性趋势预测">非线性趋势预测&lt;/h4>
&lt;p>指数曲线，使用指数曲线取回归数据。本上算是广义线性回归。对于指数曲线，只要取&lt;span class="math">\(\ln\)&lt;/span>，就可以变成线性回归。 &lt;span class="math">\[
\hat{Y}_t=b_0b_1^t
\]&lt;/span>&lt;/p>
&lt;p>修正指数曲线：加了一个常数偏置&lt;span class="math">\(K\)&lt;/span>。 &lt;span class="math">\[
\hat{Y}_t=K+b_0b_1^t
\]&lt;/span>&lt;/p>
&lt;p>Gompertz曲线： &lt;span class="math">\[
\hat{Y}_t=Ka^{b^{t}}
\]&lt;/span>&lt;/p>
&lt;p>Logistics曲线 &lt;span class="math">\[
\hat{Y}_t=\frac{1}{K+ab^t}
\]&lt;/span>&lt;/p>
&lt;p>多阶曲线：高阶多项式拟合。 &lt;span class="math">\[
\hat{Y}_t=b_0+b_1t++b_2t^2+\dotsb++b_kt^k
\]&lt;/span>&lt;/p>
&lt;h3 id="季节因素考虑">季节因素考虑&lt;/h3>
&lt;blockquote>
&lt;p>季节指数:刻画序列在一个年度个月或各季度的典型季节特征。&lt;/p>
&lt;/blockquote></description></item><item><title>机器学习之广义线性模型</title><link>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</guid><description>
&lt;h2 id="机器学习之广义线性模型">机器学习之广义线性模型&lt;!-- omit in toc -->&lt;/h2>
&lt;p>在机器学习中，我们常常是从线性回归和Logistics回归这两种模型入手。大多数人在学的时候时将其当成两个独立的模型去学习的，线性回归用来拟合直线，Logistics回归用来分类。实际上，这两种模型都是一个更广泛模型的特例，这就是广义线性模型（Generalized Linear Models, 简称GLM）。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#引子线性回归和logistics回归">引子：线性回归和Logistics回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从概率的角度解释回归">从概率的角度解释回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族指数族">指数型分布族（指数族）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#自然指数族">自然指数族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数分散族">指数分散族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#广义线性模型与指数型分布族核心">广义线性模型与指数型分布族（核心）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性的体现">线性的体现&lt;/a>&lt;/li>
&lt;li>&lt;a href="#连接函数与激活函数">连接函数与激活函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#标准连接">标准连接&lt;/a>&lt;/li>
&lt;li>&lt;a href="#广义线性模型的参数关系">广义线性模型的参数关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#广义线性模型的最大似然估计">广义线性模型的最大似然估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#标准连接函数下的广义线性模型最大似然估计">标准连接函数下的广义线性模型最大似然估计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#logistics回归优化举例">Logistics回归优化举例&lt;/a>&lt;/li>
&lt;li>&lt;a href="#补充牛顿法的简化方法之一fisher分数法">补充：牛顿法的简化方法之一Fisher分数法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#回答引子的疑问最大似然估计形势下的迭代优化">回答引子的疑问，最大似然估计形势下的迭代优化&lt;/a>&lt;/li>
&lt;li>&lt;a href="#广义线性模型的求解irls算法">广义线性模型的求解（IRLS算法）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#参考文献">参考文献&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="引子线性回归和logistics回归">引子：线性回归和Logistics回归&lt;/h2>
&lt;p>如果刚学完线性回归和Logistics回归，那么是否会注意到，二者的梯度更新步骤都是(虽然&lt;span class="math">\(h_{\vec\theta}(\vec x^{(i)})\)&lt;/span>的定义不同)： &lt;span class="math">\[
\theta_j=\theta_j-\alpha(h_{\vec\theta}(\vec x^{(i)})-y^{(i)})x_j^{(i)}\\
h_{\vec\theta}(\vec x^{(i)})=\begin{cases}
\vec{\theta}^T \vec{x},\quad线性回归\\
\frac{1}{1+e^{-\vec{\theta}^T \vec{x}}},\quad Logistics回归\end{cases}
\]&lt;/span> 其中，&lt;span class="math">\(\vec\theta, \vec x^{(i)}\)&lt;/span>分别是参数向量，第&lt;span class="math">\(i\)&lt;/span>个观测数据的向量。下标&lt;span class="math">\(j\)&lt;/span>表示第&lt;span class="math">\(j\)&lt;/span>个分量，&lt;span class="math">\(\alpha\)&lt;/span>表示更新的步长。那么他们二者为什么都有相同的更新公式呢？(只有&lt;span class="math">\(h_{\vec{\theta}}(\vec{x})\)&lt;/span>的具体表现形式不同)。第一个原因是他们本质上都可以从最大似然估计推导出来；第二个原因则是它们二者都是一种更普遍的模型的特殊情况，这个模型就是&lt;strong>广义线性模型&lt;/strong>。&lt;/p>
&lt;h2 id="从概率的角度解释回归">从概率的角度解释回归&lt;/h2>
&lt;p>如果我们详细看统计回归的过程，就能发现它有两步组成。第一步是参数估计，确定特定模型中的未知参数，即求模型的参数&lt;span class="math">\(\Theta\)&lt;/span>；第二部根据已经确定的模型与参数，预测新数据的函数值，即求&lt;span class="math">\(y_{pred}\)&lt;/span>。&lt;/p>
&lt;p>我们使用参数估计的过程主要是使用MLE、MAP、Bayesian等准则（推荐文章&lt;a href="https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf">ML, MAP, and Bayesian --- The Holy Trinity of Parameter Estimation and Data Prediction&lt;/a>），这种方式往往是以上述某个准则推导出的概率最大的值作为参数估计的结果。因此，在第二部预测步骤时，给定参数估计结果&lt;span class="math">\(\theta\)&lt;/span>和自变量&lt;span class="math">\(x\)&lt;/span>后，通过模型得到的预测结果往往也不是实际真实值，而是&lt;strong>真实值和某个概率分布相关的误差组合起来的结果&lt;/strong>（我们希望这个组合出来的概率分布期望就是真实值）。&lt;/p>
&lt;p>回看线性回归的结果，其预测模型是一条直线，但是真实的数据点并不一定在直线上，而是以某个概率分布在预测模型周围。&lt;/p>
&lt;img src="../images/linear_regression.png" alt="线性回归" />
&lt;center>
图中线性回归为根据身高预测体重
&lt;/center>
&lt;p>以线性回归模型为例，假设回归函数为&lt;span class="math">\(y=\mathbf{\theta}^T\mathbf{x}\)&lt;/span> (&lt;span class="math">\(\theta,x\)&lt;/span>为向量), 对于每对观测结果&lt;span class="math">\((x^{(i)},y^{(i)})\)&lt;/span>，都有 &lt;span class="math">\[y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}\]&lt;/span> 其中 &lt;span class="math">\(\epsilon\)&lt;/span>为误差，基于一种合理的假设（中心极限定理），我们可以认为误差的分布服从正态分布(又称高斯分布)，即 &lt;span class="math">\(\epsilon \sim N(0,\sigma^2)\)&lt;/span> ，那么，我们可以认为&lt;span class="math">\(y^{(i)} \sim N(\theta^Tx^{(i)},\sigma^2)\)&lt;/span>,根据正态分布的概率公式 &lt;span class="math">\[P(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\]&lt;/span>&lt;/p>
&lt;p>我们可以通过最大似然估计求出模型的参数&lt;span class="math">\(\mathbf{\theta}\)&lt;/span>，进而得到线性模型&lt;span class="math">\(y=\mathbf{\theta}^T\mathbf{x}\)&lt;/span>。我们不难发现，&lt;span class="math">\(\mathbf{\theta}^T\mathbf{x}\)&lt;/span>正好是&lt;span class="math">\(y\)&lt;/span>这个高斯随机变量的期望！&lt;/p>
&lt;p>线性回归这条直线&lt;span class="math">\(y=f(x)\)&lt;/span>的真实含义其实是：&lt;strong>回归模型中，对于取特定的自变量&lt;span class="math">\(x^{(i)}\)&lt;/span>，其因变量概率的期望是&lt;span class="math">\(f(x^{(i)})\)&lt;/span>。同样，在Logistics回归中，最终的预测结果也是二项分布中取0或1的期望&lt;/strong>，其以0.5作为阈值的原因也在于此，若以大于0.5的期望取1，那么我们就认为结果是1；若以小于0.5的期望取1（取0概率大于0.5），那么我们就认为结果是0，本质上就是这么直白。&lt;/p>
&lt;p>此外，也可从&lt;span class="math">\(y_{pred}\)&lt;/span>为一个统计量的角度理解。&lt;/p>
&lt;p>我们之前已经说过，我们使用回归模型预测的值，其实也并不是精确值，而是预测值概率的期望。广义线性模型和概率的关系，就是我们的&lt;strong>预测值&lt;span class="math">\(y_{pred}\)&lt;/span>的概率分布服从指数族分布。而指数族的参数通过连接函数和线性函数连接到一起&lt;/strong>，即&lt;span class="math">\(g(u)=\theta^T x\)&lt;/span>，这一点我们之后再说。下面我们先看指数族。&lt;/p>
&lt;h2 id="指数型分布族指数族">指数型分布族（指数族）&lt;/h2>
&lt;p>指数型分布族是指数分布族的推广，囊括了正态分布族、二项分布族、伽马分布族、多项分布族常见分布等等。具体定义形式如下：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>指数型分布族&lt;/strong>：一个概率分布族&lt;span class="math">\(\mathfrak{p}=\{p_{\theta}(x);\theta∈\varTheta\}\)&lt;/span>可称为&lt;strong>指数型分布族&lt;/strong>，假如&lt;span class="math">\(\mathfrak{p}\)&lt;/span>中的分布（分布列或密度函数）都可表示为如下形式： &lt;span class="math">\[p_\theta(x)=h(x)c(\theta)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\}\tag{1}\]&lt;/span> 其中，k为自然数；&lt;span class="math">\(\theta\)&lt;/span>可以是数字，也可以是向量。分布的支撑&lt;span class="math">\(\{x:p(x)&amp;gt;0\}\)&lt;/span>与参数&lt;span class="math">\(\theta\)&lt;/span>无关；诸&lt;span class="math">\(c(\theta),c_1(\theta),\dotsb,c_k(\theta)\)&lt;/span>是定义在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上的函数；诸&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>是&lt;span class="math">\(x\)&lt;/span>的函数，称为充分统计向量，但&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>线性无关。&lt;span class="math">\(h(x)\)&lt;/span>也只是&lt;span class="math">\(x\)&lt;/span>的函数，且&lt;span class="math">\(h(x)&amp;gt;0\)&lt;/span>，通常是一个常数。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;span class="math">\(c(\theta)\)&lt;/span>是作为归一化参数存在的，称为叫做配分函数(partition function)。 &lt;span class="math">\[c(\theta)^{-1} = \int h(x) \exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\} dx\]&lt;/span> 此外，指数族还有另一种表述方式，就是将外面的&lt;span class="math">\(c(\theta)\)&lt;/span>放到指数符号中： &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)-A(\theta)\right\}\tag{2}\]&lt;/span> 由于通常&lt;span class="math">\(A(\theta)\)&lt;/span>含有&lt;span class="math">\(\log\)&lt;/span>符号，该部分也称为“Log Partition Function”，易知&lt;span class="math">\(A(\theta)=\ln c(\theta)\)&lt;/span>。 如果我们使用向量值函数来表达指数型分布族可写为: &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)-A(\theta)\right\}\tag{3}\]&lt;/span>&lt;/p>
&lt;p>从上述定义可知，一个分布族是不是指数型分布族的&lt;strong>关键在于其概率分布能否改写为定义中方式&lt;/strong>。&lt;/p>
&lt;h3 id="指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/h3>
&lt;p>下面我们使用&lt;strong>向量值函数&lt;/strong>将式(4)进行进一步改造。&lt;/p>
&lt;blockquote>
&lt;p>向量值函数：有时也称为向量函数，是一个单变量或多变量的、&lt;strong>值域是多维向量或者无穷维向量的集合的函数&lt;/strong>。向量值函数的输入可以是一个标量或者一个向量，输出是向量，定义域的维度和值域的维度是不相关的。&lt;/p>
&lt;/blockquote>
&lt;p>对于&lt;span class="math">\(\theta\)&lt;/span>的一系列函数&lt;span class="math">\(c_1(\theta),c_2(\theta),\dotsb\)&lt;/span>和充分统计量向量&lt;span class="math">\(T_1(x),T_2(x),\dotsb\)&lt;/span>，我们写出列向量形式： &lt;span class="math">\[
\mathbf{C}(\theta)=\begin{bmatrix}c_1(\theta)\\c_2(\theta)\\\vdots\\c_k(\theta)\end{bmatrix}
\mathbf{T}(x)=\begin{bmatrix}T_1(x)\\T_2(x)\\\vdots\\T_k(x)\end{bmatrix}
\]&lt;/span> 那么式（3）可写成 &lt;span class="math">\[
p(x;\theta)=h(x)\exp\left\{\mathbf{C}^T(\theta)\mathbf{T}(x)-A(\theta)\right\}\tag{4}
\]&lt;/span> 其中，&lt;span class="math">\(\mathbf{C}(\theta),\mathbf{T}(x)\)&lt;/span>都是向量值函数，&lt;span class="math">\(h(x),A(\theta)\)&lt;/span>都是普通函数。通常文章会把&lt;span class="math">\(A(\theta)\)&lt;/span>写成&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的形式，这两种本质上是等价的，但是&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的参数形式更加统一，为主流用法。由于&lt;span class="math">\(\mathbf{C}(\theta)\)&lt;/span>的计算结果本质上就是一个向量，我们可令向量值函数&lt;span class="math">\(\mathbf{C(\theta)}=\eta\)&lt;/span>，那么式（4）可表示为： &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{T}(x)-A(\eta)\right\}\tag{5}
\]&lt;/span> 这就是其他资料中的常见形式。其中&lt;span class="math">\(\eta=\mathbf{C}(\theta)\)&lt;/span>，参数&lt;span class="math">\(η\)&lt;/span>通常叫做自然参数(natural parameter)或者标准参数(canonical parameter)。这里注明：&lt;span class="math">\(A(\theta)\)&lt;/span>与&lt;span class="math">\(A(\eta)\)&lt;/span>实际上是两个不同的函数，但是可以通过&lt;span class="math">\(\eta=\mathbf{C}(\theta),\theta=\mathbf{C}^{-1}(\eta)\)&lt;/span>进行互换，因此在后文对他们不做区分。此外，&lt;span class="math">\(\eta,\theta\)&lt;/span>是一一对应的，这里先不加证明地写出这个引理。&lt;/p>
&lt;blockquote>
&lt;p>引理1：在指数族中函数&lt;span class="math">\(C(\cdot)\)&lt;/span>总是&lt;strong>单调连续的(存在逆函数)&lt;/strong>，所以自然参数&lt;span class="math">\(η\)&lt;/span>和原始参数&lt;span class="math">\(θ\)&lt;/span>是&lt;strong>存在一一映射关系的&lt;/strong>。 &lt;span class="math">\[
\eta=\mathbf{C}(\theta)\\
\theta=\mathbf{C}^{-1}(\eta)
\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>在指数型分布族中，使用标准参数&lt;span class="math">\(η\)&lt;/span>表示的公式形式称为&lt;strong>指数族分布的标准形式(canonical form)&lt;/strong>，在标准形式下，分布的参数是&lt;span class="math">\(η\)&lt;/span>。&lt;strong>实际上，从原始分布向指数型分布转换的过程就是将&lt;span class="math">\(\theta\)&lt;/span>转换为&lt;span class="math">\(\eta\)&lt;/span>的过程&lt;/strong>。&lt;/p>
&lt;p>广义线性模型的&lt;strong>预测值概率分布都属于指数型分布族&lt;/strong>。&lt;/p>
&lt;p>具体关于指数型分布式的细节请看笔记&lt;a href="概率统计随机过程之指数型分布族应用.md">概率统计随机过程之指数型分布族应用.md&lt;/a>&lt;/p>
&lt;h3 id="自然指数族">自然指数族&lt;/h3>
&lt;p>我们在式（5）中给出了指数型分布族的一般形式 &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{T}(x)-A(\eta)\right\}\tag{5}
\]&lt;/span> 但是对于广义线性模型的应用场景而言，还是复杂了一些，因此有一种简化的&lt;strong>自然指数族&lt;/strong>。在自然指数族中，&lt;span class="math">\(\mathbf{T}(\mathbf{x})=\mathbf{x}\)&lt;/span>，不存在类似于&lt;span class="math">\(x^2,x^3,\log(x),\frac{1}{x}\)&lt;/span>这种带有函数关系的充分统计量，其可以简化写成： &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{x}-A(\eta)\right\}\tag{6}
\]&lt;/span> 二项分布，负二项分布，伯努利分布，泊松分布，参数&lt;span class="math">\(\alpha\)&lt;/span>已知的Gamma分布，已知方差的高斯分布，参数&lt;span class="math">\(\lambda\)&lt;/span>已知的逆高斯分布（又称Wald分布）等都可以写成自然指数族形式，其他分布如卡方分布、Beta分布、帕累托分布，对数正态分布，一般正态分布，一般Gamma分布则无法写成自然指数族的形式。他们是否是自然指数族的核心就在于是不是充分统计量&lt;span class="math">\(T(x)=x\)&lt;/span>。&lt;/p>
&lt;h3 id="指数分散族">指数分散族&lt;/h3>
&lt;p>在自然指数族的基础上，研究者们为了方便探究分布的期望和方差，对自然指数族做了少些变形得到指数分散族。其处理方法是将自然指数族的规范形式(式(6))的规范（自然）参数&lt;span class="math">\(\eta\)&lt;/span>拆分成与位置（期望）相关的位置函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>以及和方差相关的分散函数&lt;span class="math">\(a(\phi)\)&lt;/span>。其形式如下： &lt;span class="math">\[
p(x;\vartheta)=\exp\{\frac{\vartheta^T x-b(\vartheta)}{a(\phi)}+c(x,\phi)\}\tag{7}
\]&lt;/span> 这种形式的指数族通常被称为指数分散族(exponential dispersion family,EDF)，&lt;span class="math">\(a(ϕ)\)&lt;/span>称为分散函数(dispersion function)，是已知的。&lt;span class="math">\(ϕ\)&lt;/span>称为分散参数(dispersion parameter)。&lt;span class="math">\(\vartheta\)&lt;/span>仍然叫自然参数(natural parameter)或者规范参数(canonical parameter)，它和自然指数族中参数差了个系数，因为两种模式中&lt;span class="math">\(\vartheta^T x,\eta^Tx\)&lt;/span>的模式都是&lt;strong>参数&lt;span class="math">\(\times\)&lt;/span>充分统计量&lt;/strong>，所以不难发现，实际上我们对自然参数做一个&lt;span class="math">\(\frac{1}{a(\phi)}\)&lt;/span>倍的缩放。需要指出的是，在广义线性模型中，&lt;span class="math">\(a(\phi)\)&lt;/span>一般是 已知的，且通常是个常数系数，如果样本之间的重要性没有区别，我们可以令&lt;span class="math">\(a(\phi)=\phi\)&lt;/span>，即 &lt;span class="math">\[
p(x;\vartheta)=\exp\{\frac{\vartheta^T x-b(\vartheta)}{\phi}+c(x,\phi)\}\tag{7.1}
\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>指数分散族形式本质上是对自然指数族的参数&lt;span class="math">\(\eta\)&lt;/span>进行了拆分，把期望参数和方差参数拆分开（二者实际是可逆的变换）&lt;/strong>。使得自然参数&lt;span class="math">\(\vartheta\)&lt;/span>仅和期望&lt;span class="math">\(μ\)&lt;/span>相关，分散参数&lt;span class="math">\(ϕ\)&lt;/span>和分布的方差参数相关。分拆后，规范参数&lt;span class="math">\(\vartheta\)&lt;/span>仅和分布的期望参数&lt;span class="math">\(μ\)&lt;/span>相关，并且和&lt;span class="math">\(μ\)&lt;/span>之间存在一一映射的函数关系，换句话说，&lt;span class="math">\(\vartheta\)&lt;/span>和&lt;span class="math">\(μ\)&lt;/span>可以互相转化。 &lt;span class="math">\[
\vartheta=f(\mu)\\
\mu=f^{−1}(\vartheta)\tag{8}
\]&lt;/span> 这一点接下来会证明。&lt;/p>
&lt;p>由笔记&lt;a href="概率统计随机过程之指数型分布族应用.md">概率统计随机过程之指数型分布族应用&lt;/a>可知，指数分散族的期望和方差可表达为： &lt;span class="math">\[
E[X]=b&amp;#39;(\vartheta)=\mu\tag{9}
\]&lt;/span> &lt;span class="math">\[
\mathrm{Var}[X]=a(\phi)b&amp;#39;&amp;#39;(\vartheta)\tag{10}
\]&lt;/span> 从期望和方差的关系，我们能发现&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>也是一一对应关系。根据式（9）可知，&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>有函数关系，且由于&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>的导数&lt;span class="math">\(b&amp;#39;&amp;#39;(\vartheta)\)&lt;/span>是方差（恒大于0）乘以一个已知数&lt;span class="math">\(a(\phi)\)&lt;/span>（式（10）结论），因此&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>的导数必然恒为正数或负数（取决于已知数&lt;span class="math">\(a(\phi)\)&lt;/span>），即&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>必为单调函数，而单调函数必存在反函数，推得必存在&lt;span class="math">\(b&amp;#39;^{-1}\)&lt;/span>，使得&lt;span class="math">\(\vartheta=b&amp;#39;^{-1}(\mu)\)&lt;/span>。因此&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>是一一对应的。&lt;/p>
&lt;p>我们定义配分函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>的二阶导数为&lt;strong>方差函数&lt;/strong>(variance function)，方差函数是一个关于期望&lt;span class="math">\(μ\)&lt;/span>的函数，即 &lt;span class="math">\[
b&amp;#39;&amp;#39;(\vartheta)=\nu(μ)\tag{11}
\]&lt;/span> 方差函数&lt;span class="math">\(ν(μ)\)&lt;/span>存在两种情况：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>方差函数是一个常量值，&lt;span class="math">\(ν(μ)=b&amp;#39;&amp;#39;(\vartheta)=C\)&lt;/span>，此时分布的方差与均值无关。典型的分布就是正态分布。&lt;/li>
&lt;li>方差函数是一个关于均值&lt;span class="math">\(μ\)&lt;/span>的函数，&lt;span class="math">\(ν(μ)=b&amp;#39;&amp;#39;(\vartheta)\)&lt;/span>，此时分布的方差与均值有关。&lt;/li>
&lt;/ol>
&lt;p>我们从一般的概率分布推导出指数族，然后其中的一个子集自然指数族，最后给它做一个变型的指数分散族，这么兜兜绕绕就是为了方便广义线性的计算与推导。&lt;/p>
&lt;h2 id="广义线性模型与指数型分布族核心">广义线性模型与指数型分布族（核心）&lt;/h2>
&lt;p>我们在前面提到了用概率的形式理解回归模型和指数型分布族，这两个概率论的概念其实是广义线性模型的核心，然而在实际应用中，不论是一般线性模型还是广义线性模型，都没有体现出概率的影子。概率论似乎从回归模型中消失了，这一节我们将概率论从幕后拖出来，展示其操作广义线性模型的真面目。&lt;/p>
&lt;p>我们在&lt;a href="#从概率的角度解释回归">从概率的角度解释回归&lt;/a>章节中指出，根据参数估计形成的预测模型给出的预测值&lt;span class="math">\(y_{pred}\)&lt;/span>实际上并不会和观测到的&lt;span class="math">\(y_{obs}\)&lt;/span>(observation)完全一致。这其中的缘由主要有两点：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>观测值&lt;span class="math">\(y_{obs}\)&lt;/span>与对应的&lt;span class="math">\(x_{obs}\)&lt;/span>并不是严格的函数关系，而是在某个与&lt;span class="math">\(x_{obs}\)&lt;/span>相关的函数附近随机波动，即&lt;span class="math">\(y_{obs}=f(x_{obs})+\varepsilon\)&lt;/span>，其中&lt;span class="math">\(\varepsilon\)&lt;/span>是随机数，服从特定分布。但是，实际场景下&lt;span class="math">\(f(x_{obs})\)&lt;/span>与&lt;span class="math">\(\varepsilon\)&lt;/span>也不一定是相加的关系。&lt;/li>
&lt;li>我们的预测模型&lt;span class="math">\(f(\cdot)\)&lt;/span>并不保证一定准确，模型的参数也是通过观测数据通过统计推断的形式如（最大似然估计、最大后验概率估计）得到的，而非精确推导。&lt;/li>
&lt;/ol>
&lt;p>因此，我们认为响应变量&lt;span class="math">\(y_{obs}\)&lt;/span>也服从带有特定参数的分布&lt;span class="math">\(Y=P(y_{obs}|x_{obs})\)&lt;/span>，即&lt;span class="math">\(y_{obs}\)&lt;/span>是个随机变量。然而，&lt;span class="math">\(y_{obs}\)&lt;/span>不是一个独立的随机变量，它和&lt;span class="math">\(x_{obs}\)&lt;/span>有着密切关系，我们可以把&lt;span class="math">\(x_{obs}\)&lt;/span>看成&lt;span class="math">\(Y\)&lt;/span>的分布参数。实际应用中，我们不会说给出&lt;span class="math">\(Y\)&lt;/span>的分布当成&lt;span class="math">\(y_{pred}\)&lt;/span>让用户或者系统使用，这样一是不好用，二是计算起来经常无法得到数值结果。所以我们有个自然而然的想法：&lt;strong>使用一个代表性的数字来替代&lt;span class="math">\(Y\)&lt;/span>的概率分布&lt;/strong>。&lt;/p>
&lt;p>那么，如果只选一个数字来代替整个概率分布，大家的第一反应基本都是&lt;strong>期望&lt;/strong>。因此，我们在建立回归模型的时候，&lt;strong>希望根据给定的&lt;span class="math">\(x_{obs}\)&lt;/span>得到的预测值&lt;span class="math">\(y_{pred}\)&lt;/span>等于&lt;span class="math">\(Y\)&lt;/span>分布的条件期望&lt;span class="math">\(E[Y|x_{obs}]\)&lt;/span>&lt;/strong>。 &lt;span class="math">\[
\mu=E[Y|x_{obs}]=y_{pred}=f(x_{obs})\tag{12}
\]&lt;/span> 式（12）是推导广义线性模型的核心。由于求概率的期望是一个抹除随机性的过程，因此在回归模型中，概率分布的痕迹被隐藏了起来。其中的&lt;span class="math">\(f(\cdot)\)&lt;/span>就是要训练的广义线性模型。&lt;/p>
&lt;p>从函数的角度来看，&lt;span class="math">\(x_{obs}\)&lt;/span>为自变量，&lt;span class="math">\(f(\cdot)\)&lt;/span>为映射关系（广义线性模型），&lt;span class="math">\(y_{pred}\)&lt;/span>为因变量，又称响应变量（Response variable）。从式（12）我们也可以看出，建立的广义线性模型其根据自变量计算得到的因变量，应等于随机变量&lt;span class="math">\(Y\)&lt;/span>的期望。&lt;/p>
&lt;p>我们要解析广义线性模型，首先就要从&lt;span class="math">\(Y\)&lt;/span>的分布谈起。经过之前章节的铺垫，我们应该已经猜到，&lt;span class="math">\(Y\)&lt;/span>&lt;strong>是属于指数型分布族&lt;/strong>，即&lt;span class="math">\(Y\sim P(y;\theta)\)&lt;/span>。这是有现实意义的，比如认为估计的连续型随机变量属于高斯分布、二分类型随机变量属于伯努利分布等等。同时，我们也认识到，之所以广义线性模型都属于指数型分布族只不过是因为我们人为地挑了容易研究的这类分布族罢了。&lt;strong>所以，从因果关系上来讲，并非广义线性模型都使用指数型分布族，而是我们先选中指数型分布族，然后把符合这些分布族的模型命名为广义线性模型&lt;/strong>。&lt;/p>
&lt;p>虽说，&lt;span class="math">\(P(y;\theta)\)&lt;/span>是指数型分布族，&lt;strong>但是广义线性模型用到的指数族模型并不像式（5）那么复杂，而是属于自然指数族或指数分散族&lt;/strong>。由于自然指数族和指数分布族是等效的，且指数分散族更适合模型的推导，同时我们一般不对样本的重要性有区分，因此我们下文主要使用&lt;span class="math">\(a(\phi)\)&lt;/span>为常函数的指数分散族的形式，如式（7.1）所示： &lt;span class="math">\[
p(x;\vartheta)=\exp\{\frac{\vartheta^T x-b(\vartheta)}{\phi}+c(x,\phi)\}\tag{7.1}
\]&lt;/span> 为了区别观测数据&lt;span class="math">\(x_{obs}\)&lt;/span>和式（7.1）pdf中的自变量&lt;span class="math">\(x\)&lt;/span>，我们使用预测值&lt;span class="math">\(y\)&lt;/span>替代式（7.1）中的&lt;span class="math">\(x\)&lt;/span>，即 &lt;span class="math">\[
p(y;\vartheta)=\exp\{\frac{\vartheta^T y-b(\vartheta)}{\phi}+c(y,\phi)\}\tag{7.2}
\]&lt;/span> &lt;strong>此式（7.2）即为在下文中使用的广义线性模型概率分布表达式&lt;/strong>。&lt;/p>
&lt;p>根据公式（9）我们可知，给定&lt;span class="math">\(\vartheta\)&lt;/span>的&lt;span class="math">\(Y\)&lt;/span>的期望为配分函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>的一阶导数: &lt;span class="math">\[
E[Y;\vartheta]=b&amp;#39;(\vartheta)=\mu\tag{9.1}
\]&lt;/span> 也就是说，我们使用期望&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>来代表&lt;span class="math">\(Y\)&lt;/span>的整个分布，并且也应是广义线性模型预测的结果&lt;span class="math">\(y_{pred}\)&lt;/span>。需要指出的是，&lt;span class="math">\(\vartheta\)&lt;/span>是&lt;span class="math">\(x_{obs}\)&lt;/span>的函数（有确定关系），因此&lt;span class="math">\(E[Y|x_{obs}]=E[Y|\vartheta]\)&lt;/span>。综合式（12）（9.1）我们有： &lt;span class="math">\[
b&amp;#39;(\vartheta)=\mu=y_{pred}=f(x_{obs})\\
\Rightarrow b&amp;#39;(\vartheta)=f(x_{obs})\tag{13}
\]&lt;/span> 从式（12）、式（9.1）、式（13）我们通过求期望的方式去除了&lt;span class="math">\(Y\)&lt;/span>的随机性，得到了一个确定性的表达式，这也将概率统计的影子从广义线性模型中消去了。如果说还是留有概率分布的痕迹的话，那么就只有指数分散族的配分函数的导数&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>还在其中。&lt;/p>
&lt;p>式（13）还说明我们&lt;strong>要求的广义线性模型的表达式和配分函数的导数&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>是存在密切关系的！&lt;/strong> 此外，我们在式（9）（10）的介绍中指出由于&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>是一一对应的，即存在反函数 &lt;span class="math">\[\vartheta=b&amp;#39;^{-1}(\mu)\tag{14}\]&lt;/span> 这个式子将会把广义线性模型中的“线性”和指数型分布族联系起来。&lt;/p>
&lt;h3 id="线性的体现">线性的体现&lt;/h3>
&lt;p>我们前面说了指数型分布族、期望，甚至得到了自变量&lt;span class="math">\(x_{obs}\)&lt;/span>与&lt;span class="math">\(y_{pred}\)&lt;/span>的某种关系&lt;span class="math">\(\mu=b&amp;#39;(\vartheta)=y_{pred}=f(x_{obs})\)&lt;/span>，然而还有一个关键点没有解决，那就是如何将概率分布的参数&lt;span class="math">\(\vartheta\)&lt;/span>和&lt;span class="math">\(x\)&lt;/span>观测值&lt;span class="math">\(x_{obs}\)&lt;/span>联系起来。我们可以通过上式发现，&lt;span class="math">\(\vartheta\)&lt;/span>和&lt;span class="math">\(x_{obs}\)&lt;/span>是通过均值&lt;span class="math">\(\mu\)&lt;/span>联系在一起的。而&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>的关系可以通过式（14）确定。然而，&lt;span class="math">\(x_{obs}\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>的关系还没有定下来。这也是广义“线性”模型中线性一词的由来，我们人为地设计一个&lt;strong>线性预测器&lt;/strong>： &lt;span class="math">\[
\kappa=\beta^T x_{obs}+b\tag{15}
\]&lt;/span> 即我们&lt;strong>设定&lt;/strong>参数&lt;span class="math">\(\kappa\)&lt;/span>是&lt;span class="math">\(x_{obs}\)&lt;/span>的线性组合，&lt;span class="math">\(x_{obs}\)&lt;/span>可以是标量也可以是向量。为了简洁性，通常会人为的为&lt;span class="math">\(x\)&lt;/span>扩充一个一维常量值1，并且把截距参数&lt;span class="math">\(b\)&lt;/span>算在&lt;span class="math">\(β\)&lt;/span>中，这样上述线性函数可以写成向量內积的形式。 &lt;span class="math">\[
\kappa=\beta^T x_{obs}\tag{15.1}
\]&lt;/span> 结合式（13）（14）我们可以得到 &lt;span class="math">\[
\mu=b&amp;#39;(\vartheta)=h(\kappa)\tag{16}
\]&lt;/span> &lt;span class="math">\[
\vartheta=b&amp;#39;^{-1}(\mu)=b&amp;#39;^{-1}(h(\kappa))=b&amp;#39;^{-1}(h(\beta^T x_{obs}))\tag{17}
\]&lt;/span> 根据式（13）的关系，显然有&lt;span class="math">\(f(x_{obs})=h(\kappa)=h(\beta^T x_{obs})\)&lt;/span>。由于&lt;span class="math">\(f\)&lt;/span>与&lt;span class="math">\(h\)&lt;/span>之间自变量&lt;span class="math">\(x_{obs},\beta^T x_{obs}\)&lt;/span>是线性变换，因此只要&lt;span class="math">\(f(\cdot)\)&lt;/span>存在，那么一般情况下，&lt;span class="math">\(h(\cdot)\)&lt;/span>必然存在。这样我们就得到了广义线性模型，我们使用式（16）来通过观测值&lt;span class="math">\(x_{obs}\)&lt;/span>与&lt;span class="math">\(\beta\)&lt;/span>的线性组合得到预测值&lt;span class="math">\(y_{pred}=\mu\)&lt;/span>。而式（17）则表明了线性预测器&lt;span class="math">\(\beta^T x_{obs}\)&lt;/span>与自然参数&lt;span class="math">\(\vartheta\)&lt;/span>的关系。现在还有一个问题，就是函数关系&lt;span class="math">\(h(\cdot)\)&lt;/span>是什么样的呢？&lt;/p>
&lt;h3 id="连接函数与激活函数">连接函数与激活函数&lt;/h3>
&lt;p>从式（16）（17）我们可以看出预测值（期望）和线性预测器之间的关系。我们将令 &lt;span class="math">\(g(\mu)=h^{-1}(\mu)\)&lt;/span>称为&lt;strong>连接函数（Link function）&lt;/strong>，之所以叫这个名字是因为它将线性预测器和最终的预测值（期望）联系到了一起，即 &lt;span class="math">\[
g(\mu)=h^{-1}\circ h(\beta^T x_{obs})=\beta^T x_{obs}=\kappa\tag{18}
\]&lt;/span> 但是在实际使用中，连接函数并不常用，反倒是连接函数的反函数&lt;span class="math">\(g^{-1}=h\)&lt;/span>更常见，因为我们可以用过&lt;span class="math">\(g^{-1}(\beta^T x_{obs})\)&lt;/span>计算出广义线性模型的预测值&lt;span class="math">\(y_{pred}\)&lt;/span>，我们称&lt;span class="math">\(h=g^{-1}\)&lt;/span>为&lt;strong>激活函数(Activation function)&lt;/strong>。 &lt;span class="math">\[
y_{pred}=g^{-1}(\kappa)=g^{-1}(\beta^T x_{obs})=h(\beta^ T x_{obs})\tag{19}
\]&lt;/span> 显然，广义线性模型中&lt;strong>通过线性预测器和激活函数就可以得到预测结果&lt;span class="math">\(y_{pred}\)&lt;/span>&lt;/strong>，而激活函数与连接函数是反函数的关系。现在，如果我们能知道连接函数，就可以完成整个广义线性模型了！&lt;/p>
&lt;p>那么现在就有一个问题，我们如何来确定连接函数呢？此外，也引出另一个问题:连接函数是唯一的吗？我们先回答后一个问题，即连接函数并不是唯一的。下面我们讨论如何找连接函数&lt;span class="math">\(g\)&lt;/span>。&lt;/p>
&lt;p>前面我们提到过&lt;span class="math">\(\vartheta\)&lt;/span>是&lt;span class="math">\(x_{obs}\)&lt;/span>的函数（有确定关系），且有&lt;span class="math">\(E[Y|x_{obs}]=E[Y|\vartheta]=\mu=h(x_{obs})\)&lt;/span>，因此本质上期望&lt;span class="math">\(\mu\)&lt;/span>也是一个关于&lt;span class="math">\(x_{obs}\)&lt;/span>的函数。那么，式（18）可简写成 &lt;span class="math">\[
g(\mu)=\beta^T x_{obs}\tag{18.1}
\]&lt;/span> 这是一个比较有意思的式子，左右两边根源自变量都是&lt;span class="math">\(x_{obs}\)&lt;/span>。那么连接函数&lt;span class="math">\(g\)&lt;/span>就必须要满足2个条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>由于&lt;span class="math">\(\beta^T x_{obs}\)&lt;/span>的取值可能是整个实数域，但是期望&lt;span class="math">\(\mu\)&lt;/span>是有范围的，比如泊松分布的均值必大于0，即&lt;span class="math">\(\mu\in (0,+\infty)\)&lt;/span>，因此&lt;span class="math">\(g\)&lt;/span>必须将&lt;span class="math">\(\mu\)&lt;/span>的取值范围映射到整个实数域，这称为&lt;strong>定义域要求&lt;/strong>。连接函数本质上，就是把实数域范围的&lt;span class="math">\(\beta^T x_{obs}\)&lt;/span>转换到特定分布合法的&lt;span class="math">\(\mu\)&lt;/span>值空间上。&lt;/li>
&lt;li>此外，我们希望&lt;span class="math">\(g\)&lt;/span>是可逆的（可微且严格单调），这样&lt;span class="math">\(\mu\)&lt;/span>与&lt;span class="math">\(\beta^T x_{obs}\)&lt;/span>就有了一一对应的关系，这是因为函数一旦不可逆&lt;span class="math">\(\beta^T x_{obs}\)&lt;/span>就可能求出多个预测值，这显然是不符合实际情况的，这称为&lt;strong>可逆要求&lt;/strong>。&lt;/li>
&lt;/ol>
&lt;p>满足以上2点，就可以初步得到一个连接函数。可以看出，上面2个要求还是相对宽泛的，不同的映射可能得到不同的线性关系组合即&lt;span class="math">\(\beta_1^Tx_{obs},\beta_2^Tx_{obs},\dotsb\)&lt;/span>，这些都满足连接函数的要求，也就是说满足定义域映射关系的可逆函数都可以作为连接函数，因此&lt;strong>连接函数并不是唯一的&lt;/strong>。&lt;/p>
&lt;p>举两个例子：&lt;/p>
&lt;p>一：伯努利分布，我们知道其期望&lt;span class="math">\(\mu\in(0,1)\)&lt;/span>，那么连接函数&lt;span class="math">\(g\)&lt;/span>应该能够将定义域为&lt;span class="math">\((0,1)\)&lt;/span>的范围映射到整个实数域&lt;span class="math">\(\R\)&lt;/span>，那么下面三个连续单调函数（可逆）都可以满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Logit：&lt;span class="math">\(\log(\frac{\mu}{1-\mu})\)&lt;/span>；&lt;/li>
&lt;li>Probit：&lt;span class="math">\(\varPhi^{-1}(\mu)\)&lt;/span>，其中&lt;span class="math">\(\varPhi^{-1}\)&lt;/span>是正态分布概率累计函数的反函数，实际上在伯努利分布中，任意定义域为&lt;span class="math">\(\R\)&lt;/span>的概率累计函数的反函数都可以作为连接函数；&lt;/li>
&lt;li>互补Log-Log：&lt;span class="math">\(\log(-\log(1-\mu))\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>可以验证以上三个函数都可以将&lt;span class="math">\(\mu\in(0,1)\)&lt;/span>映射到实数域&lt;span class="math">\(\R\)&lt;/span>。&lt;/p>
&lt;p>二：泊松分布，泊松分布的期望值&lt;span class="math">\(\mu&amp;gt;0\)&lt;/span>，那么连接函数&lt;span class="math">\(g\)&lt;/span>需要将定义域&lt;span class="math">\((0,\infty)\)&lt;/span>的范围映射到整个实数域&lt;span class="math">\(\R\)&lt;/span>，那么下面两个连续单调函数（可逆）都可以满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Log函数：&lt;span class="math">\(\log(\mu)\)&lt;/span>;&lt;/li>
&lt;li>初等函数：&lt;span class="math">\(\mu-\frac{1}{\mu}\)&lt;/span>，虽然此函数在全局并不是连续单调的，但是在&lt;span class="math">\(\mu&amp;gt;0\)&lt;/span>时是全局单调的，存在反函数&lt;span class="math">\(y=\frac{x+\sqrt{x^2+4}}{2}\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;p>既然连接函数并不是唯一的，那么我们就要从中选出最为合适的连接函数。下一节，我们介绍一个比较常用的连接函数选择方式：&lt;strong>标准连接函数&lt;/strong>。&lt;/p>
&lt;h3 id="标准连接">标准连接&lt;/h3>
&lt;p>我们之前说个，连接函数并不是唯一的，那么选择哪一个连接函数比较好呢？这里我们推荐一种&lt;strong>比较自然的选择&lt;/strong>，注意只是说比较自然，但不一定是最合适的，在不同场景下某些连接函数确实表现地比其他连接函数更好。&lt;/p>
&lt;p>&lt;strong>所谓比较自然的选择，就是选择连接函数使得&lt;span class="math">\(g(\mu)=\vartheta=\beta^Tx_{obs}\)&lt;/span>。首先，&lt;span class="math">\(\vartheta\)&lt;/span>的取值范围就是&lt;span class="math">\(\R\)&lt;/span>，因此可以当成连接函数的值域，同时，我们也能保证常见情景下，这样选择的连接函数是可逆的（可用指数型分布族配分函数的二阶导数含义证明）。选择标准连接函数的一大优势就是可以极大地简化数学运算，非常契合广义线性模型的最大似然估计，我们在下一章节会详细讨论这个问题，这里先记住这个结论&lt;/strong>。&lt;/p>
&lt;p>根据式（16）与标准连接函数的条件&lt;span class="math">\(g(\mu)=\vartheta=\beta^Tx_{obs}\)&lt;/span>可以推出： &lt;span class="math">\[
g(\mu)=g(b&amp;#39;(\vartheta))=\vartheta\tag{20}
\]&lt;/span> 其中，&lt;span class="math">\(\vartheta\)&lt;/span>经过两次变换&lt;span class="math">\(g\circ b&amp;#39;\)&lt;/span>又变回了&lt;span class="math">\(\vartheta\)&lt;/span>，显然有&lt;span class="math">\(g\circ b&amp;#39;\)&lt;/span>为恒等变换，那么&lt;span class="math">\(g\)&lt;/span>与&lt;span class="math">\(b&amp;#39;\)&lt;/span>就互为反函数！即有 &lt;span class="math">\[
g^{-1}=b&amp;#39;\tag{21}
\]&lt;/span> 这样我们的标准连接函数就可以根据指数型分布族的配分函数&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>的反函数推得了！我们终于不用再在一堆连接函数中盲目地搜索、构造，只需要通过&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>就能求得。同时，我们知道，激活函数与连接函数也是互为反函数，那么根据式（21）的关系也不难发现： &lt;span class="math">\[
b&amp;#39;=h\tag{22}
\]&lt;/span> &lt;strong>指数型分布族的配分函数的一阶导数正好就是激活函数的形式&lt;/strong>！我们最终得到了一个既简洁又优美的结果！&lt;/p>
&lt;p>下表列出了常用广义线性模型的配分函数、激活函数、连接函数以及典型使用场景。注：在自然指数族中，我们只允许充分统计量&lt;span class="math">\(T(x)=x\)&lt;/span>，在其他文献中有别的设置方式，因此标准连接函数会略有所不同，大多数只是系数的差别。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">分布&lt;/th>
&lt;th align="center">一般形式&lt;/th>
&lt;th align="center">原参数与自然参数、分散参数关系&lt;/th>
&lt;th align="center">指数分散族形式&lt;/th>
&lt;th align="center">使用场景&lt;/th>
&lt;th align="center">配分函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>&lt;/th>
&lt;th align="center">激活函数&lt;span class="math">\(\mu=b&amp;#39;(\vartheta)=g^{-1}(\vartheta)\)&lt;/span>&lt;/th>
&lt;th align="center">连接函数&lt;span class="math">\(\beta^Tx=b&amp;#39;^{-1}(\mu)=g(\mu)\)&lt;/span>&lt;/th>
&lt;th align="center">分布的支撑&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">正态分布&lt;br>已知&lt;span class="math">\(\sigma\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\mu\\\phi=\sigma^2\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\frac{\vartheta x}{\phi}-\frac{\vartheta^2}{2\phi}-[\frac{x^2}{2\phi}+\ln(\sqrt{2\pi\phi})]\}\)&lt;/span>&lt;/td>
&lt;td align="center">随机误差服从正态分布，一般线性回归&lt;/td>
&lt;td align="center">&lt;span class="math">\(\frac{1}{2}\vartheta^2\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu=\vartheta\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\mu\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\((-\infty,+\infty)\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">逆高斯分布&lt;br>已知&lt;span class="math">\(\lambda(&amp;gt;0)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\sqrt{\frac{\lambda}{2\pi x^3}}e^{-\frac{\lambda(x-\mu)^2}{2\mu^2x}}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\frac{1}{2\mu^2}\\\phi=-\frac{1}{\lambda}&amp;lt;0\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\frac{\vartheta x}{\phi}-\frac{\sqrt{2\vartheta}}{\phi}+(\frac{1}{2\phi x}+\ln(\sqrt{\frac{-1}{2\pi\phi x^3}}))\}\)&lt;/span>&lt;/td>
&lt;td align="center">逆高斯分布描述的是布朗运动中到达固定距离所需时间的分布&lt;/td>
&lt;td align="center">&lt;span class="math">\(\sqrt{2\vartheta}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu=(2\vartheta)^{-1/2}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=(2\mu^2)^{-1}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\((0,+\infty)\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">伯努利分布&lt;/td>
&lt;td align="center">&lt;span class="math">\(p^x(1-p)^{1-x}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\ln\frac{p}{1-p}\\\phi=1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\vartheta x-\ln(1+e^{\vartheta})\}\)&lt;/span>&lt;/td>
&lt;td align="center">典型的二选一，比如抛硬币&lt;/td>
&lt;td align="center">&lt;span class="math">\(\ln(1+e^\vartheta)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu=\frac{1}{1+e^{-\vartheta}}\)&lt;/span>又称sigmoid函数&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\ln\frac{\mu}{1-\mu}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\{0,1\}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">二项分布(已知&lt;span class="math">\(n\)&lt;/span>)&lt;/td>
&lt;td align="center">&lt;span class="math">\(C_n^xp^x(1-p)^{n-x}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\ln\frac{p}{1-p}\\\phi=1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\vartheta x-n\ln(1+e^\vartheta)+\ln C_n^x\}\)&lt;/span>&lt;/td>
&lt;td align="center">在n次尝试中，概率为&lt;span class="math">\(p\)&lt;/span>的事件出现&lt;span class="math">\(x\)&lt;/span>次的概率&lt;/td>
&lt;td align="center">&lt;span class="math">\(n\ln(1+e^\vartheta)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu=\frac{n}{1+e^{-\vartheta}}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\ln\frac{\mu}{n-\mu}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\{0,1,\dotsb,n\}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">负二项分布(已知成功次数&lt;span class="math">\(r\)&lt;/span>)&lt;/td>
&lt;td align="center">&lt;span class="math">\(C_{r+x-1}^{r-1} p^r(1-p)^{x}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\ln(1-p)\\\phi=1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\vartheta x+r\ln(1-e^\vartheta)+\ln C_{r+x-1}^{r-1}\}\)&lt;/span>&lt;/td>
&lt;td align="center">在成功次数为&lt;span class="math">\(r\)&lt;/span>时，失败次数的分布，第&lt;span class="math">\(r\)&lt;/span>次必然是成功的&lt;/td>
&lt;td align="center">&lt;span class="math">\(-r\ln(1-e^\vartheta)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu=\frac{re^\vartheta}{1-e^\vartheta}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\ln\frac{\mu}{r+\mu}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\{0,1,2,\dotsb\}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">泊松分布&lt;/td>
&lt;td align="center">&lt;span class="math">\(e^{-\lambda}\frac{\lambda^x}{x!}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\ln\lambda\\\phi=1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\vartheta x-e^\vartheta-\ln(x!)\}\)&lt;/span>&lt;/td>
&lt;td align="center">表示在单位时间内，项目出现次数的概率分布，广泛应用于排队论&lt;/td>
&lt;td align="center">&lt;span class="math">\(e^\vartheta\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu=e^\vartheta\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\ln\mu\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\{0,1,2,\dotsb\}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">指数分布&lt;/td>
&lt;td align="center">&lt;span class="math">\(\lambda e^{-\lambda x}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\lambda\\\phi=-1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\frac{\vartheta x-\ln\lambda}{-1}\}\)&lt;/span>&lt;/td>
&lt;td align="center">基础分布，用途广泛&lt;/td>
&lt;td align="center">&lt;span class="math">\(\ln\vartheta\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu=1/\vartheta\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=1/\mu\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\((0,+\infty)\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">GAMMA分布(已知&lt;span class="math">\(k&amp;gt;0\)&lt;/span>)&lt;/td>
&lt;td align="center">&lt;span class="math">\(\frac{1}{\theta ^{k}\Gamma (k)}x^{k-1}e^{-x/\theta }\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=1/\theta\\\phi=-1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\frac{\vartheta x-k\ln\vartheta}{-1}+(k-1)\ln x-\ln\Gamma(k)\}\)&lt;/span>&lt;/td>
&lt;td align="center">可看成是指数分布的加和，具有可加性&lt;/td>
&lt;td align="center">&lt;span class="math">\(k\ln\vartheta\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu=k/\vartheta\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=k/\mu\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\((0,+\infty)\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">分类分布(共&lt;span class="math">\(m\)&lt;/span>个分类)&lt;/td>
&lt;td align="center">&lt;span class="math">\(\prod_{i=1}^m\theta_i^{x_i}\\\sum_i\theta_i=1\\\sum_i x_i=1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta=\begin{bmatrix}\ln(\theta_1/\theta_m)\\\ln(\theta_2/\theta_m)\\\vdots\\\ln(\theta_{m-1}/\theta_m)\\\ln(\theta_m/\theta_m)\end{bmatrix}\\\phi=1\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\exp\{\vartheta x-\ln\sum_{i=1}^me^{\vartheta_i}\}\)&lt;/span>&lt;/td>
&lt;td align="center">伯努利分布的拓展，多个当中只有一个发生&lt;/td>
&lt;td align="center">&lt;span class="math">\(\ln\sum_{i=1}^me^{\vartheta_i}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\mu_i=\frac{e^\vartheta_i}{\sum_{i=1}^me^{\vartheta_i}}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vartheta_i=\ln\frac{\vartheta_i}{\vartheta_m}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\{1,2,\dotsb,k\}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="广义线性模型的参数关系">广义线性模型的参数关系&lt;/h3>
&lt;p>我们在上表中，可以看出广义线性模型的表达式等于激活函数，即&lt;span class="math">\(f_{GLM}(x)=g^{-1}(\beta^T x)\)&lt;/span>，如果我们梳理一下上面的内容，可总结出广义线性模型的三大组成部分。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>一个指数族分布(指数分散族)作为响应变量&lt;span class="math">\(Y\)&lt;/span>概率分布&lt;span class="math">\(p(Y;\vartheta)\)&lt;/span>，被称为随机组件(random component)。&lt;/li>
&lt;li>一个线性预测器&lt;span class="math">\(\vartheta=β^Tx_{obs}\)&lt;/span>，被称为系统组件(systematic component)。&lt;/li>
&lt;li>一个连接函数&lt;span class="math">\(g\)&lt;/span>使得&lt;span class="math">\(E[Y]=\mu=g(\vartheta)=g(\beta^T x_{obs})\)&lt;/span>，描述了随机组件和系统组件之间的关系。&lt;/li>
&lt;/ol>
&lt;p>用图像表达他们的关系为（假设使用标准连接函数）：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/广义线性分布关系图.drawio.svg" alt="广义线性分布关系图" />&lt;p class="caption">广义线性分布关系图&lt;/p>
&lt;/div>
&lt;p>先从图坐往右看，待定参数&lt;span class="math">\(\beta\)&lt;/span>与观测值&lt;span class="math">\(x_{obs}\)&lt;/span>组成线性预测期，其结果为指数分散族的参数&lt;span class="math">\(\vartheta\)&lt;/span>，且其分散参数&lt;span class="math">\(\phi\)&lt;/span>已知。由于指数分散族和常使用的一般概率分布表达式是可以相互转换的，我们也可以得到原参数&lt;span class="math">\(\theta\)&lt;/span>和自然参数&lt;span class="math">\(\vartheta\)&lt;/span>的转换关系，可以证明它们是一一对应的。&lt;/p>
&lt;p>再从图右往左看，由于观测结果&lt;span class="math">\(y_{obs}\)&lt;/span>与&lt;span class="math">\(x_{obs}\)&lt;/span>存在相关性，而非确定函数关系，我们认为其&lt;span class="math">\(y_{obs}\)&lt;/span>服从特定的分布，我们认为这种分布是指数型分布族（指数分散族），因此可以用&lt;span class="math">\(P(y;\vartheta)\)&lt;/span>来表达&lt;span class="math">\(y_{obs}\)&lt;/span>的分布，用随机变量&lt;span class="math">\(Y\)&lt;/span>表示。当然，这个随机变量也可以用原始参数的概率分布&lt;span class="math">\(P(y;\theta)\)&lt;/span>表示。但是，在实际使用中，我们更希望得到一个具体值，而非一个分布，因此我们应使用最具代表性的值——期望&lt;span class="math">\(\mu\)&lt;/span>来表示分布，即期望应等于广义线性模型的预测值&lt;span class="math">\(\mu=y_{pred}\)&lt;/span>。同时，指数分散族有一个非常好的性质，其配分函数的导数&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>正好等于期望&lt;span class="math">\(\mu\)&lt;/span>。而原始参数的概率分布&lt;span class="math">\(P(y;\theta)\)&lt;/span>与期望&lt;span class="math">\(\mu\)&lt;/span>的关系&lt;span class="math">\(\varphi(\theta)\)&lt;/span>则需要根据具体情况计算，因此在广义线性模型中不太常用。&lt;/p>
&lt;p>由于&lt;span class="math">\(\mu\)&lt;/span>和线性预测期直接不是相等关系，需要一个桥梁，即连接函数&lt;span class="math">\(g(\mu)\)&lt;/span>，它阐述了期望和线性预测期之间的关系：&lt;span class="math">\(g(\mu)=\vartheta=\beta^T x_{obs}\)&lt;/span>。我们可以证明这个连接函数必存在反函数&lt;span class="math">\(g^{-1}\)&lt;/span>，这样我们就可以使用这个反函数来预测结果，即&lt;span class="math">\(g^{-1}(\vartheta)=g^{-1}(\beta^T x)\)&lt;/span>。我们将这个反函数称之为激活函数。最后我们发现配分函数的导数&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>与激活函数&lt;span class="math">\(g^{-1}(\vartheta)\)&lt;/span>有着一样的作用，因此我们得到激活函数就是配分函数的导数，即&lt;span class="math">\(b&amp;#39;=g^{-1}\)&lt;/span>。它们就是广义线性模型的函数&lt;span class="math">\(f(x_{obs})=b&amp;#39;(\beta^T x_{obs})=g^{-1}(\beta^T x_{obs})\)&lt;/span>。&lt;/p>
&lt;h2 id="广义线性模型的最大似然估计">广义线性模型的最大似然估计&lt;/h2>
&lt;p>本笔记的最后分两步，首先我们将说明使用标准连接函数时，使用最大似然估计方法我们可以得到非常简洁的解析结果；然后我们将首位呼应，回收文章开头引子中提出的疑问。&lt;/p>
&lt;h3 id="标准连接函数下的广义线性模型最大似然估计">标准连接函数下的广义线性模型最大似然估计&lt;/h3>
&lt;p>最大似然估计是广义线性模型中参数估计的一般方法，其目的是最大化似然函数，具体细节可参考笔记&lt;a href="概率统计随机过程之最大似然估计拓展.md">《概率统计随机过程之最大似然估计拓展.md》&lt;/a>。其模型通常假设如下：我们有一组&lt;span class="math">\(n\)&lt;/span>个观测值&lt;span class="math">\((x_i,y_i)\in(\R^p\times \R),i=1,2,\dotsb,n\)&lt;/span>，其中&lt;span class="math">\(x_i\)&lt;/span>是一个&lt;span class="math">\(p\)&lt;/span>维向量，&lt;span class="math">\(y_i\)&lt;/span>是观测结果是一个实数。观测值&lt;span class="math">\(x_i\)&lt;/span>会影响到广义线性模型的参数&lt;span class="math">\(\vartheta\)&lt;/span>，或者说&lt;span class="math">\(\vartheta\)&lt;/span>是&lt;span class="math">\(x_i\)&lt;/span>的函数，即&lt;span class="math">\(\vartheta=\vartheta(x_i)\)&lt;/span>。在最大似然估计中，我们假设各个观测值之间是独立的，那么每一个观测值发生的概率即为： &lt;span class="math">\[
p(y_i)=\exp\{\frac{\vartheta(x_i)y_i-b(\vartheta(x_i))}{\phi}+c(y_i,\phi)\}\tag{23}
\]&lt;/span> 显然，观测值&lt;span class="math">\(x_i\)&lt;/span>通过其因变量&lt;span class="math">\(\vartheta(x_i)\)&lt;/span>影响概率与期望，&lt;span class="math">\(x_i\)&lt;/span>给定时&lt;span class="math">\(\vartheta\)&lt;/span>也确定，然后&lt;span class="math">\(b(\vartheta)\)&lt;/span>及其一阶导数也确定了，最终可以确定&lt;span class="math">\(\mu=b&amp;#39;(\vartheta)\)&lt;/span>。我们先将独立的观测值相乘取&lt;span class="math">\(log\)&lt;/span>得到对数似然函数： &lt;span class="math">\[
\begin{aligned}
l_n(\vartheta(X);Y)&amp;amp;=\log \prod_{i=1}^n \exp\{\frac{\vartheta(x_i)y_i-b(\vartheta(x_i))}{\phi}+c(y_i,\phi)\}\\
&amp;amp;=\sum_{i=1}^n \frac{\vartheta(x_i)y_i-b(\vartheta(x_i))}{\phi}+c(y_i,\phi)
\end{aligned}\tag{24}
\]&lt;/span> 下面我们根据参数&lt;span class="math">\(\vartheta\)&lt;/span>，期望&lt;span class="math">\(\mu\)&lt;/span>、线性预测器&lt;span class="math">\(\beta^T x_i\)&lt;/span>以及连接函数&lt;span class="math">\(g\)&lt;/span>之间的关系对式（24）进行变形： &lt;span class="math">\[
\left .\begin{aligned}
式(9.1)\Rightarrow \vartheta=b&amp;#39;^{-1}(\mu)\\
式(18.1)\Rightarrow \mu=g^{-1}(\beta^T x_i)
\end{aligned}\right\}\Rightarrow \vartheta(x_i) = b&amp;#39;^{-1}\circ g^{-1}(\beta^T x_i)\\
\Rightarrow \vartheta(x_i)=(g\circ b&amp;#39;)^{-1}(\beta^T x_i)\tag{25}
\]&lt;/span> 上式集中体现了&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(x_i\)&lt;/span>之间的关系，代入式（24）可将变量&lt;span class="math">\(\vartheta(x_i)\)&lt;/span>替换成&lt;span class="math">\(\beta\)&lt;/span>： &lt;span class="math">\[
l_n(\beta;Y;X)=\sum_{i=1}^n \frac{(g\circ b&amp;#39;)^{-1}(\beta^T x_i)y_i-b[(g\circ b&amp;#39;)^{-1}(\beta^T x_i)]}{\phi}+c(y_i,\phi)\tag{26}
\]&lt;/span> 在场景中，&lt;span class="math">\((x_i,y_i)\)&lt;/span>都是已经获得的观测值，只有参数&lt;span class="math">\(\beta\)&lt;/span>是未知的，最大似然估计就是求&lt;span class="math">\(\beta\)&lt;/span>使得对数似然函数（等效于似然函数）最大。&lt;/p>
&lt;p>一般情况下，式（26）的计算并不是平凡的，但是如果我们选用&lt;strong>标准连接函数&lt;/strong>，这个式子就能得到极大简化！&lt;strong>根据式（21）有：&lt;span class="math">\(g=b&amp;#39;^{-1}\)&lt;/span>，这使得式（26）中的函数复合&lt;span class="math">\(g\circ b&amp;#39;=I\)&lt;/span>，即二者抵消，是一个恒等变换&lt;/strong>！此时，式（26）可简化为： &lt;span class="math">\[
l_n(\beta;Y;X)=\sum_{i=1}^n \frac{\beta^T x_iy_i-b(\beta^T x_i)}{\phi}+c(y_i,\phi)\tag{26.1}
\]&lt;/span> 其中，&lt;span class="math">\(\beta^T x_iy_i\)&lt;/span>是一个简单的线性函数；由式（10）可知&lt;span class="math">\(b(\beta^T x_i)\)&lt;/span>二阶导海森矩阵正定是一个严格凸函数；而最后的&lt;span class="math">\(c(y_i,\phi)\)&lt;/span>与&lt;span class="math">\(\beta\)&lt;/span>无关，求导时不造成影响。&lt;strong>这使得采取标准连接函数的最大似然估计可以通过凸优化的方法求得唯一极值&lt;/strong>，大大简化优化流程，免除了函数复合&lt;span class="math">\(g\circ b&amp;#39;\)&lt;/span>在求导计算中复杂情形，这也是我们愿意选用标准连接函数的&lt;strong>核心原因&lt;/strong>。&lt;/p>
&lt;p>附标准连接函数下最大似然估计的一二阶导数：&lt;/p>
&lt;p>一阶导数： &lt;span class="math">\[
\nabla_\beta l_n(\beta;Y;X)=\sum_{i=1}^n \frac{y_ix_i-b&amp;#39;(\beta^T x_i)x_i}{\phi};x_i\in \R^p\tag{27}
\]&lt;/span> 二阶导数： &lt;span class="math">\[
\nabla^2_\beta l_n(\beta;Y;X)=\sum_{i=1}^n\frac{-b&amp;#39;&amp;#39;(\beta^T x_i)x_ix_i^T}{\phi};x_i\in\R^p;x_i x_i^T \text{positive definite}\tag{28}\\
\forall y\neq 0\in \R^p\quad y^T x_i x_i^T y=(x_i^T y)^T (x_i^Ty)&amp;gt;0
\]&lt;/span>&lt;/p>
&lt;h4 id="logistics回归优化举例">Logistics回归优化举例&lt;/h4>
&lt;p>伯努利分布又叫两点分布或者0-1分布，是最简单的概率分布形式之一，其对应的广义线性模型就是Logistics回归，即二项分类。常见伯努利分布写成： &lt;span class="math">\[
p(y;\theta)=\theta^y(1-\theta)^{1-y},y\in\{0,1\},\theta\in[0,1]
\]&lt;/span> 转写为指数型分布族形式为： &lt;span class="math">\[
\begin{aligned}
p(y;\theta)&amp;amp;=\exp\{y\ln{\theta}+(1-y)\ln{(1-\theta)}\}\\
&amp;amp;=\exp\{y\ln(\frac{\theta}{1-\theta})+\ln{(1-\theta)}\}
\end{aligned}
\]&lt;/span> 令&lt;span class="math">\(\vartheta=\ln(\frac{\theta}{1-\theta})\)&lt;/span>将其转换成指数分散族： &lt;span class="math">\[
p(y;\vartheta)=\exp\{y\vartheta-\log(1+e^\vartheta)\}
\]&lt;/span> 其&lt;span class="math">\(n\)&lt;/span>个观测值组成的对数似然函数为： &lt;span class="math">\[
l_n(\vartheta(X);Y)=\sum_{i=1}^n (y_i\vartheta(x_i)-\log(1+e^{\vartheta(x_i)}))
\]&lt;/span> 我们采用标准连接函数： &lt;span class="math">\[
\vartheta(x_i)=\beta^T x_i
\]&lt;/span> 则对数似然函数可写为： &lt;span class="math">\[
l_n(\beta;Y;X)=\sum_{i=1}^n (\beta^T x_iy_i-\log(1+e^{\beta^Tx_i}))
\]&lt;/span> 对&lt;span class="math">\(\beta\)&lt;/span>求一阶，二阶导数分别为： &lt;span class="math">\[
\nabla_\beta l_n(\beta;Y;X)=\sum_{i=1}^n (y_ix_i-\frac{e^{\beta^Tx_i}}{1+e^{\beta^T x_i}}x_i);x_i\in \R^p\\
H_{l_n}(\beta)=\nabla^2_\beta l_n(\beta;Y;X)=\sum_{i=1}^n \frac{e^{\beta^Tx_i}}{(1+e^{\beta^T x_i})^2}x_ix_i^T;x_i\in\R^p;
\]&lt;/span> 注意，&lt;span class="math">\(x_ix_i^T\)&lt;/span>是由向量张成的矩阵。那么根据牛顿法，其优化迭代步骤为： &lt;span class="math">\[
\beta^{(k+1)}=\beta^{(k)}-H_{l_n}(\beta^{(k)})^{-1}\nabla_\beta l_n(\beta^{(k)};Y;X)
\]&lt;/span>&lt;/p>
&lt;h4 id="补充牛顿法的简化方法之一fisher分数法">补充：牛顿法的简化方法之一Fisher分数法&lt;/h4>
&lt;p>由于计算海森矩阵求和这一步很繁琐，因此我们可以用期望来替代求和操作（具体原理参见笔记&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>中最大似然估计与相对熵（KL散度）、交叉熵的等价性那一节）。因此有： &lt;span class="math">\[
H_{l_n}(\beta)=\nabla^2_\beta l_n(\beta;Y;X)=\sum_{i=1}^n\frac{-b&amp;#39;&amp;#39;(\beta^T x_i)x_ix_i^T}{\phi}\\
\simeq E[H_{l_n}(\beta)]=-I(\beta)
\]&lt;/span> 其中，&lt;span class="math">\(I(\beta)\)&lt;/span>是Fisher信息量。所以海森矩阵也可以被Fisher信息矩阵替代，即为： &lt;span class="math">\[
\beta^{(k+1)}=\beta^{(k)}+I(\beta^{(k)})^{-1}\nabla_\beta l_n(\beta^{(k)};Y;X)
\]&lt;/span> 这种方法称为Fisher分数法，这算是一种近似的拟牛顿法，其收敛速度和牛顿法相近，但是计算量降低。&lt;/p>
&lt;h3 id="回答引子的疑问最大似然估计形势下的迭代优化">回答引子的疑问，最大似然估计形势下的迭代优化&lt;/h3>
&lt;p>还记得在文章开头时的引子吗？&lt;/p>
&lt;blockquote>
&lt;p>如果刚学完线性回归和Logistics回归，那么是否会注意到，二者的梯度更新步骤都是(虽然&lt;span class="math">\(h_{\vec\theta}(\vec x^{(i)})\)&lt;/span>的定义不同)： &lt;span class="math">\[
\theta_j=\theta_j-\alpha(h_{\vec\theta}(\vec x^{(i)})-y^{(i)})x_j^{(i)}\\
h_{\vec\theta}(\vec x^{(i)})=\begin{cases}
\vec{\theta}^T \vec{x},\quad线性回归\\
\frac{1}{1+e^{-\vec{\theta}^T \vec{x}}},\quad Logistics回归\end{cases}
\]&lt;/span> 其中，&lt;span class="math">\(\vec\theta, \vec x^{(i)}\)&lt;/span>分别是参数向量，第&lt;span class="math">\(i\)&lt;/span>个观测数据的向量。下标&lt;span class="math">\(j\)&lt;/span>表示第&lt;span class="math">\(j\)&lt;/span>个分量，&lt;span class="math">\(\alpha\)&lt;/span>表示更新的步长。&lt;/p>
&lt;/blockquote>
&lt;p>这一是因为我们通过指数型分布族（指数分散族）给出了广义线性模型响应变量&lt;span class="math">\(Y\)&lt;/span>的统一形式，第二点就是由于我们使用的是最大似然估计，它的似然函数求导步骤与指数分散族非常契合。&lt;strong>实际上引子中的例子就是使用标准连接函数时一阶导数的应用&lt;/strong>。在梯度下降法中，通用的迭代公式为： &lt;span class="math">\[
\theta_j=\theta_j-\alpha\frac{\partial }{\partial \theta_j}L(\theta)
\]&lt;/span> 代入式（27）即为(变量&lt;span class="math">\(\beta\)&lt;/span>替换为&lt;span class="math">\(\theta\)&lt;/span>，另外最大似然估计要变成最小化损失函数，加个负号): &lt;span class="math">\[
\theta=\theta-\alpha\sum_{i=1}^n -\frac{y_ix_i-b&amp;#39;(\theta^T x_i)x_i}{\phi}=\theta-\alpha\sum_{i=1}^n(b&amp;#39;(\theta^T x_i)-y_i)x_i/\phi
\]&lt;/span> 其中，&lt;span class="math">\(b&amp;#39;(\theta^T x_i)\)&lt;/span>写成激活函数形式为&lt;span class="math">\(h_{\vec\theta}(\vec x^{(i)})\)&lt;/span>，采用单步迭代不需要求和&lt;span class="math">\(\sum\)&lt;/span>，如果分散系数为1，那么最终结果的各维度分量就是引子中的形式。&lt;/p>
&lt;h2 id="广义线性模型的求解irls算法">广义线性模型的求解（IRLS算法）&lt;/h2>
&lt;p>通常情况下，如果我们不考虑计算的复杂程度，广义线性模型可以使用梯度下降法或者牛顿进行求解。在这里，我们介绍一种广义线性模型的较常用的优化算法Iteratively Re-wighted Least Squares算法。这个算法能够将循环求和运算变换成矩阵运算，提供一个较为整体化的便捷计算形式。&lt;/p>
&lt;p>TODO&lt;/p>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;a href="https://zhangzhenhu.github.io/blog/glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">https://zhangzhenhu.github.io/blog/glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=mc1y8m9-hOM&amp;amp;list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0&amp;amp;index=21">https://www.youtube.com/watch?v=mc1y8m9-hOM&amp;amp;list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0&amp;amp;index=21&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>线性代数与矩阵之矩阵的正定性，二次型与合同矩阵</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%9F%A9%E9%98%B5%E7%9A%84%E6%AD%A3%E5%AE%9A%E6%80%A7%E4%BA%8C%E6%AC%A1%E5%9E%8B%E4%B8%8E%E5%90%88%E5%90%8C%E7%9F%A9%E9%98%B5/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%9F%A9%E9%98%B5%E7%9A%84%E6%AD%A3%E5%AE%9A%E6%80%A7%E4%BA%8C%E6%AC%A1%E5%9E%8B%E4%B8%8E%E5%90%88%E5%90%8C%E7%9F%A9%E9%98%B5/</guid><description>
&lt;h2 id="线性代数与矩阵之矩阵的正定性二次型与合同矩阵">线性代数与矩阵之矩阵的正定性，二次型与合同矩阵&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#对称埃米特矩阵与正定性">对称（埃米特）矩阵与正定性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正定矩阵性质">正定矩阵性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二次型与最小值">二次型与最小值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#合同矩阵了解">合同矩阵（了解）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#合同性质">合同性质&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>我们在对称矩阵的基础上定义了正定、半正定、负定、半负定的概念，并引入二次型，其与最大最小值相关。最后我们简介了矩阵的合同。&lt;/p>
&lt;h2 id="对称埃米特矩阵与正定性">对称（埃米特）矩阵与正定性&lt;/h2>
&lt;p>对称（埃米特）矩阵与矩阵的正定性，通常&lt;strong>正定性是定义在对称矩阵（或埃米特矩阵）上&lt;/strong>的，如果一个矩阵不是对称（埃米特）矩阵，就不具备讨论正定性的前提条件。&lt;/p>
&lt;blockquote>
&lt;p>一个&lt;span class="math">\(n×n\)&lt;/span>的实对称矩阵&lt;span class="math">\(A\)&lt;/span>是&lt;strong>正定的&lt;/strong>，当且仅当对于所有的非零实系数向量&lt;span class="math">\(x\)&lt;/span>，都有&lt;span class="math">\(x^TAx&amp;gt;0\)&lt;/span>，其中&lt;span class="math">\(x^T\)&lt;/span>表示x的转置。&lt;/p>
&lt;p>类似的，如果&lt;span class="math">\(x^TAx\geq 0\)&lt;/span>，则&lt;span class="math">\(A\)&lt;/span>称为&lt;strong>半正定矩阵&lt;/strong>；如果&lt;span class="math">\(x^TAx&amp;lt;0\)&lt;/span>，则&lt;span class="math">\(A\)&lt;/span>称为&lt;strong>负定矩阵&lt;/strong>；如果&lt;span class="math">\(x^TAx\leq 0\)&lt;/span>，则&lt;span class="math">\(A\)&lt;/span>称为&lt;strong>半负定矩阵&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>注：如果是复数域的埃米特矩阵，那么上面一定中的转置都有替换成共轭转置。&lt;/p>
&lt;p>正定性还有这几个等价命题：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的所有特征值为正。对于正定矩阵的特征向量&lt;span class="math">\(y\)&lt;/span>，有&lt;span class="math">\(y^TAy&amp;gt;0\Rightarrow y^T\lambda y=\lambda |y|^2&amp;gt;0\Rightarrow \lambda&amp;gt;0\)&lt;/span>。&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的所有主元为正。证明思路：主元与二次型展开的平方项系数一致。&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的顺序主子式为正。&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>与单位阵&lt;span class="math">\(I\)&lt;/span>合同，即存在可逆矩阵&lt;span class="math">\(C\)&lt;/span>，使得&lt;span class="math">\(A=C^TIC\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;p>类似的，半正定矩阵有以下等价命题：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的所有特征值为非负。&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的所有主元为非负。&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的顺序主子式为非负。&lt;/li>
&lt;/ol>
&lt;p>我们可以通过例子说明，非对称矩阵可以在满足上述1，2，3的前提下不满足正定矩阵的定义&lt;span class="math">\(x^TAx&amp;gt;0\)&lt;/span>。如下例 &lt;span class="math">\[
\begin{bmatrix}
1&amp;amp;-100\\0&amp;amp;1
\end{bmatrix}
\]&lt;/span> 显然，其主元、特征值都是1，顺序主子式也都大于0，但是对于&lt;span class="math">\(x^TAx\)&lt;/span>，我们随便找一个向量&lt;span class="math">\(x=[1,1]^T\)&lt;/span>，则 &lt;span class="math">\[
[1\quad 1]\begin{bmatrix}1&amp;amp;-100\\0&amp;amp;1\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix}=-98&amp;lt;0
\]&lt;/span> 这并不满足&lt;span class="math">\(x^TAX&amp;gt;0\)&lt;/span>。这说明，&lt;strong>正定性必须建立在对称矩阵上&lt;/strong>。&lt;/p>
&lt;h3 id="正定矩阵性质">正定矩阵性质&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>是正定矩阵，它的逆矩阵&lt;span class="math">\(A^{-1}\)&lt;/span>也是正定矩阵。因为逆矩阵的特征值是原矩阵的倒数&lt;span class="math">\(1\over \lambda\)&lt;/span>，因此也都是正数。&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A,B\)&lt;/span>是正定矩阵，那么&lt;span class="math">\(A+B\)&lt;/span>也是正定矩阵。证明：&lt;span class="math">\(x^T(A+B)x=x^TAx+x^TBx&amp;gt;0\)&lt;/span>&lt;/li>
&lt;li>如果&lt;span class="math">\(A\)&lt;/span>是一个&lt;span class="math">\(m×n\)&lt;/span>长方形矩阵，则&lt;span class="math">\(A^TA\)&lt;/span>是对称方阵，且一定半正定。证明：&lt;span class="math">\(x^TA^TAx=(Ax)^TAx=|Ax|^2\geq 0\)&lt;/span>。当&lt;span class="math">\(A\)&lt;/span>的秩为&lt;span class="math">\(n\)&lt;/span>时，&lt;span class="math">\(A^TA\)&lt;/span>为秩为&lt;span class="math">\(n\)&lt;/span>的&lt;span class="math">\(n\)&lt;/span>阶方阵，此时&lt;span class="math">\(A^TA\)&lt;/span>的零空间为零维空间，此时&lt;span class="math">\(A^TA\)&lt;/span>为正定矩阵。&lt;/li>
&lt;/ol>
&lt;h2 id="二次型与最小值">二次型与最小值&lt;/h2>
&lt;p>有了对称矩阵和定义在对称矩阵上的正定矩阵，我们由&lt;span class="math">\(x^TAx\)&lt;/span>引出二次型的概念。所谓“二次型”就是多元二次函数的矩阵表示形态，但是这个多元二次函数只包含二次项不包含一次项和常数项。这是因为在高阶多项式中，主要性质大多由最高阶项决定。&lt;/p>
&lt;blockquote>
&lt;p>二次型：对&lt;span class="math">\(n\)&lt;/span>维实向量&lt;span class="math">\(x∈R^n\)&lt;/span>&lt;span class="math">\(n\)&lt;/span>阶实对称阵&lt;span class="math">\(A\)&lt;/span>，&lt;span class="math">\(f(\vec x)=\vec x^TA\vec x\)&lt;/span>称为二次型。复数域上变为共轭转置即可。&lt;/p>
&lt;/blockquote>
&lt;p>由二项式展开式： &lt;span class="math">\[
\begin{aligned}
f(x_1,x_2,\dotsb,x_n)&amp;amp;=c_{11}x_1^2+c_{22}x_2^2+\dotsb+c_{nn}x_n^2\\
&amp;amp;+c_{12}x_1x_2+c_{13}x_1x_3+\dotsb+c_{1n}x_1x_n\\
&amp;amp;+\dotsb+c_{n-1,n}x_{n-1}x_n
\end{aligned}
\]&lt;/span> 与二次型&lt;span class="math">\(f(\vec x)=\vec x^TA\vec x\)&lt;/span>关系可知，其系数矩阵&lt;span class="math">\(A\)&lt;/span>与二项式系数关系为： &lt;span class="math">\[
A=\begin{bmatrix}
c_{11}&amp;amp;0.5c_{12}&amp;amp;0.5c_{13}&amp;amp;\dotsb&amp;amp;0.5c_{1,n}\\
c_{21}&amp;amp;c_{22}&amp;amp;0.5c_{23}&amp;amp;\dotsb&amp;amp;0.5c_{2,n}\\
\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots&amp;amp;\\
0.15c_{1,n}&amp;amp;0.5c_{2,n}&amp;amp;0.5c_{3,n}&amp;amp;\dotsb&amp;amp;c_{n,n}\\
\end{bmatrix}
\]&lt;/span> 显然，矩阵&lt;span class="math">\(A\)&lt;/span>是对称矩阵。当&lt;span class="math">\(A\)&lt;/span>是（半）正定矩阵时，二次型有最小值；当&lt;span class="math">\(A\)&lt;/span>是（半）负定矩阵时二次型有最大值。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/正定负定不定半正定.png" alt="正定负定不定半正定.png" />&lt;p class="caption">正定负定不定半正定.png&lt;/p>
&lt;/div>
&lt;h2 id="合同矩阵了解">合同矩阵（了解）&lt;/h2>
&lt;p>合同矩阵，在线性代数，特别是二次型理论中，常常用到矩阵间的合同关系。&lt;/p>
&lt;blockquote>
&lt;p>合同矩阵：设&lt;span class="math">\(A,B\)&lt;/span>是两个&lt;span class="math">\(n\)&lt;/span>阶方阵，若存在可逆矩阵&lt;span class="math">\(C\)&lt;/span>，使得： &lt;span class="math">\[B=C^TAC\]&lt;/span> 则称方阵&lt;span class="math">\(A\)&lt;/span>与&lt;span class="math">\(B\)&lt;/span>合同，记作&lt;span class="math">\(A≃B\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h3 id="合同性质">合同性质&lt;/h3>
&lt;p>合同关系是一个&lt;strong>等价关系&lt;/strong>，也就是说满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>反身性：任意矩阵都与其自身合同；&lt;/li>
&lt;li>对称性：&lt;span class="math">\(A\)&lt;/span>合同于&lt;span class="math">\(B\)&lt;/span>，则可以推出&lt;span class="math">\(B\)&lt;/span>合同于&lt;span class="math">\(A\)&lt;/span>；&lt;/li>
&lt;li>传递性：&lt;span class="math">\(A\)&lt;/span>合同于&lt;span class="math">\(B\)&lt;/span>，&lt;span class="math">\(B\)&lt;/span>合同于&lt;span class="math">\(C\)&lt;/span>，则可以推出&lt;span class="math">\(A\)&lt;/span>合同于&lt;span class="math">\(C\)&lt;/span>；&lt;/li>
&lt;li>此外，由于&lt;span class="math">\(B=C^TAC\)&lt;/span>中，要求&lt;span class="math">\(C\)&lt;/span>可逆，所以合同矩阵的秩相同。&lt;/li>
&lt;/ol>
&lt;p>两个实对称矩阵合同的充要条件是&lt;strong>它们的正负惯性指数相同&lt;/strong>。&lt;/p>
&lt;p>其中，提到了正负惯性指数，是指：&lt;/p>
&lt;blockquote>
&lt;p>正惯性指数，简称正惯数，是线性代数里矩阵的&lt;strong>正的特征值个数&lt;/strong>，也即是规范型里的系数&amp;quot;1&amp;quot;的个数。&lt;/p>
&lt;/blockquote>
&lt;p>实二次型的标准形中，系数为正的平方项的个数等于二次型的正惯性指数。&lt;/p>
&lt;p>此外，合同变化与二次型的化简有关。&lt;/p></description></item><item><title>线性代数与矩阵之特征值与特征向量</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%89%B9%E5%BE%81%E5%80%BC%E4%B8%8E%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%89%B9%E5%BE%81%E5%80%BC%E4%B8%8E%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/</guid><description>
&lt;h2 id="线性代数与矩阵之特征值与特征向量">线性代数与矩阵之特征值与特征向量&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#传统的特征值与特征向量介绍方法">传统的特征值与特征向量介绍方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征向量特殊在哪里">特征向量特殊在哪里&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征基">特征基&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征基的存在性与特征子空间">特征基的存在性与特征（子）空间&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#特征值都不相同">特征值都不相同&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征值重根与特征向量个数">特征值重根与特征向量个数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征子空间">特征（子）空间&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#特征值的一些性质">特征值的一些性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征值分解与矩阵的幂">特征值分解与矩阵的幂&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩阵幂的快捷计算">矩阵幂的快捷计算&lt;/a>&lt;/li>
&lt;li>&lt;a href="#投影矩阵的特征值与特征向量">投影矩阵的特征值与特征向量&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>特征值与特征向量是线性代数与矩阵中非常重要且深刻的两个概念，然而我当年在学习的时候基本上只学会了如何计算特征值和特征向量，对他们的意义、由来、用法不求甚解，很长一段时间都不知为什么会引入这两个奇怪的东西。当再次看到这二者时，我觉得还是记录下一些想法，必备后用。&lt;/p>
&lt;p>注意，我们默认只在&lt;strong>方阵&lt;/strong>中讨论特征值和特征向量。&lt;/p>
&lt;h2 id="传统的特征值与特征向量介绍方法">传统的特征值与特征向量介绍方法&lt;/h2>
&lt;p>国内一些典型的大学线代教科书，例如用的非常广泛但也被称为教科书中的耻辱柱的《线性代数》（同济版）。对特征值与特征向量的介绍很直接，上来就会给你下个定义：&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(A\)&lt;/span>是&lt;span class="math">\(n\)&lt;/span>阶方阵，如果数&lt;span class="math">\(\lambda\)&lt;/span>和&lt;span class="math">\(n\)&lt;/span>维非零列向量&lt;span class="math">\(x\)&lt;/span>使关系是 &lt;span class="math">\[Ax=\lambda x\tag{1}\]&lt;/span> 成立，那么，这样的数&lt;span class="math">\(\lambda\)&lt;/span>称为矩阵&lt;span class="math">\(A\)&lt;/span>的&lt;strong>特征值&lt;/strong>，非零向量&lt;span class="math">\(x\)&lt;/span>称为&lt;span class="math">\(A\)&lt;/span>的对应与特征值&lt;span class="math">\(\lambda\)&lt;/span>的&lt;strong>特征向量&lt;/strong>。&lt;/p>
&lt;p>（1）式也可写成 &lt;span class="math">\[(A-\lambda I)x=0\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>以上是同济版线性代数教材对特征值和特征向量的第一印象介绍。&lt;/p>
&lt;p>接下来，大多数教科书就会介绍如何根据行列式&lt;span class="math">\(\det(A-\lambda I)=0\)&lt;/span>求出特征值，再将求出的各个特征值代入式（1）求出特征向量，接下来就开始举例子做题了。&lt;/p>
&lt;p>最后，大家经过艰苦的刷题和复习，线性代数考了高分，过了段时间，就将这个有些奇怪的特征值和特征向量抛掷脑后。反正，当时我是这么过来的，^_^。&lt;/p>
&lt;h2 id="特征向量特殊在哪里">特征向量特殊在哪里&lt;/h2>
&lt;p>很多人并没想过，这个特征向量，为什么特殊，它又是怎么被称为“特征”的。刚接触这个定义会觉得有些微妙，似乎是个很巧妙的东西，又说不出个因为所以，有点懵，怎么就是“特征”值，“特征”向量了呢？我们从特征向量入手。&lt;/p>
&lt;p>我们知道，矩阵是线性变化的一种表示方法。空间中的向量与矩阵的乘积本质上是对该向量长度和方向的变换。大多数向量在经过矩阵的变换后，长度和方向都会发生变化。如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/矩阵乘法线性变换.gif" alt="矩阵乘法线性变换.gif" />&lt;p class="caption">矩阵乘法线性变换.gif&lt;/p>
&lt;/div>
&lt;p>图中向量&lt;span class="math">\(v\)&lt;/span>原来所在的方向的直线可以称之为向量&lt;span class="math">\(v\)&lt;/span>张成的空间，对这个概念有疑惑的话，可以参考笔记&lt;a href="线性代数与矩阵之理解向量、线性变换与矩阵乘法.md">线性代数与矩阵之理解向量、线性变换与矩阵乘法&lt;/a>，线性变换之后，向量&lt;span class="math">\(v\)&lt;/span>大概率会离开原来所在的一维子空间到。这种离开原来子空间的向量并没有什么特殊的。基本上随便找一个空间中的向量，都会发生这种改变。那么，有没有一些特殊的向量，经过矩阵&lt;span class="math">\(A\)&lt;/span>的变换之后不会离开原来的一维子空间呢？&lt;/p>
&lt;div class="figure">
&lt;img src="../images/特征向量.gif" alt="特征向量.gif" />&lt;p class="caption">特征向量.gif&lt;/p>
&lt;/div>
&lt;p>还真有。如上所示，对于矩阵&lt;span class="math">\(\begin{bmatrix}3&amp;amp;1\\0&amp;amp;2\end{bmatrix}\)&lt;/span>而言，向量&lt;span class="math">\((1,-1)^T\)&lt;/span>在经过矩阵变换之后变成了&lt;span class="math">\((2,-2)^T\)&lt;/span>，仍旧位于原来的子空间中，只是长度变长了。同时，不难发现所有在向量&lt;span class="math">\((1,-1)^T\)&lt;/span>同一条直线上的向量都满足这种方向不变性。那么这些方向不变的向量相对于大多数方向会改变的向量是特殊的存在，此时我们再回看传统特征向量的定义： &lt;span class="math">\[
Ax=\lambda x\tag{1}
\]&lt;/span> 不正是说的是向量&lt;span class="math">\(x\)&lt;/span>在经过矩阵&lt;span class="math">\(A\)&lt;/span>的变换后，方向没有变化，和原向量是共线的&lt;span class="math">\(\lambda x\)&lt;/span>，只是长度发生了改变。对于特征向量而言，矩阵变化只是长度的伸缩，而长度伸缩的大小，正是特征值&lt;span class="math">\(\lambda\)&lt;/span>。&lt;/p>
&lt;p>我们再看特征这个词的解释：特别的征象、标志；特点。我们说一个事物的特征，比如昆虫的特征是一对触角，两对翅（忽略极少数特例），三对足。特征体现了一种&lt;strong>不变性&lt;/strong>，不管其他方面怎么变，比如颜色、大小、生活地点，只要它的三个特性“一对触角，两对翅（忽略极少数特例），三对足”没变，那么它就是昆虫。如果它的特征，如足数目变了，比如蜘蛛、蜈蚣，那么它就不是昆虫（是不是很多人不知道蜘蛛不是昆虫，哈哈）。而&lt;strong>特征向量也是不变性的体现，它的方向在矩阵变换中总是不变的&lt;/strong>。&lt;/p>
&lt;p>现在，我们可以承认，特征向量（以及附带的特征值）确实具有特殊性，那么这种特殊性有什么好处呢？我们需要回到向量在空间中表示的角度来考虑。&lt;/p>
&lt;h2 id="特征基">特征基&lt;/h2>
&lt;p>正常情况下，向量都是用标准正交基表示的，用矩阵表示就是单位向量&lt;span class="math">\(I\)&lt;/span>。例如，向量&lt;span class="math">\(v=(3,2)^T\)&lt;/span>就是说在基&lt;span class="math">\(e_1=(1,0)^T,e_2=(0,1)^T\)&lt;/span>两个单位向量下，由3份&lt;span class="math">\(e_1\)&lt;/span>和2份&lt;span class="math">\(e_2\)&lt;/span>合成的向量，即&lt;span class="math">\(v=3e_1+2e_2\)&lt;/span>。在经过某个矩阵&lt;span class="math">\(A\)&lt;/span>的线性变换后，比如还是上面的例子&lt;span class="math">\(\begin{bmatrix}3&amp;amp;1\\0&amp;amp;2\end{bmatrix}\)&lt;/span>，其结果&lt;span class="math">\((11,4)^T\)&lt;/span>很难和原向量&lt;span class="math">\((3,2)^T\)&lt;/span>看出直接的联系。&lt;/p>
&lt;p>如果我们换一种思路，用另一组基去表示向量&lt;span class="math">\(v\)&lt;/span>，能不能让矩阵代表的线性变换看起来更直观，更容易呢？比方说，借助特征向量的方向不变性？&lt;/p>
&lt;p>我们可以尝试下。以&lt;span class="math">\(v=(3,2)^T,A=\begin{bmatrix}3&amp;amp;1\\0&amp;amp;2\end{bmatrix}\)&lt;/span>为例。我们根据特征向量的求法，可得两个特征向量分别为&lt;span class="math">\(t_1=(1,0)^T,t_2=(1,-1)^T\)&lt;/span>，对应特征值分别为&lt;span class="math">\(\lambda_1=3,\lambda_2=2\)&lt;/span>。&lt;/p>
&lt;p>那么&lt;span class="math">\(v\)&lt;/span>用&lt;span class="math">\(t_1,t_2\)&lt;/span>可表示为&lt;span class="math">\(v=5t_1-2t_2\)&lt;/span>，即在基&lt;span class="math">\(t_1=(1,0)^T,t_2=(1,-1)^T\)&lt;/span>表示下应为&lt;span class="math">\((5,-2)^T\)&lt;/span>，那么 &lt;span class="math">\[Av=A(5t_1-2t_2)=5At_1-2At_2=5\lambda_1t_1-2\lambda_2t_2=15t_1-4t_2\]&lt;/span> 在基&lt;span class="math">\(t_1,t_2\)&lt;/span>下，结果为&lt;span class="math">\((15,-4)\)&lt;/span>正好分别将第一、二个分量扩大了&lt;span class="math">\(\lambda_1,\lambda_2\)&lt;/span>倍！过程非常直观，计算非常方便！那些原来没有规律的变换，在特征向量组成的基表示下，&lt;strong>只是在各个分量方向不变是的伸缩变化&lt;/strong>。而其在标准正交基&lt;span class="math">\(e_1,e_2\)&lt;/span>下的结果正是&lt;span class="math">\(15t_1-4t_2=(11,4)^T\)&lt;/span>。结果没有任何区别！&lt;/p>
&lt;p>因此，为了线性变换的便利，我们引入该线性变换（矩阵）对应特征基来表示一个向量：&lt;/p>
&lt;blockquote>
&lt;p>线性变换的特征基：空间的一组基，使得某线性变换在这组基下只是坐标轴方向上的伸缩变换(乘以一个标量&lt;span class="math">\(\lambda\)&lt;/span>)，不同轴上的伸缩比例不同。若&lt;span class="math">\(T=(t_1,t_2,\dotsb,t_n)\)&lt;/span>是矩阵&lt;span class="math">\(A\)&lt;/span>的一组特征基，向量&lt;span class="math">\(x\)&lt;/span>在特征基&lt;span class="math">\(T\)&lt;/span>的表示下为&lt;span class="math">\(x=(x_1,x_2,\dotsb,x_n)\)&lt;/span>，那么&lt;span class="math">\(Ax\)&lt;/span>的结果在&lt;span class="math">\(T\)&lt;/span>的表示下是： &lt;span class="math">\[Ax=(\lambda_1x_1,\lambda_2x_2,\dotsb,\lambda_nx_n)\]&lt;/span> 其中，&lt;span class="math">\(\lambda_1,\lambda_2,\dotsb,\lambda_n\)&lt;/span>分别是特征向量&lt;span class="math">\(t_1,t_2,\dotsb,t_n\)&lt;/span>对应的特征值。&lt;/p>
&lt;/blockquote>
&lt;p>注意，这里的&lt;span class="math">\(x=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是在特征基下的表示结果，即&lt;span class="math">\(x=x_1t_1+x_2t_2+\dotsb+x_nt_n\)&lt;/span>，而不是标准正交基下的表示结果。&lt;/p>
&lt;p>矩阵&lt;span class="math">\(A=\begin{bmatrix}3&amp;amp;1\\0&amp;amp;2\end{bmatrix}\)&lt;/span>的特征基的变换过程如下&lt;/p>
&lt;div class="figure">
&lt;img src="../images/特征基.gif" alt="特征基" />&lt;p class="caption">特征基&lt;/p>
&lt;/div>
&lt;p>所以使用特征向量组成的特征基来应对相应的线性变换，可以大大降低计算复杂度，这在求矩阵的幕、图像压缩、解微分方程等领域得到了大量应用，后面我们会举几个例子。&lt;/p>
&lt;p>现在有一个问题，既然特征基那么好用，那么特征基总是存在嘛？换句话说，我们总是能够找到足够多的线性无关的特征向量组成特征基嘛？&lt;/p>
&lt;h3 id="特征基的存在性与特征子空间">特征基的存在性与特征（子）空间&lt;/h3>
&lt;p>一般我们求特征向量的步骤是先根据&lt;span class="math">\(\det(A-\lambda I)=0\)&lt;/span>解特征值的一元高次方程，然后在代入&lt;span class="math">\(Ax=\lambda x\)&lt;/span>求特征向量。那么 &lt;span class="math">\[\det(A-\lambda I)=0\tag{2}\]&lt;/span> 这个式子的解，就决定了特征向量的存在性。我们知道，在复数域下，n次方程必然有n个根，&lt;strong>而有没有重根，将是决定特征基是否存在的关键&lt;/strong>。&lt;/p>
&lt;h4 id="特征值都不相同">特征值都不相同&lt;/h4>
&lt;p>对于方程组 &lt;span class="math">\[(A-\lambda I)x=0\]&lt;/span> 由于式（2）&lt;span class="math">\(\det(A-\lambda I)=0\)&lt;/span>，即&lt;span class="math">\(A-\lambda I\)&lt;/span>并不是满秩的（奇异矩阵），由此其必存在至少一维零空间，此时&lt;span class="math">\(x\)&lt;/span>是属于零空间的任意向量。&lt;/p>
&lt;p>对于存在&lt;span class="math">\(n\)&lt;/span>个不同特征值的场景，&lt;span class="math">\(A-\lambda I\)&lt;/span>的维数为&lt;span class="math">\(n-1\)&lt;/span>，由于&lt;span class="math">\(rank(Col(A-\lambda I))+rank(N(A-\lambda I))=n\)&lt;/span>，则我们可以得到&lt;span class="math">\(n\)&lt;/span>个特征向量&lt;span class="math">\((x_1,x_2\dotsb,x_n)\)&lt;/span>处在&lt;span class="math">\(n\)&lt;/span>个一维的零空间中。现在需要证明的是，这&lt;span class="math">\(n\)&lt;/span>个特征向量是线性无关的。&lt;/p>
&lt;blockquote>
&lt;p>我们使用反证法来证明。先假设这&lt;span class="math">\(n\)&lt;/span>个特征向量线性相关，则存在&lt;span class="math">\(n\)&lt;/span>个不全为零的常数&lt;span class="math">\((c^{(1)}_i)\)&lt;/span>使得如下式子成立： &lt;span class="math">\[c^{(1)}_1 x_1+c^{(1)}_2 x_2+\dotsb+c^{(1)}_n x_n=0\tag{3}\]&lt;/span> 用矩阵&lt;span class="math">\(A\)&lt;/span>左乘式(3)，根据&lt;span class="math">\(Ax_i =\lambda_ix_i\)&lt;/span>有： &lt;span class="math">\[c^{(1)}_1\lambda_1 x_1+c^{(1)}_2\lambda_2 x_2+\dotsb+c^{(1)}_n\lambda_n x_n=0\tag{4}\]&lt;/span> 现使用&lt;span class="math">\(式(4)-\lambda_n×式(3)\)&lt;/span>有: &lt;span class="math">\[c^{(1)}_1(\lambda_1-\lambda_n) x_1+c^{(1)}_2(\lambda_2-\lambda_n) x_2+\dotsb+c^{(1)}_{n-1}(\lambda_{n-1}-\lambda_n) x_{n-1}=0\tag{5}\]&lt;/span> 由于所有的&lt;span class="math">\(\lambda_i\)&lt;/span>都不相等，所以&lt;span class="math">\(\lambda_i-\lambda_n\neq 0(i\neq n)\)&lt;/span>。 我们令&lt;span class="math">\(c^{(2)}_i=c^{(1)}_i(\lambda_i-\lambda_n)\)&lt;/span> &lt;span class="math">\[c^{(2)}_1 x_1+c^{(2)}_2 x_2+\dotsb+c^{(2)}_{n-1} x_{n-1}=0\tag{6}\]&lt;/span> 式(6)与式(3)形式一样，但是少一个&lt;span class="math">\(x_n\)&lt;/span>，我们仿照之前的步骤，同样对&lt;span class="math">\(A×式(6)-\lambda_{n-1}×式(6)\)&lt;/span>得到: &lt;span class="math">\[c^{(2)}_1(\lambda_1-\lambda_{n-1}) x_1+c^{(2)}_2(\lambda_2-\lambda_{n-1}) x_2+\dotsb+c^{(2)}_{n-2}(\lambda_{n-2}-\lambda_{n-1}) x_{n-2}=0\tag{7}\]&lt;/span> 这次我们把&lt;span class="math">\(x_{n-1}\)&lt;/span>消掉了。 按照前面的乘以矩阵&lt;span class="math">\(A\)&lt;/span>再减去&lt;span class="math">\(\lambda\)&lt;/span>的步骤重复进行&lt;span class="math">\(n−2\)&lt;/span>次（每次都用一个不同的单个字符代替&lt;span class="math">\(x_i\)&lt;/span>前面的系数）后，可得： &lt;span class="math">\[c^{n-2}_1 (\lambda_1-\lambda_3)x_1+c^{n-2}_2(\lambda_2-\lambda_3)x_2=0\tag{8}\]&lt;/span> 同样的，令&lt;span class="math">\(c^{n-2}_1 (\lambda_1-\lambda_3)=c^{n-1}_1,c^{n-2}_2(\lambda_2-\lambda_3)=c^{(n-1)}_2\)&lt;/span>即可得到: &lt;span class="math">\[c^{(n-1)}_1x_1 +c^{(n-1)}_2x_2=0\tag{9}\]&lt;/span> 最后，我们使用&lt;span class="math">\(式(9)-\lambda_2×式(9)\)&lt;/span>有: &lt;span class="math">\[c^{(n-1)}_1Ax_1 +c^{(n-1)}_2Ax_2-c^{(n-1)}_1\lambda_2x_1-c^{(n-1)}_2\lambda_2x_2\\=c^{(n-1)}_1(\lambda_1-\lambda_2)x_1=0\tag{10}\]&lt;/span> 我们最后令&lt;span class="math">\(c^{(n)}_1=c^{(n-1)}_1(\lambda_1-\lambda_2)\)&lt;/span>。由于特征向量不为零，所以只能是&lt;span class="math">\(c^{(n)}_1\)&lt;/span>。&lt;/p>
&lt;p>而&lt;span class="math">\(c^{(n)}_1=c^{(n-1)}_1(\lambda_1-\lambda_2)=c^{(n-2)}_1(\lambda_1-\lambda_3)(\lambda_1-\lambda_2)=\dotsb=c^{1}_1(\lambda_1-\lambda_2)(\lambda_1-\lambda_3)\dotsb(\lambda_1-\lambda_n)\)&lt;/span>，又因为各特征值都不相等，所以只能是&lt;span class="math">\(c^{(1)}_1=0\)&lt;/span>。将其代回式（9）可得：&lt;span class="math">\(c_2^{(n-1)}=0\)&lt;/span>&lt;/p>
&lt;p>而&lt;span class="math">\(c_2^{(n-1)}=c^{(1)}_2(\lambda_2-\lambda_3)(\lambda_2-\lambda_4)\dotsb(\lambda_2-\lambda_n)\)&lt;/span>，而各特征值都不相等，所以只能是&lt;span class="math">\(c^{(1)}_2=0\)&lt;/span>。我们逐步从后往前反推，即可得到 &lt;span class="math">\[c^{(1)}_i=0,\forall i=\{1,2,\dotsb,n\}\]&lt;/span> 则说明前面的假设：存在&lt;span class="math">\(n\)&lt;/span>个不全为零的常数&lt;span class="math">\((c^{(1)}_i)\)&lt;/span>使式（3）为0不成立，因此&lt;strong>矩阵不同特征值对应的特征向量线性无关&lt;/strong>得证。&lt;/p>
&lt;/blockquote>
&lt;p>需要指出，我们所说的&lt;span class="math">\(n\)&lt;/span>个不同特征值，不一定非要都是实数，也可以是复数。复数特征值和特征向量同样满足&lt;span class="math">\(Ax=\lambda x\)&lt;/span>的一系列特性。我们以逆时针旋转&lt;span class="math">\(\frac{\pi}{2}\)&lt;/span>的旋转矩阵&lt;span class="math">\(R=\begin{bmatrix}0&amp;amp;-1\\1&amp;amp;0\end{bmatrix}\)&lt;/span>为例： &lt;img src="../images/旋转矩阵.gif" alt="旋转矩阵.gif" />&lt;/p>
&lt;p>显然，经过逆时针90°旋转，空间中所有的非零向量方向都发生了改变。我们计算特征值：&lt;span class="math">\(\det(R-\lambda I)=\lambda^2+1=0\Rightarrow \lambda_1=i,\lambda_2=-i\)&lt;/span>。代入算出对应的特征向量分别为&lt;span class="math">\(x_1=(1,-i)^T,x_2=(-i,1)^T\)&lt;/span>。可见，在此场景下的特征向量无法用实数域向量表示，但是用&lt;span class="math">\(x_1,x_2\)&lt;/span>组成特征基，其矩阵计算与实数域结果依然一致。&lt;/p>
&lt;h4 id="特征值重根与特征向量个数">特征值重根与特征向量个数&lt;/h4>
&lt;p>前一节已经说明不同的特征值必然带来线性不相关的特征向量，那么当特征值有&lt;span class="math">\(r\)&lt;/span>重根时，线性无关特征向量是不是也会有&lt;span class="math">\(r\)&lt;/span>个呢？&lt;/p>
&lt;p>答案是不一定。而且&lt;span class="math">\(r\)&lt;/span>重根的特征向量可能从1~r个不等。&lt;/p>
&lt;p>典型的&lt;span class="math">\(r\)&lt;/span>重特征值有&lt;span class="math">\(r\)&lt;/span>个线性无关特征向量的矩阵是单位阵&lt;span class="math">\(I_{n×n}\)&lt;/span>，特征值&lt;span class="math">\(\det(I-\lambda I)=0\)&lt;/span>，所有的特征值都是1，同时空间中所有的向量都可以作为特征向量（&lt;span class="math">\(Ix=1×x\)&lt;/span>），所以可以从空间中取出&lt;span class="math">\(n\)&lt;/span>个线性无关特征向量。&lt;/p>
&lt;p>典型的线性无关特征向量个数小于特征根重数的矩阵是主对角元素有相等时的三角矩阵。具体例子如 &lt;span class="math">\[
\begin{bmatrix}
1&amp;amp;2\\0&amp;amp;1
\end{bmatrix},
\begin{bmatrix}
1&amp;amp;2&amp;amp;3\\0&amp;amp;1&amp;amp;1\\0&amp;amp;0&amp;amp;1
\end{bmatrix}
\]&lt;/span> 这两个矩阵都只有一个线性无关的特征向量。&lt;/p>
&lt;h4 id="特征子空间">特征（子）空间&lt;/h4>
&lt;p>我们将所有有着&lt;strong>相同特征值的特征向量组成的空间，还包括零向量&lt;/strong>（但要注意零向量本身不是特征向量），叫做一个特征（子）空间。也可以说是相同特征值所对应的特征向量张成的（子）空间。矩阵&lt;span class="math">\(A\)&lt;/span>有多少个不同特征值，就有多少个特征子空间。&lt;/p>
&lt;p>由于特征子空间具有&lt;strong>几何特性&lt;/strong>，我们将特征子空间的维度成为&lt;strong>几何重数&lt;/strong>，而特征值的重数是&lt;strong>方程解出来的，具有代数特征&lt;/strong>，我们称特征值的重数为&lt;strong>代数重数&lt;/strong>。&lt;/p>
&lt;p>我们知道行列式&lt;span class="math">\(\det(A-\lambda I)=0\)&lt;/span>，特征子空间作为&lt;span class="math">\((A-\lambda I)x=0\)&lt;/span>的零空间必然有解，因此特征空间的维度必然是大于等于1。如果特征值重数是1，那么特征空间的维度必然等于1。而对于特征值根的重数大于1（即代数重数大于1），特征空间的维度取决于&lt;span class="math">\((A-\lambda I)x=0\)&lt;/span>的零空间的维度，此维度小于代数重数，即有&lt;span class="math">\(1\leq 几何充数 \leq 代数重数\)&lt;/span>。&lt;/p>
&lt;p>如果矩阵&lt;span class="math">\(A\)&lt;/span>各特征子空间的直和等于原完整空间&lt;span class="math">\(V^n\)&lt;/span>，那么&lt;span class="math">\(A\)&lt;/span>就有了&lt;span class="math">\(n\)&lt;/span>个线性无关的特征向量，此时这些特征向量可以构成一组特征基。&lt;/p>
&lt;h2 id="特征值的一些性质">特征值的一些性质&lt;/h2>
&lt;p>说了那么久的特征向量，我们再来看看特征值。特征值算是特征向量的副产品，虽然说我们在求解的时候通常是先求特征值，再求特征向量。&lt;/p>
&lt;p>特征值几个常见的性质如下：&lt;/p>
&lt;blockquote>
&lt;p>性质1：矩阵&lt;span class="math">\(A\)&lt;/span>的特征值和等于矩阵的迹：&lt;span class="math">\(\sum\limits_i \lambda_i=tr(A)\)&lt;/span>&lt;/p>
&lt;p>性质2：矩阵&lt;span class="math">\(A\)&lt;/span>的特征值积等于其行列式：&lt;span class="math">\(\prod\limits_i \lambda_i=\det(A)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这两个性质的证明可借助矩阵的特征多项式。我们可以将&lt;span class="math">\(\det(A-\lambda I)\)&lt;/span>写成零点式形式： &lt;span class="math">\[
f(\lambda)=(\lambda_1-\lambda)(\lambda_2-\lambda)\dotsb(\lambda_n-\lambda)
\]&lt;/span> 其中，&lt;span class="math">\(\lambda_i\)&lt;/span>都是特征值，且有可能相等也有可能是复数。显然，根据展开式系数有&lt;span class="math">\(\lambda^0=\prod\limits_i \lambda_i,\lambda^1=(-1)^{n-1}\sum\limits_i \lambda_i\)&lt;/span>。我们再根据行列式&lt;span class="math">\(\det(A-\lambda I)\)&lt;/span>的展开项对应可得&lt;span class="math">\(\lambda^0=\det(A),\lambda^1=(-1)^{n-1}\sum\limits_i a_{ii}\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>性质3：若&lt;span class="math">\(λ\)&lt;/span>是可逆阵&lt;span class="math">\(A\)&lt;/span>的一个特征根，&lt;span class="math">\(x\)&lt;/span>为对应的特征向量，则&lt;span class="math">\(1/λ\)&lt;/span>是&lt;span class="math">\(A\)&lt;/span>的逆的一个特征根，&lt;span class="math">\(x\)&lt;/span>仍为对应的特征向量。&lt;/p>
&lt;/blockquote>
&lt;p>证明：若&lt;span class="math">\(x\)&lt;/span>为&lt;span class="math">\(A\)&lt;/span>对应的一个特征向量，&lt;span class="math">\(Ax=\lambda x\Rightarrow A^{-1}Ax=A^{-1}\lambda x\Rightarrow Ix=\lambda A^{-1}x\Rightarrow 1/\lambda x=A^{-1}x\)&lt;/span>。得证。&lt;/p>
&lt;p>另外，如果某个特征值为0，说明矩阵&lt;span class="math">\(A\)&lt;/span>是奇异矩阵，特征向量&lt;span class="math">\(x\)&lt;/span>为了满足&lt;span class="math">\(Ax=0\)&lt;/span>必须位于&lt;span class="math">\(A\)&lt;/span>的零空间内。在不可逆矩阵中，特征值就不能套用上面的性质。&lt;/p>
&lt;blockquote>
&lt;p>性质4：若&lt;span class="math">\(λ\)&lt;/span>是方阵A的一个特征根，&lt;span class="math">\(x\)&lt;/span>为对应的特征向量，则&lt;span class="math">\(λ\)&lt;/span>的&lt;span class="math">\(m\)&lt;/span>次方是&lt;span class="math">\(A\)&lt;/span>的&lt;span class="math">\(m\)&lt;/span>次方的一个特征根，&lt;span class="math">\(x\)&lt;/span>仍为对应的特征向量。&lt;/p>
&lt;/blockquote>
&lt;p>证明：若&lt;span class="math">\(x\)&lt;/span>为&lt;span class="math">\(A\)&lt;/span>对应的一个特征向量，&lt;span class="math">\(A^mx=A^{m-1}(Ax)=A^{m-1}\lambda x=\lambda A^{m-2}(Ax)=\dotsb=\lambda^m x\)&lt;/span>，即&lt;span class="math">\(A^mx=\lambda^m x\)&lt;/span>。得证。&lt;/p>
&lt;blockquote>
&lt;p>性质5：矩阵&lt;span class="math">\(A\)&lt;/span>与&lt;span class="math">\(A^T\)&lt;/span>拥有相同的特征值。&lt;/p>
&lt;/blockquote>
&lt;p>证明：我们通过行列式&lt;span class="math">\(\det(A-\lambda I)\)&lt;/span>计算得到矩阵&lt;span class="math">\(A\)&lt;/span>的特征值，根据行列式转置不变的性质，我们有&lt;span class="math">\(\det(A-\lambda I)=\det(A-\lambda I)^T=\det(A^T-\lambda I^T)=\det(A^T-\lambda I)\)&lt;/span>，即为&lt;span class="math">\(A^T\)&lt;/span>计算特征值的行列式。因此，矩阵&lt;span class="math">\(A\)&lt;/span>与&lt;span class="math">\(A^T\)&lt;/span>拥有相同的特征值。&lt;/p>
&lt;blockquote>
&lt;p>性质6：对于任何实数矩阵，如果其特征值为实数，那么特征向量是实向量。&lt;/p>
&lt;/blockquote>
&lt;p>证明：我们从矩阵的空间来理解比较容易。对于&lt;span class="math">\(n\)&lt;/span>阶实数矩阵而言，它是代表&lt;span class="math">\(\R^n\)&lt;/span>空间中的线性变换，这意味着任意&lt;span class="math">\(\R^n\)&lt;/span>中的向量，经过矩阵变换之后必然仍然在&lt;span class="math">\(\R^n\)&lt;/span>空间中。对于矩阵&lt;span class="math">\(A\)&lt;/span>，其求特征值所用的行列式所代表的矩阵&lt;span class="math">\(A-\lambda I\)&lt;/span>，如果也是实数矩阵，那么也是&lt;span class="math">\(\R^n\)&lt;/span>空间中的线性变换，同样也要满足&lt;span class="math">\(\R^n\)&lt;/span>线性空间中的变换的封闭性。而当特征值&lt;span class="math">\(\lambda\)&lt;/span>为实数时，&lt;span class="math">\(A-\lambda I\)&lt;/span>显然也是实数矩阵，那么特征向量&lt;span class="math">\(x\)&lt;/span>使得&lt;span class="math">\((A-\lambda I)x=0\)&lt;/span>就位于&lt;span class="math">\(A-\lambda I\)&lt;/span>的零空间中。矩阵的零空间&lt;span class="math">\(N(A)\)&lt;/span>和矩阵的行空间&lt;span class="math">\(Raw(A)\)&lt;/span>是&lt;span class="math">\(\R^n\)&lt;/span>中互补的两个子空间，因此&lt;span class="math">\(N(A),Raw(A)\)&lt;/span>都是实数向量的空间，因此位于零空间&lt;span class="math">\(N(A)\)&lt;/span>中的特征向量必然也是实数向量。&lt;/p>
&lt;h2 id="特征值分解与矩阵的幂">特征值分解与矩阵的幂&lt;/h2>
&lt;p>如果我们有了&lt;span class="math">\(n\)&lt;/span>个线性无关的特征向量，那么我们能够做一些新的改变，我们将矩阵&lt;span class="math">\(A\)&lt;/span>的具有&lt;span class="math">\(n\)&lt;/span>个线性无关的特征向量&lt;span class="math">\(q_i\)&lt;/span>作为列向量，组成一个可逆方阵&lt;span class="math">\(Q\)&lt;/span>： &lt;span class="math">\[
Q=[q_1,q_2,\dotsb,q_n]
\]&lt;/span> 将其与原矩阵&lt;span class="math">\(A\)&lt;/span>相乘可得： &lt;span class="math">\[
AQ=A[q_1,q_2,\dotsb,q_n]=[\lambda_1 q_1,\lambda_2 q_2,\dotsb,\lambda_n q_n]\\
=Q\begin{bmatrix}
\lambda_1&amp;amp;0&amp;amp;\dotsb&amp;amp;0\\
0&amp;amp;\lambda_2&amp;amp;\dotsb&amp;amp;0\\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\
0&amp;amp;0&amp;amp;\dotsb&amp;amp;\lambda_1\\
\end{bmatrix}=Q\Lambda
\]&lt;/span> 这里的矩阵&lt;span class="math">\(\Lambda\)&lt;/span>为对角阵，它的非零元素就是矩阵&lt;span class="math">\(A\)&lt;/span>的特征值。因为矩阵&lt;span class="math">\(Q\)&lt;/span>中的列向量线性无关，因此逆矩阵&lt;span class="math">\(Q^{-1}\)&lt;/span>存在。在等式两侧左乘逆矩阵&lt;span class="math">\(Q^{-1}\)&lt;/span>，得到&lt;span class="math">\(\Lambda=Q^{-1}AQ\)&lt;/span>。相应地，&lt;span class="math">\(A=Q\Lambda Q^{-1}\)&lt;/span>。我们称之为矩阵的&lt;strong>特征分解&lt;/strong>，特征分解的过程也称为&lt;strong>相似对角化&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>特征分解（Eigendecomposition），又称谱分解（Spectral decomposition）是将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法。 &lt;span class="math">\(A\)&lt;/span>可以被分解为&lt;span class="math">\(A=Q\Lambda {Q}^{-1}\)&lt;/span>，其中&lt;span class="math">\(Q\)&lt;/span>是&lt;span class="math">\(N×N\)&lt;/span>方阵，且其第&lt;span class="math">\(i\)&lt;/span>列为&lt;span class="math">\(A\)&lt;/span>的特征向量&lt;span class="math">\(q_i\)&lt;/span>。&lt;span class="math">\(Λ\)&lt;/span>是对角矩阵，其对角线上的元素为对应的特征值，也即&lt;span class="math">\(\Lambda_{ii}=\lambda_i\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>由于&lt;span class="math">\(Q\)&lt;/span>是由特征向量组成的矩阵，并且可逆，因此&lt;span class="math">\(Q\)&lt;/span>的必然是非奇异矩阵，它的列向量组必然是线性无关的。这也推出了矩阵可特征值分解的一个充要条件：&lt;/p>
&lt;blockquote>
&lt;p>定理：矩阵&lt;span class="math">\(A\)&lt;/span>有&lt;span class="math">\(n\)&lt;/span>个线性无关的特征向量&lt;span class="math">\(\Leftrightarrow\)&lt;/span>矩阵&lt;span class="math">\(A\)&lt;/span>可特征值分解。&lt;/p>
&lt;/blockquote>
&lt;p>在联合之前特征基、特征子空间、代数重数、几何重数的概念，我们将上述定理做出推广：&lt;/p>
&lt;blockquote>
&lt;p>推论：矩阵&lt;span class="math">\(A\)&lt;/span>有&lt;span class="math">\(n\)&lt;/span>个线性无关的特征向量&lt;span class="math">\(\Leftrightarrow\)&lt;/span>矩阵&lt;span class="math">\(A\)&lt;/span>存在一组特征基&lt;span class="math">\(\Leftrightarrow\)&lt;/span>矩阵&lt;span class="math">\(A\)&lt;/span>所有的特征值的几何重数等于代数重数&lt;span class="math">\(\Leftrightarrow\)&lt;/span>矩阵&lt;span class="math">\(A\)&lt;/span>所以特征子空间的直和为完整空间&lt;span class="math">\(\Leftrightarrow\)&lt;/span>矩阵&lt;span class="math">\(A\)&lt;/span>可特征值分解&lt;/p>
&lt;/blockquote>
&lt;p>需要注意的是，&lt;strong>可特征值分解和矩阵&lt;span class="math">\(A\)&lt;/span>本身是不是非奇异无关，只和&lt;span class="math">\(A\)&lt;/span>的特征值数量有关&lt;/strong>，奇异矩阵也可能有特征值分解，如下例： &lt;span class="math">\[
A=\begin{bmatrix}
1&amp;amp;1\\0&amp;amp;0
\end{bmatrix}=\begin{bmatrix}
1&amp;amp;-1\\0&amp;amp;1
\end{bmatrix}\begin{bmatrix}
1&amp;amp;0\\0&amp;amp;0
\end{bmatrix}\begin{bmatrix}
1&amp;amp;1\\0&amp;amp;1
\end{bmatrix}
\]&lt;/span> 而非奇异矩阵也有可能没有特征值分解，例如&lt;span class="math">\(\begin{bmatrix}1&amp;amp;2\\0&amp;amp;1\end{bmatrix}\)&lt;/span>。&lt;/p>
&lt;h3 id="矩阵幂的快捷计算">矩阵幂的快捷计算&lt;/h3>
&lt;p>特征值分解的一个重要应用是计算矩阵的幂。根据特征分解公式&lt;span class="math">\(\mathbf{A}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}\)&lt;/span>，有 &lt;span class="math">\[
A^m=(Q\Lambda Q^{-1})^m=Q\Lambda Q^{-1}Q\Lambda Q^{-1}\dotsb Q\Lambda Q^{-1}=Q\Lambda^m Q^{-1}
\]&lt;/span> 这大大方便了矩阵的幂计算。&lt;/p>
&lt;p>同时，我们观察&lt;span class="math">\(A^m\)&lt;/span>分解后的结果&lt;span class="math">\(Q\Lambda^m Q^{-1}\)&lt;/span>，如果当&lt;span class="math">\(m\rightarrow ∞\)&lt;/span>时，&lt;span class="math">\(A^m\rightarrow 0\)&lt;/span>，我们称矩阵&lt;span class="math">\(A\)&lt;/span>是&lt;strong>稳定的&lt;/strong>。那么根据&lt;span class="math">\(A^m=Q\Lambda^m Q^{-1}\)&lt;/span>，只有当&lt;span class="math">\(\Lambda^m\rightarrow 0\)&lt;/span>时，&lt;span class="math">\(A\)&lt;/span>才是稳定的。这就要求&lt;span class="math">\(A\)&lt;/span>的最大特征值的模要小于1，即&lt;span class="math">\(|\max \lambda_i|&amp;lt;1\)&lt;/span>。&lt;/p>
&lt;p>此外，当&lt;span class="math">\(m\)&lt;/span>很大时，我们也发现最大的特征值&lt;span class="math">\(\max \lambda_i\)&lt;/span>对矩阵的幂产生的作用最大，因此我们也可用矩阵的最大特征值估计一些结果。&lt;/p>
&lt;h2 id="投影矩阵的特征值与特征向量">投影矩阵的特征值与特征向量&lt;/h2>
&lt;p>假设投影矩阵&lt;span class="math">\(P\)&lt;/span>是一个&lt;span class="math">\(n×n\)&lt;/span>维矩阵。由于&lt;span class="math">\(P\)&lt;/span>是幂等的，即&lt;span class="math">\(P^2=P\)&lt;/span>，因此对于特征向量&lt;span class="math">\(x\)&lt;/span>有 &lt;span class="math">\[
\left .
\begin{aligned}
P^2x=\lambda^2 x\\
Px=\lambda x\\
P^2=P
\end{aligned}
\right \}\Rightarrow \lambda^2 x = \lambda x
\]&lt;/span> 由于特征向量不为0，因此有 &lt;span class="math">\[
\lambda^2=\lambda\Rightarrow \lambda=0,1
\]&lt;/span> 即投影矩阵的特征值只能是0或1。&lt;/p>
&lt;p>若该投影矩阵的列空间&lt;span class="math">\(Col(P)\)&lt;/span>的秩为&lt;span class="math">\(r\)&lt;/span>，那么任意列空间的元素都是特征向量，因为满足&lt;span class="math">\(Px=1×x\)&lt;/span>，所以我们可以从列空间中挑选出&lt;span class="math">\(r\)&lt;/span>个线性无关的特征向量。同时，对于投影矩阵的零空间&lt;span class="math">\(N(P)\)&lt;/span>，其是&lt;span class="math">\(n-r\)&lt;/span>维的，其中所有的向量皆满足&lt;span class="math">\(Px=0=0×x\)&lt;/span>，因此所有零空间中的向量也是&lt;span class="math">\(P\)&lt;/span>的特征向量，从而我们也能从零空间中找出&lt;span class="math">\(n-r\)&lt;/span>个线性无关的特征向量。&lt;/p>
&lt;p>我们还知道投影矩阵是对称矩阵，即&lt;span class="math">\(P^T=P\)&lt;/span>，因此其列空间和行空间是一样的。根据矩阵四类空间的性质（参考笔记&lt;a href="线性代数与矩阵之四类空间.md">线性代数与矩阵之四类空间&lt;/a>），零矩阵和行矩阵垂直，因此零矩阵中元素皆垂直于行空间元素，列空间等于行空间的&lt;span class="math">\(r\)&lt;/span>个特征向量必然也垂直与零空间中的&lt;span class="math">\(n-r\)&lt;/span>个特征向量。这样我们就找到了&lt;span class="math">\(n\)&lt;/span>个线性无关的特征向量，即投影矩阵&lt;span class="math">\(P\)&lt;/span>必然可特征分解。（所有的对称矩阵都满足这个条件，因此所有的对称矩阵都是可特征分解的。）&lt;/p>
&lt;p>总结：投影矩阵必然可特征分解，且其特征值只有0和1。&lt;/p></description></item><item><title>线性代数与矩阵之正交（酉）矩阵与正交化</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E6%AD%A3%E4%BA%A4%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%8E%E6%AD%A3%E4%BA%A4%E5%8C%96/</link><pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E6%AD%A3%E4%BA%A4%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%8E%E6%AD%A3%E4%BA%A4%E5%8C%96/</guid><description>
&lt;h2 id="线性代数与矩阵之正交酉矩阵与正交化">线性代数与矩阵之正交（酉）矩阵与正交化&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#正交向量组">正交向量组&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正交矩阵">正交矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#酉矩阵">酉矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#施密特正交化">施密特正交化&lt;/a>&lt;/li>
&lt;li>&lt;a href="#qr分解">QR分解&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>本笔记将系统的介绍矩阵中正交的相关概念，包括正交向量（组），正交矩阵以及酉矩阵、施密特正交化以及矩阵的QR分解等。&lt;/p>
&lt;h2 id="正交向量组">正交向量组&lt;/h2>
&lt;blockquote>
&lt;p>正交向量组：一组非零的两两正交(即内积为0)的向量构成的向量组，用数学语言可写为：有一组向量&lt;span class="math">\((q_1,q_2,\dotsb,q_n)\)&lt;/span>，向量之间两两满足 &lt;span class="math">\[q_i^Tq_j=&amp;lt;q_i,q_j&amp;gt;=\begin{cases}
0,i\neq j\\
a&amp;gt;0,i=j
\end{cases}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>如果我们要求当&lt;span class="math">\(i=j\)&lt;/span>时的内积&lt;span class="math">\(a=1\)&lt;/span>，则称&lt;span class="math">\((q_1,q_2,\dotsb,q_n)\)&lt;/span>为标准正交向量组，所为“标准”就是其每个向量模长为单位长度1，相应的将向量长度化为1的过程就叫做&lt;strong>标准化（Normalization）&lt;/strong>。（此外，如果这个标准正交向量组的个数等于空间的维度&lt;span class="math">\(n\)&lt;/span>，那么这一组标准正交向量可以作为空间的标准正交基。因为非零正交的向量必然线性无关）&lt;/p>
&lt;h2 id="正交矩阵">正交矩阵&lt;/h2>
&lt;p>在矩阵论中，正交矩阵（英语：orthogonal matrix）是一个方阵&lt;span class="math">\(Q\)&lt;/span>，其&lt;strong>元素为实数&lt;/strong>，而且&lt;strong>行向量与列向量皆为正交的单位向量&lt;/strong>，使得该矩阵的&lt;strong>转置矩阵&lt;/strong>有如下特性： &lt;span class="math">\[Q^{T}Q=QQ^{T}=I\]&lt;/span> 显而易见，&lt;span class="math">\(Q^{T}=Q^{-1}\)&lt;/span>即&lt;strong>正交矩阵的转置为其逆矩阵&lt;/strong>。这算是正交矩阵最重要的性质。这也说明，矩阵&lt;span class="math">\(Q\)&lt;/span>的列向量组&lt;span class="math">\((Cq_1,Cq_2,\dotsb,Cq_n)\)&lt;/span>是一组标准正交向量。典型的正交矩阵有单位阵&lt;span class="math">\(I\)&lt;/span>，置换矩阵，标准化的Hadamard等。&lt;/p>
&lt;h3 id="酉矩阵">酉矩阵&lt;/h3>
&lt;p>酉矩阵是正交矩阵在复数空间&lt;span class="math">\(\mathbb{C}^n\)&lt;/span>的推广。酉矩阵（又译作幺正矩阵，英语：unitary matrix）是一个&lt;span class="math">\(n×n\)&lt;/span>复数方块矩阵&lt;span class="math">\(U\)&lt;/span>，其满足以下性质： &lt;span class="math">\[U^\ast U=UU^{\ast}=I_{n}\]&lt;/span> 其中&lt;span class="math">\(U\ast\)&lt;/span>是&lt;span class="math">\(U\)&lt;/span>的共轭转置，&lt;span class="math">\(I_n\)&lt;/span>是&lt;span class="math">\(n×n\)&lt;/span>单位矩阵。同样的，酉矩阵的逆矩阵就是共轭转置： &lt;span class="math">\[
U^{-1}=U^{*}
\]&lt;/span> 显然，&lt;span class="math">\(U\)&lt;/span>的列（行）向量组是在&lt;span class="math">\(\mathbb{C}^n\)&lt;/span>上的一组标准正交基。&lt;/p>
&lt;h2 id="施密特正交化">施密特正交化&lt;/h2>
&lt;p>请看网页资料&lt;a href="../网页资料/线性代数与矩阵之Gram-Schmidt正交化.html">线性代数与矩阵之Gram-Schmidt正交化.html&lt;/a>&lt;/p>
&lt;p>原文地址&lt;a href="https://ccjou.wordpress.com/2010/04/22/gram-schmidt-正交化與-qr-分解/">https://ccjou.wordpress.com/2010/04/22/gram-schmidt-正交化與-qr-分解/&lt;/a>&lt;/p>
&lt;h2 id="qr分解">QR分解&lt;/h2>
&lt;p>某种程度上来说，矩阵的QR分解就是施密特正交化的矩阵化表示。对矩阵&lt;span class="math">\(A\)&lt;/span>进行施密特正交化得到&lt;span class="math">\(Q\)&lt;/span>，而求取&lt;span class="math">\(R\)&lt;/span>的过程是使用已经求取的标准正交基反推原来的列向量。&lt;/p></description></item><item><title>线性代数与矩阵之投影与子空间</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E6%8A%95%E5%BD%B1%E4%B8%8E%E5%AD%90%E7%A9%BA%E9%97%B4/</link><pubDate>Tue, 10 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E6%8A%95%E5%BD%B1%E4%B8%8E%E5%AD%90%E7%A9%BA%E9%97%B4/</guid><description>
&lt;h2 id="线性代数与矩阵之投影与子空间">线性代数与矩阵之投影与子空间&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#引子">引子&lt;/a>&lt;/li>
&lt;li>&lt;a href="#投影从向量到子空间投影">投影——从向量到子空间投影&lt;/a>&lt;/li>
&lt;li>&lt;a href="#投影的最小误差要求">投影的最小误差要求&lt;/a>&lt;/li>
&lt;li>&lt;a href="#向量到向量的投影">向量到向量的投影&lt;/a>&lt;/li>
&lt;li>&lt;a href="#向量的投影矩阵">向量的投影矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#子空间投影">子空间投影&lt;/a>&lt;/li>
&lt;li>&lt;a href="#投影矩阵与子空间关系">投影矩阵与子空间关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#再谈投影和误差">再谈投影和误差&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小二乘法与子空间投影">最小二乘法与（子）空间投影&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小二乘法">最小二乘法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#ata不可逆时的处理">&lt;span class="math">\(A^TA\)&lt;/span>不可逆时的处理&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="引子">引子&lt;/h2>
&lt;p>注：本文中默认矩阵&lt;span class="math">\(A\)&lt;/span>的形式为：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/bordermatrix.png" alt="bordermatrix" />&lt;p class="caption">bordermatrix&lt;/p>
&lt;/div>
&lt;p>在&lt;a href="线性代数与矩阵之四类空间.md">《线性代数与矩阵之四类空间》&lt;/a>笔记中，我们提到了四个空间与方程的可解性，讨论的都是解存在以及解系的情况。那如果&lt;span class="math">\(Ax=b\)&lt;/span>这个线性方程组中，解不存在怎么办？比如我们需要知道一个人的平面位置，但是我们经过多次间接测量&lt;span class="math">\((x_i,y_i)\)&lt;/span>（假设间接测量和实际位置存在某种线性关系），得到了很多组结果： &lt;span class="math">\[
\begin{bmatrix}
x_1&amp;amp;y_1\\x_2&amp;amp;y_2\\x_3&amp;amp;y_3\\\vdots&amp;amp;\vdots\\x_m&amp;amp;y_m\\
\end{bmatrix}\begin{bmatrix}\theta_1\\\theta_2\end{bmatrix}=\begin{bmatrix}b_1\\b_2\end{bmatrix}
\]&lt;/span> 显然，通常这个方程组时不可解的，那么有没有一种方法，能得出尽量精确的解呢？&lt;/p>
&lt;p>答案是有的，尽量近似的解获得方式如下（暂时不考虑&lt;span class="math">\(A^TA\)&lt;/span>不可逆）： &lt;span class="math">\[
A^TA\Theta=A^Tb\Rightarrow \hat{\Theta}=(A^TA)^{-1}A^Tb
\]&lt;/span> 注意，这里的矩阵&lt;span class="math">\(A\)&lt;/span>&lt;strong>并不是方阵&lt;/strong>，因此&lt;span class="math">\((A^TA)^{-1}\neq A^{-1}(A^T)^{-1}\)&lt;/span>，&lt;span class="math">\(A,A^T\)&lt;/span>的逆不存在的。&lt;/p>
&lt;p>其中，&lt;span class="math">\(A^Tb\)&lt;/span>和衍生出来的投影矩阵&lt;span class="math">\(A(A^TA)^{-1}A^Tb\)&lt;/span>的由来就是这篇笔记的重点。&lt;/p>
&lt;h2 id="投影从向量到子空间投影">投影——从向量到子空间投影&lt;/h2>
&lt;p>想象我们站在冬日的阳光下，阳光照在我们身上。在给我们带来温暖与惬意的同时，也在平地上留下了一个影子，这就是投影在生活中最常见的情形。而此时的影子是三维世界的我们在二维平面成的像，当然阳光的角度，地面是不是斜坡都会影响影子的最终形态。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/投影影子.jpeg" alt="投影影子" />&lt;p class="caption">投影影子&lt;/p>
&lt;/div>
&lt;p>线性代数与矩阵中也存在类似的过程——&lt;strong>投影&lt;/strong>。投影是一种高维事物（如向量）往低维子空间投射影子的过程，而这个低维子空间中的影子应该是子空间中&lt;strong>与原来事物相似&lt;/strong>的存在。如果用严格的数学语言描述：&lt;/p>
&lt;blockquote>
&lt;p>投影：有一属于向量空间&lt;span class="math">\(V\)&lt;/span>的子空间&lt;span class="math">\(W\)&lt;/span>，该子空间的一个投影&lt;span class="math">\(P\)&lt;/span>需使得&lt;span class="math">\(\forall u\in V,P(u)\in W\)&lt;/span>，并且&lt;span class="math">\(\forall u\in W\)&lt;/span>，有&lt;span class="math">\(P(u)=u\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>上面这句话可解释为，&lt;span class="math">\(P\)&lt;/span>是向量空间&lt;span class="math">\(V\)&lt;/span>到子空间&lt;span class="math">\(W\)&lt;/span>投影，那么&lt;span class="math">\(P\)&lt;/span>将所有&lt;span class="math">\(V\)&lt;/span>中的元素都映射到&lt;span class="math">\(W\)&lt;/span>中，而且&lt;span class="math">\(P\)&lt;/span>在&lt;span class="math">\(W\)&lt;/span>上是恒等变换。虽然这个定义没有问题，但是满足这个定义的投影&lt;span class="math">\(P\)&lt;/span>显然不是唯一的，如下图所示，各个&lt;span class="math">\(p _i\)&lt;/span>都可以算作投影的影子。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/投影结果.drawio.svg" alt="投影结果.drawio.svg" />&lt;p class="caption">投影结果.drawio.svg&lt;/p>
&lt;/div>
&lt;p>在这些影子中，总得有好有坏吧。我们可以设立一个指标：&lt;strong>影子向量和原来向量的差别或误差&lt;/strong>。差别（误差）越小，也就是越近似，那么这个影子就保留了原向量尽可能多的信息，因此最好的投影会让二者具有“最近似性”关系。&lt;/p>
&lt;p>其实，这种追求最小误差的投影，叫做&lt;strong>正交投影&lt;/strong>，正交投影是我们用得最多的投影方式，一个子空间的正交投影是唯一的。&lt;strong>斜投影&lt;/strong>有时用来提及非正交投影，斜投影常常会扭曲向量的尺寸，因此用的不多。我们这里先提一下这些概念，本文其实说的投影都是正交投影。&lt;/p>
&lt;h3 id="投影的最小误差要求">投影的最小误差要求&lt;/h3>
&lt;p>那么，如果体现投影的“最近似”特性呢？由于从高维空间到子空间的投影是不唯一的，就像阳光可以顺着不同的方向照射，同一个地面产生的影子也是不一样的。如果原来的向量&lt;span class="math">\(\vec{b}\)&lt;/span>产生的投影向量是&lt;span class="math">\(\vec{p}\)&lt;/span>，那么它们之间的误差就是&lt;span class="math">\(\vec{e}=\vec{b}-\vec{p}\)&lt;/span>。“最近似”特性就是希望误差&lt;span class="math">\(\vec{e}\)&lt;/span>最小。根据点与空间的关系可知，&lt;strong>最小化&lt;span class="math">\(\vec{e}\)&lt;/span>相当于让其成为向量到空间的距离&lt;/strong>。&lt;/p>
&lt;p>那么这个空间&lt;span class="math">\(V\)&lt;/span>到子空间&lt;span class="math">\(W\)&lt;/span>的投影&lt;span class="math">\(P\)&lt;/span>到底怎么求？以及如何在低维子空间中找到任意&lt;span class="math">\(V\)&lt;/span>中向量的影子呢？就是这一章节的阐述重点。&lt;/p>
&lt;h3 id="向量到向量的投影">向量到向量的投影&lt;/h3>
&lt;p>空间中，最简单的子空间就是一维子空间（除了零向量），直接来说就是一个向量所在的过原点的直线，且该一维子空间由这个向量确定。一维子空间的投影是最基本的场景，也是其他更复杂场景的基础。一个向量往另一向量确定的一维子空间投影应该是什么呢？根据我们之前的要求，投影的影子应该是和原来向量差距最小的的。基本场景如下图所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/向量投影.drawio.svg" alt="向量投影" />&lt;p class="caption">向量投影&lt;/p>
&lt;/div>
&lt;p>在上面这个图中，&lt;span class="math">\(\vec{b}\)&lt;/span>为原向量，&lt;span class="math">\(\vec{p}\)&lt;/span>为其在向量&lt;span class="math">\(\vec{a}\)&lt;/span>所在的一维子空间的投影向量。根据向量的减法关系可知：&lt;span class="math">\(\vec{b}-\vec{p}=\vec{e}\)&lt;/span>，其中&lt;span class="math">\(\vec{e}\)&lt;/span>是&lt;span class="math">\(\vec{b},\;\vec{p}\)&lt;/span>的误差。用小学的知识也能知道，&lt;strong>当&lt;span class="math">\(\vec{e}\)&lt;/span>垂直于一维子空间所在直线&lt;/strong>的时候，误差最小。所以投影应该满足： &lt;span class="math">\[
\vec{e}\perp\vec{a}\\
\Leftrightarrow \vec{a}^T\vec{e}=0或\vec{a}\cdot \vec{e}=0
\]&lt;/span> 我们将&lt;span class="math">\(\vec{e}=\vec{b}-\vec{p}\)&lt;/span>代入，可得 &lt;span class="math">\[
\vec{a}^T(\vec{b}-\vec{p})=0\Leftrightarrow \vec{a}^T\vec{b}=\vec{a}^T\vec{p}
\]&lt;/span> 并且，&lt;span class="math">\(\vec{p}\)&lt;/span>与&lt;span class="math">\(\vec{a}\)&lt;/span>都是一维子空间的向量，二者共线，必然有&lt;span class="math">\(\vec{p}=x\vec{a}\)&lt;/span>，其中&lt;span class="math">\(x\)&lt;/span>是一个标量。代入上式可得： &lt;span class="math">\[
\vec{a}^T\vec{b}=\vec{a}^T(x\vec{a})\Leftrightarrow x=(\vec{a}^T\vec{a})^{-1}\vec{a}^T\vec{b}
\]&lt;/span> &lt;span class="math">\(x\)&lt;/span>是&lt;span class="math">\(\vec{p}\)&lt;/span>与&lt;span class="math">\(\vec{a}\)&lt;/span>的倍数关系，由&lt;span class="math">\(\vec{p}=x\vec{a}\)&lt;/span>可得： &lt;span class="math">\[
\vec{p}=\vec{a}(\vec{a}^T\vec{a})^{-1}\vec{a}^T\vec{b}
\]&lt;/span> 那么，对于任意的&lt;span class="math">\(\vec{b}\in \R^n\)&lt;/span>，为了追求误差最小化，其投影到&lt;span class="math">\(\vec{a}\)&lt;/span>所属的一维子空间投影矩阵为： &lt;span class="math">\[
P_a=\vec{a}(\vec{a}^T\vec{a})^{-1}\vec{a}^T
\]&lt;/span> 其中，&lt;span class="math">\(P_a\)&lt;/span>表示投影到向量&lt;span class="math">\(\vec{a}\)&lt;/span>所在子空间的投影矩阵。由于从&lt;span class="math">\(\vec{e}\perp\vec{a}\)&lt;/span>到投影矩阵&lt;span class="math">\(P_a\)&lt;/span>的推导过程都是充要关系，因此这种最小化误差的投影与投影矩阵&lt;span class="math">\(P\)&lt;/span>是一一对应的。&lt;/p>
&lt;h3 id="向量的投影矩阵">向量的投影矩阵&lt;/h3>
&lt;p>上一小节，我们得到了某向量所在的一维子空间的投影矩阵&lt;span class="math">\(P_a=\vec{a}(\vec{a}^T\vec{a})^{-1}\vec{a}^T\)&lt;/span>。我们现在来研究一些&lt;span class="math">\(P\)&lt;/span>有哪些特性与性质。&lt;/p>
&lt;p>首先，&lt;span class="math">\(P_a\)&lt;/span>与&lt;span class="math">\(\vec{a}\)&lt;/span>的关系非常密切，由&lt;span class="math">\(P_a=kaa^T,k=(\vec{a}^T\vec{a})^{-1}\)&lt;/span>可知，&lt;span class="math">\(P_a\)&lt;/span>中的每个列向量都是&lt;span class="math">\(\vec{a}\)&lt;/span>的倍数，即列向量之间都是线性相关的。这意味着&lt;span class="math">\(P_a\)&lt;/span>的列空间（值域）等于&lt;span class="math">\(\vec{a}\)&lt;/span>所在的一维子空间，&lt;span class="math">\(Col(P_a)=\R^1_a\)&lt;/span>。这满足了投影定义中，将所有&lt;span class="math">\(V\)&lt;/span>中的元素都映射到子空间&lt;span class="math">\(W\)&lt;/span>中的要求。&lt;/p>
&lt;p>同时，&lt;span class="math">\(P_a\)&lt;/span>的列向量的最大线性无关组为1，即&lt;span class="math">\(P_a\)&lt;/span>是个秩1矩阵。投影所保留的维度正好对应了一维子空间的维度，这也体现了&lt;span class="math">\(P_a\)&lt;/span>与一维子空间的映射关系。&lt;/p>
&lt;p>我们之前，在定义投影的时候也提过，&lt;span class="math">\(\forall u\in W\)&lt;/span>，有&lt;span class="math">\(P(u)=u\)&lt;/span>，也就是子空间中向量的投影是其本身。如果对一个已经经过投影的向量再做一次投影操作（因为第一次投影操作以及让影子属于子空间了），应该不再有变化，这从几何观点很容易理解。不难发现，&lt;span class="math">\(P_aP_a=\vec{a}(\vec{a}^T\vec{a})^{-1}[\vec{a}^T\vec{a}(\vec{a}^T\vec{a})^{-1}]_{=1}\vec{a}^T=\vec{a}(\vec{a}^T\vec{a})^{-1}\vec{a}^T=P_a\)&lt;/span>，即&lt;span class="math">\(P_a^2=P_a\)&lt;/span>。我们称这个性质为&lt;strong>幂等性&lt;/strong>，满足这个性质的矩阵为幂等矩阵。投影矩阵的幂等性还可以从信息的角度理解，投影矩阵的作用是将与子空间无关的其他维度剥离，只留下子空间维度的信息。因此经过投影的向量已经没有了其他维度信息，在投影（剥离）一次不改变结果。&lt;/p>
&lt;p>此外，我们还发现，投影矩阵是一个对称矩阵，即&lt;span class="math">\(P_a^T=P_a\)&lt;/span>。投影矩阵的对称性涉及到自伴随算子与空间的映射关系，是一个深刻且值得探讨的结论，但是需要的数学知识较深，这里不再讨论。&lt;/p>
&lt;p>总结，向量到一维子空间的投影矩阵有如下性质：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>投影矩阵&lt;span class="math">\(P_a\)&lt;/span>可有一维子空间所在向量&lt;span class="math">\(\vec{a}\)&lt;/span>构成，因此其列空间为该子空间。&lt;/li>
&lt;li>&lt;span class="math">\(P_a\)&lt;/span>的秩等于子空间维度。&lt;/li>
&lt;li>&lt;span class="math">\(P_a\)&lt;/span>是幂等矩阵；&lt;/li>
&lt;li>&lt;span class="math">\(P_a\)&lt;/span>是对称矩阵。&lt;/li>
&lt;/ol>
&lt;h3 id="子空间投影">子空间投影&lt;/h3>
&lt;p>我们已经讨论了向量到向量所在的一维子空间的投影，那么当子空间维数不仅仅是一维的，而是高维的子空间，那么其投影该怎么求呢？&lt;/p>
&lt;p>方法和一维子空间类似。假设我们要求矩阵&lt;span class="math">\(A\)&lt;/span>确定的投影矩阵&lt;span class="math">\(P_A\)&lt;/span>，由于子空间等于由矩阵&lt;span class="math">\(A\)&lt;/span>的列空间，整体空间中有一任意非零向量&lt;span class="math">\(\vec{b}\)&lt;/span>，其在子空间的投影为&lt;span class="math">\(\vec{p}\)&lt;/span>，因为&lt;span class="math">\(\vec{p}\)&lt;/span>位于&lt;span class="math">\(A\)&lt;/span>的列空间，所以可以用&lt;span class="math">\(A\)&lt;/span>的列向量的线性组合&lt;span class="math">\(\hat{x}_1\vec{c}_1+\hat{x}_2\vec{c}_2+\dotsb+\hat{x}_n\vec{c}_n\)&lt;/span>表示，其中&lt;span class="math">\(\vec{c}_i\)&lt;/span>是&lt;span class="math">\(A\)&lt;/span>的一个&lt;span class="math">\(m×1\)&lt;/span>的列向量，将其写成矩阵形式为：&lt;span class="math">\(\vec{p}=A\hat{x}\)&lt;/span>。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/3dim_projection.png" alt="3dim_projection.png" />&lt;p class="caption">3dim_projection.png&lt;/p>
&lt;/div>
&lt;p>同样的，我们希望误差&lt;span class="math">\(\vec{e}=\vec{b}-\vec{p}\)&lt;/span>最小，那么根据高中立体几何知识可知：&lt;strong>只有当误差&lt;span class="math">\(\vec{e}\)&lt;/span>垂直与子空间时，&lt;span class="math">\(e\)&lt;/span>的长度最短&lt;/strong>。此时，&lt;span class="math">\(\vec{e}\)&lt;/span>应垂直于子空间中所有向量，包括&lt;span class="math">\(A\)&lt;/span>的所有列向量，即 &lt;span class="math">\[
\vec{e}\perp Col(A)\\
\Leftrightarrow A^T\vec{e}=\vec{0}或 c_i\cdot \vec{e}=0\;\forall i\in\{1,\dotsb,n\}
\]&lt;/span> 其中，&lt;span class="math">\(Col(A)\)&lt;/span>表示矩阵&lt;span class="math">\(A\)&lt;/span>的列空间，&lt;span class="math">\(c_i\)&lt;/span>是矩阵&lt;span class="math">\(A\)&lt;/span>的一列。将&lt;span class="math">\(\vec{e}=\vec{b}-\vec{p},\;\vec{p}=A\hat{x}\)&lt;/span>代入上式可得： &lt;span class="math">\[
A^T(\vec{b}-\vec{p})=0\Leftrightarrow A^T\vec{b}=A^T\vec{p}\Leftrightarrow A^T\vec{b}=A^TA\hat{x}
\]&lt;/span> 我们先只考虑&lt;span class="math">\(A^TA\)&lt;/span>是可逆的情形，可得： &lt;span class="math">\[
\hat{x}=(A^TA)^{-1}A^T\vec{b}
\]&lt;/span> 而&lt;span class="math">\(\hat{x}\)&lt;/span>又是&lt;span class="math">\(\vec{p}\)&lt;/span>用矩阵&lt;span class="math">\(A\)&lt;/span>列向量线性表示的系数，即&lt;span class="math">\(\vec{p}=A\hat{x}\)&lt;/span>。综上可得，对于空间中任意向量&lt;span class="math">\(\vec{b}\in \R^m\)&lt;/span>，在误差&lt;span class="math">\(\vec{e}\)&lt;/span>最小化的正交投影要求下，其投影到子空间&lt;span class="math">\(Col(A)\)&lt;/span>的结果为： &lt;span class="math">\[
\vec{p}=A(A^TA)^{-1}A^T\vec{b}
\]&lt;/span> 相应的，由&lt;span class="math">\(A\)&lt;/span>所确定的投影矩阵可定义为： &lt;span class="math">\[
P_A=A(A^TA)^{-1}A^T
\]&lt;/span>&lt;/p>
&lt;h3 id="投影矩阵与子空间关系">投影矩阵与子空间关系&lt;/h3>
&lt;p>上一小节已经求出了投影矩阵&lt;span class="math">\(P_A=A(A^TA)^{-1}A^T\)&lt;/span>，其中&lt;span class="math">\(A_{m×n}\)&lt;/span>为&lt;span class="math">\(m×n\)&lt;/span>矩阵，那么投影矩阵与&lt;span class="math">\(A\)&lt;/span>所构成的子空间是什么关系呢？&lt;/p>
&lt;p>我们首先研究&lt;span class="math">\(P_A\)&lt;/span>的列空间的秩。由于我们先假设&lt;span class="math">\(A^TA\)&lt;/span>是可逆的，因此其为一个&lt;span class="math">\(n×n\)&lt;/span>维可逆矩阵，因此有&lt;span class="math">\(Rank(A^TA)=n\)&lt;/span>。同时由于两个秩为&lt;span class="math">\(r\)&lt;/span>的矩阵乘积秩必然也不大于&lt;span class="math">\(r\)&lt;/span>（这是因为矩阵右乘或左乘分别是到其行空间或列空间的映射，因此矩阵乘法不可能升维），所以&lt;span class="math">\(Rank(A)=Rank(A^T)\geq Rank(A^TA)=n\)&lt;/span>。而又因为矩阵的秩小于等于行数和列数，即&lt;span class="math">\(Rank(A)=Rank(A^T)\leq \min\{m,n\}\)&lt;/span>，所以有&lt;span class="math">\(Rank(A)=Rank(A^T)=n\leq m\)&lt;/span>。&lt;/p>
&lt;p>有了矩阵秩的信息，我们再来看看投影矩阵&lt;span class="math">\(P_A\)&lt;/span>的秩，我们发现&lt;span class="math">\(P_A\)&lt;/span>的秩并不好求，因此我们这里用一个夹逼的技巧，看看矩阵&lt;span class="math">\(P_AA\)&lt;/span>的秩，易得 &lt;span class="math">\[
P_AA=A\underbrace{(A^TA)^{-1}A^TA}_{=I}=A
\]&lt;/span> 根据&lt;strong>两个矩阵乘积的秩必然不大于任一个矩阵的秩&lt;/strong>这一特性，有： &lt;span class="math">\[
\left .
\begin{aligned}
rank(P_A)≥rank(P_AA)=rank(A)=n\\
rank(P_A)=rank(A(A^TA)^{-1}A^T)≤rank(A)=n
\end{aligned}
\right \}
\Rightarrow rank(P_A)=n
\]&lt;/span> 即投影矩阵&lt;span class="math">\(P_A\)&lt;/span>的秩为&lt;span class="math">\(n\)&lt;/span>，等于子空间的维度。&lt;/p>
&lt;p>有了&lt;span class="math">\(P_A\)&lt;/span>的秩，我们再由矩阵乘法的积的列空间特性进行进一步推导。对于矩阵乘法&lt;span class="math">\(S=AX\)&lt;/span>，如果我们将&lt;span class="math">\(X\)&lt;/span>写成列向量组的形式&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_k)\)&lt;/span>，则&lt;span class="math">\(S=AX\)&lt;/span>的乘积为&lt;span class="math">\((Ax_1,Ax_2,\dotsb,Ax_k)\)&lt;/span>。显然，矩阵&lt;span class="math">\(S\)&lt;/span>的每一列&lt;span class="math">\(s_i=Ax_i ∈ Col(A)\)&lt;/span>，因而&lt;span class="math">\(S\)&lt;/span>列向量组的线性组合也在&lt;span class="math">\(Col(A)\)&lt;/span>中，即&lt;span class="math">\(Col(S)\subseteq Col(A)\)&lt;/span>。依据此结论，我们可推断&lt;span class="math">\(Col(P_A)\subseteq Col(A)\)&lt;/span>。再综合&lt;span class="math">\(P_A\)&lt;/span>的秩为&lt;span class="math">\(n\)&lt;/span>等于&lt;span class="math">\(rank(A)\)&lt;/span>，我们能够断定&lt;span class="math">\(Col(P_A)=Col(A)\)&lt;/span>，即&lt;span class="math">\(A\)&lt;/span>所构成的子空间等于投影矩阵&lt;span class="math">\(P_A\)&lt;/span>的列空间。&lt;/p>
&lt;p>最后我们来看看投影矩阵还有哪些性质。根据一维子空间中的投影研究应该具有幂等性和对称性。不难验证：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>幂等性：&lt;span class="math">\(P_AP_A=A(A^TA)^{-1}\underbrace{A^TA(A^TA)^{-1}}_{=I}A^T=A(A^TA)^{-1}A^T=P_A\)&lt;/span>&lt;/li>
&lt;li>幂等矩阵的迹等于幂等矩阵的秩，即&lt;span class="math">\(tr(A)=rank(A)\)&lt;/span>（通过特征值证明）&lt;/li>
&lt;li>对称性：&lt;span class="math">\(P_A^T=(A(A^TA)^{-1}A^T)^T=(A^T)^T((A^TA)^{-1})^TA^T=A(A^TA)^{-1}A^T=P_A\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;h3 id="再谈投影和误差">再谈投影和误差&lt;/h3>
&lt;p>通过投影，我们把空间中任一向量&lt;span class="math">\(\vec{b}\)&lt;/span>分成了投影子空间的部分&lt;span class="math">\(\vec{p}\)&lt;/span>和误差部分&lt;span class="math">\(\vec{e}\)&lt;/span>。从信息的角度，投影矩阵的作用就是保留子空间中的信息分量&lt;span class="math">\(\vec{p}\)&lt;/span>，去掉非子空间中的分量&lt;span class="math">\(\vec{e}\)&lt;/span>。之前，我们已经分析出，投影子空间等于矩阵&lt;span class="math">\(A\)&lt;/span>的列空间&lt;span class="math">\(Col(A)\)&lt;/span>。那么误差&lt;span class="math">\(\vec{e}\)&lt;/span>所在的空间有没有什么特殊之处呢？&lt;/p>
&lt;p>从之前的推导过程中，我们指出对于任意误差&lt;span class="math">\(\vec{e}\)&lt;/span>，为了使其最小化，应让其垂直与列空间&lt;span class="math">\(Col(A)\)&lt;/span>，即 &lt;span class="math">\[
\forall \vec{e},有\vec{e}\perp Col(A)
\]&lt;/span> 哈，这里很直观的告诉我们，所有&lt;span class="math">\(\vec{e}\)&lt;/span>所组成的子空间，应该垂直于列空间。而根据笔记&lt;a href="线性代数与矩阵之四类空间.md">线性代数与矩阵之四类空间&lt;/a>的内容，&lt;strong>垂直于列空间的空间正是左零空间&lt;/strong>！&lt;/p>
&lt;div class="figure">
&lt;img src="../images/投影分割空间.png" alt="投影分割空间.png" />&lt;p class="caption">投影分割空间.png&lt;/p>
&lt;/div>
&lt;p>这也就是说，我们永远可以把一个&lt;span class="math">\(\R^m\)&lt;/span>中的向量，通过投影拆分成两个分量：一个在&lt;span class="math">\(A\)&lt;/span>的列空间中，另一个分量垂直于&lt;span class="math">\(A\)&lt;/span>的列空间，即在&lt;span class="math">\(A\)&lt;/span>的左零空间中。同时这两个空间的直和，正好等于完整的空间&lt;span class="math">\(\R^m\)&lt;/span>！&lt;/p>
&lt;blockquote>
&lt;p>直和：设&lt;span class="math">\(V_1,V_2\)&lt;/span>是线性空间&lt;span class="math">\(V\)&lt;/span>的子空间，如果&lt;span class="math">\(V_1+V_2\)&lt;/span>中的每个向量分解式&lt;span class="math">\(\vec{\alpha}=\vec{\alpha}_1+\vec{\alpha}_2\)&lt;/span>唯一，其中&lt;span class="math">\(\vec{\alpha}_1∈V_1,\vec{\alpha}_2∈V_2\)&lt;/span>。那么称这个和为直和(direct sum)，记为&lt;span class="math">\(V_1⊕V_2\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h2 id="最小二乘法与子空间投影">最小二乘法与（子）空间投影&lt;/h2>
&lt;p>我们说了这么长时间的投影，再回头看&lt;a href="#引子">引子&lt;/a>中的那个问题，是不是有点理解投影的作用了？&lt;/p>
&lt;p>对于一个方程数大于变量数的线性方程组而言（&lt;span class="math">\(m&amp;gt;n\)&lt;/span>），一般是没有解的。而投影就是让这个线性方程组有一个最近似的解&lt;span class="math">\(\vec{\hat{x}}\)&lt;/span>，使误差&lt;span class="math">\(\vec{e}=\vec{b}-A\vec{\hat{x}}\)&lt;/span>最小。这个方法在拟合方程如最小二乘法中得到了广泛应用。&lt;/p>
&lt;h3 id="最小二乘法">最小二乘法&lt;/h3>
&lt;p>“最小平方法”是对线性方程组，即方程个数比未知数更多的方程组，以回归分析求得近似解的标准方法，其核心是将&lt;strong>残差总和最小化&lt;/strong>，残差即观测值与模型提供的拟合值之间的差距，对应到矩阵化处理，就是使投影的误差最小。&lt;/p>
&lt;p>下面我们用一个例子来说明最小二乘法的矩阵化处理方法。&lt;/p>
&lt;p>假设在平面直角坐标系中有三个点&lt;span class="math">\(p_1,p_2,p_3\)&lt;/span>，分别为&lt;span class="math">\(\{(1,1), (2,2), (3,2)\}\)&lt;/span>需要求一条直线尽可能的经过这三个点，虽然不可能有直线完全经过三点，但可以要求直线与这三个点的误差尽量小。我们假设这条直线可写成&lt;span class="math">\(b=C+Dt\)&lt;/span>的形式，将数据代入写成方程组： &lt;span class="math">\[
\begin{cases}
C+D=1\\
C+2D=2\\
C+3D=2
\end{cases}\Leftrightarrow
\underbrace{\begin{bmatrix}
1&amp;amp;1\\1&amp;amp;2\\1&amp;amp;3\\
\end{bmatrix}}_A
\underbrace{\begin{bmatrix}
C\\D\\
\end{bmatrix}}_{\vec{x}}=
\underbrace{\begin{bmatrix}
1\\2\\2\\
\end{bmatrix}}_{\vec{b}}
\]&lt;/span>&lt;/p>
&lt;div class="figure">
&lt;img src="../images/最小二乘法.png" alt="最小二乘法.png" />&lt;p class="caption">最小二乘法.png&lt;/p>
&lt;/div>
&lt;p>显然，这个的方程&lt;span class="math">\(A\vec{x}=\vec{b}\)&lt;/span>是无解的，我们根据之前向量投影思路，解决办法就是求其最优解，最优解的含义即为误差最小： &lt;span class="math">\[
\min\|\vec{e}\|^2=\|A\vec{\hat{x}}-\vec{b}\|^2
\]&lt;/span> 我们需要的就是寻找具有&lt;strong>最小误差平方和&lt;/strong>的解&lt;span class="math">\(\vec{\hat{x}}\)&lt;/span>，这也是“最小二乘”这一名称的由来。&lt;/p>
&lt;p>前面投影章节已经指出，正交投影矩阵&lt;span class="math">\(P_A\)&lt;/span>能够最小化误差，所以&lt;span class="math">\(\vec{\hat{x}}\)&lt;/span>的值应该等于&lt;span class="math">\(\vec{b}\)&lt;/span>在&lt;span class="math">\(A\)&lt;/span>所在子空间的正交投影，即： &lt;span class="math">\[
\hat{x}=(A^TA)^{-1}A^T\vec{b}
\]&lt;/span> 根据上式解&lt;span class="math">\(A\vec{\hat{x}}=\vec{b}\)&lt;/span>可得： &lt;span class="math">\[
\hat{x}=\begin{bmatrix}
\hat{C}\\\hat{D}\\
\end{bmatrix}=\left (\begin{bmatrix}
1&amp;amp;1&amp;amp;1\\1&amp;amp;2&amp;amp;3\\
\end{bmatrix}\begin{bmatrix}
1&amp;amp;1\\1&amp;amp;2\\1&amp;amp;3\\
\end{bmatrix}\right)^{-1}\begin{bmatrix}
1&amp;amp;1&amp;amp;1\\1&amp;amp;2&amp;amp;3\\
\end{bmatrix}\begin{bmatrix}
1\\2\\2\\
\end{bmatrix}=\begin{bmatrix}
\frac{2}{3}\\\\\frac{1}{2}\\
\end{bmatrix}
\]&lt;/span> 即为方程误差最小的最优解。我们也可以验证，误差&lt;span class="math">\(\vec{e}=\vec{b}-A\vec{\hat{x}}=[1/6\; -1/3\; 1/6]^T\)&lt;/span>，&lt;span class="math">\(A^T\vec{e}=\vec{0}\Rightarrow Col(A)\perp\vec{e}\)&lt;/span>，投影向量&lt;span class="math">\(\vec{p}=A\vec{\hat{x}}\)&lt;/span>与&lt;span class="math">\(\vec{e}\)&lt;/span>正交，并且&lt;span class="math">\(\vec{e}\)&lt;/span>与矩阵&lt;span class="math">\(A\)&lt;/span>的列空间&lt;span class="math">\(Col(A)\)&lt;/span>正交。&lt;/p>
&lt;p>最小二乘法是投影矩阵的典型应用，再补充一个小点，在实数空间&lt;span class="math">\(\R^n\)&lt;/span>中，投影矩阵&lt;span class="math">\(P_A=A(A^TA)^{-1}A^T\)&lt;/span>；而在复数空间&lt;span class="math">\(\mathbb{C}^n\)&lt;/span>中，需要使用共轭转置代替，即&lt;span class="math">\(P_A=A(A^HA)^{-1}A^H\)&lt;/span>。&lt;/p>
&lt;h3 id="ata不可逆时的处理">&lt;span class="math">\(A^TA\)&lt;/span>不可逆时的处理&lt;/h3>
&lt;p>之前，我们一直假设&lt;span class="math">\(A^TA\)&lt;/span>是可逆的。然而，确实存在其不可逆的场景，比如一些未知量&lt;span class="math">\(x_i,x_j\)&lt;/span>存在线性关系。&lt;/p>
&lt;p>最简单的方法当时是从中挑出一些列，筛选出线性无关的特征，不保留相同的特征，保证不存在线性相关的特征，从而组成一个更小的矩阵&lt;span class="math">\(\tilde{A}\)&lt;/span>，使&lt;span class="math">\(\tilde{A}^T\tilde{A}\)&lt;/span>可逆。&lt;/p>
&lt;p>其次，我们可以增加样本量，即多增加几个方程，看看能不能破坏未知量&lt;span class="math">\(x_i,x_j\)&lt;/span>存在线性关系。&lt;/p>
&lt;p>最后，还可以采用正则化方法，对于正则化的方法，常见的是L1正则项和L2正则项。这超出了本文的阐述范围，有兴趣的读者可以找相关资料了解。&lt;/p></description></item><item><title>线性代数与矩阵之四类空间</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E5%9B%9B%E7%B1%BB%E7%A9%BA%E9%97%B4/</link><pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E5%9B%9B%E7%B1%BB%E7%A9%BA%E9%97%B4/</guid><description>
&lt;h2 id="线性代数与矩阵之四类空间">线性代数与矩阵之四类空间&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#矩阵的列空间与行空间">矩阵的列空间与行空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#列空间与线性方程组的解">列空间与线性方程组的解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩阵的零空间与左零空间">矩阵的零空间与左零空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#零空间与线性方程组的通解">零空间与线性方程组的通解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#各空间关系">各空间关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#秩的关系">秩的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正交关系">正交关系&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#子空间互补">子空间互补&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#四类空间与线性方程组的可解性">四类空间与线性方程组的可解性&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>对于一个&lt;span class="math">\(m×n\)&lt;/span>的矩阵&lt;span class="math">\(A\)&lt;/span>， &lt;span class="math">\[
A=\begin{bmatrix}
a_{11}&amp;amp;a_{12}&amp;amp;a_{13}&amp;amp;\dotsb&amp;amp;a_{1n}\\
a_{21}&amp;amp;a_{22}&amp;amp;a_{23}&amp;amp;\dotsb&amp;amp;a_{2n}\\
a_{31}&amp;amp;a_{32}&amp;amp;a_{33}&amp;amp;\dotsb&amp;amp;a_{3n}\\
\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\
a_{n1}&amp;amp;a_{n2}&amp;amp;a_{n3}&amp;amp;\dotsb&amp;amp;a_{nn}\\
\end{bmatrix}
\]&lt;/span> 皆有四类空间，即矩阵的&lt;strong>列向量组成的列空间（Column Space）&lt;/strong>、矩阵&lt;strong>行向量组成的行空间（Row Space）&lt;/strong>、零空间（Null Space）以及左零空间（Left Null Space）。&lt;/p>
&lt;p>注：&lt;strong>本文在没有特殊说明的时候，矩阵&lt;span class="math">\(A\)&lt;/span>都是此&lt;span class="math">\(m\times n\)&lt;/span>矩阵&lt;/strong>。&lt;/p>
&lt;h2 id="矩阵的列空间与行空间">矩阵的列空间与行空间&lt;/h2>
&lt;p>矩阵&lt;span class="math">\(A_{m\times n}\)&lt;/span>根据横向和纵向，我们可以将其分成列向量组或行向量组&lt;/p>
&lt;div class="figure">
&lt;img src="../images/bordermatrix.png" alt="bordermatrix" />&lt;p class="caption">bordermatrix&lt;/p>
&lt;/div>
&lt;p>其中，列向量组表示&lt;span class="math">\(A=(c_1,c_2\dotsb,c_n)\)&lt;/span>，行向量组表示&lt;span class="math">\(A=(r_1,r_2,\dotsb,r_m)^T\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>列（行）空间：用&lt;span class="math">\(A\)&lt;/span>的所有列向量所张成的空间，称为列空间，记作&lt;span class="math">\(C(A)或Col(A)\)&lt;/span>；用&lt;span class="math">\(A\)&lt;/span>的所有行向量所张成的空间，称为行空间，记作&lt;span class="math">\(R(A)或Raw(A)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>向量组张成空间的概念参见笔记&lt;a href="线性代数与矩阵之理解向量、线性变换与矩阵乘法.md">线性代数与矩阵之理解向量、线性变换与矩阵乘法&lt;/a>。每一个&lt;span class="math">\(c_i\)&lt;/span>是&lt;span class="math">\(m\times 1\)&lt;/span>向量，因此列空间必是&lt;span class="math">\(R^m\)&lt;/span>空间的子空间，同理行空间是&lt;span class="math">\(R^n\)&lt;/span>空间的子空间。&lt;/p>
&lt;p>如果从矩阵乘以向量&lt;span class="math">\(Ax=y\)&lt;/span>的角度来看列空间，我们可以发现，&lt;span class="math">\(Ax\)&lt;/span>的结果是列向量组&lt;span class="math">\((c_1,c_2\dotsb,c_n)\)&lt;/span>的线性组合，即&lt;span class="math">\(y\)&lt;/span>必然是列空间的向量。这表明列空间等同于该矩阵左乘向量的值域空间。同理，行空间是&lt;span class="math">\(x^TA=y^T\)&lt;/span>的值域空间。&lt;/p>
&lt;h3 id="列空间与线性方程组的解">列空间与线性方程组的解&lt;/h3>
&lt;p>如果将线性方程组写成矩阵的形式&lt;span class="math">\(Ax=b\)&lt;/span>，如果要求此方程有解，那么向量&lt;span class="math">\(b\)&lt;/span>必须要在矩阵&lt;span class="math">\(A\)&lt;/span>的列空间中。&lt;/p>
&lt;h2 id="矩阵的零空间与左零空间">矩阵的零空间与左零空间&lt;/h2>
&lt;blockquote>
&lt;p>零空间：所有满足&lt;span class="math">\(Ax=0\)&lt;/span>的向量&lt;span class="math">\(x\)&lt;/span>的集合就称之为矩阵&lt;span class="math">\(A\)&lt;/span>的零空间，记为&lt;span class="math">\(N(A)或Null(A)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>如果矩阵A的各列线性无关，则&lt;span class="math">\(x\)&lt;/span>就只有零向量这个唯一解，如果&lt;span class="math">\(A\)&lt;/span>的各列线性相关(意味着降维了，进一步是行列式值为零)，那么&lt;span class="math">\(x\)&lt;/span>就有非零解。由于解&lt;span class="math">\(x\)&lt;/span>是&lt;span class="math">\(n\times 1\)&lt;/span>向量，因此零空间是&lt;span class="math">\(R^n\)&lt;/span>空间的子空间(通常小于&lt;span class="math">\(n\)&lt;/span>)。&lt;/p>
&lt;blockquote>
&lt;p>左零空间：所有满足&lt;span class="math">\(y^TA=A^Ty=0\)&lt;/span>的向量&lt;span class="math">\(y^T或y\)&lt;/span>的集合就称之为矩阵&lt;span class="math">\(A\)&lt;/span>的左零空间，记为&lt;span class="math">\(N(A^T)或Null(A^T)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>显然，左零空间就是因为&lt;span class="math">\(y^T\)&lt;/span>向量在矩阵左侧。由于解&lt;span class="math">\(y\)&lt;/span>是&lt;span class="math">\(m\times 1\)&lt;/span>向量，因此左零空间是&lt;span class="math">\(R^m\)&lt;/span>空间的子空间(通常小于&lt;span class="math">\(m\)&lt;/span>)。&lt;/p>
&lt;h3 id="零空间与线性方程组的通解">零空间与线性方程组的通解&lt;/h3>
&lt;p>无论是零空间还是左零空间都是&lt;strong>解&lt;/strong>的特性。利用零空间和方程的一个特解，我们可以构造方程的通解。 &lt;span class="math">\[
Ax=b\Rightarrow Ax^\ast+Ax&amp;#39;=b
\]&lt;/span> 其中，&lt;span class="math">\(Ax^\ast=0\)&lt;/span>，&lt;span class="math">\(x^\ast\)&lt;/span>是零空间的仍一元素，而&lt;span class="math">\(x&amp;#39;\)&lt;/span>是方程组的任一个特解。&lt;span class="math">\(x^\ast\)&lt;/span>可以用零空间的一组基来表示，即 &lt;span class="math">\[
x^\ast=(z_1,z_2,\dotsb,z_r)\begin{bmatrix}
x_1\\x_2\\\dotsb\\z_r
\end{bmatrix}
\]&lt;/span> 其中，&lt;span class="math">\((z_1,z_2,\dotsb,z_r)\)&lt;/span>表示零空间的一组基，&lt;span class="math">\(z_i\)&lt;/span>为&lt;span class="math">\(n\times 1\)&lt;/span>维向量，下标&lt;span class="math">\(r\)&lt;/span>为零空间的维度。&lt;/p>
&lt;p>对于&lt;span class="math">\(Ax=0\)&lt;/span>的通解，还可以使用简化列阶梯形矩阵来求解，Matlab正是用此方法进行求解的。&lt;/p>
&lt;blockquote>
&lt;p>简化列阶梯形矩阵：简化列阶梯形矩阵或简约行梯形式矩阵（reduced row echelon form），也称作行规范形矩阵（row canonical form），如果满足额外的条件：每个首项系数是1，且是其所在列的唯一的非零元素。&lt;/p>
&lt;/blockquote>
&lt;p>例如： &lt;span class="math">\[{\displaystyle \left[{\begin{array}{ccccc}1&amp;amp;0&amp;amp;a_{1}&amp;amp;0&amp;amp;b_{1}\\0&amp;amp;1&amp;amp;a_{2}&amp;amp;0&amp;amp;b_{2}\\0&amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;b_{3}\end{array}}\right]}\]&lt;/span>&lt;/p>
&lt;p>简化列阶梯形矩阵与线性方程组的通解有关系，如果我们将所有基本列放到一起，如上例中是一个&lt;span class="math">\(I_{3\times 3}\)&lt;/span>，剩下的自由列为&lt;span class="math">\(F=\begin{bmatrix}a_1&amp;amp;b_1\\a_2&amp;amp;b_2\\0&amp;amp;b_3\end{bmatrix}\)&lt;/span>。在求解&lt;span class="math">\(Ax=0\)&lt;/span>时，我们分别把自由列（第3，5）列取（1，0）和（0，1）可得，零空间的一组基为： &lt;span class="math">\[
k_1\begin{bmatrix}-a_1\\-a_2\\1\\0\\0\end{bmatrix}+k_2\begin{bmatrix}
-b_1\\-b_2\\0\\-b_3\\1\end{bmatrix}
\]&lt;/span>&lt;/p>
&lt;h2 id="各空间关系">各空间关系&lt;/h2>
&lt;p>对于一个矩阵&lt;span class="math">\(A_{m\times n}\)&lt;/span>，其四个空间为&lt;/p>
&lt;ul>
&lt;li>列空间&lt;span class="math">\(C(A)\)&lt;/span>，是空间&lt;span class="math">\(R^m\)&lt;/span>的子空间，秩&lt;span class="math">\(r\)&lt;/span>为&lt;span class="math">\(A\)&lt;/span>的基本列个数，或&lt;span class="math">\(C(A)\)&lt;/span>的维度。&lt;/li>
&lt;li>零空间&lt;span class="math">\(N(A)\)&lt;/span>，是空间&lt;span class="math">\(R^n\)&lt;/span>的子空间，秩&lt;span class="math">\(n-r\)&lt;/span>为&lt;span class="math">\(A\)&lt;/span>的自由列个数，或&lt;span class="math">\(N(A)\)&lt;/span>的维度。&lt;/li>
&lt;li>行空间&lt;span class="math">\(C(A^T)=R(A)\)&lt;/span>，是空间&lt;span class="math">\(R^n\)&lt;/span>的子空间，秩&lt;span class="math">\(r\)&lt;/span>为&lt;span class="math">\(A^T\)&lt;/span>的基本列个数，或&lt;span class="math">\(R(A)\)&lt;/span>的维度。&lt;/li>
&lt;li>左零空间&lt;span class="math">\(N(A^T)\)&lt;/span>，是空间&lt;span class="math">\(R^m\)&lt;/span>的子空间，秩&lt;span class="math">\(m-r\)&lt;/span>为&lt;span class="math">\(A^T\)&lt;/span>的自由列个数，或&lt;span class="math">\(C(A^T)\)&lt;/span>的维度。&lt;/li>
&lt;/ul>
&lt;p>各空间的关系总结如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/four-linear-subspaces.png" alt="four-linear-subspaces.png" />&lt;p class="caption">four-linear-subspaces.png&lt;/p>
&lt;/div>
&lt;h3 id="秩的关系">秩的关系&lt;/h3>
&lt;p>一个基本的秩的结论是列空间维度等于行空间维度，即&lt;span class="math">\(Rank(A)=Rank(A^T)\)&lt;/span>。这个结论太过显然，很多时候我们都未自习思考过。&lt;/p>
&lt;p>我们再来看列空间和零空间之间秩的关系。在&lt;span class="math">\(A_{m\times n}\)&lt;/span>中，列向量组中向量个数有&lt;span class="math">\(n\)&lt;/span>个，其中有&lt;span class="math">\(r_c\)&lt;/span>个线性无关的向量，那么根据线性无关向量个数和子空间维度的关系，我们可以知道列空间&lt;span class="math">\(C(A)\)&lt;/span>的秩为&lt;span class="math">\(r_c\)&lt;/span>。（算是矩阵值域的特性）&lt;/p>
&lt;p>而当我们在看方程组&lt;span class="math">\(Ax=0\)&lt;/span>时，当通过高斯消元法，可以得到一个上三角矩阵，但是主对角线元素不一定都是非0的，我们将主对角线非0的列叫基本列，主对角线等于0的自由列。根据三角矩阵秩与主对角元素的关系可知，基本列个数为&lt;span class="math">\(r_c\)&lt;/span>，那么自由列个数就是&lt;span class="math">\(n-c_r\)&lt;/span>。&lt;/p>
&lt;p>在解方程&lt;span class="math">\(x\)&lt;/span>过程中，我们给自由列任一的系数，而基本变量用自由变量来表示；也就是说，自由变量确定了一个值，基本变量也就随之确定了一个值。上面这个解集形式也被称为方程组的“通解”，因为它给出了方程组所有解的显示表示。自由列的个数解决了自由变量的个数，也就决定了零空间的维度，因此零空间维度是&lt;span class="math">\(n-r_c\)&lt;/span>。&lt;/p>
&lt;p>综上所述：&lt;span class="math">\(Rank(C(A))+Rank(N(A))=n\)&lt;/span>。同理，有&lt;span class="math">\(Rank(R(A))+Rank(N(A^T))=m\)&lt;/span>。这二者是从线性方程组与解的关系得来的。&lt;/p>
&lt;h3 id="正交关系">正交关系&lt;/h3>
&lt;p>先说结论：&lt;strong>列空间与左零空间是&lt;span class="math">\(R^m\)&lt;/span>的子空间，且二者正交；行空间与零空间是&lt;span class="math">\(R^n\)&lt;/span>的子空间，且二者正交&lt;/strong>。&lt;/p>
&lt;p>再看看其中的关键词：&lt;strong>正交&lt;/strong>，准确的说是&lt;strong>子空间正交&lt;/strong>。那么什么是子空间正交呢？&lt;/p>
&lt;blockquote>
&lt;p>子空间正交：子空间&lt;span class="math">\(S\)&lt;/span>与子空间&lt;span class="math">\(T\)&lt;/span>正交，则&lt;span class="math">\(S\)&lt;/span>中的&lt;strong>任意&lt;/strong>一个向量都和T中的&lt;strong>任意&lt;/strong>向量正交。&lt;/p>
&lt;/blockquote>
&lt;p>什么是向量正交，这个就太基本了，不再赘述。如果两个子空间正交，那么他们一定只能相交于零向量。因为，如果他们相较于某个非零向量&lt;span class="math">\(\vec{v}\)&lt;/span>，那么&lt;span class="math">\(\vec{v}\in S\cap T\)&lt;/span>，则要求&lt;span class="math">\(\vec{v}\)&lt;/span>必须和自己正交，而和自己正交的向量只有零向量，与假设矛盾。&lt;/p>
&lt;p>但是，按照如上定义如果我们要验证子空间的正交性，那么需要验证任意的向量，是很麻烦的。根据线性空间与基的关系，我们可以有如下推论：&lt;/p>
&lt;blockquote>
&lt;p>推论：如果向量和（子）空间的一组基正交，那么该向量和（子）空间正交。&lt;/p>
&lt;/blockquote>
&lt;p>根据此推论，我们再来看看式子&lt;span class="math">\(Ax=0\)&lt;/span>，显然&lt;span class="math">\(x_{n\times 1}\)&lt;/span>是零空间的任意向量。我们将矩阵&lt;span class="math">\(A\)&lt;/span>写成行向量组的形式，即 &lt;span class="math">\[
Ax=\begin{bmatrix}
r_1\\r_2\\ \vdots\\r_m
\end{bmatrix} x_{n\times 1}=\begin{bmatrix}
0\\0\\ \vdots\\0
\end{bmatrix}_{m\times 1}
\]&lt;/span> 注意，此处每一个&lt;span class="math">\(r_i\)&lt;/span>是一个&lt;span class="math">\(1\times n\)&lt;/span>的行向量。根据行空间的定义，它是由行向量组张成的空间，&lt;strong>因此这些行向量必然包含行空间的一组基&lt;/strong>。&lt;/p>
&lt;p>同时，由于&lt;span class="math">\(r_i x=0,i\in \{1,2,\cdots,m\}\)&lt;/span>，因此任一行向量与零空间的向量&lt;span class="math">\(x\)&lt;/span>都是正交的。又因为行向量必然包含行空间的一组基，那么零空间中的向量&lt;span class="math">\(x\)&lt;/span>正交于行空间的一组基，推得&lt;span class="math">\(x\)&lt;/span>正交于行空间。同时，&lt;span class="math">\(x\)&lt;/span>又是零空间中的任意向量，因此零空间是正交于行空间的。&lt;/p>
&lt;p>虽然有些啰嗦，但是我们再来阐述下列空间与左零空间的正交性。我们看式子&lt;span class="math">\(y^TA=0\)&lt;/span>，显然&lt;span class="math">\(y_{m\times 1}\)&lt;/span>是左零空间任意向量。我们将矩阵&lt;span class="math">\(A\)&lt;/span>写成列向量组的形式，即 &lt;span class="math">\[
y^TA=y^T\begin{bmatrix}
c_1&amp;amp;c_2&amp;amp;\cdots&amp;amp;c_n
\end{bmatrix}=\begin{bmatrix}
0&amp;amp;0&amp;amp; \cdots&amp;amp;0
\end{bmatrix}_{1\times n}
\]&lt;/span> 注意，此处每一个&lt;span class="math">\(c_j\)&lt;/span>是一个&lt;span class="math">\(m\times 1\)&lt;/span>的列向量。根据列空间的定义，它是由列向量组张成的空间，&lt;strong>因此这些列向量必然包含列空间的一组基&lt;/strong>。&lt;/p>
&lt;p>同时，由于&lt;span class="math">\(y^T c_j=0,j\in \{1,2,\cdots,n\}\)&lt;/span>，因此任一列向量与左零空间的向量&lt;span class="math">\(y^T\)&lt;/span>都是正交的。又因为列向量必然包含列空间的一组基，那么左零空间中的向量&lt;span class="math">\(y^T\)&lt;/span>正交于列空间的一组基，推得&lt;span class="math">\(y^T\)&lt;/span>正交于列空间。同时，&lt;span class="math">\(y^T\)&lt;/span>又是左零空间中的任意向量，因此左零空间是正交于列空间的。&lt;/p>
&lt;h4 id="子空间互补">子空间互补&lt;/h4>
&lt;blockquote>
&lt;p>补空间：在数学领域线性代数和泛函分析中，空间&lt;span class="math">\(V\)&lt;/span>的子空间&lt;span class="math">\(W\)&lt;/span>的正交补是正交于&lt;span class="math">\(W\)&lt;/span>中所有向量的所有&lt;span class="math">\(V\)&lt;/span>中向量的集合，其张成的空间成为&lt;span class="math">\(W^{\perp}\)&lt;/span>，即 &lt;span class="math">\[W^{\perp}=\{x\in V:\forall y\in W,&amp;lt;x,y&amp;gt;=0\}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>我们再联系四个空间秩的关系，有&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(Rank(Raw(A))+Rank(N(A^T))=Rank(Col(A))+Rank(N(A^T))=m\)&lt;/span>可知，&lt;span class="math">\(R^m\)&lt;/span>被切割成&lt;span class="math">\(R^r\)&lt;/span>的列空间和&lt;span class="math">\(R^{m-r}\)&lt;/span>的左零空间。也可以说在&lt;span class="math">\(R^m\)&lt;/span>中列空间和左零空间&lt;strong>互补&lt;/strong>。&lt;/li>
&lt;li>&lt;span class="math">\(Rank(Col(A))+Rank(N(A))=Rank(Raw(A))+Rank(N(A))=n\)&lt;/span>可知，&lt;span class="math">\(R^n\)&lt;/span>被切割成&lt;span class="math">\(R^r\)&lt;/span>的行空间和&lt;span class="math">\(R^{n-r}\)&lt;/span>的零空间。也可以说在&lt;span class="math">\(R^n\)&lt;/span>中行空间和零空间&lt;strong>互补&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>互补的子空间组合在一起就是完整的空间，这种组合到一起的操作又称直和。&lt;/p>
&lt;h2 id="四类空间与线性方程组的可解性">四类空间与线性方程组的可解性&lt;/h2>
&lt;p>MIT著名的线性代数课程是从线性方程组的解的角度引出四类空间的。会看最初的定义&lt;/p>
&lt;p>矩阵&lt;span class="math">\(A_{m\times n}\)&lt;/span>根据横向和纵向，我们可以将其分成列向量组或行向量组&lt;/p>
&lt;div class="figure">
&lt;img src="../images/bordermatrix.png" alt="bordermatrix" />&lt;p class="caption">bordermatrix&lt;/p>
&lt;/div>
&lt;p>其中，列向量组表示&lt;span class="math">\(A=(c_1,c_2\dotsb,c_n)\)&lt;/span>，行向量组表示&lt;span class="math">\(A=(r_1,r_2,\dotsb,r_m)^T\)&lt;/span>。&lt;/p>
&lt;p>在矩阵&lt;span class="math">\(A\)&lt;/span>与向量&lt;span class="math">\(x\)&lt;/span>的乘积&lt;span class="math">\(Ax=(c_1,c_2\dotsb,c_n)x=b\)&lt;/span>中，&lt;span class="math">\(b\)&lt;/span>是值，体现的是列向量的线性组合，所有可能的&lt;span class="math">\(b\)&lt;/span>组成了列空间即为值域，也就是说只有&lt;span class="math">\(b\)&lt;/span>是列空间的元素，方程才有解。同理，&lt;span class="math">\(y^TA=b^T\)&lt;/span>也只有当&lt;span class="math">\(b^T\)&lt;/span>是行空间的元素时，&lt;span class="math">\(y^T\)&lt;/span>才有解。&lt;/p>
&lt;p>当矩阵为列满秩，即&lt;span class="math">\(Rank(A)=n\leq m\)&lt;/span>时，方程只可能有0个或1个解。&lt;span class="math">\(b\)&lt;/span>在列空间中，只有一个解；不在时没有解。&lt;/p>
&lt;p>线性方程组&lt;span class="math">\(Ax=0\)&lt;/span>是特殊的值0，所有让此方程组成立的&lt;span class="math">\(x\)&lt;/span>组成了&lt;span class="math">\(A\)&lt;/span>的零空间。在此方程中，零向量总是方程的解。当&lt;span class="math">\(Rank(A)=n\leq m\)&lt;/span>时，有且只有一个解：零向量。只有当&lt;span class="math">\(Rank(A)&amp;lt;n\)&lt;/span>时，才存在非零解，非零解一旦存在，就不是一个，而是一个解空间，即零空间，且零空间与列空间维度和为&lt;span class="math">\(n\)&lt;/span>。&lt;/p>
&lt;p>如果对于方程&lt;span class="math">\(Ax=b\)&lt;/span>有解，且&lt;span class="math">\(Ax=0\)&lt;/span>存在非零向量解&lt;span class="math">\(x_n\)&lt;/span>，那么&lt;span class="math">\(Ax=b\)&lt;/span>的解就是一个解系，其组成为任一个特解&lt;span class="math">\(x_p\)&lt;/span>与零空间中向量的任意线性组合，即&lt;span class="math">\(x=x_p+任意一组N(A)中的向量和\)&lt;/span>。因为&lt;span class="math">\(A(x_p+x_n)=Ax_p+Ax_n=b+0=b,\; x_n=k_1x_{n1}+k_2x_{n2}+\cdots,\;x_{ni}\in N(A)\)&lt;/span>。&lt;/p>
&lt;p>这里都只讨论&lt;span class="math">\(Ax=b\)&lt;/span>有解的情形，四个空间也有可解性密切相关。当&lt;span class="math">\(Ax=b\)&lt;/span>不可解，即&lt;span class="math">\(b\)&lt;/span>不在列空间中，我们是否可以求出&lt;span class="math">\(x\)&lt;/span>的最优或最近似解呢？这就会涉及到向量与矩阵的投影。&lt;/p></description></item><item><title>线性代数与矩阵之逆矩阵</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E9%80%86%E7%9F%A9%E9%98%B5/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E9%80%86%E7%9F%A9%E9%98%B5/</guid><description>
&lt;ul>
&lt;li>&lt;a href="#逆矩阵">逆矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#补充了解广义逆">补充了解：广义逆&lt;/a>&lt;/li>
&lt;li>&lt;a href="#逆矩阵存在条件">逆矩阵存在条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#逆矩阵的求法">逆矩阵的求法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#初等变换法">初等变换法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#伴随矩阵求逆矩阵">伴随矩阵求逆矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩阵分解法">矩阵分解法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特殊矩阵的逆">特殊矩阵的逆&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对角矩阵的逆">对角矩阵的逆&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正交矩阵的逆">正交矩阵的逆&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分块矩阵的逆">分块矩阵的逆&lt;/a>&lt;/li>
&lt;li>&lt;a href="#逆矩阵的性质">逆矩阵的性质&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="线性代数与矩阵之逆矩阵">线性代数与矩阵之逆矩阵&lt;!-- omit in toc -->&lt;/h2>
&lt;p>逆矩阵，是线性代数和矩阵分析中非常常见也是非常关键的一个问题。本笔记将根据自己常遇到的逆矩阵相关问题，整理有关逆矩阵定义、常用求法、存在条件以及常用性质等相关内容，并对广义逆做简单介绍。&lt;/p>
&lt;h2 id="逆矩阵">逆矩阵&lt;/h2>
&lt;blockquote>
&lt;p>逆矩阵（inverse matrix），又称乘法反方阵、反矩阵。在线性代数中，给定一个&lt;span class="math">\(n\)&lt;/span>阶方阵&lt;span class="math">\(\mathbf {A}\)&lt;/span>，若存在一&lt;span class="math">\(n\)&lt;/span>阶方阵&lt;span class="math">\(\mathbf{B}\)&lt;/span>，使得&lt;span class="math">\(\mathbf{AB}=\mathbf{BA}=\mathbf{I}_n\)&lt;/span>，其中&lt;span class="math">\(\mathbf{I}_n\)&lt;/span>为&lt;span class="math">\(n\)&lt;/span>阶单位矩阵，则称&lt;span class="math">\(\mathbf{A}\)&lt;/span>是可逆的，且&lt;span class="math">\(\mathbf{B}\)&lt;/span>是&lt;span class="math">\(\mathbf{A}\)&lt;/span>的&lt;strong>逆矩阵&lt;/strong>，记作&lt;span class="math">\(\mathbf {A} ^{-1}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>注意：我们在谈论一般的逆矩阵时只针对&lt;strong>方阵&lt;/strong>，非方阵不存在逆矩阵，但是可以有广义逆矩阵。&lt;/p>
&lt;blockquote>
&lt;p>可逆矩阵：如果&lt;span class="math">\(A\)&lt;/span>有逆矩阵，我们称为可逆矩阵，或非奇异矩阵。不可逆的矩阵也叫做奇异矩阵。&lt;/p>
&lt;/blockquote>
&lt;h3 id="补充了解广义逆">补充了解：广义逆&lt;/h3>
&lt;blockquote>
&lt;p>广义逆（Generalized inverse），是线性代数中针对矩阵的一种运算。一个矩阵&lt;span class="math">\(A\)&lt;/span>的广义逆叫做&lt;span class="math">\(A\)&lt;/span>的广义逆阵，是指具有部分逆矩阵的特性，但是不一定具有逆矩阵的所有特性的另一矩阵。假设一矩阵&lt;span class="math">\(A\in \mathbb {R} ^{n\times m}\)&lt;/span>及另一矩阵&lt;span class="math">\(A^{\mathrm {g} }\in \mathbb {R} ^{m\times n}\)&lt;/span>，若&lt;span class="math">\(A^{\mathrm {g} }\)&lt;/span>满足&lt;span class="math">\(AA^{\mathrm {g} }A=A\)&lt;/span>，则&lt;span class="math">\(A^{\mathrm {g} }\)&lt;/span>即为&lt;span class="math">\(A\)&lt;/span>的广义逆阵。&lt;/p>
&lt;/blockquote>
&lt;p>广义逆也称为伪逆（pseudoinverse），有些时候，伪逆特指&lt;strong>摩尔－彭若斯广义逆&lt;/strong>。此外，还有针对非方阵的单边逆矩阵，如左逆矩阵、右逆矩阵等。&lt;/p>
&lt;h2 id="逆矩阵存在条件">逆矩阵存在条件&lt;/h2>
&lt;p>矩阵&lt;span class="math">\(A_{n\times n}\)&lt;/span>可逆的核心是&lt;strong>不能降维&lt;/strong>，即&lt;span class="math">\(A_{n\times n}\)&lt;/span>必须&lt;span class="math">\(n\)&lt;/span>维空间的一个表示。其等效表述有：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\det(A)\neq 0\)&lt;/span>。行列式不为0，表明&lt;span class="math">\(n\)&lt;/span>维体积不等于0，若有一个维度消失，那么其高维体积必是0。&lt;/li>
&lt;li>&lt;span class="math">\(\mathrm{Rank}(A)=n\)&lt;/span>。这个好理解，矩阵的秩就是可描述空间的维度。&lt;/li>
&lt;li>存在初等变换矩阵&lt;span class="math">\(P\)&lt;/span>，使得&lt;span class="math">\(PA=I\)&lt;/span>。我们知道初等变换不会增加维度，而单位矩阵&lt;span class="math">\(I\)&lt;/span>的维度是&lt;span class="math">\(n\)&lt;/span>，这也意味着矩阵&lt;span class="math">\(A，I\)&lt;/span>秩相等。&lt;/li>
&lt;li>&lt;span class="math">\(A\)&lt;/span>的所有特征值都不为0。特征值等效于该矩阵&lt;span class="math">\(n\)&lt;/span>维特征基的尺度变换，如果其中某个特征值为0，必然使得该特征向量所在维度坍缩，从而降维。&lt;/li>
&lt;li>齐次线性方程组&lt;span class="math">\(Ax=0\)&lt;/span>仅有零解或非齐次线性方程组&lt;span class="math">\(Ax=b\)&lt;/span>有唯一解。零空间维0维，等效于列空间为&lt;span class="math">\(n\)&lt;/span>维。&lt;/li>
&lt;li>&lt;span class="math">\(A\)&lt;/span>的行（列）向量组线性无关。最大线性无关组中的向量个数即为空间维数。&lt;/li>
&lt;li>任一&lt;span class="math">\(n\)&lt;/span>维向量可由&lt;span class="math">\(A\)&lt;/span>的行（列）向量组线性表示。这表明&lt;span class="math">\(A\)&lt;/span>的最大线性无关组至少为&lt;span class="math">\(n\)&lt;/span>。&lt;/li>
&lt;li>&lt;span class="math">\(A\)&lt;/span>可表示成有限个初等矩阵的乘积。&lt;/li>
&lt;/ul>
&lt;p>我们在这一小结，开头说了可逆的核心是&lt;strong>不能降维&lt;/strong>，这又是什么理由呢？从变换的角度可逆矩阵是将&lt;span class="math">\(n\)&lt;/span>维空间中点从一个位置，&lt;strong>一一对应的变换到另一个位置&lt;/strong>。其中不能有多个点变换到同一个点上（通常是降维），也不能有一个点变换到多个点情况（这个一般不会）。当多个点变换到同一个点上时，我们无法从变换后的点（像）精确、唯一的找到到底原来时哪一个点（原）变过来的，也就意味着无法找到一个变换方法把像点变回去，这就是不可逆的变换，即为不可逆矩阵。&lt;/p>
&lt;h2 id="逆矩阵的求法">逆矩阵的求法&lt;/h2>
&lt;p>除了使用定义求矩阵的逆外，常见的逆矩阵的直接求法包含&lt;strong>初等变换法&lt;/strong>和&lt;strong>伴随矩阵法&lt;/strong>。此外，&lt;strong>矩阵分解&lt;/strong>对矩阵求逆也有很大的帮助。&lt;/p>
&lt;p>求逆矩阵一直是矩阵研究中的一个重点问题，也是计算量非常大的一个问题。因此，也有很多逆矩阵的技巧，例如分块矩阵求逆，利用性质求逆等等，我们在本文中不再详述。&lt;/p>
&lt;h3 id="初等变换法">初等变换法&lt;/h3>
&lt;p>求逆矩阵最基本的方法是初等变换法。如果要求方阵&lt;span class="math">\(A\)&lt;/span>的逆矩阵，用初等变换法是：&lt;/p>
&lt;blockquote>
&lt;p>将矩阵&lt;span class="math">\(A\)&lt;/span>与单位矩阵&lt;span class="math">\(I\)&lt;/span>排成一个新的矩阵&lt;span class="math">\((A \quad I)\)&lt;/span>，称为增广矩阵，将此增广矩阵&lt;span class="math">\((A\quad I)\)&lt;/span>做初等行变换，将它化成&lt;span class="math">\((I\quad B)\)&lt;/span>的形式，则 &lt;span class="math">\[B=A^{-1}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>举个例子。&lt;/p>
&lt;p>例1：求矩阵&lt;span class="math">\(A\)&lt;/span>的逆矩阵。 &lt;span class="math">\[A= \begin{bmatrix} 1&amp;amp;0&amp;amp;-2\\ -3&amp;amp;1&amp;amp;4\\ 2&amp;amp;-3&amp;amp;4\end{bmatrix} \]&lt;/span> 这是一个三阶的矩阵，最简便有效的方法是初等变换法。我们将矩阵与单位矩阵排在一起，然后做初等变换。 &lt;span class="math">\[\begin{aligned}(A\quad I)&amp;amp;=\begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;\vdots&amp;amp;1&amp;amp;0&amp;amp;0\\ -3&amp;amp;1&amp;amp;4 &amp;amp;\vdots&amp;amp; 0&amp;amp;1&amp;amp;0\\ 2&amp;amp;-3&amp;amp;4 &amp;amp;\vdots&amp;amp; 0&amp;amp;0&amp;amp;1\end{bmatrix}\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;\vdots&amp;amp;1&amp;amp;0&amp;amp;0\\ 0&amp;amp;1&amp;amp;-2 &amp;amp;\vdots&amp;amp; 3&amp;amp;1&amp;amp;0\\ 0&amp;amp;-3&amp;amp;8 &amp;amp;\vdots&amp;amp; -2&amp;amp;0&amp;amp;1\end{bmatrix}\\ &amp;amp;\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;\vdots&amp;amp;1&amp;amp;0&amp;amp;0\\ 0&amp;amp;1&amp;amp;-2 &amp;amp;\vdots&amp;amp; 3&amp;amp;1&amp;amp;0\\ 0&amp;amp;0&amp;amp;2 &amp;amp;\vdots&amp;amp; 7&amp;amp;3&amp;amp;1\end{bmatrix}\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;0&amp;amp;\vdots&amp;amp;8&amp;amp;3&amp;amp;1\\ 0&amp;amp;1&amp;amp;0 &amp;amp;\vdots&amp;amp; 10&amp;amp;4&amp;amp;1\\ 0&amp;amp;0&amp;amp;2 &amp;amp;\vdots&amp;amp; 7&amp;amp;3&amp;amp;1\end{bmatrix}\\&amp;amp;\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;0&amp;amp;\vdots&amp;amp;8&amp;amp;3&amp;amp;1\\ 0&amp;amp;1&amp;amp;0 &amp;amp;\vdots&amp;amp; 10&amp;amp;4&amp;amp;1\\ 0&amp;amp;0&amp;amp;1 &amp;amp;\vdots&amp;amp; \frac{7}{2}&amp;amp;\frac{3}{2}&amp;amp;\frac{1}{2}\end{bmatrix} \end{aligned}\]&lt;/span>&lt;/p>
&lt;p>所以我们得到&lt;/p>
&lt;p>&lt;span class="math">\[A^{-1}= \begin{bmatrix} 8&amp;amp;3&amp;amp;1\\ 10&amp;amp;4&amp;amp;1\\\frac{7}{2}&amp;amp;\frac{3}{2}&amp;amp;\frac{1}{2}\end{bmatrix} \]&lt;/span>&lt;/p>
&lt;p>我们再来看一个四阶矩阵的逆矩阵。&lt;/p>
&lt;p>例2：求矩阵&lt;span class="math">\(A\)&lt;/span>的逆矩阵。 &lt;span class="math">\[A=\begin{bmatrix}1&amp;amp;2&amp;amp;3&amp;amp;4\\ 2&amp;amp;3&amp;amp;1&amp;amp;2\\ 1&amp;amp;1&amp;amp;1&amp;amp;-1\\ 1&amp;amp;0&amp;amp;-2&amp;amp;-6\end{bmatrix}\]&lt;/span> 我们将下述矩阵做初等变换&lt;/p>
&lt;p>&lt;span class="math">\[ \begin{aligned} (A\quad I)&amp;amp;= \begin{bmatrix}1&amp;amp;2&amp;amp;3&amp;amp;4 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;0&amp;amp;0\\ 2&amp;amp;3&amp;amp;1&amp;amp;2 &amp;amp;\vdots &amp;amp;0&amp;amp;1&amp;amp;0&amp;amp;0\\ 1&amp;amp;1&amp;amp;1&amp;amp;-1 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;0\\ 1&amp;amp;0&amp;amp;-2&amp;amp;-6 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;1\end{bmatrix}\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;-6 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;1\\ 2&amp;amp;3&amp;amp;1&amp;amp;2 &amp;amp;\vdots &amp;amp;0&amp;amp;1&amp;amp;0&amp;amp;0\\ 1&amp;amp;1&amp;amp;1&amp;amp;-1 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;0\\ 1&amp;amp;2&amp;amp;3&amp;amp;4 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;0&amp;amp;0 \end{bmatrix} \\&amp;amp; \sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;-6 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;1\\ 0&amp;amp;3&amp;amp;5&amp;amp;14 &amp;amp;\vdots &amp;amp;0&amp;amp;1&amp;amp;0&amp;amp;-2\\ 0&amp;amp;1&amp;amp;3&amp;amp;5 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;-1\\ 0&amp;amp;2&amp;amp;5&amp;amp;10 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;0&amp;amp;-1 \end{bmatrix}\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;-6 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;1\\ 0&amp;amp;1&amp;amp;3&amp;amp;5 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;-1 \\ 0&amp;amp;3&amp;amp;5&amp;amp;14 &amp;amp;\vdots &amp;amp;0&amp;amp;1&amp;amp;0&amp;amp;-2 \\ 0&amp;amp;2&amp;amp;5&amp;amp;10 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;0&amp;amp;-1 \end{bmatrix}\\&amp;amp;\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;-6 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;1\\ 0&amp;amp;1&amp;amp;3&amp;amp;5 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;-1 \\ 0&amp;amp;0&amp;amp;-4&amp;amp;-1 &amp;amp;\vdots &amp;amp;0&amp;amp;1&amp;amp;-3&amp;amp;1 \\ 0&amp;amp;0&amp;amp;-1&amp;amp;0 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;-2&amp;amp;1 \end{bmatrix}\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;-6 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;1\\ 0&amp;amp;1&amp;amp;3&amp;amp;5 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;-1 \\ 0&amp;amp;0&amp;amp;-1&amp;amp;0 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;-2&amp;amp;1 \\ 0&amp;amp;0&amp;amp;-4&amp;amp;-1 &amp;amp;\vdots &amp;amp;0&amp;amp;1&amp;amp;-3&amp;amp;1 \end{bmatrix}\\&amp;amp;\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;-6 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;1\\ 0&amp;amp;1&amp;amp;3&amp;amp;5 &amp;amp;\vdots &amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;-1 \\ 0&amp;amp;0&amp;amp;-1&amp;amp;0 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;-2&amp;amp;1 \\ 0&amp;amp;0&amp;amp;0&amp;amp;-1 &amp;amp;\vdots &amp;amp;-4&amp;amp;1&amp;amp;5&amp;amp;-3 \end{bmatrix}\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;-2&amp;amp;0 &amp;amp;\vdots &amp;amp;24&amp;amp;-6&amp;amp;-30&amp;amp;19\\ 0&amp;amp;1&amp;amp;3&amp;amp;0 &amp;amp;\vdots &amp;amp;-20&amp;amp;5&amp;amp;26&amp;amp;-16 \\ 0&amp;amp;0&amp;amp;-1&amp;amp;0 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;-2&amp;amp;1 \\ 0&amp;amp;0&amp;amp;0&amp;amp;-1 &amp;amp;\vdots &amp;amp;-4&amp;amp;1&amp;amp;5&amp;amp;-3 \end{bmatrix} \\ &amp;amp;\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;0&amp;amp;0 &amp;amp;\vdots &amp;amp;22&amp;amp;-6&amp;amp;-26&amp;amp;17\\ 0&amp;amp;1&amp;amp;0&amp;amp;0 &amp;amp;\vdots &amp;amp;-17&amp;amp;5&amp;amp;20&amp;amp;-13 \\ 0&amp;amp;0&amp;amp;-1&amp;amp;0 &amp;amp;\vdots &amp;amp;1&amp;amp;0&amp;amp;-2&amp;amp;1 \\ 0&amp;amp;0&amp;amp;0&amp;amp;-1 &amp;amp;\vdots &amp;amp;-4&amp;amp;1&amp;amp;5&amp;amp;-3 \end{bmatrix}\sim \begin{bmatrix} 1&amp;amp;0&amp;amp;0&amp;amp;0 &amp;amp;\vdots &amp;amp;22&amp;amp;-6&amp;amp;-26&amp;amp;17\\ 0&amp;amp;1&amp;amp;0&amp;amp;0 &amp;amp;\vdots &amp;amp;-17&amp;amp;5&amp;amp;20&amp;amp;-13 \\ 0&amp;amp;0&amp;amp;1&amp;amp;0 &amp;amp;\vdots &amp;amp;-1&amp;amp;0&amp;amp;2&amp;amp;-1 \\ 0&amp;amp;0&amp;amp;0&amp;amp;1 &amp;amp;\vdots &amp;amp;4&amp;amp;-1&amp;amp;-5&amp;amp;3 \end{bmatrix} \end{aligned}\]&lt;/span>&lt;/p>
&lt;p>所以，我们得到 &lt;span class="math">\[A^{-1}= \begin{bmatrix} 22&amp;amp;-6&amp;amp;-26&amp;amp;17\\ -17&amp;amp;5&amp;amp;20&amp;amp;-13 \\ -1&amp;amp;0&amp;amp;2&amp;amp;-1 \\ 4&amp;amp;-1&amp;amp;-5&amp;amp;3 \end{bmatrix} \]&lt;/span>&lt;/p>
&lt;h3 id="伴随矩阵求逆矩阵">伴随矩阵求逆矩阵&lt;/h3>
&lt;p>用伴随矩阵求逆的公式很简单： &lt;span class="math">\[A^{-1}=\frac{1}{\det(A)}A^\ast=\frac{1}{\det(A)}C^T\]&lt;/span> 其中，&lt;span class="math">\(\det(A)\)&lt;/span>表示矩阵&lt;span class="math">\(A\)&lt;/span>的行列式，&lt;span class="math">\(A^\ast\)&lt;/span>表示伴随矩阵，&lt;span class="math">\(C\)&lt;/span>表示余子矩阵。显然&lt;span class="math">\(C^T=A^\ast\)&lt;/span>&lt;/p>
&lt;p>矩阵的行列式我们都知道，那么余子矩阵和伴随矩阵又是什么呢？我们接下来介绍相关内容。首先，从代数余子式开始。&lt;/p>
&lt;blockquote>
&lt;p>元素&lt;span class="math">\(a_{ij}\)&lt;/span>代数余子式：在矩阵&lt;span class="math">\(A_{n\times n}\)&lt;/span>所对应的&lt;span class="math">\(n\)&lt;/span>阶行列式中，划去元素&lt;span class="math">\(a_{ij}\)&lt;/span>所在的第&lt;span class="math">\(i\)&lt;/span>行与第&lt;span class="math">\(j\)&lt;/span>列的元素，剩下的元素不改变原来的顺序所构成的n-1阶&lt;strong>行列式&lt;/strong>称为&lt;strong>元素&lt;span class="math">\(a_{ij}\)&lt;/span>的余子式&lt;/strong>。数学表示上计作&lt;span class="math">\(M_{ij}\)&lt;/span>。&lt;span class="math">\(a_{ij}\)&lt;/span>的&lt;strong>代数余子式&lt;/strong>：&lt;span class="math">\(c_{ij}= (-1)^{i+j} M_{ij}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>余子式和代数余子式都是&lt;span class="math">\(n-1\)&lt;/span>维行列式，也是一个简简单单的数字。区别就在于有没有乘以&lt;span class="math">\((-1)^{i+j}\)&lt;/span>。&lt;/p>
&lt;p>例如对矩阵 &lt;span class="math">\[\begin{bmatrix}\,\,\,1&amp;amp;4&amp;amp;7\\\,\,\,3&amp;amp;0&amp;amp;5\\-1&amp;amp;9&amp;amp;\!11\\\end{bmatrix}\]&lt;/span> 要计算代数余子式&lt;span class="math">\(c_{23}\)&lt;/span>。首先计算余子式&lt;span class="math">\(M_{23}\)&lt;/span>，也就是原矩阵去掉第2行和第3列后的子矩阵的行列式： &lt;span class="math">\[\begin{vmatrix}\,\,1&amp;amp;4&amp;amp;\Box \,\\\,\Box &amp;amp;\Box &amp;amp;\Box \,\\-1&amp;amp;9&amp;amp;\Box \,\\\end{vmatrix},即\begin{vmatrix}\,\,\,1&amp;amp;4\,\\-1&amp;amp;9\,\\\end{vmatrix}=9-(-4)=13\]&lt;/span> 因此，&lt;span class="math">\(c_{23}\)&lt;/span>等于&lt;span class="math">\((-1)^{2+3}M_{23}=-13\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(A\)&lt;/span>的余子矩阵是指将&lt;span class="math">\(A\)&lt;/span>的&lt;span class="math">\((i, j)\)&lt;/span>项代数余子式&lt;span class="math">\(c_{ij}\)&lt;/span>摆在第i行第j列所得到的矩阵，记为&lt;span class="math">\(C\)&lt;/span>。 &lt;span class="math">\[C=\begin{bmatrix}\,\,c_{11}&amp;amp;\dotsb&amp;amp;c_{1n} \,\\\,\vdots &amp;amp;\vdots &amp;amp;\vdots \,\\c_{n1}&amp;amp;\dotsb&amp;amp;c_{nn} \,\\\end{bmatrix}\]&lt;/span> 其中，&lt;span class="math">\(c_{ij}\)&lt;/span>是元素&lt;span class="math">\(a_{ij}\)&lt;/span>的代数余子式&lt;/p>
&lt;p>余子矩阵&lt;span class="math">\(C\)&lt;/span>的&lt;strong>转置&lt;/strong>矩阵称为&lt;span class="math">\(A\)&lt;/span>的伴随矩阵&lt;span class="math">\(A^\ast\)&lt;/span>。 &lt;span class="math">\[
A^\ast=C^T
\]&lt;/span> 伴随矩阵与逆矩阵联系密切，并且当A可逆时可以用来计算它的逆矩阵。 &lt;span class="math">\[A^{-1}=\frac{1}{\det(A)}A^\ast=\frac{1}{\det(A)}C^T\]&lt;/span>&lt;/p>
&lt;p>例3：设&lt;span class="math">\(A=A=\begin{pmatrix}2&amp;amp;3&amp;amp;4\\ 2&amp;amp;1&amp;amp;1\\ -1&amp;amp;1&amp;amp;2\end{pmatrix}\)&lt;/span>，求&lt;span class="math">\(A^{-1}\)&lt;/span>。&lt;/p>
&lt;p>我们先求行列式： &lt;span class="math">\[
|A|=\begin{vmatrix}2&amp;amp;3&amp;amp;4\\ 2&amp;amp;1&amp;amp;1\\ -1&amp;amp;1&amp;amp;2\end{vmatrix}=\begin{vmatrix}0&amp;amp;5&amp;amp;8\\ 0&amp;amp;3&amp;amp;5\\ -1&amp;amp;1&amp;amp;2\end{vmatrix}=(-1)(-1)^{3+1}\begin{vmatrix}5&amp;amp;8\\ 3&amp;amp;5\end{vmatrix}=-1
\]&lt;/span> 所以矩阵可逆。现在我们来求伴随矩阵。 &lt;span class="math">\[
c_{11}=\begin{vmatrix}1&amp;amp;1\\1&amp;amp;2\end{vmatrix}=1,\quad c_{12}=(-1)^{1+2}\begin{vmatrix}2&amp;amp;1\\ -1&amp;amp;2\end{vmatrix}=-5,\quad c_{13}=\begin{vmatrix}2&amp;amp;1\\-1&amp;amp;1\end{vmatrix}=3\\
c_{21}=(-1)^{2+1}\begin{vmatrix}3&amp;amp;4\\1&amp;amp;2\end{vmatrix}=-2,\quad c_{22}=\begin{vmatrix}2&amp;amp;4\\ -1&amp;amp;2\end{vmatrix}=8,\quad c_{23}=(-1)^{2+3}=\begin{vmatrix}2&amp;amp;2\\-1&amp;amp;1\end{vmatrix}=-5\\
c_{31}=\begin{vmatrix}3&amp;amp;4\\1&amp;amp;1\end{vmatrix}=-1,\quad c_{32}=(-1)^{3+2}\begin{vmatrix}2&amp;amp;4\\ 2&amp;amp;1\end{vmatrix}=6,\quad c_{33}=\begin{vmatrix}2&amp;amp;3\\2&amp;amp;1\end{vmatrix}=-4\\
C=\begin{pmatrix}1&amp;amp;-5&amp;amp;3\\ -2&amp;amp;8&amp;amp;-5\\ -1&amp;amp;6&amp;amp;-4\end{pmatrix}
\]&lt;/span> 所以 &lt;span class="math">\[
A^\ast=C^T=\begin{pmatrix}1&amp;amp;-2&amp;amp;-1\\ -5&amp;amp;8&amp;amp;6\\ 3&amp;amp;-5&amp;amp;-4\end{pmatrix}\\
A^{-1}=\frac{1}{|A|}A^*=-1\begin{pmatrix}1&amp;amp;-2&amp;amp;-1\\ -5&amp;amp;8&amp;amp;6\\ 3&amp;amp;-5&amp;amp;-4\end{pmatrix}=\begin{pmatrix}-1&amp;amp;2&amp;amp;1\\ 5&amp;amp;-8&amp;amp;-6\\ -3&amp;amp;5&amp;amp;4\end{pmatrix}
\]&lt;/span>&lt;/p>
&lt;h3 id="矩阵分解法">矩阵分解法&lt;/h3>
&lt;p>矩阵分解是矩阵求逆非常有效的方式，可以根据不同的分解方式衍生出不同的求逆公式。我们下面列出几种矩阵分解的求逆公式。&lt;/p>
&lt;ul>
&lt;li>LU分解求逆：&lt;span class="math">\(A=LU, A^{-1}=U^{-1}L^{-1}\)&lt;/span>，由于三角矩阵逆容易求，由此简化了求逆计算。&lt;/li>
&lt;li>QR分解求逆：&lt;span class="math">\(A=QR, A^{-1}=R^{-1}Q^{T}\)&lt;/span>，其中两个矩阵分别为正交矩阵&lt;span class="math">\(Q\)&lt;/span>和上三角矩阵&lt;span class="math">\(R\)&lt;/span>。正交矩阵的逆等于转置。&lt;/li>
&lt;li>SVD分解求逆：&lt;span class="math">\(A=UDV^T, A^{-1}=VD^{-1}U^T\)&lt;/span>，其中&lt;span class="math">\(U,V\)&lt;/span>为正交矩阵、&lt;span class="math">\(D\)&lt;/span>为对角矩阵。正交矩阵的逆等于转置。对角矩阵逆为各个元素倒数。&lt;/li>
&lt;/ul>
&lt;p>当然，矩阵分解的具体步骤超出了本文的叙述范围，需要了解的读者可以自行查找矩阵分解的相关内容。&lt;/p>
&lt;h2 id="特殊矩阵的逆">特殊矩阵的逆&lt;/h2>
&lt;p>我们知道矩阵的初等变换有三种，&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>换行(列)变换：交换两行（列）。&lt;/li>
&lt;li>倍法变换：将行列式的某一行（列）的所有元素同乘以数k。&lt;/li>
&lt;li>消法变换：把行列式的某一行（列）的所有元素乘以一个数k并加到另一行（列）的对应元素上。&lt;/li>
&lt;/ol>
&lt;p>而这三种变换分别对应了三种矩阵，即对角矩阵、置换矩阵和消元矩阵。巧的是，这三种矩阵的逆矩阵都有十分简便的求法。以下分别说明。&lt;/p>
&lt;h3 id="对角矩阵的逆">对角矩阵的逆&lt;/h3>
&lt;p>对角矩阵的逆矩阵是最容易、直观的，即对角元素取倒数即可。 &lt;span class="math">\[
\Lambda=\begin{bmatrix}
a_{11}&amp;amp;0&amp;amp;\dotsb&amp;amp;0\\0&amp;amp;a_{22}&amp;amp;\dotsb&amp;amp;0\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\0&amp;amp;0&amp;amp;\dotsb&amp;amp;a_{nn}
\end{bmatrix}\Rightarrow
\Lambda^{-1}=\begin{bmatrix}
\frac{1}{a_{11}}&amp;amp;0&amp;amp;\dotsb&amp;amp;0\\0&amp;amp;\frac{1}{a_{22}}&amp;amp;\dotsb&amp;amp;0\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\0&amp;amp;0&amp;amp;\dotsb&amp;amp;\frac{1}{a_{nn}}
\end{bmatrix}
\]&lt;/span> 因为对角矩阵对应线性变换中的&lt;strong>缩放变换&lt;/strong>，右乘对应相应列的缩放，左乘对应相应行的缩放。其逆矩阵就相当于把缩放的倍数变回来，即缩放大小的倒数。&lt;/p>
&lt;h3 id="正交矩阵的逆">正交矩阵的逆&lt;/h3>
&lt;p>由于正交矩阵定义为&lt;span class="math">\(QQ^T=I\)&lt;/span>，所以显然有： &lt;span class="math">\[
Q^{-1}=Q^T
\]&lt;/span> 利用正交矩阵的逆可以快速计算置换矩阵（permutation matrix）的逆矩阵。&lt;/p>
&lt;blockquote>
&lt;p>置换矩阵：是一种系数只由0和1组成的方块矩阵。置换矩阵的每一行和每一列都恰好有一个1，其余元素都是0。&lt;/p>
&lt;/blockquote>
&lt;p>在线性代数中，每个n阶的置换矩阵都代表了一个对n个元素（n维空间的基）的置换。当一个矩阵乘上一个置换矩阵时，所得到的是原来矩阵的横行（置换矩阵在左）或纵列（置换矩阵在右）经过置换后得到的矩阵。一个置换矩阵&lt;span class="math">\(P_π\)&lt;/span>必然是&lt;strong>正交矩阵&lt;/strong>（即满足&lt;span class="math">\(P_{{\pi }}P_{{\pi }}^{{T}}=I\)&lt;/span>），并且它的逆也是置换矩阵： &lt;span class="math">\[P_{{\pi }}^{{-1}}=P_{{\pi ^{{-1}}}}=P_{{\pi }}^{{T}}\]&lt;/span>&lt;/p>
&lt;h3 id="分块矩阵的逆">分块矩阵的逆&lt;/h3>
&lt;p>分块矩阵逆的两种常用情形如下，分别是上三角分块矩阵和下三角分块矩阵。其他分块模式需要考虑各分块是否可逆，具体可参见维基百科：&lt;a href="https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion">分块矩阵求逆&lt;/a>。 &lt;span class="math">\[
U=\begin{bmatrix}A&amp;amp;C\\0&amp;amp;B\end{bmatrix}\Rightarrow U^{-1}=\begin{bmatrix}A^{-1}&amp;amp;-A^{-1}CB^{-1}\\0&amp;amp;B^{-1}\end{bmatrix}\\
L=\begin{bmatrix}A&amp;amp;0\\C&amp;amp;B\end{bmatrix}\Rightarrow L^{-1}=\begin{bmatrix}A^{-1}&amp;amp;0\\-B^{-1}CA^{-1}&amp;amp;B^{-1}\end{bmatrix}\\
\]&lt;/span> 在初等变换中的消法变换（只考虑一次消去一个行或列）在消去特定行（列）其他元素时，如果按照从上到下的顺序消元，那么每一个消元矩阵都是一个下三角矩阵。虽然消元矩阵不完全符合上面给出的两个分块矩阵求逆公式，但是可以根据维基百科中的其他公式求出：消元矩阵&lt;span class="math">\(C\)&lt;/span>的逆为&lt;strong>对角线元素不变，其他元素取反&lt;/strong>。&lt;/p>
&lt;p>从线性方程组消元的角度可以很快理解这个逆的，消去的元素需要取反再加回来，即能恢复原样。&lt;/p>
&lt;h2 id="逆矩阵的性质">逆矩阵的性质&lt;/h2>
&lt;ul>
&lt;li>若矩阵&lt;span class="math">\(A\)&lt;/span>存在逆矩阵&lt;span class="math">\(A^{-1}\)&lt;/span>，则&lt;span class="math">\(A^{-1}\)&lt;/span>唯一。反证法可得。&lt;/li>
&lt;li>&lt;span class="math">\((A^{-1})^{-1}=A\)&lt;/span>，显然。&lt;/li>
&lt;li>&lt;span class="math">\((kA)^{-1}=k^{-1}A^{-1}\)&lt;/span>，其中&lt;span class="math">\(k\)&lt;/span>是常数。显然。&lt;/li>
&lt;li>&lt;span class="math">\(\det(A^{-1})=(\det(A))^{-1}\)&lt;/span>。行列式可以看做是有向面积或体积的概念在一般的欧几里得空间中的推广。或者说，在欧几里得空间中，行列式描述的是一个线性变换对“体积”所造成的影响。因此，&lt;span class="math">\(A^{-1}\)&lt;/span>相当于是将体积变换回去，导致其行列式为原来的倒数。&lt;/li>
&lt;li>&lt;span class="math">\((A_1A_2\dotsb A_k)^{-1}=A_k^{-1}\dotsb A_2^{-1}A_1^{-1}\)&lt;/span>，矩阵乘法结合律+逆矩阵唯一性可证。特别地，若矩阵&lt;span class="math">\(A_1=A_2=\dotsb=A_n\)&lt;/span>，有&lt;span class="math">\((A^n)^{-1}=(A^{-1})^n\)&lt;/span>&lt;/li>
&lt;li>矩阵转置的逆矩阵。&lt;span class="math">\((A^T)^{-1}=(A^{-1})^T\)&lt;/span>。注意，转置和求逆可交换不像是指数的相互交换。其可交换性是一种美妙的巧合。&lt;/li>
&lt;/ul>
&lt;p>证明：矩阵求逆、转置的可交换性。 &lt;span class="math">\[
AA^{-1}=I\\
(AA^{-1})^T=I^T=I\\
\Rightarrow (A^{-1})^TA^T=I\\
\Rightarrow (A^T)^{-1}=(A^{-1})^T
\]&lt;/span>&lt;/p></description></item><item><title>概率统计随机过程之C-R不等式</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8Bc-r%E4%B8%8D%E7%AD%89%E5%BC%8F/</link><pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8Bc-r%E4%B8%8D%E7%AD%89%E5%BC%8F/</guid><description>
&lt;h2 id="概率统计随机过程之c-r不等式">概率统计随机过程之C-R不等式&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#前置条件">前置条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单参数c-r正则分布族">单参数C-R正则分布族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#费舍尔信息量">费舍尔信息量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#c-r不等式">C-R不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单参数c-r不等式">单参数C-R不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单参数c-r不等式等号成立条件">单参数C-R不等式等号成立条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多参数c-r不等式">多参数C-R不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#c-r不等式应用">C-R不等式应用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#求umvue">求UMVUE&lt;/a>&lt;/li>
&lt;li>&lt;a href="#估计的效率和有效性">估计的效率和有效性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#在渐进正态性中的应用">在渐进正态性中的应用&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Cramer-Rao不等式是另一个判别无偏估计是否为UMVUE的方法，但是Cramer-Rao不等式有更深层的含义。&lt;/p>
&lt;p>我们知道估计量始终会是一个随机变量，有自己的概率分布，而不是一个准确的值。Cramer-Rao除了给出了Cramer-Rao正则分布族这种费舍尔信息的存在条件，还有另一个更重要的贡献：&lt;strong>C-R不等式&lt;/strong>，可以说给了统计学理论上的绝望。&lt;/p>
&lt;p>C-R不等式，其实就是在说：统计，对真实的概率分布参数估计能力是有限的。举个不太恰当的类比，有点像量子理论中的测不准原理 （二者证明有相似之处哦）。C-R不等式告诉我们，无论我们如何抽样充足，无论我们统计方法如何科学，我们对参数的估计值，永远不可能无限逼近是逻辑上的真实值！&lt;/p>
&lt;p>回到C-R不等式和UMVUE的关系上来，其思想如下：设&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一切无偏估计构成的集合，所有的这些&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>中的无偏估计的方差必有一个下界（一定非负），这个下界称为C-R下界。如果&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>中某一个估计量&lt;span class="math">\(\hat g\)&lt;/span>的方差达到了这个下界，则&lt;span class="math">\(\hat{g}\)&lt;/span>就一定是参数的UMVUE，当然会对样本分布族和&lt;span class="math">\(\hat{g}\)&lt;/span>有一些正则条件。当时，使用这种下界的方法，都一个缺点，即&lt;strong>C-R不等式给出的下界经常比实际的下界更小一些&lt;/strong>。这一情况下，C-R不等式就无法判断UMVUE的存在性。此外，C-R不等式还有其他一些用处，比如计算估计的效率、有效估计等等。&lt;/p>
&lt;h2 id="前置条件">前置条件&lt;/h2>
&lt;p>C-R不等式成立需要样本分布族满足一些正则条件，适合这些条件的分布族称为&lt;strong>C-R正则分布族&lt;/strong>。&lt;/p>
&lt;h3 id="单参数c-r正则分布族">单参数C-R正则分布族&lt;/h3>
&lt;blockquote>
&lt;p>定义1：单参数Cramer-Rao正则分布族：若单参数概率分布族&lt;span class="math">\(p(x;\theta)\)&lt;/span>，&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>属于Cramer-Rao正则分布族，则需要满足以下五个条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>参数空间&lt;span class="math">\(\varTheta\)&lt;/span>是直线上的开区间；&lt;/li>
&lt;li>&lt;span class="math">\(\frac{\partial p(x;\theta)}{\partial\theta}\)&lt;/span>对所有&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>都存在；&lt;/li>
&lt;li>分布的支撑&lt;span class="math">\(\{x:p(x;\theta)&amp;gt;0\}\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关，即分布族具有共同的支撑；&lt;/li>
&lt;li>&lt;span class="math">\(p(x;\theta)\)&lt;/span>的微分与积分运算可交换；&lt;/li>
&lt;li>对所有&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，期望 &lt;span class="math">\[0&amp;lt;I(\theta)=E[(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2]&amp;lt;+\infty\tag{1}\]&lt;/span> 其中，&lt;span class="math">\(I(\theta)\)&lt;/span>为分布&lt;span class="math">\(p(x;\theta)\)&lt;/span>中含有&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>费舍尔信息量&lt;/strong>，简称信息量。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h3 id="费舍尔信息量">费舍尔信息量&lt;/h3>
&lt;p>上面，我们用式（1）定义了费舍尔信息量。其具体解释可以理解为样本中关于&lt;span class="math">\(\theta\)&lt;/span>有多少信息。&lt;span class="math">\(I(\theta)\)&lt;/span>越大，意味着样本中含有位置参数&lt;span class="math">\(\theta\)&lt;/span>的信息越多，该参数越容易估计。&lt;span class="math">\(I(\theta)\)&lt;/span>也可解释成&lt;strong>单个样品&lt;/strong>提供的信息量，由于简单抽样中，各个样品是i.i.d的，故每个样品提供的信息量&lt;span class="math">\(I(\theta)\)&lt;/span>也是一样多的，即整个样本&lt;span class="math">\((X_1,\dotsb,X_n)\)&lt;/span>所含信息量为&lt;span class="math">\(nI(\theta)\)&lt;/span>。&lt;/p>
&lt;h2 id="c-r不等式">C-R不等式&lt;/h2>
&lt;h3 id="单参数c-r不等式">单参数C-R不等式&lt;/h3>
&lt;blockquote>
&lt;p>定理1：设&lt;span class="math">\(\mathcal{F}=\{f(x;\theta),\theta\in\varTheta\}\)&lt;/span>是C-R正则分布族，&lt;span class="math">\(g(\theta)\)&lt;/span>是定义在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上的可微函数，设&lt;span class="math">\(X=(X_1,X_2,\dotsb,X_n)\)&lt;/span>是由总体&lt;span class="math">\(f(x;\theta)\in\mathcal{F}\)&lt;/span>中抽取的简单随机样本，&lt;span class="math">\(\hat g(X)\)&lt;/span>是&lt;span class="math">\(g(\theta)\)&lt;/span>的任一无偏估计，且满足下列条件： &lt;span class="math">\[\int\dotsb\int \hat{g}(\bm{x})f(\bm{x},\theta)\mathrm{d}\bm{x}\]&lt;/span> 可在积分号下对&lt;span class="math">\(\theta\)&lt;/span>求导数，此出&lt;span class="math">\(\mathrm{d}\bm{x}=\mathrm{d}x_1\dotsb\mathrm{d}x_n\)&lt;/span>，则有： &lt;span class="math">\[D[\hat{g}(X)]\geq \frac{(\hat g&amp;#39;(\theta))^2}{nI(\theta)},\forall \theta\in \varTheta\tag{2}\]&lt;/span> 其中,&lt;span class="math">\(I(\theta)\)&lt;/span>为Fisher信息量。&lt;/p>
&lt;/blockquote>
&lt;p>特别地，当&lt;span class="math">\(\hat g(\theta)=\theta\)&lt;/span>时，式(2)变成 &amp;gt;&lt;span class="math">\[D[\hat{g}(X)]\geq \frac{1}{nI(\theta)},\forall \theta\in \varTheta\tag{3}\]&lt;/span> 当&lt;span class="math">\(f(x;\theta)\)&lt;/span>为离散概率分布列时，式（2）变成 &amp;gt;&lt;span class="math">\[D[\hat{g}(X)]\geq \frac{(\hat g&amp;#39;(\theta))^2}{n\sum\limits_i\left\{[\frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]^2f(x_i;\theta)\right\}},\forall \theta\in \varTheta\tag{4}\]&lt;/span>&lt;/p>
&lt;p>证明：C-R不等式的证明本质上是柯西-施瓦茨不等式的应用。&lt;/p>
&lt;blockquote>
&lt;p>首先，在概率论中，柯西-施瓦茨不等式形式为： &lt;span class="math">\[\mathrm{Var}(X)\cdot\mathrm{Var}(Y)\geq[\mathrm{Cov}(X,Y)]^2\]&lt;/span> 我们再来看看C-R不等式，式（2）： &lt;span class="math">\[D[\hat{g}(X)]\geq \frac{(\hat g&amp;#39;(\theta))^2}{nI(\theta)},\forall \theta\in \varTheta\]&lt;/span> 和柯西施瓦茨不等式对比下，&lt;span class="math">\(D[\hat{g}(X)]\)&lt;/span>是方差。根据&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>笔记中的内容，可知&lt;span class="math">\(nI(\theta)\)&lt;/span>其实是&lt;strong>分数函数的方差&lt;/strong>。前面多了一个&lt;span class="math">\(n\)&lt;/span>是因为，此时有&lt;span class="math">\(n\)&lt;/span>个i.i.d简单抽样出来的随机变量，是多维随机变量场景。&lt;/p>
&lt;p>为了阐述清晰，在这里我们在写一遍关于分数函数&lt;span class="math">\(S(\bm{x})\)&lt;/span>的相关证明。由于随机样本中每一样品都是i.i.d的，所以有&lt;span class="math">\(f(\bm{x};\theta)=\prod\limits_{i=1}^n f(x_i;\theta)\)&lt;/span>，那么分数函数可记 &lt;span class="math">\[S(\bm{x};\theta)=\frac{\partial\log{f(\bm{x};\theta)}}{\partial\theta}=\sum_{i=1}^n \frac{\partial\log{f(x_i;\theta)}}{\partial\theta}\tag{5}\]&lt;/span> 使用和&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>中一样的方法，可知： &lt;span class="math">\[\begin{aligned}
E[S(\bm{x};\theta)]&amp;amp;=\sum_{i=1}^n E[\frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]=\sum_{i=1}^n \int\frac{1}{f(x_i;\theta)}\frac{\partial f(x_i;\theta)}{\partial\theta}\cdot f(x_i;\theta)\mathrm{d}x_i\\
&amp;amp;=\sum_{i=1}^n \int\frac{\partial f(x_i;\theta)}{\partial\theta}\mathrm{d}x_i=\sum_{i=1}^n \frac{\partial }{\partial\theta}\int f(x_i;\theta)\mathrm{d}x_i=\sum_{i=1}^n \frac{\partial 1}{\partial\theta}=0
\end{aligned}\]&lt;/span> 注意，C-R正则族的条件（2）保证了导数的存在，条件（4）保证了积分、微分顺序可交换。由于分数函数的期望为0，因此分数函数的方差为 &lt;span class="math">\[\begin{aligned}
D[S(\bm{x};\theta)]&amp;amp;=D[\sum_{i=1}^n \frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]=\sum_{i=1}^n D[\frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]\\
&amp;amp;=\sum_{i=1}^n \{E[(\frac{\partial\log{f(x_i;\theta)}}{\partial\theta})^2]-(\underbrace{E[\frac{\partial\log{f(x_i;\theta)}}{\partial\theta}]}_{=0})^2\}\\
(x_i都是i.i.d)&amp;amp;=n\cdot E[(\frac{\partial\log{f(x_i;\theta)}}{\partial\theta})^2]=nI(\theta)
\end{aligned}\]&lt;/span> 由上可知，&lt;span class="math">\(nI(\theta)\)&lt;/span>是分数函数的方差。那么式（2）可转换为要证： &lt;span class="math">\[D[\hat{g}(X)]\cdot D[S(X;\theta)]\geq (\hat g&amp;#39;(\theta))^2,\forall \theta\in \varTheta\]&lt;/span> 再将上式和柯西-施瓦茨不等式对比下，发现区别就是将&lt;span class="math">\(\hat g&amp;#39;(\theta)\)&lt;/span>换成&lt;span class="math">\(\hat g(X)\)&lt;/span>与&lt;span class="math">\(S(X;\theta)\)&lt;/span>的相关系数。注意，&lt;span class="math">\(\hat g&amp;#39;(\theta)\)&lt;/span>是关于&lt;span class="math">\(\theta\)&lt;/span>的函数，而&lt;span class="math">\(\hat g(X)\)&lt;/span>与&lt;span class="math">\(S(X;\theta)\)&lt;/span>的相关系数会将&lt;span class="math">\(X\)&lt;/span>消掉，只剩下&lt;span class="math">\(\theta\)&lt;/span>。下面我们就来验证这一点： &lt;span class="math">\[\begin{aligned}
\mathrm{Cov}(\hat{g}(X),S(X;\theta))&amp;amp;=E[\hat{g}(X)\cdot S(X;\theta)]-E[\hat{g}(X)]\underbrace{E[S(X;\theta)}_{=0}]\\
&amp;amp;=E[\hat{g}(X)\cdot S(X;\theta)]\\
&amp;amp;=\int\dotsb\int \hat{g}(\bm{x})(\frac{\partial\log{f(x_i;\theta)}}{\partial\theta})f(\bm{x};\theta)\mathrm{d}\bm{x}\\
&amp;amp;=\int\dotsb\int \hat{g}(\bm{x})\frac{\partial f(\bm{x};\theta)}{\partial \theta}\mathrm{d}\bm{x}\\
&amp;amp;=\frac{\partial}{\partial \theta}\underbrace{\int\dotsb\int \hat{g}(\bm{x})f(\bm{x};\theta)\mathrm{d}\bm{x}}_{E[\hat{g}(X)]}\\
&amp;amp;\because \hat g(X)是g(\theta)的任一无偏估计\\
&amp;amp;\therefore E[\hat g(X)]=g(\theta)\\
&amp;amp;=\frac{\partial{g(\theta)}}{\partial{\theta}}=g&amp;#39;(\theta)
\end{aligned}\]&lt;/span> 即，&lt;span class="math">\(\mathrm{Cov}(\hat{g}(X),S(X;\theta))=g&amp;#39;(\theta)\)&lt;/span>。这样C-R不等式就完全转变成了柯西-施瓦茨不等式的形式。C-R不等式得证。&lt;/p>
&lt;/blockquote>
&lt;p>C-R不等式表明，&lt;strong>在给定分布族、样本后，我们的估计能力是有限的&lt;/strong>，无论用什么估计方式，其方差最小也是C-R不等式给出的结果。如果希望方差无限小，唯一的途径就是样本数量无限大。&lt;/p>
&lt;h3 id="单参数c-r不等式等号成立条件">单参数C-R不等式等号成立条件&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>若样本分布族&lt;strong>非指数族&lt;/strong>，任何&lt;span class="math">\(g(\theta)\)&lt;/span>的任何无偏估计，其方差不能处处达到C-R不等式下界。这意味着，非指数族就没法用C-R不等式来求证UMVUE。&lt;/li>
&lt;li>即使样本的总体是指数族，&lt;span class="math">\(f(\bm{x};\theta)=C(\theta)\exp{Q(\theta)T(\bm x)}h(\bm x)\)&lt;/span>，也不是让和&lt;span class="math">\(g(\theta)\)&lt;/span>都能找到无偏估计&lt;span class="math">\(\hat{g}(X)\)&lt;/span>，使其方差处处达到C-R下界。唯有&lt;span class="math">\(g(\theta)=E[aT(X)+b]\)&lt;/span>时才有，即&lt;span class="math">\(\hat{g}(X)=aT(X)+b\)&lt;/span>（线性函数）的情形才有，此处&lt;span class="math">\(a\neq 0,b\)&lt;/span>与&lt;span class="math">\(X\)&lt;/span>无关，但可以是&lt;span class="math">\(\theta\)&lt;/span>的函数。&lt;/li>
&lt;/ol>
&lt;p>从上面两个条件，我们不能发现：&lt;strong>用C-R不等于求UMVUE是很受限的&lt;/strong>。&lt;/p>
&lt;h3 id="多参数c-r不等式">多参数C-R不等式&lt;/h3>
&lt;p>TODO用到时再说。&lt;/p>
&lt;h2 id="c-r不等式应用">C-R不等式应用&lt;/h2>
&lt;h3 id="求umvue">求UMVUE&lt;/h3>
&lt;p>当分布族满足正则分布族条件时，我们可以计算&lt;span class="math">\(\frac{g&amp;#39;(\theta)}{nI(\theta)}\)&lt;/span>。然后再计算估计量的方差&lt;span class="math">\(D[\hat g(X)]\)&lt;/span>。如果二者相等，且估计量是无偏估计，那么此估计量就是UMVUE。&lt;/p>
&lt;p>这个方法对于指数族都是很好用的，因为指数族都是C-R正则分布族，而且可以求出费舍尔信息。但是，其缺点也很明显。一是因为很多分布族不满足C-R正则条件；二是一些UMVUE的实际方差确实比C-R不等式给出的更大，因此即使一个估计量方差大于C-R下界，那它也可能是UMVUE。即C-R不等式是必要条件，不是充分条件。&lt;/p>
&lt;h3 id="估计的效率和有效性">估计的效率和有效性&lt;/h3>
&lt;p>无偏估计的效率定义很简单，就是C-R界与估计方差的比值：&lt;/p>
&lt;blockquote>
&lt;p>定义2:&lt;strong>无偏估计的效率&lt;/strong>。设&lt;span class="math">\(\hat{g}(X)\)&lt;/span>为&lt;span class="math">\(g(\theta)\)&lt;/span>的无偏估计，比值 &lt;span class="math">\[e_{\hat{g}}(\theta)=\frac{[g&amp;#39;(\theta)]^2/nI(\theta)}{D[\hat{g}(X)]}\]&lt;/span> 称为无偏估计&lt;span class="math">\(\hat{g}(X)\)&lt;/span>的效率。&lt;/p>
&lt;/blockquote>
&lt;p>显然，根据C-R不等式必有&lt;span class="math">\(0&amp;lt;e_{\hat{g}}(\theta)\leq 1\)&lt;/span>。&lt;/p>
&lt;ul>
&lt;li>当&lt;span class="math">\(e_{\hat{g}}(\theta)=1\)&lt;/span>，则称&lt;span class="math">\(\hat{g}(X)\)&lt;/span>是&lt;span class="math">\(g(\theta)\)&lt;/span>的&lt;strong>有效估计&lt;/strong>（UMVUE）（有效估计是UMVUE，但是UMVUE不一定是有效估计）；&lt;/li>
&lt;li>若&lt;span class="math">\(\hat{g}(X)\)&lt;/span>不是&lt;span class="math">\(g(\theta)\)&lt;/span>的有效估计，但是&lt;span class="math">\(\lim\limits_{n\rightarrow \infty}e_{\hat{g}}(\theta)=1\)&lt;/span>，则称&lt;span class="math">\(\hat{g}(X)\)&lt;/span>是&lt;span class="math">\(g(\theta)\)&lt;/span>的&lt;strong>渐进有效估计&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>虽然有效估计是无偏估计中最好的，但是从常用分布来看有效估计并不多，渐进有效估计不少。&lt;/p>
&lt;h3 id="在渐进正态性中的应用">在渐进正态性中的应用&lt;/h3>
&lt;p>在一定条件下，最大似然估计具有渐进正态性。我们将通过如下定理阐释。需要指出的是，定理是以连续分布的形式给出，但是对于离散场景也是适用的。&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(p(x;\theta)\)&lt;/span>是某密度函数，其参数空间&lt;span class="math">\(\varTheta=\{\theta\}\)&lt;/span>是直线上的非退化区间（即不是一个点），假如：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，&lt;span class="math">\(p(x;\theta)\)&lt;/span>对&lt;span class="math">\(\theta\)&lt;/span>如下偏导都存在：&lt;span class="math">\(\frac{\partial\ln p}{\partial\theta},\frac{\partial^2\ln p}{\partial\theta^2},\frac{\partial^3\ln p}{\partial\theta^3}\)&lt;/span>&lt;/li>
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，有&lt;span class="math">\(|\frac{\partial\ln p}{\partial\theta}|&amp;lt;F_1(x),|\frac{\partial^2\ln p}{\partial\theta^2}|&amp;lt;F_2(x),\frac{\partial^3\ln p}{\partial\theta^3}&amp;lt;H(x)\)&lt;/span>成立，其中&lt;span class="math">\(F_1(x)\)&lt;/span>与&lt;span class="math">\(F_2(x)\)&lt;/span>在实数轴上可积，而&lt;span class="math">\(H(x)\)&lt;/span>满足：&lt;span class="math">\(\int_{-\infty}^\infty H(x)p(x;\theta)&amp;lt;M\)&lt;/span>，这里&lt;span class="math">\(M\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关。&lt;/li>
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，有&lt;span class="math">\(0&amp;lt;I(\theta)=E[(\frac{\partial\ln p}{\partial \theta})^2]&amp;lt;+\infty\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>则在参数真值&lt;span class="math">\(\theta\)&lt;/span>为参数空间&lt;span class="math">\(\varTheta\)&lt;/span>内点的情况下，其似然方程有一个解存在，且此解&lt;span class="math">\(\hat\theta_n=\theta(x_1,x_2,\dotsb,x_n)\)&lt;/span>依概率收敛于&lt;span class="math">\(\theta\)&lt;/span>，且： &lt;span class="math">\[
\hat\theta_n\sim AN(\theta,[nI(\theta)]^{-1})
\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这个定理的意义在于给定了最大似然分布有渐进正态性的条件，其中渐进方差（体现大样本效率）完全由&lt;strong>样本数量&lt;span class="math">\(n\)&lt;/span>和分布的费舍尔信息量&lt;span class="math">\(I(\theta)\)&lt;/span>决定&lt;/strong>，且费舍尔信息量越大（分布中含有&lt;span class="math">\(\theta\)&lt;/span>）的信息越多，渐进方差在同等样本数量下越小，从而最大似然估计效果越好。&lt;/p></description></item><item><title>概率统计随机过程之条件期望与重期望公式</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9D%A1%E4%BB%B6%E6%9C%9F%E6%9C%9B%E4%B8%8E%E9%87%8D%E6%9C%9F%E6%9C%9B%E5%85%AC%E5%BC%8F/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9D%A1%E4%BB%B6%E6%9C%9F%E6%9C%9B%E4%B8%8E%E9%87%8D%E6%9C%9F%E6%9C%9B%E5%85%AC%E5%BC%8F/</guid><description>
&lt;h2 id="概率统计随机过程之条件期望与重期望公式">概率统计随机过程之条件期望与重期望公式&lt;!-- omit in toc -->&lt;/h2>
&lt;p>之前对条件期望的理解有一些偏差，现在重新看了下条件期望的内容与重期望公式。注意（X|Y）的条件期望实际上是关于Y的函数，而重期望公式则与分区加权求和有着本质联系，提供了求X期望的另一种方式。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#条件数学期望">条件数学期望&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重期望公式">重期望公式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#随机个随机变量和的数学期望">随机个随机变量和的数学期望&lt;/a>&lt;/li>
&lt;li>&lt;a href="#条件期望的其他推论">条件期望的其他推论&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="条件数学期望">条件数学期望&lt;/h2>
&lt;p>如果我们对条件分布求期望，则称为&lt;strong>条件数学期望&lt;/strong>。在离散分布列和连续密度函数的定义方式如下，以二维举例：&lt;/p>
&lt;blockquote>
&lt;p>&lt;span class="math">\(X\)&lt;/span>关于&lt;span class="math">\(Y=y\)&lt;/span>的条件期望： &lt;span class="math">\[E(X|Y=y)=\begin{cases}\sum\limits_i x_iP(X=x_i|Y=y),\qquad(X,Y)为二维离散随机变量\\
\int_{-\infty}^{\infty}xp(x|y)\mathrm{d}x,\qquad(X,Y)为二维连续随机变量\end{cases}\tag{1}\]&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(Y\)&lt;/span>关于&lt;span class="math">\(X=x\)&lt;/span>的条件期望： &lt;span class="math">\[E(Y|X=x)=\begin{cases}\sum\limits_i y_iP(Y=y_i|X=x),\qquad(X,Y)为二维离散随机变量\\
\int_{-\infty}^{\infty}yp(y|x)\mathrm{d}y,\qquad(X,Y)为二维连续随机变量\end{cases}\tag{2}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>注意，&lt;span class="math">\(E(X|Y=y)\)&lt;/span>是在&lt;span class="math">\(y\)&lt;/span>为特定值时，对&lt;span class="math">\(x\)&lt;/span>求和/积分，抹去了&lt;span class="math">\(x\)&lt;/span>的随机性，得到一个关于&lt;span class="math">\(y\)&lt;/span>的函数。同理，&lt;span class="math">\(E(Y|X=x)\)&lt;/span>抹去的是&lt;span class="math">\(y\)&lt;/span>的随机性，得到一个关于&lt;span class="math">\(x\)&lt;/span>的函数。&lt;/p>
&lt;p>条件期望&lt;span class="math">\(E(X|Y=y)\)&lt;/span>和无条件期望&lt;span class="math">\(E(X)\)&lt;/span>的一大区别是，&lt;span class="math">\(E(X)\)&lt;/span>是一个数，而条件期望&lt;span class="math">\(E(X|Y)\)&lt;/span>是一个函数&lt;span class="math">\(g(y)\)&lt;/span>。&lt;/p>
&lt;p>举个例子，如用&lt;span class="math">\(X\)&lt;/span>表示中国成年人的身高，则&lt;span class="math">\(E(X)=170\)&lt;/span>表示中国成年人的平均身高为170 cm，是一个具体的数字。若用&lt;span class="math">\(Y\)&lt;/span>表示中国成年人的足长，则&lt;span class="math">\(E(X|Y=y)\)&lt;/span>表示足长为&lt;span class="math">\(y\)&lt;/span>的中国成年人的平均身高，根据研究可知 &lt;span class="math">\[
E(X|Y=y)=6.876y
\]&lt;/span> 这显然是一个与&lt;span class="math">\(y\)&lt;/span>相关的函数，对&lt;span class="math">\(y\)&lt;/span>的不同取值，条件期望的取值也在变化。可以记： &lt;span class="math">\[
g(y)=E(X|Y=y)
\]&lt;/span> 进一步，还可以将条件期望看成是随机变量&lt;span class="math">\(Y\)&lt;/span>的函数，即&lt;span class="math">\(E(X|Y)=g(Y)\)&lt;/span>，而将&lt;span class="math">\(E(X|Y=y)\)&lt;/span>看成是&lt;span class="math">\(Y=y\)&lt;/span>时&lt;span class="math">\(E(X|Y)\)&lt;/span>的一个取值。从这个角度来看，&lt;strong>&lt;span class="math">\(E(X|Y)\)&lt;/span>也是一个随机变量&lt;/strong>。&lt;/p>
&lt;p>如果条件期望也是一个随机数，那么条件期望的期望是什么呢？下面就用重期望公式做进一步说明。&lt;/p>
&lt;h2 id="重期望公式">重期望公式&lt;/h2>
&lt;p>前面提到，&lt;span class="math">\(g(Y)=E(X|Y)\)&lt;/span>也是一个随机变量，如果我们对其求期望，以连续函数为例,注意随机变量是&lt;span class="math">\(Y\)&lt;/span>： &lt;span class="math">\[
E[g(Y)]=\int_{-\infty}^\infty E(X|Y=y) p_Y(y)\mathrm{d}y
\]&lt;/span> 我们将条件期望的定义（1）式代入可得： &lt;span class="math">\[
\begin{aligned}
E[g(Y)]&amp;amp;=\int_{-\infty}^\infty[\int_{-\infty}^\infty xp(x|Y=y)\mathrm{d}x]\;p_{_Y}(y)\mathrm{d}y\\
(全概率公式)&amp;amp;=\int_{-\infty}^\infty\int_{-\infty}^\infty xp(x,y)\mathrm{d}x\mathrm{d}y\\
（提出x）&amp;amp;=\int_{-\infty}^\infty x\{\int_{-\infty}^\infty p(x,y)\mathrm{d}y\}\mathrm{d}x\\
(求x的边际pdf)&amp;amp;=\int_{-\infty}^\infty xp_{_X}(x)\mathrm{d}x\\
&amp;amp;=E(X)
\end{aligned}\tag{3}
\]&lt;/span> 我们“惊讶”的发现，条件期望的期望竟然是&lt;span class="math">\(X\)&lt;/span>的无条件期望！由此，我们给出重期望公式：&lt;/p>
&lt;blockquote>
&lt;p>定理：（重期望公式）设&lt;span class="math">\((X,Y)\)&lt;/span>是二维随机变量，且&lt;span class="math">\(E(X)\)&lt;/span>存在，则&lt;/p>
&lt;p>&lt;span class="math">\[E(X)=E[E(X|Y)]\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>重期望公式是概率论中比较深刻的一个结论。我们也可以换个角度理解：我们找到一个与&lt;span class="math">\(X\)&lt;/span>相关的量&lt;span class="math">\(Y\)&lt;/span>，用&lt;span class="math">\(Y\)&lt;/span>的不同取值（要互斥）把&lt;span class="math">\(X\)&lt;/span>划分成若干小区域（场景），现在小区域上求&lt;span class="math">\(X\)&lt;/span>的期望或均值，然后再根据&lt;span class="math">\(Y\)&lt;/span>的出现概率对各个小区域的期望&lt;span class="math">\(E(X_{y_i})\)&lt;/span>求加权平均，即可求出整体&lt;span class="math">\(X\)&lt;/span>的期望。&lt;/p>
&lt;p>具体一些，重期望公式也可以写成如下形式： &lt;span class="math">\[
E(X)=\begin{cases}\sum\limits_i E(X|Y=y_i)P(Y=y_i),\qquad 离散场景\\
\int_{-\infty}^\infty E(X|Y=y)P_{_Y}(y)\mathrm{d}y,\qquad 连续场景\end{cases}
\]&lt;/span>&lt;/p>
&lt;h3 id="随机个随机变量和的数学期望">随机个随机变量和的数学期望&lt;/h3>
&lt;p>设&lt;span class="math">\(X_1,X_2,\dotsb\)&lt;/span>为一系列独立同分布的随机变量，随机变量&lt;span class="math">\(N\)&lt;/span>只取正整数值，且&lt;span class="math">\(N\)&lt;/span>与&lt;span class="math">\(\{X_n\}\)&lt;/span>独立，证明： &lt;span class="math">\[
E(\sum_{i=1}^N X_i)=E(X_1)E(N)
\]&lt;/span>&lt;/p>
&lt;p>证明：由重期望公式可知： &lt;span class="math">\[
\begin{aligned}
E(\sum_{i=1}^N X_i)&amp;amp;=E[E(\sum_{i=1}^N X_i | N)]\\
&amp;amp;=\sum_{i=1}^\infty E(\sum_{i=1}^N X_i | N=n)P(N=n)\\
（\{X_n\}与N独立）&amp;amp;=\sum_{i=1}^\infty E(\sum_{i=1}^n X_i)P(N=n)\\
（\{X_n\}i.i.d）&amp;amp;=\sum_{i=1}^\infty nE(X_1)P(N=n)\\
&amp;amp;=E(X_1)\sum_{i=1}^\infty nP(N=n)\\
&amp;amp;=E(X_1)E(N)
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;h2 id="条件期望的其他推论">条件期望的其他推论&lt;/h2>
&lt;ul>
&lt;li>&lt;span class="math">\(\mathrm{Var}(X)=E[\mathrm{Var}(X|Y)]+\mathrm{Var}[E(X|Y)]\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>证明： &lt;span class="math">\[
\left .
\begin{aligned}
&amp;amp;E[\mathrm{Var}(X|Y)]=E\{E(X^2|Y)-[E(X|Y)]^2\}=E(X^2)-E[E^2(X|Y)]\\
\\
&amp;amp;\mathrm{Var}[E(X|Y)]=E[E^2(X|Y)]-[\underbrace{E\cdot E(X|Y)}_{E(X)}]^2=E[E^2(X|Y)]-[E(X)]^2
\end{aligned}
\right\}\Rightarrow\\
E[\mathrm{Var}(X|Y)]+\mathrm{Var}[E(X|Y)]=E(X^2)-E[E^2(X|Y)]+E[E^2(X|Y)]-[E(X)]^2\\
=E(X^2)-E^2(X)=\mathrm{Var}(X)
\]&lt;/span>&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(E[f(Y)|Y]=f(Y)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>证明： 当随机变量&lt;span class="math">\(Y\)&lt;/span>取到固定值&lt;span class="math">\(y\)&lt;/span>时（&lt;span class="math">\(Y=y\)&lt;/span>），就不存在随机性了。所以对于&lt;span class="math">\(\forall Y=y\)&lt;/span>，有 &lt;span class="math">\[
E[f(Y)|Y=y]=E[f(Y=y)|Y=y]=E[f(y)]=f(y)
\]&lt;/span> 所以，有&lt;span class="math">\(E[f(Y)|Y]=f(Y)\)&lt;/span>。&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(E[g(X)\cdot Y|X]=g(X)E[Y|X]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(E(XY)=E[X\cdot E(Y|X)]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathrm{Cov}[X,E(Y|X)]=\mathrm{Cov}(X,Y)\)&lt;/span>&lt;/li>
&lt;/ul></description></item><item><title>概率统计随机过程之抽样的分布</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%8A%BD%E6%A0%B7%E7%9A%84%E5%88%86%E5%B8%83/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%8A%BD%E6%A0%B7%E7%9A%84%E5%88%86%E5%B8%83/</guid><description>
&lt;h2 id="概率统计随机过程之抽样的分布统计量的分布">概率统计随机过程之抽样的分布（统计量的分布）&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#样本均值的抽样分布正态分布">样本均值的抽样分布——正态分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本方差的抽样分布">样本方差的抽样分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#卡方分布">卡方分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本方差的抽样分布服从chi2n分布">样本方差的抽样分布服从&lt;span class="math">\(\\chi^2(n)\)&lt;/span>分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本均值与样本方差比值的分布">样本均值与样本方差比值的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#t分布">t分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#tips名字来源">TIPS名字来源&lt;/a>&lt;/li>
&lt;li>&lt;a href="#t分布介绍">t分布介绍&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#样本均值与样本方差比值服从t分布">样本均值与样本方差比值服从t分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个独立正态样本方差比的分布">两个独立正态样本方差比的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#f分布">F分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个独立正态样本方差比的服从f分布">两个独立正态样本方差比的服从F分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态总体下的抽样分布总结">正态总体下的抽样分布总结&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录">附录&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录1-gamma分布可加性证明">附录1-Gamma分布可加性证明&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="样本均值的抽样分布正态分布">样本均值的抽样分布——正态分布&lt;/h2>
&lt;p>样本均值的分布问题是被最早研究的问题，关于它的研究为中心极限定理的出现提供了巨大帮助。&lt;/p>
&lt;blockquote>
&lt;p>定理1：设&lt;span class="math">\(\{x_1,x_2,\dotsb,x_n\}\)&lt;/span>是来自某个总体的样本,&lt;span class="math">\(\bar{x}\)&lt;/span>为其样本均值：&lt;/p>
&lt;p>(1)若总体分布为&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>，则&lt;span class="math">\(\bar{x}\)&lt;/span>的精确（抽样）分布为&lt;span class="math">\(N(\mu,\sigma^2/n)\)&lt;/span>;&lt;/p>
&lt;p>(2)若总体的分布未知或不是正态分布，但&lt;span class="math">\(E(x)=\mu,Var(x)=\sigma^2\)&lt;/span>存在，则&lt;span class="math">\(n\)&lt;/span>较大时&lt;span class="math">\(\bar{x}\)&lt;/span>的渐进分布为&lt;span class="math">\(N(\mu,\sigma^2/n)\)&lt;/span>（通常&lt;span class="math">\(n&amp;gt;30\)&lt;/span>就接近于正态分布），常记为&lt;span class="math">\(\bar{x}\dot{\sim} N(\mu,\sigma^2/n)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>（1）样本中的每个样品都是独立同分布的随机变量且服从&lt;span class="math">\(x_i\sim N(\mu,\sigma^2)\)&lt;/span>，则根据正态分布的线性性质，n个i.i.d的正态随机变量和为&lt;span class="math">\(N(n\mu,n\sigma^2)\)&lt;/span>。同样根据正态分布的线性性质，和再除以&lt;span class="math">\(1/n\)&lt;/span>，有&lt;span class="math">\(\bar{x}\sim N(\mu,\sigma^2/n)\)&lt;/span>。&lt;/p>
&lt;p>（2）就是独立同分布的中心极限定理（林德伯格-莱维中心极限定理）的结果。证明的话是用随机变量分布列的特征函数收敛到正态分布的特征函数的思路。&lt;/p>
&lt;p>需要指出，此处的抽样分布一般都是放回抽样，对于无放回抽样，样本均值的标准误差需要添加一个修正系数： &lt;span class="math">\[
\sigma^2_{\bar{x}}=\sigma^2/n\times \frac{N-n}{N-1}&amp;lt;\sigma^2/n
\]&lt;/span> 显然不放回抽样的样本均值的标准误差更小。&lt;/p>
&lt;h2 id="样本方差的抽样分布">样本方差的抽样分布&lt;/h2>
&lt;h3 id="卡方分布">卡方分布&lt;/h3>
&lt;blockquote>
&lt;p>定义1：卡方分布&lt;span class="math">\(\chi^2(n)\)&lt;/span>的概率密度是 &lt;span class="math">\[f(x)=\begin{cases}
\frac{1}{2^{n/2}\Gamma(n/2)}x^{{n\over2} -1}e^{-x\over 2},&amp;amp;x&amp;gt;0\\
0,&amp;amp;\text{其他}
\end{cases}\]&lt;/span> 其中参数&lt;span class="math">\(n\)&lt;/span>称为自由度，&lt;span class="math">\(\Gamma(x)=\int_0^{\infty}t^{x-1}e^{-t} \mathrm{d}t\)&lt;/span>为伽马函数。&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../images/卡方分布.png" alt="卡方分布.png" />&lt;p class="caption">卡方分布.png&lt;/p>
&lt;/div>
&lt;p>一些没用的观察：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>自由度为2时，卡方分布就是一个的指数分布。&lt;/li>
&lt;li>&lt;span class="math">\(n&amp;gt;2\)&lt;/span>，单峰曲线，且在&lt;span class="math">\(x=n-2\)&lt;/span>时取最大值。&lt;/li>
&lt;li>卡方分布不对称，但是&lt;span class="math">\(n\)&lt;/span>越大越对称，且趋向于正态分布。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>定理2（服从卡方分布）：如果随机变量&lt;span class="math">\(x_1,x_2,\dotsb,x_n\)&lt;/span>独立，且服从&lt;span class="math">\(N(0,1)\)&lt;/span>分布，那么 &lt;span class="math">\[
\sum_{i=1}^n x_i^2 \sim \chi^2(n)
\]&lt;/span> 即标准正态分布的&lt;strong>平方和&lt;/strong>是卡方分布，自由度是&lt;span class="math">\(n\)&lt;/span>表示加数的个数。&lt;/p>
&lt;/blockquote>
&lt;p>证明：令&lt;span class="math">\(y=x_i^2 ≥ 0\)&lt;/span>，其分布函数为&lt;span class="math">\(F_{y}(y)\)&lt;/span>，所以当&lt;span class="math">\(y≤0\)&lt;/span>时有&lt;span class="math">\(F_{y}(y)=0\)&lt;/span>，当&lt;span class="math">\(y&amp;gt;0\)&lt;/span>时有 &lt;span class="math">\[\begin{aligned}
F_{y}(y)&amp;amp;=P(x_i^2≤y)=P(-\sqrt{y}≤x≤\sqrt{y})\\
&amp;amp;=\int_{-\sqrt{y}}^0 p_x(x) dx + \int_{0}^{\sqrt{y}} p_x(x) dx\\
&amp;amp;=F_x(\sqrt{y}) - F_x(-\sqrt{y})
\end{aligned}\]&lt;/span> 我们把上式对&lt;span class="math">\(y\)&lt;/span>求导，有： &lt;span class="math">\[\begin{aligned}
p_y(y)&amp;amp;=F_y&amp;#39;(y)=F&amp;#39;_x(\sqrt{y}) - F&amp;#39;_x(-\sqrt{y})\\
&amp;amp;=p_x(\sqrt{y})\cdot (\sqrt{y})&amp;#39;-p_x(-\sqrt{y})\cdot (-\sqrt{y})&amp;#39;\\
&amp;amp;=[p_x(\sqrt{y})+p_x(-\sqrt{y})]/(2\sqrt{y})\\
&amp;amp;=\frac{1}{\sqrt{2\pi}}y^{-1/2}e^{-y/2},y&amp;gt;0\\
&amp;amp;=\frac{(1/2)^{1/2}}{\Gamma(1/2)}y^{\frac{1}{2}-1}e^{-\frac{1}{2}y}=Ga(\frac{1}{2},\frac{1}{2})\end{aligned}\]&lt;/span> 即单个标准正态分布的平方服从&lt;span class="math">\(Ga(\frac{1}{2},\frac{1}{2})\)&lt;/span>。而Gamma分布是有可加性的（证明见&lt;a href="#附录1-gamma分布可加性证明">附录1-Gamma分布可加性证明&lt;/a>）。因此，n个标准正态分布的平方的和服从&lt;span class="math">\(Ga(\frac{n}{2},\frac{1}{2})\)&lt;/span>。将&lt;span class="math">\(Ga(\frac{n}{2},\frac{1}{2})\)&lt;/span>写出可发现就等于&lt;span class="math">\(\chi^2(n)\)&lt;/span>。得证。&lt;/p>
&lt;p>卡方分布性质:若&lt;span class="math">\(X\sim \chi^2(n)\)&lt;/span>，则有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(E(X)=n,D(X)=2n\)&lt;/span>。分部积分可得，或者直接用特征函数求。&lt;/li>
&lt;li>由中心极限定理可知，&lt;span class="math">\(X\sim \chi^2(n)\)&lt;/span>，&lt;span class="math">\(n\)&lt;/span>充分大，&lt;span class="math">\(\frac{X-n}{\sqrt{2n}}\overset{近似}{\sim} N(0,1)\)&lt;/span>。参见中心极限定理的林德伯格－列维形式。&lt;/li>
&lt;li>卡方分布可加性：&lt;span class="math">\(X\sim \chi^2(n),Y\sim \chi^2(m),X,Y\)&lt;/span>独立，则&lt;span class="math">\(X+Y\sim \chi^2(m+n)\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>推论：&lt;span class="math">\(X_i\sim \chi^2(m_i)\)&lt;/span>，各个&lt;span class="math">\(X_i\)&lt;/span>独立，则&lt;span class="math">\(\sum_{i=1}^n X_i\sim \chi^2(\sum_{i=1}^n m_i)\)&lt;/span>&lt;/li>
&lt;li>简单论证：&lt;span class="math">\(\chi^2\)&lt;/span>分布是特殊的Gamma分布（即&lt;span class="math">\(Ga(\frac{n}{2},\frac{1}{2})\)&lt;/span>），而Gamma分布有可加性，所以&lt;span class="math">\(\chi^2\)&lt;/span>分布也有可加性。也可以从&lt;span class="math">\(\chi^2\)&lt;/span>分布的构成来看，它是由多个随机变量&lt;span class="math">\(X_i\sim N(0,1)\)&lt;/span>加出来，因此卡方分布的相加无非就是多几个标准正态分布相加的事，因此不改变分布类型，只改变分布参数。&lt;/li>
&lt;/ul>
&lt;h3 id="样本方差的抽样分布服从chi2n分布">样本方差的抽样分布服从&lt;span class="math">\(\chi^2(n)\)&lt;/span>分布&lt;/h3>
&lt;blockquote>
&lt;p>引理1： N维随机变量线性变换的分布。设在两个n维随机变量&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)&amp;#39;\)&lt;/span>与&lt;span class="math">\(Y=(_1,y_2,\dotsb,y_n)&amp;#39;\)&lt;/span>间存在一个线性变换关系&lt;span class="math">\(Y=AX\)&lt;/span>，其中&lt;span class="math">\(A=(a_{ij})\)&lt;/span>为一个&lt;span class="math">\(n\times n\)&lt;/span>的n阶方阵，则它们的期望向量和方差（协方差）矩阵之间有如下关系： &lt;span class="math">\[E(Y)=AE(X)\\Var(Y)=AVar(X)A&amp;#39;\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>（1）首先矩阵变换是一种线性变换，求期望（无论是积分还是求和）也是线性变换，有&lt;span class="math">\(E(Y)=E(AX)\)&lt;/span>，&lt;span class="math">\(A\)&lt;/span>都是常数，所以可以把它当成线性变换的系数提出来，就有了&lt;span class="math">\(E(Y)=AE(X)\)&lt;/span>。&lt;/p>
&lt;p>（2）由于方差（协方差）运算中有变量之间的乘法，所以不是线性运算。我们老实根据定义求解，根据协方差矩阵的定义有： &lt;span class="math">\[\begin{aligned}
Var(Y)&amp;amp;=E[(Y-E(Y))(Y-E(Y))&amp;#39;]\\
&amp;amp;\overset{Y=AX}{=}E[AX-E(AX)(AX-E(AX))&amp;#39;]\\
&amp;amp;=E[AX-AE(X)(AX-AE(X))&amp;#39;]\\
&amp;amp;\overset{\text{结合律}}{=}E[A(X-E(X))(A(X-E(X)))&amp;#39;]\\
&amp;amp;\overset{(AB)&amp;#39;=B&amp;#39;A&amp;#39;}{=}E[A(X-E(X))(X-E(X))&amp;#39;A&amp;#39;]\\
&amp;amp;=AE[(X-E(X))(X-E(X))&amp;#39;]A&amp;#39;\\
&amp;amp;=AVar(X)A&amp;#39;
\end{aligned}\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理3：设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>的样本，其样本均值和样本方差分别为&lt;span class="math">\(\bar{x}\)&lt;/span>和&lt;span class="math">\(s^2\)&lt;/span>，则有&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\frac{(n-1)s^2}{\sigma^2}\sim \chi^2(n-1)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\bar{x},s^2\)&lt;/span>相互独立。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>首先对于n维随机变量&lt;span class="math">\(X\)&lt;/span>，期望&lt;span class="math">\(E(X)=\underbrace{(\mu,\mu,\dotsb,\mu)&amp;#39;}_{n个}\)&lt;/span>；n维随机变量的自协方差矩阵为&lt;span class="math">\(Var(X)=\sigma^2I\)&lt;/span>，因为各维度之间是独立的（&lt;span class="math">\(\Rightarrow\)&lt;/span>不相关），所以只有对角线上的元素&lt;span class="math">\(conv(x_i,x_i)=\sigma^2\)&lt;/span>，其他&lt;span class="math">\(conv(x_i,x_j)=0,i\neq j\)&lt;/span>。（conv表示协方差）。&lt;/p>
&lt;p>我们看 &lt;span class="math">\[(n-1)s^2=\sum_{i=1}^n (x-\bar{x})^2=\sum_{i=1}^n x_i^2 - 2\underbrace{\sum_{i=1}^n x_i}_{=n\bar{x}}\bar{x}+n\bar{x}^2=(\sum_{i=1}^n x_i^2)- n\bar{x}^2\]&lt;/span>&lt;/p>
&lt;p>我们在处理样本方差的时候，最需要注意的一点就是&lt;strong>样本样品和样本均值不独立&lt;/strong>，此外，&lt;strong>一般分布都是看可加性，而上式是两个随机变量相减&lt;/strong>。所以我们需要通过&lt;code>引理1&lt;/code>构造一组不相关的随机变量相加，再通过正态分布不相关=独立的性质进行计算。证明&lt;code>定理3&lt;/code>最精巧的一步在于构造的统计量为&lt;span class="math">\(Y=AX\)&lt;/span>，&lt;span class="math">\(A\)&lt;/span>是个正交矩阵(&lt;span class="math">\(AA&amp;#39;=I\)&lt;/span>)，因为根据&lt;code>引理1&lt;/code>的方差公式，正交矩阵不改变&lt;span class="math">\(Var(X)\)&lt;/span>，既 &lt;span class="math">\[Var(Y)=AVar(X)A&amp;#39;=A\sigma^2I A&amp;#39;=\sigma^2I\]&lt;/span> 由于&lt;span class="math">\(Y\)&lt;/span>各元素是独立的正态分布随机变量&lt;span class="math">\(\{x_i\}\)&lt;/span>的线性组合，即&lt;span class="math">\(Y\)&lt;/span>各维度也服从正态分布;而&lt;span class="math">\(Y\)&lt;/span>的协方差矩阵为&lt;span class="math">\(I\)&lt;/span>，说明各维度之间不相关，又因为正态分布的不相关和独立等价，所以&lt;span class="math">\(Y=(y_1,y_2,\dotsb,y_n)&amp;#39;\)&lt;/span>的各个分量相互独立，且其方差都是&lt;span class="math">\(\sigma^2\)&lt;/span>。&lt;/p>
&lt;p>如果我们想把&lt;span class="math">\((n-1)s^2\)&lt;/span>往&lt;span class="math">\(\chi^2(n-1)\)&lt;/span>分布上靠，首先就要把其变换成&lt;strong>独立的正态分布平方的和&lt;/strong>。同时正交矩阵&lt;span class="math">\(A\)&lt;/span>不改变原n维向量的模（的平方），即&lt;span class="math">\(\sum_{i=1}^n y_i^2=Y&amp;#39;Y=(AX)&amp;#39;AX=X&amp;#39;(A&amp;#39;A)X=X&amp;#39;X=\sum_{i=1}^n x_i^2\)&lt;/span>。这正是&lt;span class="math">\((n-1)s^2\)&lt;/span>的前半部分。下一步关键是如果改造&lt;span class="math">\(\bar{x}\)&lt;/span>，由于&lt;span class="math">\(\sum_{i=1}^n y_i^2=\sum_{i=1}^n x_i^2\)&lt;/span>都很像自由度为n的&lt;span class="math">\(\chi^2(n)\)&lt;/span>分布（只是“像”）。我们希望&lt;span class="math">\(n\bar{x}^2\)&lt;/span>变成某个&lt;span class="math">\(y_k^2\)&lt;/span>，这样就可以正好减掉一个自由度，变成&lt;span class="math">\(\chi^2(n-1)\)&lt;/span>，这就需要更精细的构造矩阵&lt;span class="math">\(A\)&lt;/span>。先来看看&lt;span class="math">\(n\bar{x}^2\)&lt;/span>的构成： &lt;span class="math">\[n\bar{x}^2=n\times (\frac{1}{n}\sum_{i=1}^n x)^2=(\sum_{i=1}^n\frac{1}{\sqrt{n}}x_i)^2\]&lt;/span> 如果我们令&lt;span class="math">\(A\)&lt;/span>的第&lt;span class="math">\(k\)&lt;/span>行为&lt;span class="math">\((\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}},\dotsb,\frac{1}{\sqrt{n}})\)&lt;/span>且&lt;span class="math">\(A_k(A_k)&amp;#39;=1\)&lt;/span>，那么&lt;span class="math">\(y_k=A_{k}X=\sqrt{n}\bar{x}\)&lt;/span>，即&lt;span class="math">\(n\bar{x}^2 = y_k^2\)&lt;/span>。这样，就使得 &lt;span class="math">\[(n-1)s^2=(\sum_{i=1}^n x_i^2)- n\bar{x}^2=(\sum_{i=1}^n y_i^2)-y_k^2,k\in\{1,2,\dotsb,n\}\]&lt;/span> 不失一般性，我们不妨让&lt;span class="math">\(k=1\)&lt;/span>，则 &lt;span class="math">\[(n-1)s^2=\sum_{i=2}^n y_i^2\]&lt;/span> 我们发现，&lt;span class="math">\((n-1)s^2\)&lt;/span>已经变成了&lt;span class="math">\(n-1\)&lt;/span>个正态分布随机变量的和，但是还不是标准正态随机变量。而根据要证的&lt;span class="math">\(\frac{(n-1)s^2}{\sigma^2}\)&lt;/span>和已知的&lt;span class="math">\(Y\)&lt;/span>各维度元素方差都是&lt;span class="math">\(\sigma^2\)&lt;/span>，因此有 &lt;span class="math">\[\frac{(n-1)s^2}{\sigma^2}=\sum_{i=2}^n (\frac{y_i}{\sigma})^2\]&lt;/span> 其中，&lt;span class="math">\(\frac{y_i}{\sigma}\sim N(\frac{\mu_i}{\sigma},1),i=2,3,\dotsb,n,\mu_i\)&lt;/span>为&lt;span class="math">\(y_i\)&lt;/span>的均值。我们希望&lt;span class="math">\(\sum_{i=2}^n (\frac{y_i}{\sigma})^2\)&lt;/span>则需要&lt;span class="math">\(\frac{y_i}{\sigma}\sim N(0,1)\)&lt;/span>，现在还差&lt;span class="math">\(\mu_i=0\)&lt;/span>，还需要进一步构造&lt;span class="math">\(A\)&lt;/span>的第&lt;span class="math">\(2\sim n\)&lt;/span>行,由于&lt;span class="math">\(x_i\)&lt;/span>是i.i.d的，即&lt;span class="math">\(x_i\)&lt;/span>的均值都是&lt;span class="math">\(\mu\)&lt;/span>，而&lt;span class="math">\(\mu_k=E(y_k)=\mu\sum_{i=1}^n a_{ki}\)&lt;/span>。要让&lt;span class="math">\(\mu_k=0\)&lt;/span>，则需要让&lt;span class="math">\(\sum_{i=1}^n a_{ki}=0\)&lt;/span>，也就是说&lt;span class="math">\(A\)&lt;/span>的第&lt;span class="math">\(2\sim n\)&lt;/span>行的和都为0，又需要&lt;span class="math">\(A\)&lt;/span>是正交矩阵，我们因此构造如下矩阵： &lt;span class="math">\[
A=\begin{bmatrix}
\frac{1}{\sqrt{n}}&amp;amp;\frac{1}{\sqrt{n}}&amp;amp;\frac{1}{\sqrt{n}}&amp;amp;\dotsb&amp;amp;\frac{1}{\sqrt{n}}\\
\frac{1}{\sqrt{2\cdot 1}}&amp;amp;-\frac{1}{\sqrt{2\cdot 1}}&amp;amp;0&amp;amp;\dotsb&amp;amp;0\\
\frac{1}{\sqrt{3\cdot 2}}&amp;amp;\frac{1}{\sqrt{3\cdot 2}}&amp;amp;-\frac{2}{\sqrt{3\cdot 2}}&amp;amp;\dotsb&amp;amp;0\\
\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots&amp;amp;\\
\frac{1}{\sqrt{n\cdot (n-1))}}&amp;amp;\frac{1}{\sqrt{n\cdot (n-1))}}&amp;amp;\frac{1}{\sqrt{n\cdot (n-1))}}&amp;amp;\dotsb&amp;amp;-\frac{n-1}{\sqrt{n\cdot (n-1))}}\\
\end{bmatrix}
\]&lt;/span> 这时有&lt;span class="math">\(\frac{y_i}{\sigma}\sim N(0,1),i=2,3,\dotsb,n;y_1=\sqrt{n}\bar{x}\)&lt;/span>，且 &lt;span class="math">\[\frac{(n-1)s^2}{\sigma^2}=\sum_{i=2}^n (\frac{y_i}{\sigma})^2\sim \chi^2(n-1)\]&lt;/span> 样本方差的抽样分布服从&lt;span class="math">\(\chi^2(n-1)\)&lt;/span>分布得证。&lt;/p>
&lt;p>通过构造的矩阵&lt;span class="math">\(A\)&lt;/span>，我们还发现&lt;span class="math">\(\bar{x}\)&lt;/span>只和&lt;span class="math">\(y_1\)&lt;/span>有关，而&lt;span class="math">\(s^2\)&lt;/span>只和&lt;span class="math">\(y_2,\dotsb,y_n\)&lt;/span>有关，同时&lt;span class="math">\(y_i\)&lt;/span>之间是相互独立的，因此&lt;span class="math">\(\bar{x},s^2\)&lt;/span>也是相互独立的。&lt;/p>
&lt;h2 id="样本均值与样本方差比值的分布">样本均值与样本方差比值的分布&lt;/h2>
&lt;h3 id="t分布">t分布&lt;/h3>
&lt;h4 id="tips名字来源">TIPS名字来源&lt;/h4>
&lt;p>t，为Student简写，则是William Sealy Gosset（戈塞特）的笔名。他当年在爱尔兰都柏林的一家酒厂工作，设计了一种后来被称为t检验的方法来评价酒的质量。因为行业机密，酒厂不允许他的工作内容外泄，所以当他后来将其发表到至今仍十分著名的一本杂志《Biometrika》时，就署了student的笔名。所以现在很多人知道student，知道t，却不知道Gosset。&lt;/p>
&lt;h4 id="t分布介绍">t分布介绍&lt;/h4>
&lt;p>表达式：t分布的概率密度为 &lt;span class="math">\[
p(x)=\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{x^2}{n} \right)^{-\frac{n+1}{2}}
\]&lt;/span> 其中参数&lt;span class="math">\(n\)&lt;/span>称为自由度，&lt;span class="math">\(\Gamma(x)=\int_0^{\infty}t^{x-1}e^{-t} \mathrm{d}t\)&lt;/span>为伽马函数。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/TStudent分布.png" alt="TStudent分布.png" />&lt;p class="caption">TStudent分布.png&lt;/p>
&lt;/div>
&lt;p>一些观察：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>t分布是比正态分布“宽容”分布，像正态分布但是集中度没有正态分布强。在描述重尾分布时更好。&lt;/li>
&lt;li>当&lt;span class="math">\(n=1\)&lt;/span>时，t分布为柯西分布，它的期望、方差都不存在。&lt;/li>
&lt;li>t分布是关于y轴对称的，所以&lt;span class="math">\(X\sim t(n),E(X)=0(n&amp;gt;1)\)&lt;/span>。当&lt;span class="math">\(n&amp;gt;2\)&lt;/span>时，t分布的方差存在，为&lt;span class="math">\(n/(n-2)\)&lt;/span>。&lt;/li>
&lt;li>当&lt;span class="math">\(n\geq 30\)&lt;/span>时，t分布于正态分布差别很小。&lt;/li>
&lt;li>t分布被广泛应用于&lt;strong>小样本假设检验&lt;/strong>。虽然是很小的样本，但是，却强大到可以轻松的排除异常值的干扰，准确把握住数据的特征（集中趋势和离散趋势）&lt;/li>
&lt;/ol>
&lt;h3 id="样本均值与样本方差比值服从t分布">样本均值与样本方差比值服从t分布&lt;/h3>
&lt;blockquote>
&lt;p>定理4（相互独立的标准正态分布与卡方分布之比服从t分布）：&lt;span class="math">\(X\sim N(0,1),Y\sim \chi^2(n),X,Y独立\)&lt;/span>，则&lt;span class="math">\(\frac{X}{\sqrt{Y/n}}\sim t(n)\)&lt;/span>，其中&lt;span class="math">\(t(n)\)&lt;/span>是自由度为n的分布。&lt;/p>
&lt;/blockquote>
&lt;p>证明：思路先求&lt;span class="math">\(\sqrt{Y/n}\)&lt;/span>的分布，然后再通过&lt;strong>独立&lt;/strong>随机变量商的分布求&lt;span class="math">\(\frac{X}{\sqrt{Y/n}}\)&lt;/span>。&lt;/p>
&lt;p>令&lt;span class="math">\(z=g(y)=\sqrt{y/n}(y\geq 0)\)&lt;/span>，则其反函数为&lt;span class="math">\(y=h(z)=nz^2(z\geq 0)\)&lt;/span>。卡方分布在&lt;span class="math">\(y&amp;lt;0\)&lt;/span>时都等于0，因此有&lt;span class="math">\(F_{z}(z)=0,z&amp;lt;0\)&lt;/span>。当&lt;span class="math">\(y,z&amp;gt;0\)&lt;/span>时，根据随机变量的单调函数分布定理有： &lt;span class="math">\[
p_{_Z}(z)=p_{_Y}(h(z))h&amp;#39;(z)=p_{_Y}(nz^2)(2nz)\\
=\frac{1}{2^{\frac{n}{2}-1}\Gamma(\frac{n}{2})}n^{\frac{n}{2}}z^{n-1}e^{-\frac{nz^2}{2}}
\]&lt;/span> 由于&lt;span class="math">\(X,Y\)&lt;/span>相互独立，所以&lt;span class="math">\(X,Z\)&lt;/span>也是独立的。联合概率密度&lt;span class="math">\(p(x,z)\)&lt;/span>就是&lt;span class="math">\(X,Z\)&lt;/span>两个概率密度的乘积，因此我们可以通过随机变量商的密度函数公式（参见笔记：&lt;a href="概率统计随机过程之随机变量函数的分布.md">概率统计随机过程之随机变量函数的分布.md&lt;/a>）可得&lt;span class="math">\(T=X/Z\)&lt;/span>的密度函数为： &lt;span class="math">\[
\begin{aligned}
p_{_T}(t;n)&amp;amp;=\int_{-\infty}^\infty p_{_Z}(z)p_{_X}(zt) |z| \mathrm{d}z (z&amp;gt;0)\\
&amp;amp;=\int_{0}^\infty \frac{1}{2^{\frac{n}{2}-1}\Gamma(\frac{n}{2})}n^{\frac{n}{2}}z^{n-1}e^{-\frac{nz^2}{2}}\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{(zt)^2}{2}}z \mathrm{d}z\\
\overset{\text{提出非积分项}}{=}&amp;amp;\frac{n^{\frac{n}{2}}}{\sqrt{\pi}2^{\frac{n-1}{2}}\Gamma(\frac{n}{2})}\int_{0}^\infty z^{n}e^{-\frac{z^2}{2}(n+t^2)}\mathrm{d}z\\
\overset{u=\frac{z^2}{2}(n+t^2)}{=}&amp;amp;\frac{1}{\sqrt{n\pi}\Gamma(\frac{n}{2})(1+\frac{t^2}{n})^{\frac{n+1}{2}}}\int_{0}^\infty u^{\frac{n+1}{2}-1}e^{-u}\mathrm{d}u\\
&amp;amp;=\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{t^2}{n} \right)^{-\frac{n+1}{2}}
\end{aligned}
\]&lt;/span> 得证。&lt;/p>
&lt;blockquote>
&lt;p>定理5：样本均值与样本方差比值服从t分布。设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>的样本，其样本均值和样本方差分别为&lt;span class="math">\(\bar{x}\)&lt;/span>和&lt;span class="math">\(s^2\)&lt;/span>，则有 &lt;span class="math">\[t=\frac{\sqrt{n}(\bar{x}-\mu)}{s}\sim t(n-1)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：首先根据&lt;code>定理3&lt;/code>，&lt;span class="math">\(\bar{x},s^2\)&lt;/span>是独立的，所以&lt;span class="math">\(\bar{x},s\)&lt;/span>也是独立的。根据&lt;code>定理1&lt;/code>有&lt;span class="math">\(\bar{x}\sim N(\mu,\sigma^2/n)\)&lt;/span>，则&lt;span class="math">\(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\sim N(0,1)\)&lt;/span>，而根据&lt;code>定理3&lt;/code>有&lt;span class="math">\(\frac{(n-1)s^2}{\sigma^2}\sim \chi^2(n-1)\)&lt;/span>。仿照&lt;code>定理5&lt;/code>的结构，我们可以构造： &lt;span class="math">\[\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\bigg / \sqrt{\frac{(n-1)s^2}{\sigma^2}\big /(n-1)}\sim t(n-1)\]&lt;/span> 化简后即可得： &lt;span class="math">\[t=\frac{\sqrt{n}(\bar{x}-\mu)}{s}\sim t(n-1)\]&lt;/span>&lt;/p>
&lt;h2 id="两个独立正态样本方差比的分布">两个独立正态样本方差比的分布&lt;/h2>
&lt;h3 id="f分布">F分布&lt;/h3>
&lt;p>F分布是1924年英国统计学家Ronald.A.Fisher爵士提出，并以其姓氏的第一个字母命名的。&lt;/p>
&lt;p>F分布的PDF表达式： &lt;span class="math">\[
p(x,n_1,n_2)=\begin{cases}
\frac{(n1/n2)^{n_1 \over 2}}{B(n_1/2,n_2/2)}x^{{n_1\over 2}-1}(1+{n_1\over n_2}x)^{-{n_1+n_2 \over 2}},x&amp;gt;0\\
0,x\leq 0
\end{cases}
\]&lt;/span> 其中，&lt;span class="math">\(n_1,n_2\)&lt;/span>都是自由度，&lt;span class="math">\(B(n_1/2,n_2/2)\)&lt;/span>是BETA函数，&lt;span class="math">\(B(m,n)=\frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}\)&lt;/span>。&lt;/p>
&lt;p>自由度为&lt;span class="math">\(m, n\)&lt;/span>的F 分布的密度函数如下图： &lt;img src="../images/F分布.png" alt="F分布.png" />&lt;/p>
&lt;p>一些观察：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(F\sim F(n_1,n_2)\)&lt;/span>，则&lt;span class="math">\(\frac{1}{F}\sim F(n_2,n_1)\)&lt;/span>;&lt;/li>
&lt;li>当&lt;span class="math">\(n_2&amp;gt;2\)&lt;/span>时，F分布存在数学期望&lt;span class="math">\(n_2/(n_2-2)\)&lt;/span>;&lt;/li>
&lt;li>当&lt;span class="math">\(n_2&amp;gt;4\)&lt;/span>时，F分布存在方差&lt;span class="math">\(\frac{2n_2^2(n_1+n_2-2)}{n_1(n_2-2)^2(n_2-4)}\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(t\sim t(n)\)&lt;/span>，则&lt;span class="math">\(t^2\sim F(1,n)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;h3 id="两个独立正态样本方差比的服从f分布">两个独立正态样本方差比的服从F分布&lt;/h3>
&lt;blockquote>
&lt;p>定理6（两个独立的卡方分布之比服从F分布）：&lt;span class="math">\(X\sim \chi^2(n_1),Y\sim \chi^2(n_2),X,Y\)&lt;/span>独立，则&lt;span class="math">\(\frac{X/n_1}{Y/n_2}\sim F(n_1.n_2)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：首先通过两独立随机变量的商的分布得到&lt;span class="math">\(\frac{X}{Y}\)&lt;/span>的分布，然后再通过随机变量的单调函数的分布得到&lt;span class="math">\(\frac{n_2}{n_1}\frac{X}{Y}\)&lt;/span>的分布。证明不难，但是比较繁琐，我直接贴图片了。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/F分布形式证明.png" alt="F分布形式证明" />&lt;p class="caption">F分布形式证明&lt;/p>
&lt;/div>
&lt;blockquote>
&lt;p>定理7：两个独立正态样本方差比的服从F分布。设&lt;span class="math">\(X,Y\)&lt;/span>是分别来自正态分布&lt;span class="math">\(N(\mu_1,\sigma_1^2),N(\mu_2,\sigma_2^2)\)&lt;/span>的容量为&lt;span class="math">\(n_1,n_2\)&lt;/span>样本,，样本方差分别为&lt;span class="math">\(s_1^2,s_2^2\)&lt;/span>，则有 &lt;span class="math">\[\frac{X_1/(n_1-1)}{X_2/(n_2-1)}=\frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}\sim F(n_1-1,n_2-1)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：由&lt;code>定理3&lt;/code>可知样本方差&lt;span class="math">\(s_1^2,s_2^2\)&lt;/span>的线性变换&lt;span class="math">\(X_1=(n_1-1)s_1^2/\sigma_1^2,X_2=(n_2-1)s_2^2/\sigma_2^2\)&lt;/span>的分布分别服从&lt;span class="math">\(\chi^2(n_1-1),\chi^2(n_2-1)\)&lt;/span>，且相互独立。在将分子分母的自由度分别代入&lt;code>定理6&lt;/code>，可得&lt;span class="math">\(\frac{X_1/(n_1-1)}{X_2/(n_2-1)}\sim F(n_1-1,n_2-1)\)&lt;/span>，&lt;code>定理7&lt;/code>得证。&lt;/p>
&lt;h2 id="正态总体下的抽样分布总结">正态总体下的抽样分布总结&lt;/h2>
&lt;div class="figure">
&lt;img src="../images/正态总体下的抽样分布.jpg" alt="正态总体下的抽样分布.jpg" />&lt;p class="caption">正态总体下的抽样分布.jpg&lt;/p>
&lt;/div>
&lt;h2 id="附录">附录&lt;/h2>
&lt;h3 id="附录1-gamma分布可加性证明">附录1-Gamma分布可加性证明&lt;/h3>
&lt;p>可以利用Gamma分布的特征函数快速证明。记Gamma分布为&lt;span class="math">\(Ga(\alpha_i,\lambda)\)&lt;/span>，则其特征函数为 &lt;span class="math">\[
\varphi_i(t)=(1-\frac{it}{\lambda})^{-\alpha_i}
\]&lt;/span> 当&lt;span class="math">\(\lambda\)&lt;/span>相同时，有 &lt;span class="math">\[Ga(\alpha_i,\lambda)+Ga(\alpha_j,\lambda)=\varphi_i(t)*\varphi_j(t)\\
=(1-\frac{it}{\lambda})^{-(\alpha_i+\alpha_j)}=Ga(\alpha_i+\alpha_j,\lambda)\]&lt;/span> 得证。&lt;/p></description></item><item><title>概率统计随机过程之指数型分布族应用</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%8C%87%E6%95%B0%E5%9E%8B%E5%88%86%E5%B8%83%E6%97%8F%E5%BA%94%E7%94%A8/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%8C%87%E6%95%B0%E5%9E%8B%E5%88%86%E5%B8%83%E6%97%8F%E5%BA%94%E7%94%A8/</guid><description>
&lt;h2 id="概率统计随机过程之指数型分布族应用">概率统计随机过程之指数型分布族应用&lt;!-- omit in toc -->&lt;/h2>
&lt;p>在学习广义线性模型的时候，其各种模型都可以通过指数型分布族的形式来表示，而指数型分布族可以给出求原始分布均值和方差的统一形式，这在机器学习、数理统计中有重要作用。此外，本文还介绍了指数型分布族使用最大似然估计来估计参数的方法。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#指数型分布族指数族">指数型分布族（指数族）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族的转换例子">指数型分布族的转换例子&lt;/a>&lt;/li>
&lt;li>&lt;a href="#伯努利分布的指数族形式">伯努利分布的指数族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多类别分布的指数族形式">多类别分布的指数族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均值未知方差已知的高斯分布的指数族形式">均值未知方差已知的高斯分布的指数族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均值方差皆未知的高斯分布的指数族形式">均值方差皆未知的高斯分布的指数族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泊松分布的指数型分布族形式">泊松分布的指数型分布族形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数族的期望与方差的统一形式">指数族的期望与方差的统一形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数族的期望">指数族的期望&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数族的方差">指数族的方差&lt;/a>&lt;/li>
&lt;li>&lt;a href="#theta与eta的一一对应缘由">&lt;span class="math">\(\\theta\)&lt;/span>与&lt;span class="math">\(\\eta\)&lt;/span>的一一对应缘由&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族的最大似然估计">指数型分布族的最大似然估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#自然指数族">自然指数族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数分散族">指数分散族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分散参数讨论">分散参数讨论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#配分函数讨论">配分函数讨论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数分散族的方差">指数分散族的方差&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="指数型分布族指数族">指数型分布族（指数族）&lt;/h2>
&lt;p>指数型分布族是指数分布族的推广，囊括了正态分布族、二项分布族、伽马分布族、多项分布族常见分布等等。具体定义形式如下：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>指数型分布族&lt;/strong>：一个概率分布族&lt;span class="math">\(\mathfrak{p}=\{p_{\theta}(x);\theta∈\varTheta\}\)&lt;/span>可称为&lt;strong>指数型分布族&lt;/strong>，假如&lt;span class="math">\(\mathfrak{p}\)&lt;/span>中的分布（分布列或密度函数）都可表示为如下形式： &lt;span class="math">\[p_\theta(x)=h(x)c(\theta)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\}\tag{1}\]&lt;/span> 其中，k为自然数；&lt;span class="math">\(\theta\)&lt;/span>可以是数字，也可以是向量。分布的支撑&lt;span class="math">\(\{x:p(x)&amp;gt;0\}\)&lt;/span>与参数&lt;span class="math">\(\theta\)&lt;/span>无关；诸&lt;span class="math">\(c(\theta),c_1(\theta),\dotsb,c_k(\theta)\)&lt;/span>是定义在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上的函数；诸&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>是&lt;span class="math">\(x\)&lt;/span>的函数，称为充分统计向量，但&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>线性无关。&lt;span class="math">\(h(x)\)&lt;/span>也只是&lt;span class="math">\(x\)&lt;/span>的函数，且&lt;span class="math">\(h(x)&amp;gt;0\)&lt;/span>，通常是一个常数。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;span class="math">\(c(\theta)\)&lt;/span>是作为归一化参数存在的，称为叫做配分函数(partition function)。 &lt;span class="math">\[c(\theta)^{-1} = \int h(x) \exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\} dx\]&lt;/span> 此外，指数族还有另一种表述方式，就是将外面的&lt;span class="math">\(c(\theta)\)&lt;/span>放到指数符号中： &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)-A(\theta)\right\}\tag{2}\]&lt;/span> 由于通常&lt;span class="math">\(A(\theta)\)&lt;/span>含有&lt;span class="math">\(\log\)&lt;/span>符号，该部分也称为“Log Partition Function”，易知&lt;span class="math">\(A(\theta)=\ln c(\theta)\)&lt;/span>。 如果我们使用向量值函数来表达指数型分布族可写为: &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\mathbf{C}^T(\theta)\mathbf{T}(x)-A(\theta)\right\}\tag{3}\]&lt;/span>&lt;/p>
&lt;p>从上述定义可知，一个分布族是不是指数型分布族的&lt;strong>关键在于其概率分布能否改写为定义中方式&lt;/strong>。&lt;/p>
&lt;h3 id="指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/h3>
&lt;p>下面我们使用&lt;strong>向量值函数&lt;/strong>将式(3)进行进一步改造。&lt;/p>
&lt;blockquote>
&lt;p>向量值函数：有时也称为向量函数，是一个单变量或多变量的、&lt;strong>值域是多维向量或者无穷维向量的集合的函数&lt;/strong>。向量值函数的输入可以是一个标量或者一个向量，输出是向量，定义域的维度和值域的维度是不相关的。&lt;/p>
&lt;/blockquote>
&lt;p>对于&lt;span class="math">\(\theta\)&lt;/span>的一系列函数&lt;span class="math">\(c_1(\theta),c_2(\theta),\dotsb\)&lt;/span>和充分统计量向量&lt;span class="math">\(T_1(x),T_2(x),\dotsb\)&lt;/span>，我们写出列向量形式： &lt;span class="math">\[
\mathbf{C}(\theta)=\begin{bmatrix}c_1(\theta)\\c_2(\theta)\\\vdots\\c_k(\theta)\end{bmatrix}
\mathbf{T}(x)=\begin{bmatrix}T_1(x)\\T_2(x)\\\vdots\\T_k(x)\end{bmatrix}
\]&lt;/span> 那么式（3）可写成 &lt;span class="math">\[
p(x;\theta)=h(x)\exp\left\{\mathbf{C}^T(\theta)\mathbf{T}(x)-A(\theta)\right\}\tag{4}
\]&lt;/span> 其中，&lt;span class="math">\(\mathbf{C}(\theta),\mathbf{T}(x)\)&lt;/span>都是向量值函数，&lt;span class="math">\(h(x),A(\theta)\)&lt;/span>都是普通函数。通常文章会把&lt;span class="math">\(A(\theta)\)&lt;/span>写成&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的形式，这两种本质上是等价的，但是&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的参数形式更加统一，为主流用法。由于&lt;span class="math">\(\mathbf{C}(\theta)\)&lt;/span>的计算结果本质上就是一个向量，我们可令向量值函数&lt;span class="math">\(\mathbf{C(\theta)}=\eta\)&lt;/span>，那么式（4）可表示为： &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{T}(x)-A(\eta)\right\}\tag{5}
\]&lt;/span> 这就是其他资料中的常见形式。其中&lt;span class="math">\(\eta=\mathbf{C}(\theta)\)&lt;/span>，参数&lt;span class="math">\(η\)&lt;/span>通常叫做自然参数(natural parameter)或者标准参数(canonical parameter)。这里注明：&lt;span class="math">\(A(\theta)\)&lt;/span>与&lt;span class="math">\(A(\eta)\)&lt;/span>实际上是两个不同的函数，但是可以通过&lt;span class="math">\(\eta=\mathbf{C}(\theta),\theta=\mathbf{C}^{-1}(\eta)\)&lt;/span>进行互换，因此在后文对他们不做区分。此外，在&lt;a href="#指数族的期望与方差的统一形式">指数族的期望与方差的统一形式&lt;/a>一节中，我们还会证明为什么&lt;span class="math">\(\eta,\theta\)&lt;/span>是一一对应的，这里先写出这个引理。&lt;/p>
&lt;blockquote>
&lt;p>引理1：在指数族中函数&lt;span class="math">\(C(\cdot)\)&lt;/span>总是&lt;strong>单调连续的(存在逆函数)&lt;/strong>，所以自然参数&lt;span class="math">\(η\)&lt;/span>和原始参数&lt;span class="math">\(θ\)&lt;/span>是&lt;strong>存在一一映射关系的&lt;/strong>。 &lt;span class="math">\[
\eta=\mathbf{C}(\theta)\\
\theta=\mathbf{C}^{-1}(\eta)
\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>在指数型分布族中，使用标准参数&lt;span class="math">\(η\)&lt;/span>表示的公式形式称为&lt;strong>指数族分布的标准形式(canonical form)&lt;/strong>，在标准形式下，分布的参数是&lt;span class="math">\(η\)&lt;/span>。&lt;strong>实际上，从原始分布向指数型分布转换的过程就是将&lt;span class="math">\(\theta\)&lt;/span>转换为&lt;span class="math">\(\eta\)&lt;/span>的过程&lt;/strong>，在下节中我们会用例子说明。&lt;/p>
&lt;p>指数分布族的意义在于给了我们常见分布一个统一的形式，我们通过此形式得出的结论，可以推广到所有符合该形式的概率分布。指数族有以下特殊之处，可能有些内容暂时不会涉及。&lt;/p>
&lt;ul>
&lt;li>指数族分布是&lt;strong>唯一有共轭先验的分布族&lt;/strong>,这就简化了后验的计算&lt;/li>
&lt;li>在特定的规范化条件下(regularity conditions),指数族分布是&lt;strong>唯一有限规模充分统计量&lt;/strong>(finite-sized sufficient statistics)的分布族,这意味着可以将数据压缩称固定规模的浓缩概括而不损失信息&lt;/li>
&lt;li>指数族分布是&lt;strong>广义线性模型&lt;/strong>(generalized linear models)的核心&lt;/li>
&lt;li>指数族分布也是&lt;strong>变分推理&lt;/strong>(variational inference)的核心&lt;/li>
&lt;/ul>
&lt;h2 id="指数型分布族的转换例子">指数型分布族的转换例子&lt;/h2>
&lt;h3 id="伯努利分布的指数族形式">伯努利分布的指数族形式&lt;/h3>
&lt;p>伯努利分布又叫两点分布或者0-1分布，是最简单的概率分布形式之一。常见伯努利分布写成： &lt;span class="math">\[
p(x;p)=p^x(1-p)^{1-x},x\in\{0,1\}
\]&lt;/span> 转写为指数型分布族形式为： &lt;span class="math">\[
\begin{aligned}
p(x;p)&amp;amp;=\exp\{x\ln{p}+(1-x)\ln{(1-p)}\}\\
&amp;amp;=\exp\{x\ln(\frac{p}{1-p})+\ln{(1-p)}\}
\end{aligned}
\]&lt;/span> 对照指数族的形式，有&lt;span class="math">\(h(x)=1;c(p)=\ln(\frac{p}{1-p});T(x)=x;A(p)=-\ln (1-p)\)&lt;/span>。写成标准形式为： &lt;span class="math">\[
p(x;\eta)=\exp\{\eta x-\ln(1+e^\eta)\}
\]&lt;/span> 标准参数&lt;span class="math">\(\eta\)&lt;/span>和原始参数&lt;span class="math">\(p\)&lt;/span>的关系为： &lt;span class="math">\[
\eta=\ln(\frac{p}{1-p})\\
p=\frac{1}{1+e^{-\eta}}\tag{6}
\]&lt;/span> 其中&lt;span class="math">\(\ln(\frac{p}{1-p})\)&lt;/span>成为logit函数，其反函数&lt;span class="math">\(\frac{1}{1+e^{-\eta}}\)&lt;/span>称为sigmoid函数。如果我们得到了&lt;span class="math">\(\eta\)&lt;/span>就可以用sigmoid函数反推得到&lt;span class="math">\(p\)&lt;/span>。&lt;/p>
&lt;h3 id="多类别分布的指数族形式">多类别分布的指数族形式&lt;/h3>
&lt;p>伯努利分布中是只有两个取值的离散随机变量的概率分布，当随机变量的取值扩展到多个(大于2个并且有限集)的时候，就是称为多类别分布。比如掷一个均匀的骰子，就是6个等概率结果的多类别分布。假设多类别分布中共有&lt;span class="math">\(m\)&lt;/span>个类别，其概率分别为&lt;span class="math">\(\theta_1,\theta_2,\dotsb,\theta_m\)&lt;/span>，那么其概率质量函数为： &lt;span class="math">\[
p(x;\mathbf{\theta})=\prod_{i=1}^m \theta_i^{x_i}\\
\sum_{i=1}^m \theta_i=1\\
\sum_{i=1}^m x_i=1
\]&lt;/span> 在一次实验的&lt;span class="math">\(m\)&lt;/span>个&lt;span class="math">\(x_i\)&lt;/span>中，只有被取到的那个&lt;span class="math">\(x_i\)&lt;/span>为1，其余的&lt;span class="math">\(x_j\)&lt;/span>都是0。我们将其改写成指数型分布族的形式： &lt;span class="math">\[
p(x;\mathbf{\theta})=\exp\{\sum_{i=1}^m x_i\ln{\theta_i}\}
\]&lt;/span> 我们注意到，由于条件&lt;span class="math">\(\sum_{i=1}^m \theta_i=1,\sum_{i=1}^m x_i=1,x_i=\{0,1\}\)&lt;/span>的限制，实际上&lt;span class="math">\(m\)&lt;/span>个&lt;span class="math">\(\theta_i,x_i\)&lt;/span>都只有&lt;span class="math">\(m-1\)&lt;/span>个自由变量，最后的&lt;span class="math">\(\theta_m,x_m\)&lt;/span>可以用&lt;span class="math">\(\theta_m=1-\sum_{i=1}^{m-1}\theta_i,x_m=1-\sum_{i=1}^{m-1}x_i\)&lt;/span>表示。因此上面的等式可以改写为： &lt;span class="math">\[
\begin{aligned}
p(x;\mathbf{\theta})&amp;amp;=\exp\{\sum_{i=1}^{m-1} x_i\ln{\theta_i}+(1-\sum_{i=1}^{m-1}x_i)\ln{(1-\sum_{i=1}^{m-1}\theta_i)}\}\\
&amp;amp;=\exp\left\{\sum_{i=1}^{m-1} x_i\ln{\frac{\theta_i}{1-\sum_{j=1}^{m-1}\theta_j}}+\ln{(1-\sum_{i=1}^{m-1}\theta_i)}\right\}\\
其中，&amp;amp;\theta_m=1-\sum_{j=1}^{m-1}\theta_j\\
&amp;amp;=\exp\left\{\sum_{i=1}^{m-1} x_i\ln{\frac{\theta_i}{\theta_m}}+\ln{\theta_m}\right\}
\end{aligned}
\]&lt;/span> 对照向量化形式的指数型分布族形式，有&lt;span class="math">\(h(x)=1;\mathbf{C}(\theta)=\begin{bmatrix}\ln(\theta_1/\theta_m)\\\ln(\theta_2/\theta_m)\\\vdots\\\ln(\theta_{m-1}/\theta_m)\end{bmatrix},\mathbf{T}(x)=\begin{bmatrix}x_1\\x_2\\\vdots\\x_{m-1}\end{bmatrix};A(\theta)=-\ln(1-\sum_{i=1}^{m-1}\theta_i)=-\ln(\theta_m)\)&lt;/span>。&lt;/p>
&lt;p>将其写成标准形式为： &lt;span class="math">\[
p(x;\eta)=\exp\{\eta^T T(x)-\ln(\sum_{i=1}^me^{\eta_i})\}
\]&lt;/span> 其中，&lt;span class="math">\(\eta=\mathbf{C}(\theta),A(\eta)=\ln(\sum_{i=1}^me^{\eta_i})\)&lt;/span>&lt;/p>
&lt;p>在&lt;span class="math">\(\theta\)&lt;/span>与&lt;span class="math">\(\eta\)&lt;/span>的换算中需要一个技巧，我们在&lt;span class="math">\(\mathbf{C}(\theta)\)&lt;/span>最后添加一项&lt;span class="math">\(c_m(\theta)=\ln(\theta_m/\theta_m)\equiv 0\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>PS:若我们将添加&lt;span class="math">\(c_m(\theta)\)&lt;/span>的&lt;span class="math">\(\mathbf{C}(\theta)\)&lt;/span>记为&lt;span class="math">\(\mathbf{\hat{C}}(\theta)\)&lt;/span>，&lt;span class="math">\(\mathbf{T}(x)\)&lt;/span>最后也添加一项&lt;span class="math">\(x_m\)&lt;/span>，记为&lt;span class="math">\(\mathbf{\hat{T}}(x)\)&lt;/span>，由于&lt;span class="math">\(c_m(\theta)=0\)&lt;/span>，因此&lt;span class="math">\(\mathbf{\hat{C}}^T(\theta)\mathbf{\hat{T}}(x)=\sum_{i=1}^{m} x_i\ln{\theta_i}=\sum_{i=1}^{m-1} x_i\ln{\theta_i}=\mathbf{C}^T(\theta)\mathbf{T}(x)\)&lt;/span>。因此，在此定义下，可以用后者替代前者得到更加工整的表达形式。&lt;/p>
&lt;/blockquote>
&lt;p>那么有： &lt;span class="math">\[
\eta_i=c_i(\theta)=\ln(\frac{\theta_i}{\theta_m})\Rightarrow \theta_i=\theta_m e^{\eta_i}\\
\Rightarrow \sum_{i=1}^m \theta_i=\theta_m\sum_{i=1}^m e^{\eta_i}=1\\
\Rightarrow \theta_m=\frac{1}{\sum_{i=1}^m e^{\eta_i}}\\
\Rightarrow\theta_i=\theta_m e^{\eta_i}=\frac{e^{\eta_i}}{\sum_{i=1}^m e^{\eta_i}}\tag{7}
\]&lt;/span> 我们将上式称为softmax函数，普遍用于多分类问题。&lt;/p>
&lt;h3 id="均值未知方差已知的高斯分布的指数族形式">均值未知方差已知的高斯分布的指数族形式&lt;/h3>
&lt;p>典型的高斯分布写成（方差&lt;span class="math">\(\sigma^2\)&lt;/span>已知）： &lt;span class="math">\[
p(x;\mu)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}
\]&lt;/span> 转写为指数族形式为： &lt;span class="math">\[
p(x;\mu)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{1}{2\sigma^2}x^2\}\cdot\exp\{\frac{1}{2\sigma^2}(2\mu x-\mu^2)\}
\]&lt;/span> 对照指数族的形式，有&lt;span class="math">\(h(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{1}{2\sigma^2}x^2\};c(\mu)=\frac{\mu}{\sigma^2};T(x)=x;A(\mu)=\frac{\mu^2}{2\sigma^2}\)&lt;/span>。写成标准形式为： &lt;span class="math">\[
p(x;\eta)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{1}{2\sigma^2}x^2\}\cdot\exp\{\eta x-\frac{\eta^2\sigma^2}{2}\}
\]&lt;/span> 当服从标准正态分布时，&lt;span class="math">\(\sigma=1\Rightarrow\eta=\mu\)&lt;/span>。&lt;/p>
&lt;h3 id="均值方差皆未知的高斯分布的指数族形式">均值方差皆未知的高斯分布的指数族形式&lt;/h3>
&lt;p>如果高斯分布的均值和方差都是未知的，那么就需要使用多个线性不相关的充分统计量来表示指数型分布族： &lt;span class="math">\[
\begin{aligned}
p(x;\mu,\sigma^2)&amp;amp;=\frac{1}{(2\pi\sigma^2)^{1/2}} \exp[ -\frac{1}{2\sigma^2}(x-\mu)^2]\\
&amp;amp;=\frac{1}{(2\pi\sigma^2)^{1/2}} \exp[-\frac{1}{2\sigma^2} x^2 +\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2]\\
&amp;amp;=\frac{1}{(2\pi)^{1/2}} \exp[-\frac{1}{2\sigma^2} x^2 +\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2-\ln{\sigma}]
\end{aligned}
\]&lt;/span> 对照指数族的形式，有&lt;span class="math">\(h(x)=\frac{1}{(2\pi)^{1/2}}, \mathbf{C}=\begin{bmatrix}\mu/\sigma^2 \\-1/(2\sigma^2)\end{bmatrix},\mathbf{T}=\begin{bmatrix}x\\x^2\end{bmatrix},A(\mu,\sigma)=\frac{1}{2\sigma^2}\mu^2+\ln{\sigma}\)&lt;/span>。&lt;/p>
&lt;p>需要指出的是，一般情况下，我们在&lt;span class="math">\(A(\mu,\sigma)\)&lt;/span>中最好不要在使用原有的&lt;span class="math">\(\mu,\sigma\)&lt;/span>，而是使用&lt;span class="math">\(\mathbf{C}\)&lt;/span>中的分量&lt;span class="math">\(c_1=\mu/\sigma^2,c_2=-1/(2\sigma^2)\)&lt;/span>表示，即 &lt;span class="math">\[
A(\mathbf{C})=\frac{-c_1^2}{4c_2}-\frac{1}{2}\log(-2c_2)\tag{8.1}
\]&lt;/span> 所以，其指数型分布族表示形式为 &lt;span class="math">\[
p(x;\mathbf{C})=\frac{1}{(2\pi)^{1/2}} \exp[\mathbf{C}^T(\mu,\sigma)\mathbf{T}(x)-A(\mathbf{C})]
\]&lt;/span> 在其他文献中，也有令&lt;span class="math">\(h(x)=1\)&lt;/span>，然后把&lt;span class="math">\(\frac{1}{(2\pi)^{1/2}}\)&lt;/span>放到&lt;span class="math">\(A(\mathbf{C})\)&lt;/span>中的，即&lt;span class="math">\(A(\mathbf{C})=\frac{-c_1^2}{4c_2}-\frac{1}{2}\log(-2c_2)+\frac{1}{2}\log(2\pi)=\frac{-c_1^2}{4c_2}+\frac{1}{2}\log(-\frac{\pi}{c_2})\)&lt;/span>，这样是等价的。&lt;/p>
&lt;h3 id="泊松分布的指数型分布族形式">泊松分布的指数型分布族形式&lt;/h3>
&lt;p>泊松分布的概率质量函数如下： &lt;span class="math">\[
p(x;\theta)=\frac{\theta^x e^{-\theta}}{x!}
\]&lt;/span> 其中，&lt;span class="math">\(x\)&lt;/span>为正整数。将其改写为指数型分布族： &lt;span class="math">\[
p(x;\theta)=\frac{1}{x!}\exp\{x\ln(\theta)-\theta\}
\]&lt;/span> 对照指数型分布族形式易知：&lt;span class="math">\(h(x)=\frac{1}{x!},c(\theta)=\ln(\theta),T(x)=x,A(\theta)=\theta\)&lt;/span>。根据&lt;span class="math">\(\eta=c(\theta)\)&lt;/span>显然有： &lt;span class="math">\[
\eta=\ln(\theta)\\
\theta=e^{\eta}
\]&lt;/span> 此时，&lt;span class="math">\(A(\eta)=e^{\eta}\)&lt;/span>。&lt;/p>
&lt;p>其他常见指数型分布族可参见&lt;a href="https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions">维基百科词条&lt;/a>。&lt;/p>
&lt;p>由此可见，不少常见的分布如泊松分布、Beta分布、Gamma分布都是指数分布族中的一员。我们对指数族形式的推导都可以应用到这些概率分布上。&lt;/p>
&lt;h2 id="指数族的期望与方差的统一形式">指数族的期望与方差的统一形式&lt;/h2>
&lt;h3 id="指数族的期望">指数族的期望&lt;/h3>
&lt;p>我们在定义指数型分布族时提过，&lt;span class="math">\(A(\eta)\)&lt;/span>作为Log配分函数(log partition function)，实现了概率分布的归一化，即: &lt;span class="math">\[
c(\theta)^{-1} = \int h(x) \exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\} dx=e^{A(\theta)}\\
\Rightarrow A(\theta)=\ln\left\{\int h(x) \exp\left [\sum_{j=1}^k c_j(\theta)T_j(x)\right ] dx\right\}
\]&lt;/span> 由于&lt;span class="math">\(\eta\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>存在一一关系，我们用&lt;span class="math">\(\eta\)&lt;/span>替代&lt;span class="math">\(\theta\)&lt;/span>得到 &lt;span class="math">\[
A(\eta)=\ln\left\{\int h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx\right\}
\]&lt;/span> 指数族有一个特点，就是我们可以通过对&lt;span class="math">\(A(η)\)&lt;/span>求（偏）导来得到关于&lt;span class="math">\(T(x)\)&lt;/span>的矩，而一阶矩和二阶矩能够推导出概率分布的期望和方差。当&lt;span class="math">\(T(x)=x\)&lt;/span>或存在&lt;span class="math">\(T_i(x)=x\)&lt;/span>的分量时，我们就可以用求导或者求偏导得到关于&lt;span class="math">\(x\)&lt;/span>的期望和方差。具体做法如下，先&lt;span class="math">\(A(\eta)\)&lt;/span>求一阶导： &lt;span class="math">\[
\begin{aligned}
\frac{dA(\eta)}{d \eta}&amp;amp;=\frac{d}{d\eta}\ln\left\{\int h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx\right\}\\
&amp;amp;=\frac{\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx}{\int h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx}\\
&amp;amp;\because \int h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)] dx=e^{A(\eta)}\\
&amp;amp;=\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)] dx\\
&amp;amp;=E[\mathbf{T}(x)]
\end{aligned}\tag{9}
\]&lt;/span> 可见，Log配分函数的一阶导就是&lt;span class="math">\(T(x)\)&lt;/span>的概率期望，显然当&lt;span class="math">\(T(x)=x\)&lt;/span>时，有&lt;span class="math">\(E(T(x))=E(x)=\mu\)&lt;/span>。对于伯努利分布、多项分布、泊松分布、高斯分布等这些&lt;span class="math">\(T(x)=x\)&lt;/span>的分布来说，&lt;span class="math">\(A(\eta)\)&lt;/span>的一阶导就是分布的均值&lt;span class="math">\(\mu\)&lt;/span>。&lt;/p>
&lt;p>下面举两个例子。第一个多类别分布，其&lt;span class="math">\(A(\eta)=\ln(\sum_{i=1}^me^{\eta_i})\)&lt;/span>，期望应该为&lt;span class="math">\(\mathbf{\theta}=\begin{bmatrix}\theta_1\\\theta_2\\\vdots\\\theta_m\end{bmatrix}\)&lt;/span>（很特殊，其期望是一个向量）。我们求&lt;span class="math">\(A(\eta)\)&lt;/span>的一阶导(&lt;span class="math">\(\eta\)&lt;/span>是向量，因此结果是向量导数)有： &lt;span class="math">\[
\frac{d A(\eta)}{d\eta}=\frac{d \ln(\sum_{i=1}^me^{\eta_i})}{d\eta}=\frac{d\sum_{i=1}^me^{\eta_i}/d\eta}{\sum_{i=1}^me^{\eta_i}}\\
=\begin{bmatrix}
\frac{e^{\eta_1}}{\sum_{i=1}^me^{\eta_i}}\\
\frac{e^{\eta_2}}{\sum_{i=1}^me^{\eta_i}}\\
\vdots\\
\frac{e^{\eta_m}}{\sum_{i=1}^me^{\eta_i}}
\end{bmatrix}=\begin{bmatrix}\theta_1\\\theta_2\\\vdots\\\theta_m\end{bmatrix}
\]&lt;/span> 即&lt;span class="math">\(\frac{d A(\eta)}{d\eta}=E[x]\)&lt;/span>。&lt;/p>
&lt;p>第二个例子，我们来看均值方差皆未知的高斯分布，有&lt;span class="math">\(\eta=\mathbf{C}(\theta)\)&lt;/span>中的分量&lt;span class="math">\(\eta_1=c_1(\mu,\sigma)=\mu/\sigma^2,\eta_2=c_2(\mu,\sigma)=-1/2\sigma^2\)&lt;/span>表示，即 &lt;span class="math">\[
A(\mathbf{\eta})=\frac{-\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2)\tag{8.2}
\]&lt;/span> 其&lt;span class="math">\(T(x)=[x\quad x^2]^T\)&lt;/span>，如果我们仅对&lt;span class="math">\(x\)&lt;/span>那一项对应的&lt;span class="math">\(\eta_1\)&lt;/span>求偏导，就可以得到概率分布的均值： &lt;span class="math">\[
\frac{\partial A(\eta)}{\partial \eta_1}=\frac{-2\eta_1}{4\eta_2}=\frac{-\mu/\sigma^2}{2\times (-{1\over 2\sigma^2})}=\mu
\]&lt;/span> 即&lt;span class="math">\(\frac{\partial A(\eta)}{\partial \eta_1}=\mu\)&lt;/span>。&lt;/p>
&lt;h3 id="指数族的方差">指数族的方差&lt;/h3>
&lt;p>在一阶导数的基础上，我们可以求出&lt;span class="math">\(A(\eta)\)&lt;/span>的二阶导，由式（9）继续求（偏）导： &lt;span class="math">\[
\begin{aligned}
\frac{d^2A(\eta)}{d\eta^2}&amp;amp;=\frac{d}{d\eta}\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)] dx\\
&amp;amp;=\int \mathbf{T}(x) h(x) \frac{d}{d\eta}\{\exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)]\} dx\\
&amp;amp;=\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)](\mathbf{T}(x)-\underbrace{\frac{d}{d\eta}A(\eta))}_{E[T(x)]} dx\\
&amp;amp;=\int \mathbf{T}^2(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)] dx\\
&amp;amp;-E[\mathbf{T}(x)]\int \mathbf{T}(x) h(x) \exp[\mathbf{\eta}^T\mathbf{T}(x)-A(\eta)] dx\\
&amp;amp;=E[\mathbf{T}^2(x)]-E[\mathbf{T}(x)]^2\\
&amp;amp;=\mathrm{Var}[\mathbf{T}(x)]
\end{aligned}\tag{10}
\]&lt;/span> 从上式可知，&lt;span class="math">\(A(η)\)&lt;/span>的二阶导数正好是&lt;span class="math">\(T(x)\)&lt;/span>的方差，对于&lt;span class="math">\(T(x)=x\)&lt;/span>的分布，就是概率分布的方差。&lt;/p>
&lt;p>我们来看均值方差皆未知的高斯分布，对&lt;span class="math">\(\eta_1\)&lt;/span>求两次偏导： &lt;span class="math">\[
\frac{\partial^2 A(\eta)}{\partial \eta_1^2}=\frac{-2}{4\eta_2}=\frac{-2}{4\times (-{1\over 2\sigma^2})}=\sigma^2
\]&lt;/span> 显然就是高斯分布的方差。&lt;/p>
&lt;p>我们再举一个泊松分布的例子，其&lt;span class="math">\(A(\eta)=e^\eta\)&lt;/span>。显然，其对&lt;span class="math">\(\eta\)&lt;/span>求任意阶导数，都是&lt;span class="math">\(e^\eta=\theta\)&lt;/span>，而泊松分布的均值和方差也都是&lt;span class="math">\(\eta\)&lt;/span>。&lt;/p>
&lt;h3 id="theta与eta的一一对应缘由">&lt;span class="math">\(\theta\)&lt;/span>与&lt;span class="math">\(\eta\)&lt;/span>的一一对应缘由&lt;/h3>
&lt;p>我们发现现函数&lt;span class="math">\(A(η)\)&lt;/span>的二阶导数是&lt;span class="math">\(T(x)\)&lt;/span>的方差，我们都知道方差肯定是大于等于0的，一个函数的二阶导数大于等于0，证明这个函数是一个&lt;strong>凸函数&lt;/strong>(convex，碗状的)， 对于凸函数，其一阶导数单调递增。而其一阶导数正好又是&lt;span class="math">\(\mu=E[T(x)]\)&lt;/span>，因此一阶导数结果&lt;span class="math">\(\mu\)&lt;/span>和函数自变量&lt;span class="math">\(\eta\)&lt;/span>是一一对应的，&lt;span class="math">\(\eta\)&lt;/span>可以用&lt;span class="math">\(\mu\)&lt;/span>的反函数表示。此外在指数型分布族中，&lt;span class="math">\(\mu\)&lt;/span>和原分布中的参数&lt;span class="math">\(\theta\)&lt;/span>有着很简单的关系，且是一一对应的。因此，&lt;span class="math">\(\eta\)&lt;/span>与参数&lt;span class="math">\(\theta\)&lt;/span>也是一一对应关系。总结来时就是： &lt;span class="math">\[
\eta\stackrel{一一对应}{\longleftrightarrow}\mu\stackrel{一一对应}{\longleftrightarrow} \theta\tag{11}
\]&lt;/span>&lt;/p>
&lt;h2 id="指数型分布族的最大似然估计">指数型分布族的最大似然估计&lt;/h2>
&lt;p>现在我们讨论下指数族的最大似然估计，我们知道指数族的自然参数&lt;span class="math">\(η\)&lt;/span>和特定分布的原始参数&lt;span class="math">\(θ\)&lt;/span>是一一对应的，二者是存在可逆关系的，所有只要我们能估计出自然参数&lt;span class="math">\(η\)&lt;/span>，就一定能通过逆函数&lt;span class="math">\(c(⋅)^{−1}\)&lt;/span>得到分布的真实参数&lt;span class="math">\(θ\)&lt;/span>的估计值，也就是说对于指数族，我们只需要推导自然参数的估计量&lt;span class="math">\(\eta\)&lt;/span>就能求出原始参数&lt;span class="math">\(\theta\)&lt;/span>。&lt;/p>
&lt;p>我们在指数型分布族中同样可以使用&lt;strong>最大似然估计&lt;/strong>来估计自然参数（标准参数）&lt;span class="math">\(\eta\)&lt;/span>。按照最大似然估计的套路（详细可见&lt;a href="概率统计随机过程之最大似然估计拓展.md">概率统计随机过程之最大似然估计拓展&lt;/a>），我们需要假设这些样本都是独立同分布（i.i.d）的，用符号&lt;span class="math">\(\mathcal{D}\)&lt;/span>表示随机变量的一个观测样本集，样本容量是&lt;span class="math">\(N\)&lt;/span>。根据式（5）的指数型分布族表达式 &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{T}(x)-A(\eta)\right\}\tag{5}
\]&lt;/span> 可得样本的联合概率密度为： &lt;span class="math">\[
\begin{aligned}
L(\eta;\mathcal{D})&amp;amp;=\prod_{k=1}^N \left\{h(x_k)\exp[\eta^T\mathbf{T}(x_k)-A(\eta)]\right\}\\
&amp;amp;=\prod_{k=1}^N h(x_k)\times\exp\left\{\eta^T\sum_{k=1}^N\mathbf{T}(x_k)-NA(\eta)\right\}
\end{aligned}\tag{12}
\]&lt;/span> 对比一下，我们发现指数族分布的联合概率仍然是指数族，只是每一个部分有了变化： &lt;span class="math">\[
h_{ML}(\mathbf{x})=\prod_{k=1}^N h(x_k)\\
\eta_{ML}=\eta\\
T_{ML}(\mathbf{x})=\sum_{k=1}^N\mathbf{T}(x_k)\\
A_{ML}(\eta)=NA(\eta)
\]&lt;/span> 除了标准参数&lt;span class="math">\(\eta\)&lt;/span>没有改变，其他部分都发生了变化，但总体还是属于指数型分布族。&lt;/p>
&lt;p>按照最大似然估计的步骤，我们对式（12）取对数&lt;span class="math">\(\ln(\log)\)&lt;/span>有： &lt;span class="math">\[
\begin{aligned}
l(\eta;\mathcal{D})&amp;amp;=\ln(L(\eta;\mathcal{D}))\\
&amp;amp;=\sum_{k=1}^n\ln(h(x_k))+\eta^T \sum_{k=1}^N\mathbf{T}(x_k)-NA(\eta)
\end{aligned}\tag{13}
\]&lt;/span> 接下来，为了求&lt;span class="math">\(\eta\)&lt;/span>的极值，我们对式（13）求&lt;span class="math">\(\eta\)&lt;/span>的导数并令其等于0： &lt;span class="math">\[
\nabla_\eta l(\eta;\mathcal{D})=\sum_{k=1}^N\mathbf{T}(x_k)-N\nabla_\eta A(\eta)=0\\
\Rightarrow \nabla_\eta A(\eta)=\frac{1}{N}\sum_{k=1}^N\mathbf{T}(x_k)\tag{14}
\]&lt;/span> 由于&lt;span class="math">\(\eta\)&lt;/span>可能是向量，对于向量的导数，我们使用梯度&lt;span class="math">\(\nabla_\eta\)&lt;/span>表示。又由式（9）可知&lt;span class="math">\(A(η)\)&lt;/span>的一阶导数等于&lt;span class="math">\(T(x)\)&lt;/span>的期望&lt;span class="math">\(E[T(x)]\)&lt;/span>，即&lt;span class="math">\(E[T(x)]=\frac{d A(\eta)}{d \eta}=\nabla_\eta A(\eta)\)&lt;/span>，令其结果为&lt;span class="math">\(\mu_{ML}\)&lt;/span>，结合公式（14）有： &lt;span class="math">\[
\mu_{ML}=E[T(x)]=\frac{1}{N}\sum_{k=1}^N\mathbf{T}(x_k)\tag{15}
\]&lt;/span> 从式(15)可以看出，指数族分布&lt;span class="math">\(T(x)\)&lt;/span>期望值（均值参数&lt;span class="math">\(\mu\)&lt;/span>）的最大似然估计等于样本的平均值。且均值参数的最大似然估计值，只和样本的统计量&lt;span class="math">\(\sum_{k=1}^N=T(x_k)\)&lt;/span>有关，而不再依赖样本的其它信息，所以&lt;span class="math">\(\sum_{k=1}^N=T(x_k)\)&lt;/span>(或者说&lt;span class="math">\(T(x)\)&lt;/span>)是指数族的充分统计量。对于满足&lt;span class="math">\(T(x)=x\)&lt;/span>的分布，比如伯努利分布、多项式分布、泊松分布等等，样本的均值就是&lt;span class="math">\(T(x)\)&lt;/span>的均值，样本的均值就是均值参数的最大似然估计值。同理，对于单变量的高斯分布，样本的方差就是方差参数的最大似然估计值。&lt;/p>
&lt;p>最后，我们结合式（11）知道，&lt;span class="math">\(\eta,\mu,\theta\)&lt;/span>是有一一对应关系的，可以通过函数和反函数相互计算。最大似然估计给出了&lt;span class="math">\(μ_{ML}\)&lt;/span>的估计值，我们就是可以换算出&lt;span class="math">\(η_{ML},\theta_{ML}\)&lt;/span>。&lt;/p>
&lt;h2 id="自然指数族">自然指数族&lt;/h2>
&lt;p>我们在式（5）中给出了指数型分布族的一般形式 &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{T}(x)-A(\eta)\right\}\tag{5}
\]&lt;/span> 但是对于广义线性模型的应用场景而言，还是复杂了一些，因此有一种简化的&lt;strong>自然指数族&lt;/strong>。在自然指数族中，&lt;span class="math">\(\mathbf{T}(\mathbf{x})=\mathbf{x}\)&lt;/span>，不存在类似于&lt;span class="math">\(x^2,x^3,\log(x),\frac{1}{x}\)&lt;/span>这种带有函数关系的充分统计量，其可以简化写成： &lt;span class="math">\[
p(x;\eta)=h(x)\exp\left\{\eta^T\mathbf{x}-A(\eta)\right\}\tag{16}
\]&lt;/span> 二项分布，负二项分布，伯努利分布，泊松分布，参数&lt;span class="math">\(\alpha\)&lt;/span>已知的Gamma分布，已知方差的高斯分布，参数&lt;span class="math">\(\lambda\)&lt;/span>已知的逆高斯分布（又称Wald分布）等都可以写成自然指数族形式，其他分布如卡方分布、Beta分布、帕累托分布，对数正态分布，一般正态分布，一般Gamma分布则无法写成自然指数族的形式。他们是否是自然指数族的核心就在于是不是充分统计量&lt;span class="math">\(T(x)=x\)&lt;/span>。&lt;/p>
&lt;h2 id="指数分散族">指数分散族&lt;/h2>
&lt;p>在自然指数族的基础上，研究者们为了方便探究分布的期望和方差，对自然指数族做了少些变形得到指数分散族。其处理方法是将自然指数族的规范形式(式(16))的规范（自然）参数&lt;span class="math">\(\eta\)&lt;/span>拆分成与位置（期望）相关的位置函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>以及和方差相关的分散函数&lt;span class="math">\(a(\phi)\)&lt;/span>。其形式如下： &lt;span class="math">\[
p(x;\vartheta)=\exp\{\frac{\vartheta^T x-b(\vartheta)}{a(\phi)}+c(x,\phi)\}\tag{17}
\]&lt;/span> 这种形式的指数族通常被称为指数分散族(exponential dispersion family,EDF)，&lt;span class="math">\(a(ϕ)\)&lt;/span>称为分散函数(dispersion function)，是已知的。&lt;span class="math">\(ϕ\)&lt;/span>称为分散参数(dispersion parameter)。&lt;span class="math">\(\vartheta\)&lt;/span>仍然叫自然参数(natural parameter)或者规范参数(canonical parameter)，它和自然指数族中参数差了个系数，因为两种模式中&lt;span class="math">\(\vartheta^T x,\eta^Tx\)&lt;/span>的模式都是&lt;strong>参数&lt;span class="math">\(\times\)&lt;/span>充分统计量&lt;/strong>，所以不难发现，实际上我们对自然参数做一个&lt;span class="math">\(\frac{1}{a(\phi)}\)&lt;/span>倍的缩放。&lt;/p>
&lt;p>&lt;strong>指数分散族形式本质上是对自然指数族的参数&lt;span class="math">\(\eta\)&lt;/span>进行了拆分，把期望参数和方差参数拆分开&lt;/strong>。使得自然参数&lt;span class="math">\(\vartheta\)&lt;/span>仅和期望&lt;span class="math">\(μ\)&lt;/span>相关，分散参数&lt;span class="math">\(ϕ\)&lt;/span>和分布的方差参数相关。分拆后，规范参数&lt;span class="math">\(\vartheta\)&lt;/span>仅和分布的期望参数&lt;span class="math">\(μ\)&lt;/span>相关，并且和&lt;span class="math">\(μ\)&lt;/span>之间存在一一映射的函数关系，换句话说，&lt;span class="math">\(\vartheta\)&lt;/span>和&lt;span class="math">\(μ\)&lt;/span>可以互相转化。 &lt;span class="math">\[
\vartheta=f(\mu)\\
\mu=f^{−1}(\vartheta)\tag{18}
\]&lt;/span> 后面在配分函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>的讨论中可以证明这一点。&lt;/p>
&lt;h3 id="分散参数讨论">分散参数讨论&lt;/h3>
&lt;p>&lt;span class="math">\(a(ϕ)\)&lt;/span>的函数形式并没有严格的要求，其函数形式并不重要。在大多数文献中，&lt;span class="math">\(a(\phi)\)&lt;/span>被定义为： &lt;span class="math">\[
a_i(\phi)=\frac{\phi}{w_i}
\]&lt;/span> 其中&lt;span class="math">\(w_i\)&lt;/span>是观测样本的权重，一般是已知的。不同的样本可以拥有不同的权重值，比如进行参数估计时，对于某些样本设置成&lt;span class="math">\(w_i=0\)&lt;/span>，这就相当于抛弃了这些样本。如果不需要对样本进行加权（大多数场景），那么直接令 &lt;span class="math">\[
a(\phi)=\phi
\]&lt;/span> 即可。分散参数和分布的方差相关，它影响着方差的大小。此外，由于随机变量&lt;span class="math">\(x\)&lt;/span>不变，&lt;strong>在指数分布族和自然分布族中，其自然参数之间差&lt;span class="math">\(\frac{1}{a(\phi)}\)&lt;/span>倍&lt;/strong>。 &lt;span class="math">\[
\eta=\frac{\vartheta}{a(\phi)}
\]&lt;/span>&lt;/p>
&lt;h3 id="配分函数讨论">配分函数讨论&lt;/h3>
&lt;p>在指数分散族中，我们将&lt;span class="math">\(b(\vartheta)\)&lt;/span>也称为配分函数，和一般形态的配分函数&lt;span class="math">\(A(\eta)\)&lt;/span>显然有如下关系： &lt;span class="math">\[
A(\eta)=\frac{b(\vartheta)}{a(\phi)}
\]&lt;/span> 在指数型分布族中，我们可以用&lt;span class="math">\(A(\eta)\)&lt;/span>的导数求出分布的矩，一阶导数是分布的期望，二阶导数是分布的方差。&lt;span class="math">\(b(\vartheta)\)&lt;/span>也有类似的作用。由于&lt;span class="math">\(\eta=\frac{\vartheta}{a(\phi)},A(\eta)=\frac{b(\vartheta)}{a(\phi)}=\frac{b(\eta\cdot a(\phi))}{a(\phi)}\)&lt;/span>，所以概率分布的期望为： &lt;span class="math">\[
E[X]=\frac{dA(\eta)}{d\eta}=\frac{d\frac{b(\vartheta)}{a(\phi)}}{d\eta}=\frac{d\frac{b(\vartheta)}{a(\phi)}}{d\frac{\vartheta}{a(\phi)}}\\
\Rightarrow E[X]=\frac{d b(\vartheta)}{d\vartheta}=b&amp;#39;(\vartheta)=\mu\tag{19}
\]&lt;/span> 同样的，我们可以推导出概率分布的方差： &lt;span class="math">\[
\mathrm{Var}[X]=A&amp;#39;&amp;#39;(\eta)=\frac{d^2\frac{b(\vartheta)}{a(\phi)}}{d[\frac{\vartheta}{a(\phi)}]^2}\\
又\because A&amp;#39;(\eta)=b&amp;#39;(\vartheta)\\
\Rightarrow \mathrm{Var}[X]=\frac{d{b&amp;#39;(\vartheta)}}{d\frac{\vartheta}{a(\phi)}}=\frac{1}{a(\phi)}\frac{db&amp;#39;(\vartheta)}{d\vartheta}\\
\Rightarrow \mathrm{Var}[X]=a(\phi)b&amp;#39;&amp;#39;(\vartheta)\tag{20}
\]&lt;/span> 由于&lt;span class="math">\(b(\vartheta)\)&lt;/span>是在&lt;span class="math">\(A(η)\)&lt;/span>的基础上拆分出去&lt;span class="math">\(a(ϕ)\)&lt;/span>，所以&lt;span class="math">\(b(\vartheta)\)&lt;/span>的二阶导数不再等于分布的方差，需要再乘上&lt;span class="math">\(a(ϕ)\)&lt;/span>才能得到分布的方差。&lt;/p>
&lt;p>从期望和方差的关系，我们能发现&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>也是一一对应关系。根据式（19）可知，&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>有函数关系，且由于&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>的导数&lt;span class="math">\(b&amp;#39;&amp;#39;(\vartheta)\)&lt;/span>是方差（恒大于0）乘以一个已知数&lt;span class="math">\(a(\phi)\)&lt;/span>（式（20）结论），因此&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>的导数必然恒为正数或负数（取决于已知数&lt;span class="math">\(a(\phi)\)&lt;/span>），即&lt;span class="math">\(b&amp;#39;(\vartheta)\)&lt;/span>必为单调函数，而单调函数必存在反函数，推得必存在&lt;span class="math">\(b&amp;#39;^{-1}\)&lt;/span>，使得&lt;span class="math">\(\vartheta=b&amp;#39;^{-1}(\mu)\)&lt;/span>。因此&lt;span class="math">\(\vartheta\)&lt;/span>与&lt;span class="math">\(\mu\)&lt;/span>是一一对应的。&lt;/p>
&lt;h3 id="指数分散族的方差">指数分散族的方差&lt;/h3>
&lt;p>在指数分散族中，分布的方差可以表示成两部分的乘积，一部分是分散函数&lt;span class="math">\(a(ϕ)\)&lt;/span>，另一部分是配分函数的二阶导数&lt;span class="math">\(b&amp;#39;&amp;#39;(\vartheta)\)&lt;/span>。其中，函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>是一个关于&lt;span class="math">\(\vartheta\)&lt;/span>的函数， 其二阶导数要么是一个常数，要么是一个关于自然参数&lt;span class="math">\(\vartheta\)&lt;/span>的函数。 而自然参数&lt;span class="math">\(\vartheta\)&lt;/span>和均值参数&lt;span class="math">\(μ\)&lt;/span>存在一一对应关系，所以一定可以把&lt;span class="math">\(\vartheta\)&lt;/span>替换成&lt;span class="math">\(μ\)&lt;/span>。&lt;/p>
&lt;p>我们定义配分函数&lt;span class="math">\(b(\vartheta)\)&lt;/span>的二阶导数为&lt;strong>方差函数&lt;/strong>(variance function)，方差函数是一个关于期望&lt;span class="math">\(μ\)&lt;/span>的函数，即 &lt;span class="math">\[
b&amp;#39;&amp;#39;(\vartheta)=\nu(μ)\tag{21}
\]&lt;/span> 方差函数&lt;span class="math">\(ν(μ)\)&lt;/span>存在两种情况：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>方差函数是一个常量值，&lt;span class="math">\(ν(μ)=b&amp;#39;&amp;#39;(\vartheta)=C\)&lt;/span>，此时分布的方差与均值无关。典型的分布就是正态分布。&lt;/li>
&lt;li>方差函数是一个关于均值&lt;span class="math">\(μ\)&lt;/span>的函数，&lt;span class="math">\(ν(μ)=b&amp;#39;&amp;#39;(\vartheta)\)&lt;/span>，此时分布的方差与均值有关。&lt;/li>
&lt;/ol>
&lt;p>方差函数，是一个平滑函数，它把分布的均值参数&lt;span class="math">\(μ\)&lt;/span>和分布的方差关联在一起。如果其值一个常数值，说明均值和方差是独立无关的；反之，如果是&lt;span class="math">\(μ\)&lt;/span>的函数，说明均值和方差是相关联的。&lt;/p>
&lt;p>例如，在高斯分布中，&lt;span class="math">\(b&amp;#39;&amp;#39;(\vartheta)=1\)&lt;/span>，所以方差和均值是相互独立的，对于其他分布，这是不成立的，高斯分布是特例。&lt;/p>
&lt;p>影响方差的，除了方差函数&lt;span class="math">\(ν(μ)\)&lt;/span>以外，还有分散参数&lt;span class="math">\(a(ϕ)=ϕ\)&lt;/span>，它起到一个缩放的作用。 参数&lt;span class="math">\(\vartheta\)&lt;/span>和&lt;span class="math">\(ϕ\)&lt;/span>&lt;strong>本质上是位置和尺度参数，位置参数反映数据的均值，尺度参数反映数据方差&lt;/strong>。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/常见分布的方差函数表.png" alt="常见分布的方差函数表" />&lt;p class="caption">常见分布的方差函数表&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之数理统计常用概念</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%B8%B8%E7%94%A8%E6%A6%82%E5%BF%B5/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%B8%B8%E7%94%A8%E6%A6%82%E5%BF%B5/</guid><description>
&lt;h2 id="概率统计随机过程之数理统计常用概念">概率统计随机过程之数理统计常用概念&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#总体与样本">总体与样本&lt;/a>&lt;/li>
&lt;li>&lt;a href="#统计量与估计量">统计量与估计量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#估计量的评价指标">估计量的评价指标&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本的经验分布函数和样本矩">样本的经验分布函数和样本矩&lt;/a>&lt;/li>
&lt;li>&lt;a href="#抽样分布">抽样分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常见统计量的抽样分布">常见统计量的抽样分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#概率分布族">概率分布族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型分布族">指数型分布族&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#次序统计量">次序统计量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#充分统计量">充分统计量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#因子分解定理">因子分解定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小充分统计量">最小充分统计量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#完全完备统计量">完全（完备）统计量&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#指数族中统计量的完全性">指数族中统计量的完全性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小充分统计量与完备性关系">最小充分统计量与完备性关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有界完全统计量及其性质">有界完全统计量及其性质&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#常用概率分布与特征数表">常用概率分布与特征数表&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="总体与样本">总体与样本&lt;/h2>
&lt;p>总体是个体的集合；样本是样品的集合。一个样本里样品个数叫样本容量&lt;span class="math">\(n\)&lt;/span>，每个样品都是随机抽取的，所以&lt;strong>样品是随机变量&lt;/strong>。每次抽取的实际值/物品，是观察/观测值。&lt;span class="math">\(n\)&lt;/span>个样品（随机变量）组成&lt;span class="math">\(n\)&lt;/span>维样本空间，因此一个样本容量为&lt;span class="math">\(n\)&lt;/span>个样本是一个&lt;span class="math">\(n\)&lt;/span>维随机变量。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">总体&lt;/th>
&lt;th align="center">&lt;span class="math">\(\overset{N\rightarrow \infty}{\leftarrow}\)&lt;/span>&lt;/th>
&lt;th align="center">个体&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;td align="center">抽样&lt;/td>
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">样本（&lt;span class="math">\(n\)&lt;/span>维）&lt;/td>
&lt;td align="center">&lt;span class="math">\(\overset{n个}{\leftarrow}\)&lt;/span>&lt;/td>
&lt;td align="center">样品&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;td align="center">等效&lt;/td>
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(n\)&lt;/span>维随机变量&lt;/td>
&lt;td align="center">&lt;span class="math">\(\overset{n个}{\leftarrow}\)&lt;/span>&lt;/td>
&lt;td align="center">随机变量&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;td align="center">观测&lt;/td>
&lt;td align="center">&lt;span class="math">\(\downarrow\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(n\)&lt;/span>维向量&lt;/td>
&lt;td align="center">&lt;span class="math">\(\overset{n个}{\leftarrow}\)&lt;/span>&lt;/td>
&lt;td align="center">观测值&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>对总体的要求：远比样本容量大，最好是无限总体。&lt;/li>
&lt;li>对样本的要求：抽样要独立、随机&lt;span class="math">\(\Rightarrow\)&lt;/span>n个独立同分布样品(i.i.d)&lt;span class="math">\(\Rightarrow\)&lt;/span>n维随机变量&lt;span class="math">\(\leftrightarrow\)&lt;/span>n个i.i.d的一维随机变量&lt;/li>
&lt;li>抽样方法：简单随机抽样、分层抽样、系统抽样、按比例抽样……&lt;/li>
&lt;/ul>
&lt;h2 id="统计量与估计量">统计量与估计量&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>统计量&lt;/strong>：通过&lt;strong>样本构造&lt;/strong>出的&lt;strong>不含任何未知量&lt;/strong>的函数（样本函数），称此函数为统计量。&lt;/p>
&lt;/blockquote>
&lt;p>统计量通过构造函数把分散在样品中的反应总体的信息按人们的要求提取出来。需要强调的是由于样本是N维随机变量，因此其构造的函数（统计量）也是随机变量（大多数是一维）。我们通过观测，得到样本观察值后，立即可算得统计量的值。&lt;/p>
&lt;p>关于统计量有以下几点说明：&lt;/p>
&lt;ul>
&lt;li>统计量&lt;strong>既是函数也是随机变量&lt;/strong>。函数是从统计量的构造方式角度来说的，它是样本空间到参数空间的一个映射；而由于其自变量是一个N维随机变量（样本），同时一个随机变量的函数也是随机变量，随机变量是从值域的角度来考虑。&lt;/li>
&lt;li>构造统计量的目的是统计推断。&lt;strong>统计推断包括：抽样分布（精确，渐进，近似）、参数估计（点，区间）、假设检验（参数，非参数）&lt;/strong>。&lt;/li>
&lt;li>统计量随机性来源于自变量的随机性，当有一组样本的观测值被取出，那么统计量的随机性就没了，值也就固定了（就是单纯函数映射关系）。&lt;/li>
&lt;li>统计量可以简单理解为&lt;strong>随机变量的函数&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>估计量&lt;/strong>：在参数估计大类的点估计中，那么用于估计未知参数的&lt;strong>统计量&lt;/strong>称为&lt;strong>点估计(量)&lt;/strong>，简称为&lt;strong>估计(量)&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>从上面的定义中，我们注意以下几点：&lt;/p>
&lt;ul>
&lt;li>估计量是一种统计量，也是由样本构造的函数。二者区别在于目的：估计量要估计一个&lt;strong>未知参数&lt;/strong>；&lt;/li>
&lt;li>要估计的未知参数设为&lt;span class="math">\(\theta\)&lt;/span>，估计量用&lt;span class="math">\(\hat{\theta}=\hat{\theta}(x_1,x_2,\dotsb,x_n)\)&lt;/span>表示，&lt;span class="math">\(\hat{\theta}\)&lt;/span>的取值范围称为&lt;strong>参数空间&lt;/strong>&lt;span class="math">\(\varTheta=\{\theta\}\)&lt;/span>，那么估计量就是一个从&lt;strong>样本空间到参数空间的映射&lt;/strong>，一个具体的观测值所得一个估计值；&lt;/li>
&lt;li>参数&lt;span class="math">\(\theta\)&lt;/span>可以是（1）分布中的未知参数（2）分布中的特征数（期望、方差、偏度、分位数……）（3）某事件的概率；&lt;/li>
&lt;li>由于构造出来以估计参数的函数不止一种，因此一个参数的估计量也不止一个。&lt;/li>
&lt;/ul>
&lt;h3 id="估计量的评价指标">估计量的评价指标&lt;/h3>
&lt;p>（1）无偏性（小样本性质）/渐进无偏性（大样本性质）：表示的是无系统偏差，虽然每次的估计值与参数会有随机偏差，但这些随机偏差期望为0；&lt;/p>
&lt;p>无偏估计&lt;strong>不具有变换不变性&lt;/strong>，典型的例子就是无偏样本方差&lt;span class="math">\(s^2=\frac{\sum (x_i-\bar{x})^2}{n-1}\)&lt;/span>的平方根&lt;span class="math">\(s\)&lt;/span>（样本标准差）是总体标准差的有偏估计。但是如果变换是&lt;strong>线性变换&lt;/strong>，那么无偏估计还是无偏的。&lt;/p>
&lt;p>（2）有效性：之前指出估计量也是随机变量，如果这个估计的期望等于位置参数则为无偏；在&lt;strong>无偏估计的基础上&lt;/strong>，我们希望每次观测得出估计值尽量差距小，这就用&lt;strong>估计量的方差&lt;/strong>来表示。方差越小（即偏离无偏估计参数的概率越小），有效性越高。&lt;/p>
&lt;p>通常，我们希望在&lt;strong>无偏估计的基础上，估计量的方差尽量小&lt;/strong>，这就是&lt;strong>一致最小方差无偏估计&lt;/strong>，简称为&lt;strong>UMVUE&lt;/strong>。&lt;/p>
&lt;p>（3）相合性（大样本性质）：当样本容量增加时，随机变量&lt;span class="math">\(\hat{\theta}\)&lt;/span>收敛于&lt;span class="math">\(\theta\)&lt;/span>，直观的讲就是随着样本容量的增大，一个估计量的值能够稳定（以很大概率）在待估参数真值的附近，这就是估计量的相合性的要求。&lt;/p>
&lt;p>根据随机变量收敛性的强弱，又可分为弱相合（&lt;span class="math">\(\hat{\theta}\overset{P}{\rightarrow}\theta\)&lt;/span>），强相合（&lt;span class="math">\(\hat{\theta}\overset{a.s.}{\rightarrow}\theta\)&lt;/span>）,r阶矩相合（&lt;span class="math">\(\hat{\theta}\overset{r}{\rightarrow}\theta\)&lt;/span>）。理论基础时&lt;strong>大数定理&lt;/strong>。相合性也可等效于无偏（或渐进无偏）+ 方差（弱、强、r阶矩）收敛到0。&lt;/p>
&lt;p>（4）渐进正态性（大样本性质）：估计量的渐进正态性来源于&lt;strong>中心极限定理&lt;/strong>，若统计量在样本容量&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>时，也渐近于正态分布，称为渐进正态性。具体可定义为：如果存在一序列&lt;span class="math">\(\{\sigma_n^2\}\)&lt;/span>，满足&lt;span class="math">\((\hat\theta_n-\theta)/\sigma_n(\theta)\overset{L}{\rightarrow}N(0,1)\)&lt;/span>，则称&lt;span class="math">\(\hat\theta_n\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的渐进正态估计，&lt;span class="math">\(\sigma_n^2\)&lt;/span>称为&lt;span class="math">\(\hat\theta_n\)&lt;/span>的渐进方差。从&lt;span class="math">\((\hat\theta_n-\theta)/\sigma_n(\theta)\)&lt;/span>来看，分子项依概率收敛于&lt;span class="math">\(\theta\)&lt;/span>的速度与分母项&lt;span class="math">\(\sigma_n(\theta)\)&lt;/span>趋近于0的速度相同时，其比值才会稳定与正态分布。因此，&lt;span class="math">\(\hat\theta_n\)&lt;/span>收敛速度与渐近方差直接相关，渐近方差越小，收敛越快。&lt;/p>
&lt;p>渐进正态性和相合性的关系类似于中心极限定理和大数定律。相合性是对估计的一种较低要求，它只要求估计序列&lt;span class="math">\(\{\hat\theta_n\}\)&lt;/span>在样本数量&lt;span class="math">\(n\)&lt;/span>增加的时候也趋近于&lt;span class="math">\(\theta\)&lt;/span>，但是并没有指出趋近的速度（例如是&lt;span class="math">\(1/n,1/\sqrt{n}\)&lt;/span>或&lt;span class="math">\(1/\ln n\)&lt;/span>）。而渐进正态性补充了这一点，收敛速度与渐进方差相关。&lt;strong>经验来看，大多数渐进正态估计都是以&lt;span class="math">\(1/\sqrt{n}\)&lt;/span>的速度收敛于被估参数的&lt;/strong>。&lt;/p>
&lt;h3 id="样本的经验分布函数和样本矩">样本的经验分布函数和样本矩&lt;/h3>
&lt;p>经验分布函数部分可参考笔记《概率统计随机过程之经验函数分布》。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>样本矩&lt;/strong>：样本的经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>的各阶矩统称为样本矩，又称矩统计量。&lt;/p>
&lt;/blockquote>
&lt;p>这个说法太形式化了，直白的说，经验分布函数的概率质量函数都是&lt;span class="math">\(\frac{1}{n}\)&lt;/span>，样本矩就是以&lt;span class="math">\(p=\frac{1}{n}\)&lt;/span>为概率的各种期望。矩统计量我们心里默默替换成用样本构造的矩相关的函数来理解就行了。大体就这下面两种：&lt;/p>
&lt;p>&lt;span class="math">\[\begin{aligned}
A_k&amp;amp;=\frac{1}{n}\sum_{i=1}^n x_i^k(k=1,2,\dotsb)&amp;amp;\text{样本k阶（原点）矩}\\
B_k&amp;amp;=\frac{1}{n}\sum_{i=1}^n (x_i-\bar x)^k(k=1,2,\dotsb) &amp;amp;\text{样本k阶中心矩}
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>我们可以容易证明：一般样本k阶原点矩是无偏的，样本k阶中心矩是有偏的。&lt;/p>
&lt;p>所谓矩估计，就是用这些样本矩构造（凑）出特定的参数，例如偏度系数&lt;span class="math">\(\hat{\beta}=B_3/B_2^{\frac{3}{2}}\)&lt;/span>。如果需要估计的未知参数没法用样本矩构造出来，则认为此参数的矩估计不存在；相对的，如果可以用多种方法构造出来，那么就有多种矩估计，可以通过有效性，无偏性再进行进一步筛选。&lt;/p>
&lt;h2 id="抽样分布">抽样分布&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>抽样分布&lt;/strong>：统计量作为一个随机变量，其概率分布称为抽样分布，也称统计量分布、随机变量函数分布。&lt;/p>
&lt;/blockquote>
&lt;p>抽样分布就是寻求特定样本的函数（统计函数）的分布，大体分为以下三类：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>精确（抽样）分布：在总体&lt;span class="math">\(X\)&lt;/span>的分布已知，如果对任意&lt;span class="math">\(n\)&lt;/span>都能导出统计量&lt;span class="math">\(T(x_1,\dotsb,x_n)\)&lt;/span>的分布的解析表达式，则为精确分布。在&lt;strong>小样本问题&lt;/strong>中很有用。一般比较难求，大多以正态总体为研究对象，例如统计三大分布&lt;span class="math">\(\chi^2,t,F\)&lt;/span>分布等。&lt;/li>
&lt;li>渐进（抽样）分布：精确分布大多数求不出来，或者很复杂。因此，退一步，在样本容量n很大或者趋近无穷时作为近似的分布，称为渐进分布。&lt;strong>大样本&lt;/strong>时可以使用，比如中心极限定理下的正态分布、卡方分布等。&lt;/li>
&lt;li>近似（抽样）分布：没啥特定规律的分布，可以在&lt;em>一定条件&lt;/em>下用近似分布，常见的有假定正态分布并用样本前两阶矩替代总体前两阶矩；还有很多随机模拟方法，MCMC，Gibs采样等等。&lt;/li>
&lt;/ol>
&lt;h3 id="常见统计量的抽样分布">常见统计量的抽样分布&lt;/h3>
&lt;p>统计抽样三大分布：卡方分布、t分布、F分布。&lt;/p>
&lt;p>见笔记《概率统计随机过程之抽样的分布》&lt;/p>
&lt;h2 id="概率分布族">概率分布族&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>分布族&lt;/strong>：&lt;strong>分布类型&lt;/strong>和&lt;strong>参数空间&lt;/strong>组成一个（概率）参数分布族。此外，分布/概率和特定条件组成，且不能由该特定条件确定具体分布的分布族成为非参数分布族。&lt;/p>
&lt;/blockquote>
&lt;p>分布族是一个看似很高端，其实就是某一类概率分布的统称，比如常见的参数分布族可写成：&lt;/p>
&lt;ul>
&lt;li>二项分布族：&lt;span class="math">\(\{b(n,p);0&amp;lt;1&amp;lt;p,n\in Z^+\}\)&lt;/span>&lt;/li>
&lt;li>泊松分布族：&lt;span class="math">\(\{p(\lambda);\lambda&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;li>正态分布族：&lt;span class="math">\(\{N(\mu,\sigma^2);-∞&amp;lt;\mu&amp;lt;∞,\sigma&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;li>均匀分布族：&lt;span class="math">\(\{U(a,b);-∞&amp;lt;a&amp;lt;b&amp;lt;∞\}\)&lt;/span>&lt;/li>
&lt;li>指数分布族：&lt;span class="math">\(\{\exp(\lambda);\lambda&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;li>伽马分布族：&lt;span class="math">\(\{Ga(\alpha,\lambda);\alpha&amp;gt;0,\lambda&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;li>贝塔分布族：&lt;span class="math">\(\{Beta(a,b);a&amp;gt;0,b&amp;gt;0\}\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>还有一些非参数分布族的例子：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\mathfrak{p}_1=\{p(x);p(x)\text{是连续分布}\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathfrak{p}_2=\{F(x);F(x)\text{的一二阶矩存在}\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathfrak{p}_3=\{p(x);p(x)\text{是对称连续分布}\}\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="指数型分布族">指数型分布族&lt;/h3>
&lt;p>区别于指数分布族，指数型分布族是指数分布族的推广，更是囊括了正态分布族、二项分布族、伽马分布族、多项分布族等等。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>指数型分布族&lt;/strong>：一个概率分布族&lt;span class="math">\(\mathfrak{p}=\{p_{\theta}(x);\theta∈\varTheta\}\)&lt;/span>可称为&lt;strong>指数型分布族&lt;/strong>，假如&lt;span class="math">\(\mathfrak{p}\)&lt;/span>中的分布（分布列或密度函数）都可表示为如下形式： &lt;span class="math">\[p_\theta(x)=h(x)c(\theta)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\}\tag{1}\]&lt;/span> 其中，k为自然数；&lt;span class="math">\(\theta\)&lt;/span>可以是数字，也可以是向量。分布的支撑&lt;span class="math">\(\{x:p(x)&amp;gt;0\}\)&lt;/span>与参数&lt;span class="math">\(\theta\)&lt;/span>无关；诸&lt;span class="math">\(c(\theta),c_1(\theta),\dotsb,c_k(\theta)\)&lt;/span>是定义在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上的函数；诸&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>是&lt;span class="math">\(x\)&lt;/span>的函数，称为充分统计向量，但&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>线性无关。&lt;span class="math">\(h(x)\)&lt;/span>也只是&lt;span class="math">\(x\)&lt;/span>的函数，且&lt;span class="math">\(h(x)&amp;gt;0\)&lt;/span>，通常是一个常数。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;span class="math">\(c(\theta)\)&lt;/span>是作为归一化参数存在的，称为叫做配分函数(partition function)。 &lt;span class="math">\[c(\theta)^{-1} = \int h(x) \exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)\right\} dx\]&lt;/span> 此外，指数族还有另一种表述方式，就是将外面的&lt;span class="math">\(c(\theta)\)&lt;/span>放到指数符号中： &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)-A(\theta)\right\}\tag{2}\]&lt;/span> 由于通常&lt;span class="math">\(A(\theta)\)&lt;/span>含有&lt;span class="math">\(\log\)&lt;/span>符号，该部分也称为“Log Partition Function”，易知&lt;span class="math">\(A(\theta)=\ln c(\theta)\)&lt;/span>。 如果我们使用向量值函数来表达指数型分布族可写为: &lt;span class="math">\[p_\theta(x)=h(x)\exp\left\{\sum_{j=1}^k c_j(\theta)T_j(x)-A(\theta)\right\}\tag{3}\]&lt;/span>&lt;/p>
&lt;p>从上述定义可知，一个分布族是不是指数型分布族的&lt;strong>关键在于其概率分布能否改写为定义中方式&lt;/strong>，其中主要有两条：一条是“分布的支撑与&lt;span class="math">\(\theta\)&lt;/span>无关”；另一条是“&lt;span class="math">\(T_1(x),\dotsb,T_k(x)\)&lt;/span>线性无关”，若其间线性相关，如&lt;span class="math">\(T_1(x)=2T_2(x)+3T_3(x)\)&lt;/span>，则把&lt;span class="math">\(T_1(x)\)&lt;/span>归拢到&lt;span class="math">\(T_2(x),T_3(x)\)&lt;/span>中即可。&lt;/p>
&lt;p>下面这张截图就是将3个常见分布族改写成指数型分布族的例子。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/指数型分布族例子.png" alt="指数型分布族例子" />&lt;p class="caption">指数型分布族例子&lt;/p>
&lt;/div>
&lt;h4 id="指数型分布族的向量化写法">指数型分布族的向量化写法&lt;/h4>
&lt;p>下面我们使用&lt;strong>向量值函数&lt;/strong>将式（3）进行进一步改造。&lt;/p>
&lt;blockquote>
&lt;p>向量值函数：有时也称为向量函数，是一个单变量或多变量的、&lt;strong>值域是多维向量或者无穷维向量的集合的函数&lt;/strong>。向量值函数的输入可以是一个标量或者一个向量，输出是向量，定义域的维度和值域的维度是不相关的。&lt;/p>
&lt;/blockquote>
&lt;p>对于&lt;span class="math">\(\theta\)&lt;/span>的一系列函数&lt;span class="math">\(c_1(\theta),c_2(\theta),\dotsb\)&lt;/span>和充分统计量向量&lt;span class="math">\(T_1(x),T_2(x),\dotsb\)&lt;/span>，我们写出列向量形式： &lt;span class="math">\[
\mathbf{C}(\theta)=\begin{bmatrix}c_1(\theta)\\c_2(\theta)\\\vdots\\c_k(\theta)\end{bmatrix}
\mathbf{T}(x)=\begin{bmatrix}T_1(x)\\T_2(x)\\\vdots\\T_k(x)\end{bmatrix}
\]&lt;/span> 那么式（3）可写成 &lt;span class="math">\[
p(x;\theta)=h(x)\exp\left\{\mathbf{C}^T(\theta)\mathbf{T}(x)-A(\theta)\right\}\tag{4}
\]&lt;/span> 其中，&lt;span class="math">\(\mathbf{C}(\theta),\mathbf{T}(x)\)&lt;/span>都是向量值函数，&lt;span class="math">\(h(x),A(\theta)\)&lt;/span>都是普通函数，通常文章会把&lt;span class="math">\(A(\theta)\)&lt;/span>写成&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的形式，这两种本质上是等价的，但是&lt;span class="math">\(A(\mathbf{C}(\theta))\)&lt;/span>的参数形式更加统一，为主流用法。&lt;/p>
&lt;p>&lt;strong>均值方差皆未知的高斯分布的指数族形式&lt;/strong>：如果高斯分布的均值和方差都是未知的，那么就需要使用多个线性不相关的充分统计量来表示指数型分布族： &lt;span class="math">\[
\begin{aligned}
p(x|\mu,\sigma^2)&amp;amp;=\frac{1}{(2\pi\sigma^2)^{1/2}} \exp[ -\frac{1}{2\sigma^2}(x-\mu)^2]\\
&amp;amp;=\frac{1}{(2\pi\sigma^2)^{1/2}} \exp[-\frac{1}{2\sigma^2} x^2 +\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2]\\
&amp;amp;=\frac{1}{(2\pi)^{1/2}} \exp[-\frac{1}{2\sigma^2} x^2 +\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2-\ln{\sigma}]
\end{aligned}
\]&lt;/span> 对照指数族的形式，有&lt;span class="math">\(h(x)=\frac{1}{(2\pi)^{1/2}}, \mathbf{C}=\begin{bmatrix}\mu/\sigma^2 \\-1/2\sigma^2\end{bmatrix},\mathbf{T}=\begin{bmatrix}x\\x^2\end{bmatrix},A(\mu,\sigma)=\frac{1}{2\sigma^2}\mu^2+\ln{\sigma}\)&lt;/span>。&lt;/p>
&lt;p>需要指出的是，一般情况下，我们在&lt;span class="math">\(A(\mu,\sigma)\)&lt;/span>中最好不要在使用原有的&lt;span class="math">\(\mu,\sigma\)&lt;/span>，而是使用&lt;span class="math">\(\mathbf{C}\)&lt;/span>中的分量&lt;span class="math">\(c_1=\mu/\sigma^2,c_2=-1/2\sigma^2\)&lt;/span>表示，即 &lt;span class="math">\[
A(\mathbf{C})=\frac{-c_1^2}{4c_2}-\frac{1}{2}\log(-2c_2)
\]&lt;/span> 在其他文献中，也有令&lt;span class="math">\(h(x)=1\)&lt;/span>，然后把&lt;span class="math">\(\frac{1}{(2\pi)^{1/2}}\)&lt;/span>放到&lt;span class="math">\(A(\mathbf{C})\)&lt;/span>中的，即&lt;span class="math">\(A(\mathbf{C})=\frac{-c_1^2}{4c_2}-\frac{1}{2}\log(-2c_2)-\frac{1}{2}\log(2\pi)\)&lt;/span>，这样是等价的。&lt;/p>
&lt;p>指数型分布族的重要性体现在以下多个方面：&lt;/p>
&lt;ul>
&lt;li>指数族分布是&lt;strong>唯一有共轭先验的分布族&lt;/strong>,这就简化了后验的计算&lt;/li>
&lt;li>在特定的规范化条件下(regularity conditions),指数族分布是&lt;strong>唯一有限规模充分统计量&lt;/strong>(finite-sized sufficient statistics)的分布族,这意味着可以将数据压缩称固定规模的浓缩概括而不损失信息&lt;/li>
&lt;li>指数族分布是&lt;strong>广义线性模型&lt;/strong>(generalized linear models)的核心&lt;/li>
&lt;li>指数族分布也是&lt;strong>变分推理&lt;/strong>(variational inference)的核心&lt;/li>
&lt;/ul>
&lt;h2 id="次序统计量">次序统计量&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>次序统计量&lt;/strong>：设&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>是取自总体&lt;span class="math">\(X\)&lt;/span>的一个样本，&lt;span class="math">\(X_{(k)}\)&lt;/span>称为该样本的的第k个次序统计量，假如每当获得样本观测值后将其&lt;strong>从小到大排序可得如下有序样本&lt;/strong>： &lt;span class="math">\[x_{(1)}≤x_{(2)}≤x_{(3)}≤\dotsb≤x_{(k)}≤\dotsb≤x_{(n)}\]&lt;/span> 其中，第k个观测值&lt;span class="math">\(x_{(k)}\)&lt;/span>就是&lt;span class="math">\(X_{(k)}\)&lt;/span>的取值，并称&lt;span class="math">\(X_{(1)},X_{(2)},\dotsb,X_{(n)}\)&lt;/span>为该&lt;strong>样本的次序统计量&lt;/strong>，特别的，&lt;span class="math">\(X_{(1)}=\min(X_1,X_2,\dotsb,X_n)\)&lt;/span>称为该样本的&lt;strong>最小次序统计量&lt;/strong>，&lt;span class="math">\(X_{(n)}=\max(X_1,X_2,\dotsb,X_n)\)&lt;/span>称为该样本的&lt;strong>最大次序统计量&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>次序统计量引申出来的概念有最小次序统计量、最大次序统计量、极差、中位数、P分位数。次序统计量作为随机变量也是有概率分布的，用的不多，等用到的时候再补充。&lt;/p>
&lt;h2 id="充分统计量">充分统计量&lt;/h2>
&lt;p>充分统计量的概念比较难理解，但十分重要。由于我看得是茆诗松的《数理统计学》，这本书里的定义包括了分布族的充分统计量，这和其他资料上的充分统计量有一个区别，就是一般资料上充分统计量都是和分布的某个参数&lt;span class="math">\(\theta\)&lt;/span>相关的，我也更倾向这样定义（更实用主义）。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>充分统计量&lt;/strong>：假如&lt;span class="math">\(\mathcal{F}=\{F_\theta,\theta\in \varTheta\}\)&lt;/span>是参数分布族（&lt;span class="math">\(\theta\)&lt;/span>可以是向量），在给定&lt;span class="math">\(T=t\)&lt;/span>下，样本&lt;span class="math">\(x\)&lt;/span>（n维随机变量）的条件分布与&lt;span class="math">\(\theta\)&lt;/span>无关，则称&lt;span class="math">\(T\)&lt;/span>&lt;strong>为参数&lt;span class="math">\(\theta\)&lt;/span>的充分统计量&lt;/strong>。 &lt;span class="math">\[P(X=x\mid T(X)=t,\,\theta )=P(X=x\mid T(X)=t)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>从上面这个式子中，我们两边同时乘以&lt;span class="math">\(P(\theta)\)&lt;/span>，有 &lt;span class="math">\[
P(X=x\mid T(X)=t,\,\theta)P(\theta)=P(X=x,\theta\mid T(X)=t)\\
=P(X=x\mid T(X)=t)P(\theta)
\]&lt;/span> 显然，有&lt;span class="math">\(P(X=x\mid T(X)=t),P(\theta)\)&lt;/span>二者独立。当我们已知&lt;span class="math">\(T(x)=t_0\)&lt;/span>以后，样本的联合概率函数中不含有&lt;span class="math">\(\theta\)&lt;/span>项，或者说&lt;span class="math">\(T(x)\)&lt;/span>完全包含了&lt;span class="math">\(\theta\)&lt;/span>对&lt;span class="math">\(X\)&lt;/span>有影响的信息。例子如下：&lt;/p>
&lt;p>设&lt;span class="math">\(x_1,x_2,\dotsb,x_n\)&lt;/span>是来自两点分布&lt;span class="math">\(b(1,p)\)&lt;/span>的一个样本，其中&lt;span class="math">\(p\in(0,1),n&amp;gt;2\)&lt;/span>，考察以下两个统计量是否为充分统计量： &lt;span class="math">\[
T_1=\sum_{i=1}^n x_i,\qquad T_2=x_1+x_2
\]&lt;/span> 首先，该样本的联合分布是： &lt;span class="math">\[
P(X_1=x_1,X_2=x_2,\dotsb,X_n=x_n)=p^{\sum_{i=1}^n x_i}(1-p)^{n-\sum_{i=1}^n x_i}
\]&lt;/span> 其中，&lt;span class="math">\(x_i\)&lt;/span>非0即1。统计量&lt;span class="math">\(T_1=\sum_{i=1}^n x_i\)&lt;/span>的分布为二项分布&lt;span class="math">\(b(n,p)\)&lt;/span>，即 &lt;span class="math">\[
P(T_1=t)={n\choose t}p^t(1-p)^{n-t},t=0,1,\dotsb,n
\]&lt;/span> 在给定&lt;span class="math">\(T_1=t\)&lt;/span>下，样本的条件分布为： &lt;span class="math">\[
\begin{aligned}
&amp;amp;P(X_1=x_1,X_2=x_2,\dotsb,X_n=x_n|T_1=t)\\
&amp;amp;=\frac{P(X_1=x_1,X_2=x_2,\dotsb,X_n=t-\sum_{i=1}^{n-1}x_i)}{P(T_1=t)}\\
&amp;amp;=\frac{p^t(1-p)^{n-t}}{{n\choose t}p^t(1-p)^{n-t}}\\
&amp;amp;=\frac{1}{{n\choose t}}
\end{aligned}
\]&lt;/span> 计算结果表明，这个条件分布与参数&lt;span class="math">\(p\)&lt;/span>无关，即他不含参数&lt;span class="math">\(p\)&lt;/span>的信息，这意味着样本中有关&lt;span class="math">\(p\)&lt;/span>的信息都含在统计量&lt;span class="math">\(T_1\)&lt;/span>中。&lt;/p>
&lt;p>在统计量&lt;span class="math">\(T_2=x_1+x_2\)&lt;/span>的分布为&lt;span class="math">\(b(2,p)\)&lt;/span>,在&lt;span class="math">\(T_2=t\)&lt;/span>下，样本的条件分布为： &lt;span class="math">\[
\begin{aligned}
&amp;amp;P(X_1=x_1,X_2=x_2,\dotsb,X_n=x_n|T_2=t)\\
&amp;amp;=\frac{P(X_1=x_1,X_2=t-x_1,\dotsb,X_n=x_n)}{P(T_2=t)}\\
&amp;amp;=\frac{p^{t+\sum_{i=3}^n x_i}(1-p)^{n-t-\sum_{i=3}^n x_i}}{{2\choose t}p^t(1-p)^{2-t}}\\
&amp;amp;=\frac{p^{\sum_{i=3}^n x_i}(1-p)^{n-2-\sum_{i=3}^n x_i}}{{2\choose t}}
\end{aligned}
\]&lt;/span> 显然此条件分布与参数&lt;span class="math">\(p\)&lt;/span>有关，即它还有参数&lt;span class="math">\(p\)&lt;/span>的信息，而样本中关于&lt;span class="math">\(p\)&lt;/span>的信息没有完全包含在统计量&lt;span class="math">\(T_2\)&lt;/span>中。&lt;/p>
&lt;p>可以设想为： &lt;span class="math">\[
\left\{样本\mathbf{x}中所含有有关\theta的信息\right\}=\\
\left\{t统计量T中含有有关\theta的信息\right\}+\left\{当T取值为t后，样本\mathbf{x}中还含有有关\theta的信息\right\}
\]&lt;/span> 如果后一项为0，那么统计量&lt;span class="math">\(T\)&lt;/span>即为充分统计量。&lt;/p>
&lt;p>我们再从统计量定义的角度反过来看，统计量是一组样本为自变量的函数。样本中包含了未知参数&lt;span class="math">\(\theta\)&lt;/span>部分信息的，&lt;span class="math">\(\theta\)&lt;/span>的统计量是将样本中的关于&lt;span class="math">\(\theta\)&lt;/span>的信息提取出来，如果我们构造这个函数（统计量）能够把样本中所有&lt;span class="math">\(\theta\)&lt;/span>信息都提取出来，那么对于估计未知参数&lt;span class="math">\(\theta\)&lt;/span>而言，样本和该关于&lt;span class="math">\(\theta\)&lt;/span>统计量效果是一样的，此时的统计量就是充分统计量。&lt;/p>
&lt;p>我们举个例子来说明充分统计量不损失有关&lt;span class="math">\(\theta\)&lt;/span>信息，比如你已经抽样的1000个数据全都写在了一张纸上，这些数据是给你写论文用的。突然有一天你的狗把你这张写满数据的纸吃掉了，这个时候假如你的数据满足指数分布，只有参数&lt;span class="math">\(\lambda\)&lt;/span>未知，且你已经提前把这些数据的的&lt;span class="math">\(\hat\lambda\)&lt;/span>算了出来，那你的狗也没坏了什么大事——因为这个充分统计量包含了这1000个数据的所有有用信息。你可以设计一个指数分布且&lt;span class="math">\(\lambda=\hat\lambda\)&lt;/span>的随机试验重新获得样本，这个新样本和过去的样本可能不完全一样，但它和老样本有相同的分布，当样本容量趋于无穷，新老两个样本应是等效的。&lt;/p>
&lt;blockquote>
&lt;p>从信息论的角度来说，就是&lt;span class="math">\(X,T(X)\)&lt;/span>中包含关于&lt;span class="math">\(\theta\)&lt;/span>的信息相同，即&lt;strong>互信息相同&lt;/strong>： &lt;span class="math">\[I{\bigl (}\theta ;T(X){\bigr )}=I(\theta ;X)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>根据&lt;strong>数据处理不等式&lt;/strong>，当我们仅对已知数据进行加工时，不可能获得额外的信息，只可能保持信息量不减，即&lt;span class="math">\(I(X;Y)≥I(X;f(Y))\)&lt;/span>，充分统计量就是在对数据&lt;span class="math">\(X\)&lt;/span>进行加工变成&lt;span class="math">\(T(X)\)&lt;/span>时，里面关于&lt;span class="math">\(\theta\)&lt;/span>的信息没有变化。&lt;/p>
&lt;p>此外，如果&lt;strong>另一个统计量&lt;span class="math">\(S(x)\)&lt;/span>和&lt;span class="math">\(T(x)\)&lt;/span>有一一对应关系&lt;/strong>，那么&lt;span class="math">\(S(x)\)&lt;/span>也是&lt;span class="math">\(\theta\)&lt;/span>的一个充分统计量。这种一一对应关系一般并不会导致信息的损失。&lt;/p>
&lt;h3 id="因子分解定理">因子分解定理&lt;/h3>
&lt;p>虽然我们说的很好听，但是实际情况下通过条件概率方式的验证并不是特别容易。因此我们给出一个叫作因子分解定理的东西。它的证明并不需要掌握，而其用法又很简单，所以这个计算的难度就大大降低了。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>因子分解定理（费希尔分解定理）&lt;/strong>：设总体概率函数（分布列或密度函数）为&lt;span class="math">\(f(x;\theta)\)&lt;/span>,&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>为样本，那么&lt;span class="math">\(T=T(X_1,X_2,\dotsb,X_n)\)&lt;/span>为充分统计量的&lt;strong>充要条件&lt;/strong>为：存在函数&lt;span class="math">\(g(t,\theta)\)&lt;/span>与&lt;span class="math">\(h(x_1,\dotsb,x_n)\)&lt;/span>使得对任意的&lt;span class="math">\(\theta\)&lt;/span>和任意一组的观测值&lt;span class="math">\(x_1,\dotsb,x_n\)&lt;/span>，都有 &lt;span class="math">\[f(x_1,\dotsb,x_n;\theta)=g(T(x_1,\dotsb,x_n),\theta)h(x_1,\dotsb,x_n)\]&lt;/span> 其中，&lt;span class="math">\(g(t,\theta)\)&lt;/span>只通过统计量&lt;span class="math">\(T(x_1,\dotsb,x_n)\)&lt;/span>的取值二依赖于样本。&lt;/p>
&lt;/blockquote>
&lt;p>这个定理把抽样的样本发生的联合概率密度分成两个部分，一是只和&lt;span class="math">\(t,\theta\)&lt;/span>相关的&lt;span class="math">\(g(t,\theta)\)&lt;/span>，其中&lt;span class="math">\(t\)&lt;/span>可以用样本&lt;span class="math">\(x_1,\dotsb,x_n\)&lt;/span>表示；另一个是只和&lt;span class="math">\(x_1,\dotsb,x_n\)&lt;/span>相关的&lt;span class="math">\(h(x_1,\dotsb,x_n)\)&lt;/span>。因子分解定理比条件概率的方法容易了许多。有一个一般性的情况是通常&lt;span class="math">\(h(x_1,\dotsb,x_n)=c\)&lt;/span>，即它通常是一个常数。然后把样本的概率函数用统计量&lt;span class="math">\(t\)&lt;/span>和&lt;span class="math">\(\theta\)&lt;/span>表示出来。&lt;/p>
&lt;h3 id="最小充分统计量">最小充分统计量&lt;/h3>
&lt;p>充分统计量做到了“用已知刻画未知”。那么更进一步的，我们当然希望充分统计量越简单，越精细越好。所以这其实就是极小充分统计量的定义。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>最小充分统计量&lt;/strong>：若一个充分统计量是任何其他充分统计量的函数，则称其是一个最小充分统计量。即，统计量&lt;span class="math">\(S(X)\)&lt;/span>是最小充分统计量当且仅当&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(S(X)\)&lt;/span>是充分统计量，&lt;/li>
&lt;li>如果&lt;span class="math">\(T(X)\)&lt;/span>是一个充分统计量，那么存在一个函数&lt;span class="math">\(f\)&lt;/span>使得&lt;span class="math">\(S(X)= f(T(X))\)&lt;/span>。&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>如果任何一个充分统计量&lt;span class="math">\(T\)&lt;/span>都可以通过加工得到&lt;span class="math">\(S\)&lt;/span>，这就说明&lt;span class="math">\(S\)&lt;/span>一定是更精细的（或者更严谨地说，一定不会变得更粗糙）充分统计量，而“任何一个”保证了最小性。我们也可以通过数据处理不等于来理解加工只可能减少信息量这一事实。一般而言，最小充分统计量不太好证出来，而且最小充分统计量也不是唯一的，只要与最小充分统计量有一一对应关系的统计量，都是最小充分统计量。&lt;/p>
&lt;p>可以证明：&lt;/p>
&lt;blockquote>
&lt;p>一个&lt;strong>充分完全统计量&lt;/strong>必是最小充分统计量。&lt;/p>
&lt;/blockquote>
&lt;p>下面我们来解释什么是完全统计量。&lt;/p>
&lt;h3 id="完全完备统计量">完全（完备）统计量&lt;/h3>
&lt;p>完全统计量，又称完备统计量。打个不大恰当的比方，完全统计量类似充分必要条件中的必要条件。我们首先定义一个辅助统计量来帮助理解完全统计量：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>辅助(ancillary)统计量&lt;/strong>:一个统计&lt;span class="math">\(V(x)\)&lt;/span>是辅助统计量，当且仅当其概率分布不依赖与任何未知量。&lt;/p>
&lt;/blockquote>
&lt;p>一个典型但没啥用的辅助统计量就是常数，即&lt;span class="math">\(V(x)\equiv C\)&lt;/span>。&lt;strong>常数是不含有任何概率分布信息的&lt;/strong>。再举一个典型例子（位移族）：&lt;/p>
&lt;blockquote>
&lt;p>已知某一概率密度函数&lt;span class="math">\(f(x)\)&lt;/span>，对其沿&lt;span class="math">\(x\)&lt;/span>轴向右平移一段未知距离&lt;span class="math">\(\mu\)&lt;/span>，则新的pdf为&lt;span class="math">\(f(x-\mu)\)&lt;/span>，现在从平移后的pdf中抽样得到i.i.d的样本&lt;span class="math">\(x_1,x_2,\dotsb,x_n\)&lt;/span>，我们想从样本估计参数&lt;span class="math">\(\mu\)&lt;/span>。如果我们构造一个统计量&lt;span class="math">\(V(x)=x_{(k)}-x_{(l)}\)&lt;/span>，其中&lt;span class="math">\(x_{(k)},x_{(l)}\)&lt;/span>是次序统计量，那么&lt;span class="math">\(V(x)\)&lt;/span>的pdf只会和&lt;span class="math">\(f(x)\)&lt;/span>有关，与&lt;span class="math">\(\mu\)&lt;/span>无关，即为辅助统计量。&lt;/p>
&lt;/blockquote>
&lt;p>这是一个过于严格的要求，下面再给出一个放松的定义：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>一阶辅助统计量&lt;/strong>：一个统计&lt;span class="math">\(V(x)\)&lt;/span>是一阶辅助统计量，当且仅当其期望&lt;span class="math">\(E[V(x)]\)&lt;/span>不依赖与任何未知量。&lt;/p>
&lt;/blockquote>
&lt;p>显然，辅助统计量必然是一阶辅助统计量。需要指出，ancillary一词的翻译辅助，有附加、附属的意思，不一定是指辅助做了什么事。&lt;/p>
&lt;p>如果对一个统计量&lt;span class="math">\(T\)&lt;/span>能找到(存在)一个非平凡（不是常函数）的辅助统计量&lt;span class="math">\(V\)&lt;/span>，使得&lt;span class="math">\(V(T(x))\neq C\)&lt;/span>，也就是说&lt;span class="math">\(V(x&amp;#39;)\)&lt;/span>可以从统计量&lt;span class="math">\(T(x)\)&lt;/span>中再提取一些和带估计参数&lt;span class="math">\(\theta\)&lt;/span>无关的信息出来。那么说明，统计量&lt;span class="math">\(T(x)\)&lt;/span>中除了含有&lt;span class="math">\(\theta\)&lt;/span>的信息之外，还有其他冗余的信息，可以进一步压缩，&lt;strong>此时我们就不能称&lt;span class="math">\(T\)&lt;/span>是完全统计量&lt;/strong>。完全统计量是纯粹性的体现，意味着不包含其他与参数估计无关的信息。统计量可以是完全但不充分的，这意味再样本信息压缩过程中，不仅仅把和待估计参数&lt;span class="math">\(\theta\)&lt;/span>无关的信息剔除掉，甚至与&lt;span class="math">\(\theta\)&lt;/span>有关的信息也可能去掉了一些，属于“有损压缩”。由此，我们给出完全统计量定义：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>完全（完备）统计量&lt;/strong>：设&lt;span class="math">\(\mathcal{F}=\{f(x,\theta),\theta\in\varTheta\}\)&lt;/span>为一分布族，&lt;span class="math">\(\varTheta\)&lt;/span>是参数空间，设统计量&lt;span class="math">\(T=T(x)\)&lt;/span>，若对任何辅助统计量&lt;span class="math">\(\varphi(T(x))\)&lt;/span>，当 &lt;span class="math">\[E[\varphi(T(x))]=0\]&lt;/span> 都有 &lt;span class="math">\[P(\varphi(T(x))=0)=1\]&lt;/span> 则称&lt;span class="math">\(T(x)\)&lt;/span>是一完全（完备）统计量。&lt;/p>
&lt;/blockquote>
&lt;p>注意，辅助统计量是和待估计参数&lt;span class="math">\(\theta\)&lt;/span>无关的随机数，因此也可以说是对&lt;span class="math">\(\forall \theta\in\varTheta\)&lt;/span>都成立。&lt;span class="math">\(P(\varphi(T(x))=0)=1\)&lt;/span>表示以1的概率为0，其实等于其他常数也无所谓，只有将常数移到另一边就可以了，都不含分布信息，效果一样的。简单的说，如果以统计量&lt;span class="math">\(T\)&lt;/span>为自变量的辅助统计量的期望为0，那么它就是常数0。&lt;/p>
&lt;p>关于统计量的充分性、完全性还可以参考网页资料文件加中的&lt;a href="../网页资料/概率论数理统计随机过程-关于充分完全统计量的一点儿思考.html">《概率论数理统计随机过程-关于充分完全统计量的一点儿思考.html》&lt;/a>一文。&lt;/p>
&lt;p>有函数处理的信息不增特性可知，若&lt;span class="math">\(T(x)\)&lt;/span>是完全统计量，则它任一函数&lt;span class="math">\(\delta(T)\)&lt;/span>也是完全统计量。&lt;/p>
&lt;h4 id="指数族中统计量的完全性">指数族中统计量的完全性&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>定理&lt;/strong>：设样本&lt;span class="math">\(x=(x_1,x_2,\dotsb,x_n)\)&lt;/span>的概率函数 &lt;span class="math">\[f(x,\theta)=h(x)c(\theta)\exp\left\{\sum_{j=1}^k \theta_j T_j(x)\right\},\theta=(\theta_1,\dotsb,\theta_k)\in\varTheta^\star\]&lt;/span> 为指数族的自然形式。令&lt;span class="math">\(T(x)=(T_1(x),\dotsb,T_k(x))\)&lt;/span>，且自然参数空间&lt;span class="math">\(\varTheta^\star\)&lt;/span>作为&lt;span class="math">\(R_k\)&lt;/span>的自己有内点，则&lt;span class="math">\(T(x)\)&lt;/span>是完全统计量。&lt;/p>
&lt;/blockquote>
&lt;p>首先，该定理一个充分条件，只适用于判别指数族的完全性。&lt;span class="math">\(\theta_j\)&lt;/span>是可以是其他参数的函数，其结果作为&lt;span class="math">\(\varTheta^\star\)&lt;/span>的空间一维。&lt;/p>
&lt;p>其次，有内点意味着&lt;span class="math">\(\varTheta^\star\)&lt;/span>不可以是K维空间的超平面，即&lt;span class="math">\(\theta=(\theta_1,\dotsb,\theta_k)\)&lt;/span>是满秩的。&lt;/p>
&lt;p>最后，即使没有内点，统计量也不一定是不完全的，需要用定义或其他方法进一步判别。&lt;/p>
&lt;h4 id="最小充分统计量与完备性关系">最小充分统计量与完备性关系&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>定理&lt;/strong>：如果最小充分统计量存在，那么任何充分完全统计量都是最小充分统计量。&lt;/p>
&lt;/blockquote>
&lt;p>这是一个充分条件，而非充要条件。最小充分统计量可能是不完全的，即其可能包含无法分离出去的冗余内容。&lt;/p>
&lt;h4 id="有界完全统计量及其性质">有界完全统计量及其性质&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>有界完全（完备）统计量&lt;/strong>：设&lt;span class="math">\(\mathcal{F}=\{f(x,\theta),\theta\in\varTheta\}\)&lt;/span>为一分布族，&lt;span class="math">\(\varTheta\)&lt;/span>是参数空间，设统计量&lt;span class="math">\(T=T(x)\)&lt;/span>，若对任何&lt;strong>有界或a.s.有界&lt;/strong>辅助统计量&lt;span class="math">\(\varphi(T(x))\)&lt;/span>，当 &lt;span class="math">\[E[\varphi(T(x))]=0\]&lt;/span> 都有 &lt;span class="math">\[P(\varphi(T(x))=0)=1\]&lt;/span> 则称&lt;span class="math">\(T(x)\)&lt;/span>是一有界完全（完备）统计量。&lt;/p>
&lt;/blockquote>
&lt;p>有界完全统计量是对完全统计量的放松，只是给辅助统计量的函数&lt;strong>添加了有界这一条件&lt;/strong>。因此，一个完全统计量（严要求）必为有界完全统计量（松要求）。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Basu定理&lt;/strong>：&lt;span class="math">\(x\)&lt;/span>是分布中抽取的一个样本，如果&lt;span class="math">\(T(x)\)&lt;/span>是一个&lt;strong>充分，且有界完全&lt;/strong>统计量，那么&lt;span class="math">\(T(x)\)&lt;/span>独立于所有的辅助统计量。&lt;/p>
&lt;/blockquote>
&lt;h2 id="常用概率分布与特征数表">常用概率分布与特征数表&lt;/h2>
&lt;div class="figure">
&lt;img src="../images/常用概率分布族.png" alt="常用概率分布族" />&lt;p class="caption">常用概率分布族&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之母函数特征函数矩母函数</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%AF%8D%E5%87%BD%E6%95%B0%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0%E7%9F%A9%E6%AF%8D%E5%87%BD%E6%95%B0/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%AF%8D%E5%87%BD%E6%95%B0%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0%E7%9F%A9%E6%AF%8D%E5%87%BD%E6%95%B0/</guid><description>
&lt;h2 id="概率统计随机过程之母函数特征函数矩母函数">概率统计随机过程之母函数特征函数矩母函数&lt;!-- omit in toc -->&lt;/h2>
&lt;p>为什么要引入母函数、特征函数（矩母函数）？因为它们是处理概率论问题的有力工具。它们能把寻求独立随机变量法和的分布的卷积运算（积分运算）转换成函数的乘法运算，还能把求分布的各阶原点矩运算变成函数的微分运算，特别的，它能把寻求随机变量序列的极限分布转换成一般的函数极限问题。为概率论提供了数学分析方面的强大武器。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#母函数只适用于非负整数离散随机变量">母函数（只适用于非负整数离散随机变量）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常见非负离散分布的母函数">常见非负离散分布的母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#离散随机变量和的分布与母函数关系">离散随机变量和的分布与母函数关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#母函数与数字特征关系">母函数与数字特征关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数">特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几种常见分布的特征函数">几种常见分布的特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数性质">特征函数性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数唯一决定分布函数">特征函数唯一决定分布函数&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="母函数只适用于非负整数离散随机变量">母函数（只适用于非负整数离散随机变量）&lt;/h2>
&lt;p>为何要定义概率母函数？概率论起步的时候由于其研究不确定性的特点，难以找到系统的方法，&lt;strong>设计一个包含某一随机变量所有信息的函数使其具有解析性&lt;/strong>便成为处理概率问题的一种方法。&lt;/p>
&lt;blockquote>
&lt;p>定义：非负整值随机变量的母函数：一个离散随机变量的概率母函数是指该随机变量的概率质量函数的幂级数表达式。&lt;/p>
&lt;p>单变量情形&lt;/p>
&lt;p>如果&lt;span class="math">\(X\)&lt;/span>是在非负整数域&lt;span class="math">\(\{0,1, ...\}\)&lt;/span>上取值的离散随机变量,那么&lt;span class="math">\(X\)&lt;/span>的概率母函数定义为 &lt;span class="math">\[G(z)=\operatorname {E} (z^{X})=\sum _{x=0}^{\infty }p(x)z^{x},\]&lt;/span> 其中&lt;span class="math">\(p\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的概率质量函数。&lt;/p>
&lt;p>多变量情形&lt;/p>
&lt;p>如果&lt;span class="math">\(X = (X1,...,Xd )\)&lt;/span>是在&lt;span class="math">\(d-\)&lt;/span>非负整数格&lt;span class="math">\(\{0,1, ...\}^d\)&lt;/span>上取值的离散随机变量, 那么&lt;span class="math">\(X\)&lt;/span>的概率母函数定义为 &lt;span class="math">\[G(z)=G(z_{1},\ldots ,z_{d})=\operatorname {E} {\bigl (}z_{1}^{X_{1}}\cdots z_{d}^{X_{d}}{\bigr )}=\\
\sum _{x_{1},\ldots ,x_{d}=0}^{\infty }p(x_{1},\ldots ,x_{d})z_{1}^{x_{1}}\cdots z_{d}^{x_{d}},\]&lt;/span> 其中&lt;span class="math">\(p\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的概率质量函数。&lt;/p>
&lt;/blockquote>
&lt;p>由于&lt;span class="math">\(\forall p(x)，有0≤p(x)≤1，\sum p(x)=1\)&lt;/span>。所以概率母函数的收敛半径≥1。引进母函数的好处是它有很好的分析性质，而一旦知道了&lt;span class="math">\(X\)&lt;/span>的母函数，那么&lt;span class="math">\(X\)&lt;/span>的分布列可以通过下式获得： &lt;span class="math">\[p_k=\frac{g^{(k)}(0)}{k!}，k=0,1,2,\dotsb\]&lt;/span> 分布列和母函数的项是&lt;strong>一一对应的&lt;/strong>。&lt;/p>
&lt;h3 id="常见非负离散分布的母函数">常见非负离散分布的母函数&lt;/h3>
&lt;p>注：以下各式中&lt;span class="math">\(p+q=1，P(X=0)=q\)&lt;/span>&lt;/p>
&lt;ul>
&lt;li>伯努利分布(0-1分布)：&lt;span class="math">\(X\sim B(1,p)\)&lt;/span> &lt;span class="math">\[g(z)=q+pz\]&lt;/span>&lt;/li>
&lt;li>二项分布：&lt;span class="math">\(X\sim B(n,p)\)&lt;/span> &lt;span class="math">\[g(z)=\sum_{k=0}^∞ C_n^kp^kq^{n-k}z^k\\
=(q+pz)^n\]&lt;/span> 0-1分布和二项分布的关系也体现出独立同分布概率联合概率的关系。&lt;/li>
&lt;li>泊松分布：&lt;span class="math">\(X\sim P(\lambda)\)&lt;/span> &lt;span class="math">\[g(z)=\sum_{k=0}^∞\frac{\lambda^k}{k!}e^{-\lambda}z^k\\
=e^{\lambda(z-1)}\]&lt;/span>&lt;/li>
&lt;li>几何分布：：&lt;span class="math">\(X\sim Geo(p)\)&lt;/span> &lt;span class="math">\[g(z)=\sum_{k=1}^∞ q^{k-1}p z^k=pz\sum_{k=1}^∞ q^{k-1}z^{k-1}\\
\because -1&amp;lt;qz&amp;lt;1\\
=pz\times \frac{1}{1-qz}=\frac{pz}{1-qz}\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="离散随机变量和的分布与母函数关系">离散随机变量和的分布与母函数关系&lt;/h3>
&lt;blockquote>
&lt;p>定理：设非负整值随机变量&lt;span class="math">\(X_1,X_2,\dots,X_n\)&lt;/span>相互独立，而&lt;span class="math">\(g_1,g_2,\dots,g_n\)&lt;/span>分别是他们的母函数，那么&lt;span class="math">\(Y=\sum\limits_1^n X_k\)&lt;/span>的母函数为 &lt;span class="math">\[g(z)=g_1(z)g_2(z)\dotsb g_n(z)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h3 id="母函数与数字特征关系">母函数与数字特征关系&lt;/h3>
&lt;p>期望：&lt;span class="math">\(E(X)=g&amp;#39;(1)=\sum\limits_{k=1}^∞ kp_kz^{k-1}|_{z=1}\)&lt;/span>。 相仿的，&lt;span class="math">\(E(z^X)=\sum\limits_{k=0}^∞ z^kp_k=g(z)\)&lt;/span>。从这里，我们可以看出&lt;strong>母函数实际上是&lt;span class="math">\(z^X\)&lt;/span>的期望。&lt;/strong>&lt;/p>
&lt;p>方差：&lt;span class="math">\(Var(x)=g&amp;#39;&amp;#39;(1)+g&amp;#39;(1)-g&amp;#39;(1)^2\)&lt;/span>&lt;/p>
&lt;h2 id="特征函数">特征函数&lt;/h2>
&lt;p>母函数为我们处理概率提供了数学分析的角度与方法，极大方便了概率的处理，但是并不是所有随机变量都是有母函数的（只有离散的非负整数随机变量才有母函数），对于一般的随机变量是否具有类似的东西呢？&lt;strong>这就是特征函数&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>定义：对任一随机变量&lt;span class="math">\(X\)&lt;/span>，称 &lt;span class="math">\[\varphi_X(t)=E(e^{itX})=\int_{-∞}^∞ e^{itx}p(x) \mathrm{d}x，-∞&amp;lt;t&amp;lt;+∞\]&lt;/span> 为随机变量&lt;span class="math">\(X\)&lt;/span>的特征函数。&lt;/p>
&lt;/blockquote>
&lt;p>说明：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>规定&lt;span class="math">\(E(\xi+i\eta)=E(\xi)+iE(\eta)\Rightarrow E(e^{itX})=E(\cos tX)+iE(\sin tX)\)&lt;/span>&lt;/li>
&lt;li>由于&lt;span class="math">\(|e^{itX}|=1\)&lt;/span>，所以对任一随机变量都有特征函数。对于离散随机变量，其为求和形式；对于连续随机变量为积分形式。&lt;/li>
&lt;li>特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>都是实变复值的。&lt;/li>
&lt;li>&lt;span class="math">\(\varphi(0)=1\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;h3 id="几种常见分布的特征函数">几种常见分布的特征函数&lt;/h3>
&lt;p>注：以下各式中&lt;span class="math">\(p+q=1，P(X=0)=q\)&lt;/span>&lt;/p>
&lt;ul>
&lt;li>伯努利分布(0-1分布)：&lt;span class="math">\(X\sim B(1,p)\)&lt;/span> &lt;span class="math">\[\varphi(t)=q+pe^{it}\]&lt;/span>&lt;/li>
&lt;li>二项分布：&lt;span class="math">\(X\sim B(n,p)\)&lt;/span>，0-1分布和二项分布的关系也体现出独立同分布概率联合概率的关系。 &lt;span class="math">\[\varphi(t)=(q+pe^{it})^n\]&lt;/span>&lt;/li>
&lt;li>泊松分布：&lt;span class="math">\(X\sim P(\lambda)\)&lt;/span> &lt;span class="math">\[\varphi(t)=\sum_{k=0}^∞\frac{\lambda^k}{k!}e^{-\lambda}e^{\lambda e^{it}}\\
=e^{\lambda(e^{it}-1)}\]&lt;/span>&lt;/li>
&lt;li>几何分布：：&lt;span class="math">\(X\sim Geo(p)\)&lt;/span> &lt;span class="math">\[\varphi(t)=\frac{pe^{it}}{1-qe^{it}}\]&lt;/span>&lt;/li>
&lt;li>均匀分布：&lt;span class="math">\(X \sim U(a,b)\)&lt;/span> &lt;span class="math">\[\varphi(t)=\frac{e^{ibt}-e^{iat}}{i(b−a)t}\]&lt;/span>&lt;/li>
&lt;li>正态分布：&lt;span class="math">\(X\sim N(\mu,\sigma^2)\)&lt;/span> &lt;span class="math">\[\varphi(t)=e^{iut-\frac{\sigma^2t^2}{2}}\]&lt;/span>&lt;/li>
&lt;li>指数分布：&lt;span class="math">\(X \sim \exp(\lambda)\)&lt;/span> &lt;span class="math">\[\varphi(t)=(1-\frac{it}{\lambda})^{-1}\]&lt;/span>&lt;/li>
&lt;li>gamma分布：&lt;span class="math">\(X \sim Ga(\alpha,\lambda)\)&lt;/span> &lt;span class="math">\[\varphi(t)=(1-\frac{it}{\lambda})^{-\alpha}\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>其他分布的特征函数可见茆诗松《概率论与数理统计教程 第二版》P219&lt;/p>
&lt;h3 id="特征函数性质">特征函数性质&lt;/h3>
&lt;blockquote>
&lt;p>性质1：&lt;span class="math">\(|\varphi(t)|\leq \varphi(0)=1\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明： &lt;span class="math">\[
|\varphi(t)|=|\int_{-∞}^∞ e^{itx}p(x) \mathrm{d}x|\overset{\text{柯西不等式}}{\le}\int_{-∞}^∞ |e^{itx}|p(x) \mathrm{d}x\\
|e^{itx}|=\sqrt{\cos^2(tx)+\sin^2(tx)}=1=e^{ix\cdot 0}\\
\Rightarrow|\varphi(t)|=\int_{-∞}^∞ 1\cdot p(x) \mathrm{d}x=\varphi(0)=1
\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>性质2：&lt;span class="math">\(\varphi(-t)=\overline{\varphi(t)}\)&lt;/span>，其中&lt;span class="math">\(\overline{\varphi(t)}\)&lt;/span>表示&lt;span class="math">\(\varphi(t)\)&lt;/span>的复共轭。&lt;/p>
&lt;/blockquote>
&lt;p>证明： &lt;span class="math">\[\varphi(-t)=\int_{-∞}^∞ e^{-itx}p(x) \mathrm{d}x=\int_{-∞}^∞ \overline{e^{itx}}p(x) \mathrm{d}x\]&lt;/span> 由于&lt;span class="math">\(p(x)\)&lt;/span>是非负实数，不影响虚数，所以&lt;span class="math">\(\int_{-∞}^∞ \overline{e^{itx}}p(x) \mathrm{d}x=\overline{\int_{-∞}^∞ e^{itx}p(x)}\mathrm{d}x=\overline{\varphi(t)}\)&lt;/span>，所以&lt;span class="math">\(\varphi(-t)=\overline{\varphi(t)}\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>性质3：&lt;span class="math">\(X\)&lt;/span>的特征函数为&lt;span class="math">\(\varphi(t)\)&lt;/span>，则&lt;span class="math">\(Y=aX+b\)&lt;/span>的特征函数为&lt;span class="math">\(e^{itb}\varphi(at)\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>证明： &lt;span class="math">\[\varphi_Y(t)=E(e^{itY})=E(e^{it(aX+b)})=E(e^{itaX})\cdot e^{itb}\]&lt;/span> 如果我们将&lt;span class="math">\(ta\)&lt;/span>作为变量整体，则有&lt;span class="math">\(E(e^{itaX})=\varphi(at)\)&lt;/span>，综上所述有&lt;span class="math">\(\varphi_Y(t)=e^{itb}\varphi(at)\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>性质4：独立随机变量的和的特征函数为每个随机变量的特征函数的积，即设&lt;span class="math">\(X_1,X_2,\dotsb X_n\)&lt;/span>相互独立，则&lt;span class="math">\(Y=\sum_{k=1}^n X_k\)&lt;/span>有 &lt;span class="math">\[\varphi_{Y}(t)=\prod_{k=1}^n \varphi_{X_k}(t)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：因为&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>相互独立，所以其随机变量的函数&lt;span class="math">\(E(e^{itX_1}),E(e^{itX_2}),\dotsb,E(e^{itX_n})\)&lt;/span>也是相互独立的，从而有 &lt;span class="math">\[\begin{aligned}
\varphi_{Y}(t)&amp;amp;=E(e^{itY})=E(e^{it\sum_{k=1}^n X_i})=E(\prod_{k=1}^n e^{itX_k})\\
&amp;amp;\overset{\text{独立性}}{=}\prod_{k=1}^n E(e^{itX_k})=\prod_{k=1}^n \varphi_{X_k}(t)
\end{aligned}\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>性质5：若&lt;span class="math">\(E(X^l)\)&lt;/span>存在，则&lt;span class="math">\(X\)&lt;/span>的特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>可&lt;span class="math">\(l\)&lt;/span>次求导，且对&lt;span class="math">\(1\leq k \leq l\)&lt;/span>，有 &lt;span class="math">\[\varphi^{(k)}(0)=i^kE(X^k)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明可以从特征函数的积分式进行微分即可。从性质5我们也可以用以下方式求期望和方差： &lt;span class="math">\[
E(X)=\frac{\varphi&amp;#39;(0)}{i},\text{Var}(X)=-\varphi&amp;#39;&amp;#39;(0)+(\varphi&amp;#39;(0))^2
\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>性质6：一致连续性。随机变量&lt;span class="math">\(X\)&lt;/span>的特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>在&lt;span class="math">\((-\infty,\infty)\)&lt;/span>上一致连续。&lt;/p>
&lt;/blockquote>
&lt;p>证明略。&lt;/p>
&lt;blockquote>
&lt;p>性质7：非负定性。随机变量&lt;span class="math">\(X\)&lt;/span>的特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>是非负定的，即对任意正整数&lt;span class="math">\(n\)&lt;/span>及&lt;span class="math">\(n\)&lt;/span>个实数&lt;span class="math">\(t_1,t_2,\dotsb,t_n\)&lt;/span>和&lt;span class="math">\(n\)&lt;/span>个复数&lt;span class="math">\(z_1,z_2,\dotsb,z_n\)&lt;/span>，有 &lt;span class="math">\[\sum_{k=1}^n\sum_{j=1}^n \varphi(t_k-t_j)z_k\bar{z_j}\geq 0\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明略。&lt;/p>
&lt;h3 id="特征函数唯一决定分布函数">特征函数唯一决定分布函数&lt;/h3>
&lt;p>由特征函数的定义可知，随机变量的分布唯一地确定了它的特征函数。我们也可以同样推出特征函数完全决定了分布，也就是说，&lt;strong>两个分布函数相等当且仅当它们所对应的特征函数相等&lt;/strong>。我们不加具体证明给出如下定理：&lt;/p>
&lt;blockquote>
&lt;p>定理：随机变量的分布函数由其特征函数&lt;strong>唯一决定&lt;/strong>。且当&lt;span class="math">\(X\)&lt;/span>为连续随机变量，其密度函数为&lt;span class="math">\(p(x)\)&lt;/span>，特征函数为&lt;span class="math">\(\varphi(t)\)&lt;/span>，如果&lt;span class="math">\(\int_{-\infty}^\infty |\varphi(t)|\mathrm{d}t&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\[p(x)=\frac{1}{2\pi} \int_{-\infty}^\infty e^{itx}\varphi(t)\mathrm{d}t\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>我们可以称由分布转换为特征函数的过程为傅里叶变换，由特征函数转换成分布的过程为傅里叶逆变换。需要指出：这里定义的傅里叶正逆变换和我们通常在通信、复变函数中定义的傅里叶变换&lt;strong>在&lt;span class="math">\(e\)&lt;/span>的指数上相差了一个负号&lt;/strong>。&lt;/p>
&lt;p>由于分布函数和特征函数的一一对应关系，我们可以把随机变量序列的收敛问题和函数的收敛问题联系起来，从而有以下定理：&lt;/p>
&lt;blockquote>
&lt;p>定理：分布函数序列&lt;span class="math">\(\{F_n(x)\}\)&lt;/span>弱收敛于分布函数&lt;span class="math">\(F(x)\)&lt;/span>的充要条件是&lt;span class="math">\(\{F_n(x)\}\)&lt;/span>的特征函数序列&lt;span class="math">\(\{\varphi_n(t)\}\)&lt;/span>收敛于&lt;span class="math">\(F(x)\)&lt;/span>的特征函数&lt;span class="math">\(\varphi(t)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>例题： &lt;img src="../images/特征函数列收敛.png" alt="特征函数列收敛" />&lt;/p></description></item><item><title>概率统计随机过程核心之大数定理和中心极限定理</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E6%A0%B8%E5%BF%83%E4%B9%8B%E5%A4%A7%E6%95%B0%E5%AE%9A%E7%90%86%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E6%A0%B8%E5%BF%83%E4%B9%8B%E5%A4%A7%E6%95%B0%E5%AE%9A%E7%90%86%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</guid><description>
&lt;h2 id="概率统计随机过程核心大数定理和中心极限定理">概率统计随机过程核心大数定理和中心极限定理&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#随机变量序列的收敛性">随机变量序列的收敛性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#依分布收敛">依分布收敛&lt;/a>&lt;/li>
&lt;li>&lt;a href="#依概率收敛">依概率收敛&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几乎处处收敛依概率1收敛">几乎处处收敛（依概率1收敛）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#收敛性之间的关系">收敛性之间的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大数定理">大数定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#弱大数定律及其几种形式">弱大数定律及其几种形式&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#切比雪夫不等于相关的大数定律">切比雪夫不等于相关的大数定律&lt;/a>&lt;/li>
&lt;li>&lt;a href="#用特征函数证明的大数定律">用特征函数证明的大数定律&lt;/a>&lt;/li>
&lt;li>&lt;a href="#八个弱大数定律的比较">八个弱大数定律的比较&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#强大数定律">强大数定律&lt;/a>&lt;/li>
&lt;li>&lt;a href="#中心极限定理">中心极限定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#独立同分布条件下的中心极限定理">独立同分布条件下的中心极限定理&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#棣莫佛拉普拉斯定理">棣莫佛－拉普拉斯定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#林德伯格列维中心极限定理">林德伯格－列维中心极限定理&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#非独立同分布条件下的中心极限定理">非独立同分布条件下的中心极限定理&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#林德伯格-费勒定理">林德伯格-费勒定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#李雅普诺夫中心极限定理">李雅普诺夫中心极限定理&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#弱大数定理和强大数定理的区别">弱大数定理和强大数定理的区别&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="随机变量序列的收敛性">随机变量序列的收敛性&lt;/h2>
&lt;h3 id="依分布收敛">依分布收敛&lt;/h3>
&lt;blockquote>
&lt;p>定义1：设随机变量序列&lt;span class="math">\(\{X_n\}\)&lt;/span>的分布函数列是&lt;span class="math">\(\{F_n(X)\}\)&lt;/span>，&lt;span class="math">\(X\)&lt;/span>的分布函数是&lt;span class="math">\(F(X)\)&lt;/span>，若在&lt;span class="math">\(F(X)\)&lt;/span>的每一个连续点都成立&lt;span class="math">\(\lim_{n→\infty}F_n(X)=F(X)\)&lt;/span>，则称&lt;span class="math">\(F_n(X)\overset{W}{\longrightarrow} F(X)\)&lt;/span>，或&lt;span class="math">\(X_n\overset{L/D}{\longrightarrow} X\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>其中&lt;span class="math">\(W,\text{Weak}\)&lt;/span>表示弱收敛，用于分布函数列；&lt;span class="math">\(L,\text{convergence in law}; D,\text{Distribution}\)&lt;/span>表示依分布收敛，用于随机变量序列。依分布收敛的本质是函数列的收敛，而非随机变量的收敛，因此我们要重点关注的是分布“函数”，在函数列弱收敛的“弱”则体现在函数不是所有点的点点收敛，而是只有在连续点才收敛。在这个定义中，我们需要注意几个点：&lt;/p>
&lt;p>（1）&lt;span class="math">\(F(x)\)&lt;/span>要求的是所有连续点，并非所有点。&lt;/p>
&lt;p>例1：一个离散随机变量序列&lt;span class="math">\(X_n\)&lt;/span>分布列如下所示： |&lt;span class="math">\(X_n\)&lt;/span>|&lt;span class="math">\(\frac{1}{n}\)&lt;/span>|&lt;span class="math">\(\overset{n→\infty}{\longrightarrow}\)&lt;/span>|&lt;span class="math">\(X\)&lt;/span>| 0 | |:---:|:-----------:|:-----------------------------------:|:-:|:-:| | P | 1 |&lt;span class="math">\({\longrightarrow}\)&lt;/span>|&lt;span class="math">\(P\)&lt;/span>| 1 | &lt;span class="math">\(X_n\)&lt;/span>依分布收敛到&lt;span class="math">\(X\)&lt;/span>，但是分布函数&lt;span class="math">\(F(x)\)&lt;/span>在间断点&lt;span class="math">\(x=0\)&lt;/span>显然是不收敛的。&lt;/p>
&lt;p>（2）只有&lt;span class="math">\(F(x)\)&lt;/span>是一个分布函数的时候，而不是任意函数，才能说是依分布收敛。&lt;/p>
&lt;h3 id="依概率收敛">依概率收敛&lt;/h3>
&lt;blockquote>
&lt;p>定义2：设&lt;span class="math">\(\{X_n\}\)&lt;/span>为一随机变量序列，&lt;span class="math">\(X\)&lt;/span>为一随机变量，如果对任意的&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>，有 &lt;span class="math">\[\lim_{n→∞}P({|X_n-X|&amp;lt;\varepsilon})=1\]&lt;/span> 则称&lt;span class="math">\(\{X_n\}\)&lt;/span>依概率收敛于&lt;span class="math">\(X\)&lt;/span>，记作&lt;span class="math">\(X_n\overset{P}{\longrightarrow}X\)&lt;/span>。&lt;span class="math">\(P\)&lt;/span>表示Probability，概率。&lt;/p>
&lt;/blockquote>
&lt;p>这个定义也有几个注意事项：&lt;/p>
&lt;p>（1）与数列极限&lt;span class="math">\(\{a_n\}\)&lt;/span>的区别。数列极限的收敛比较好理解，就是逐渐逼近某个点。比如下右图中&lt;span class="math">\(y=(\frac{\sin x}{x})^2\)&lt;/span>所示，点序列含逐渐趋向于0。而依概率收敛，是指偏离收敛目标的概率趋于0，以下左图为例，Gamma分布的概率密度函数随&lt;span class="math">\(\beta\)&lt;/span>值减小，逐渐集中到0附近，也就是说取值大于&lt;span class="math">\(0+\varepsilon\)&lt;/span>的概率会越来越小并趋于0，但是仍然有取到一个远大于0的值的可能性。所以依概率收敛是从概率密度/质量函数的角度理解的。 &lt;img src="../images/依概率收敛与数列收敛区别.png" alt="依概率收敛与数列收敛区别.png" />&lt;/p>
&lt;p>（2）&lt;span class="math">\(X\)&lt;/span>既可以是随机变量，也可以是一常数（退化分布）。（但是，我遇到的场景基本上都是常数）。&lt;/p>
&lt;p>依概率收敛的等价形式：&lt;/p>
&lt;ul>
&lt;li>若&lt;span class="math">\(\lim_{n→∞} E(X_n) = c\)&lt;/span>，且&lt;span class="math">\(\lim_{n→∞} \text{var}(X_n) = 0\)&lt;/span>，则&lt;span class="math">\(X_n\overset{P}{\longrightarrow}c\)&lt;/span>。(可用切比雪夫不等于证明)&lt;/li>
&lt;/ul>
&lt;p>依概率收敛的性质：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>四则运算。设&lt;span class="math">\({X_n},{Y_n}\)&lt;/span>是两个随机变量序列，&lt;span class="math">\(X,Y\)&lt;/span>是两个随机变量（常数也可以），如果&lt;span class="math">\(X_n\overset{P}{\longrightarrow}X,Y_n\overset{P}{\longrightarrow}Y\)&lt;/span> &lt;span class="math">\[1)~~ X_n\pm Y_n \overset{P}{\longrightarrow} X\pm Y\\
2)~~ X_n\times Y_n \overset{P}{\longrightarrow} X\times Y\\
3)~~ X_n\div Y_n \overset{P}{\longrightarrow} X\div Y\]&lt;/span>&lt;/li>
&lt;li>适用于函数。如果&lt;span class="math">\(X_n\overset{P}{\longrightarrow}X,g(x)\)&lt;/span>是直线上的连续函数，则：&lt;span class="math">\(g(X_n)\overset{P}{\longrightarrow}g(X)\)&lt;/span>。&lt;/li>
&lt;li>依概率收敛与依分布收敛的关系。依概率收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>依分布收敛；当二者收敛到同一常数时，有依概率收敛&lt;span class="math">\(\overset{P}{\longrightarrow} C\Leftrightarrow \)&lt;/span>依分布收敛&lt;span class="math">\(\overset{P}{\longrightarrow} C\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;h3 id="几乎处处收敛依概率1收敛">几乎处处收敛（依概率1收敛）&lt;/h3>
&lt;blockquote>
&lt;p>定义3：设&lt;span class="math">\(\{X_n\}\)&lt;/span>为一随机变量序列，&lt;span class="math">\(X\)&lt;/span>为一随机变量，如果有 &lt;span class="math">\[\lim_{n→∞}P({X_n=X})=1\]&lt;/span> 则称&lt;span class="math">\(\{X_n\}\)&lt;/span>几乎处处（依概率1）收敛于&lt;span class="math">\(X\)&lt;/span>，记作&lt;span class="math">\(X_n\overset{a.s/a.e}{\longrightarrow}X\)&lt;/span>。&lt;span class="math">\(a.s/a.e\)&lt;/span>表示almost surely 或 almost everywhere，几乎处处。&lt;/p>
&lt;/blockquote>
&lt;p>显然，几乎处处收敛（依概率1收敛）的收敛性比依概率收敛更强，与&lt;span class="math">\(\varepsilon\)&lt;/span>无关。&lt;/p>
&lt;h3 id="收敛性之间的关系">收敛性之间的关系&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>几乎处处收敛（依概率1收敛）&lt;span class="math">\(\Rightarrow\)&lt;/span>依概率收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>依分布收敛。&lt;/li>
&lt;li>&lt;span class="math">\(L^p\)&lt;/span>收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>依概率收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>依分布收敛。&lt;/li>
&lt;li>&lt;span class="math">\(L^p\)&lt;/span>收敛与几乎处处收敛（依概率1收敛）之间互相不可推导，即没有等价性。&lt;/li>
&lt;li>依概率收敛与一系列弱大数定律相关。&lt;/li>
&lt;li>几乎处处收敛（依概率1收敛）与强大数定律相关。&lt;/li>
&lt;li>在收敛到同一常数时，依概率收敛与依分布收敛等价。&lt;/li>
&lt;/ol>
&lt;h2 id="大数定理">大数定理&lt;/h2>
&lt;p>在数学与统计学中，大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。这个定律核心是，&lt;strong>样本数量越多，则其算术平均值就有越高的概率接近期望值&lt;/strong>。&lt;/p>
&lt;p>大数定律很重要，因为它“说明”了一些随机事件的均值的&lt;strong>长期稳定性&lt;/strong>。人们发现，在重复试验中，随着试验次数的增加，事件发生的频率趋于一个稳定值；人们同时也发现，在对物理量的测量实践中，测定值的算术平均也具有稳定性。比如，我们向上抛一枚硬币，硬币落下后哪一面朝上是偶然的，但当我们上抛硬币的次数足够多后，达到上万次甚至几十万几百万次以后，我们就会发现，硬币每一面向上的次数约占总次数的二分之一，亦即偶然之中包含着必然。&lt;/p>
&lt;p>&lt;strong>切比雪夫不等式的一个特殊情况&lt;/strong>、&lt;strong>辛钦定理&lt;/strong>和&lt;strong>伯努利大数定律&lt;/strong>等等都概括了这一现象，都可以称为大数定律。而这几种大数定律都是&lt;strong>依概率收敛&lt;/strong>的，而相对于几乎处处收敛较弱，所以又是&lt;strong>弱大数定律&lt;/strong>的几种表现形式。对应的，能够证明为&lt;strong>几乎处处收敛&lt;/strong>的大数定律称为&lt;strong>强大数定律&lt;/strong>。&lt;/p>
&lt;h3 id="弱大数定律及其几种形式">弱大数定律及其几种形式&lt;/h3>
&lt;p>弱大数定律主要描述一系列&lt;strong>依概率收敛&lt;/strong>的&lt;strong>随机变量序列&lt;/strong>，利用在序列序数趋近于无穷时，序列与收敛目标的距离小于一小正数&lt;span class="math">\(\varepsilon\)&lt;/span>这种模式定义。具体描述为，当随机变量序列&lt;span class="math">\(\{X_n\}\)&lt;/span>&lt;strong>满足一些条件时&lt;/strong>有： &lt;span class="math">\[\frac{1}{n}\sum_{i=1}^n X_i\overset{P}{\longrightarrow}E(\frac{1}{n}\sum_{i=1}^n X_i)=\frac{1}{n}\sum_{i=1}^n E(X_i)\\
即\lim_{n\rightarrow\infty} P\{|\frac{1}{n}\sum_{i=1}^n X_i-\frac{1}{n}\sum_{i=1}^n E(X_i)|&amp;lt;\varepsilon\}=1\tag{1}\]&lt;/span> 也就是说，这些随机变量的平均值趋近于其各期望的和的平均值。注意，我们在第一项中不需要取平均的期望，虽然是随机变量，但是其平均仍然具有稳定性（趋向于确定性）。&lt;/p>
&lt;p>历史上，有很多人名命名的弱大数定律，以下分两大类共八个弱大数定律来具体阐释。&lt;/p>
&lt;h4 id="切比雪夫不等于相关的大数定律">切比雪夫不等于相关的大数定律&lt;/h4>
&lt;blockquote>
&lt;p>（一）切比雪夫大数定律：设&lt;span class="math">\(\{X_n\}\)&lt;/span>为&lt;strong>两两不相关&lt;/strong>的随机变量序列，方差为：&lt;span class="math">\({\displaystyle \operatorname {Var} (X_{i})=\sigma_i^{2}\quad (i=1,\ 2,\ \dots )}\)&lt;/span>，且&lt;strong>有一致上界&lt;/strong>，即&lt;span class="math">\(\text{var}(X_i)\leq c\)&lt;/span>，对任意&lt;span class="math">\(i\)&lt;/span>成立。则有式&lt;span class="math">\((1)\)&lt;/span>成立。&lt;/p>
&lt;/blockquote>
&lt;p>证明：利用切比雪夫不等式。 &lt;span class="math">\[\begin{aligned}
P\{|\frac{1}{n}\sum_{i=1}^n X_i-E(\frac{1}{n}\sum_{i=1}^n X_i)|\geq \varepsilon\}&amp;amp;\leq \frac{\text{var}(\frac{1}{n}\sum_{i=1}^n X_i)}{\varepsilon^2}\\
\overset{两两不相关}{=}\frac{\sum_{i=1}^n\text{var}(X_i)}{n^2\varepsilon^2}\overset{\text{var}(X_i)\leq c}{\leq} \frac{nc}{n^2\varepsilon^2}&amp;amp;=\frac{c}{n\varepsilon^2}
\end{aligned}\]&lt;/span> 显然当&lt;span class="math">\(n→∞\)&lt;/span>时，有&lt;span class="math">\(\frac{c}{n\varepsilon^2}→0\)&lt;/span>。切比雪夫大数定律得证。&lt;/p>
&lt;blockquote>
&lt;p>（二）独立同分布场合的大数定律：设&lt;span class="math">\(\{X_n\}\)&lt;/span>为独立同分布的随机变量序列，且方差&lt;span class="math">\(\sigma^2\)&lt;/span>存在，则有式&lt;span class="math">\((1)\)&lt;/span>成立。&lt;/p>
&lt;/blockquote>
&lt;p>这个大数定律没有以人名命名，是一直被普遍认为比较直观的大数定律。独立同分布是比两两不相关更强的条件，且由于是i.i.d的，所以方差存在，等同于有一致上界。因此可以由&lt;span class="math">\((一)\Rightarrow (二)\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>（三）伯努利大数定律：设在&lt;span class="math">\(n\)&lt;/span>次&lt;strong>独立重复伯努利&lt;/strong>试验中，事件&lt;span class="math">\(X\)&lt;/span>发生的次数为&lt;span class="math">\(n_{x}\)&lt;/span>，事件&lt;span class="math">\(X\)&lt;/span>在每次试验中发生的总体概率为&lt;span class="math">\(p\)&lt;/span>，&lt;span class="math">\(\frac{n_{x}}{n}\)&lt;/span>代表样本发生事件&lt;span class="math">\(X\)&lt;/span>的频率。则对任意正数&lt;span class="math">\(\varepsilon &amp;gt;0\)&lt;/span>，伯努利大数定律表明： &lt;span class="math">\[\lim _{n\to \infty }{P{\left\{\left|{\frac {n_{x}}{n}}-p\right|&amp;lt;\varepsilon \right\}}}=1\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>换言之，事件发生的&lt;strong>频率依概率收敛于事件的总体概率&lt;/strong>。该定理以严格的数学形式表达了频率的稳定性，也就是说当&lt;span class="math">\(n\)&lt;/span>很大时，事件发生的频率于总体概率有较大偏差的可能性很小。&lt;/p>
&lt;p>伯努利大数定律是第一个被明确提出的大数定律。原始证明没有用切比雪夫不等于证明（当时还没有此不等式），而是用了很繁琐的方式。今天，我们可以直接从（一）或（二）证明出来。因为&lt;span class="math">\(n\)&lt;/span>次独立重复伯努利是&lt;span class="math">\(n\)&lt;/span>个i.i.d的随机变量序列，且方差为&lt;span class="math">\(p(1-p)\)&lt;/span>存在，显然（三）就是（二）的一种特殊情况。&lt;/p>
&lt;blockquote>
&lt;p>（四）马尔可夫大数定律&lt;/p>
&lt;/blockquote>
&lt;p>切比雪夫大数定理的进一步&lt;/p>
&lt;blockquote>
&lt;p>（五）泊松大数定律&lt;/p>
&lt;/blockquote>
&lt;p>伯努利大数定律大数定理的进一步&lt;/p>
&lt;blockquote>
&lt;p>（六）伯恩斯坦大数定律&lt;/p>
&lt;/blockquote>
&lt;p>有相互独立性扩展为序列渐进不相关（比独立性更弱的要求）&lt;/p>
&lt;blockquote>
&lt;p>（七）格涅坚科大数定律&lt;/p>
&lt;/blockquote>
&lt;p>矩估计的理论基础&lt;/p>
&lt;h4 id="用特征函数证明的大数定律">用特征函数证明的大数定律&lt;/h4>
&lt;blockquote>
&lt;p>（八）辛钦大数定理：陈述为：&lt;strong>独立同分布&lt;/strong>的样本均值&lt;strong>依概率收敛&lt;/strong>于期望值。 &lt;span class="math">\[
{\displaystyle {\overline {X}}_{n}\ {\xrightarrow {P}}\ \mu \quad {\textrm {as}}\quad n\to \infty }
\]&lt;/span> 也就是说对于任意正数&lt;span class="math">\(ε\)&lt;/span>, &lt;span class="math">\[\lim_{n\to \infty }P\left(\,|{\overline {X}}_{n}-\mu |&amp;gt;\varepsilon \,\right)=0 或者\\
\lim_{n\to \infty }P\left(\,|{\overline {X}}_{n}-\mu |&amp;lt;\varepsilon \,\right)=1\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h4 id="八个弱大数定律的比较">八个弱大数定律的比较&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">定律&lt;/th>
&lt;th align="center">分布情况&lt;/th>
&lt;th align="center">期望&lt;/th>
&lt;th align="center">方差&lt;/th>
&lt;th align="center">结论&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">辛钦大数定律&lt;/td>
&lt;td align="center">相互独立且同分布&lt;/td>
&lt;td align="center">存在&lt;/td>
&lt;td align="center">无要求（存在即能相等）&lt;/td>
&lt;td align="center">估算期望&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">切比雪夫大数定律&lt;/td>
&lt;td align="center">相互独立（不必同分布）&lt;/td>
&lt;td align="center">相同&lt;/td>
&lt;td align="center">相同&lt;/td>
&lt;td align="center">估算期望&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">伯努利大数定律&lt;/td>
&lt;td align="center">二项分布&lt;/td>
&lt;td align="center">相同&lt;/td>
&lt;td align="center">相同&lt;/td>
&lt;td align="center">频率=概率&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">相同点：n−&amp;gt;+∞,依概率趋近&lt;/td>
&lt;td align="center">条件逐渐变得严格&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="强大数定律">强大数定律&lt;/h3>
&lt;p>后面的数学家在弱大数定理的基础上证明出了更好的强大数定理。&lt;/p>
&lt;p>强大数定律指出，样本均值以概率1收敛于期望值。a.s. 表示almost surely. &lt;span class="math">\[
{\displaystyle {\overline {X}}_{n}\ {\xrightarrow {\text{a.s.}}}\ \mu \quad {\textrm {as}}\quad n\to \infty }\]&lt;/span> 即 &lt;span class="math">\[
{\displaystyle P\left(\lim_{n\to \infty }{\overline {X}}_{n}=\mu \right)=1}\]&lt;/span>&lt;/p>
&lt;h2 id="中心极限定理">中心极限定理&lt;/h2>
&lt;p>中心极限定理是概率论中的&lt;strong>一组定理&lt;/strong>。中心极限定理说明，在适当的条件下，&lt;strong>大量相互独立随机变量&lt;/strong>的均值经适当标准化后&lt;strong>依分布收敛于正态分布&lt;/strong>。这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量之和近似服从正态分布的条件。&lt;/p>
&lt;h3 id="独立同分布条件下的中心极限定理">独立同分布条件下的中心极限定理&lt;/h3>
&lt;h4 id="棣莫佛拉普拉斯定理">棣莫佛－拉普拉斯定理&lt;/h4>
&lt;p>棣莫佛－拉普拉斯（de Moivre - Laplace）定理是中央极限定理的最初版本，讨论了服从二项分布的随机变量序列。它指出，参数为&lt;span class="math">\(n, p\)&lt;/span>的二项分布以&lt;span class="math">\(np\)&lt;/span>为均值、&lt;span class="math">\(np(1-p)\)&lt;/span>为方差的正态分布为极限。&lt;/p>
&lt;p>若&lt;span class="math">\(X\sim B(n,p)\)&lt;/span>是&lt;span class="math">\(n\)&lt;/span>次伯努利实验中事件&lt;span class="math">\(A\)&lt;/span>出现的次数，每次试验成功的概率为&lt;span class="math">\(p\)&lt;/span>，且&lt;span class="math">\(q=1-p\)&lt;/span>，则对任意有限区间&lt;span class="math">\([a,b]\)&lt;/span>：&lt;/p>
&lt;p>令&lt;span class="math">\(x_{k}={\frac {k-np}{\sqrt {npq}}}\)&lt;/span>(标准化&lt;span class="math">\(x_k\)&lt;/span>)，当&lt;span class="math">\(n\to {\infty }\)&lt;/span>时&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\({\displaystyle P(X=k)\to {\frac {1}{\sqrt {npq}}}\cdot {\frac {1}{\sqrt {2\pi }}}e^{-{\frac {1}{2}}x_{\mu_{n}}^{2}}}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\({\displaystyle P(a\leq {\frac {X-np}{\sqrt {npq}}}\leq {b})\to \int _{a}^{b}\varphi (x)dx}\)&lt;/span>，其中&lt;span class="math">\(\varphi (x)={\frac {1}{\sqrt {2\pi }}}e^{-{\frac {x^{2}}{2}}}(-\infty &amp;lt;x&amp;lt;\infty)\)&lt;/span>.&lt;/li>
&lt;/ol>
&lt;p>棣莫弗－拉普拉斯定理指出二项分布的极限为正态分布。&lt;/p>
&lt;h4 id="林德伯格列维中心极限定理">林德伯格－列维中心极限定理&lt;/h4>
&lt;p>林德伯格－列维（Lindeberg-Levy）定理，是棣莫佛－拉普拉斯定理的扩展，讨论&lt;strong>独立同分布&lt;/strong>随机变量序列均值的中心极限定理。它表明，&lt;strong>独立同分布&lt;/strong>(iid)、且&lt;strong>数学期望和方差有限&lt;/strong>的随机变量序列均值的标准化和以标准正态分布为极限。&lt;/p>
&lt;p>设随机变量&lt;span class="math">\(X_{1},X_{2},\cdots ,X_{n}\)&lt;/span>独立同分布，且具有&lt;strong>有限的数学期望和方差&lt;/strong>&lt;span class="math">\(E(X_{i})=\mu\)&lt;/span>，&lt;span class="math">\(D(X_{i})=\sigma ^{2}\neq 0(i=1,2,\cdots ,n)\)&lt;/span>。记 &lt;span class="math">\[
{\bar{X}}={\frac {1}{n}}\sum_{i=1}^{n}X_{i}，\zeta_{n}={\frac {{\bar {X}}-\mu }{\sigma /{\sqrt {n}}}}，\]&lt;/span> 则 &lt;span class="math">\[\lim_{n\rightarrow \infty }P\left(\zeta_{n}\leq z\right)=\Phi \left(z\right)
\]&lt;/span> 其中&lt;span class="math">\(\Phi (z)\)&lt;/span>是标准正态分布的分布函数。&lt;/p>
&lt;h3 id="非独立同分布条件下的中心极限定理">非独立同分布条件下的中心极限定理&lt;/h3>
&lt;h4 id="林德伯格-费勒定理">林德伯格-费勒定理&lt;/h4>
&lt;p>TODO 林德伯格条件&lt;/p>
&lt;p>林德伯格－费勒定理，是中心极限定理的高级形式，是对林德伯格－列维定理的扩展，讨论&lt;strong>独立，但不同分布&lt;/strong>的情况下的随机变量和。它表明，&lt;strong>满足林德伯格条件时&lt;/strong>，独立，但不同分布的随机变量序列的标准化和依然以标准正态分布为极限。&lt;/p>
&lt;h4 id="李雅普诺夫中心极限定理">李雅普诺夫中心极限定理&lt;/h4>
&lt;p>TODO 李雅普诺夫条件&lt;/p>
&lt;h2 id="弱大数定理和强大数定理的区别">弱大数定理和强大数定理的区别&lt;/h2>
&lt;p>作者：runze Zheng 链接：&lt;a href="https://www.zhihu.com/question/21110761/answer/23815273">https://www.zhihu.com/question/21110761/answer/23815273&lt;/a> 来源：知乎。著作权归作者所有。&lt;/p>
&lt;p>强弱大数定律都是在说：随着样本数的增大，用样本的平均数来估计总体的平均数，是靠谱的。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>强弱大数定律的前提条件一样：要求独立同分布iid的随机序列，要求其期望存在。&lt;/li>
&lt;li>强弱大数定律的结论不同（废话）。弱大数定律比较早被证明出来，弱大数定律表示样本均值“&lt;strong>依概率收敛&lt;/strong>”于总体均值；而强大数定律是比较晚被证明出来的，它证明了样本均值可以“&lt;strong>以概率为1收敛&lt;/strong>”于总体均值。简单的来说，就是数学家先证明了弱大数定律，后来在没有改变前提的情况下把弱大数定律推进了一步，得到了更厉害的强大数定律。&lt;/li>
&lt;li>弱大数定律和强大数定律的区别在于，前者是“依概率收敛(convergence in probability)”，后者是“几乎确定收敛(almost surely convergence)或以概率为1收敛、几乎处处收敛”。后者比前者强，满足后者的必定满足前者，而满足前者的未必满足后者。&lt;/li>
&lt;/ol>
&lt;p>依概率收敛的例子：考虑下图，图中的每条线都代表一个数列，虚线表示一个非常小的区间。总的来说每个数列都越来越趋近0，且大部分时候不会超过虚线所表示的小边界，但是，偶尔会有一两条线超过虚线、然后再回到虚线之内。而且我们不能保证，有没有哪一个数列会在未来再次超出虚线的范围然后再回来——虽然概率很小。注意虚线的范围可以是任意小的实数，此图中大约是，可以把这个边界缩小到，甚至，随你喜欢，这个性质始终存在。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/弱大数定理.jpg" alt="弱大数定理" />&lt;p class="caption">弱大数定理&lt;/p>
&lt;/div>
&lt;p>几乎处处收敛的例子：图中的黑线表示一个随机数列，这个数列在大约n=200之后进入了一个我们定的小边界（用虚线表示），之后我们可以确定，它再也不会超出虚线所表示的边界（超出这个边界的概率是0）。跟上面的例子一样，虚线所表示的边界可以定得任意小，而一定会有一个n值，当这个数列超过了n值之后，超出这个边界的概率就是0了。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/强大数定理.jpg" alt="强大数定理" />&lt;p class="caption">强大数定理&lt;/p>
&lt;/div>
&lt;p>弱大数定律是较早被数学家最早证明的，即对于独立同分布的随机序列&lt;span class="math">\(X_1,X_2,\dotsb,X_n,\dotsb\)&lt;/span>，只要总体均值&lt;span class="math">\(\mu\)&lt;/span>存在，那么样本均值会随着&lt;span class="math">\(n\)&lt;/span>增大而“依概率收敛”到总体均值，就是弱大数定律。但是弱大数定律“依概率收敛”不够完美，随着增大，样本均值有没有可能（即使概率很小）偶然偏离总体均值很多呢？后来数学家们证明了强大数定律，就是告诉我们不用担心，&lt;span class="math">\(S_n=\frac{1}{n}\sum_{i=1}^n X_i\)&lt;/span>会“几乎处处收敛”到&lt;span class="math">\(\mu\)&lt;/span>。&lt;/p></description></item><item><title>概率统计随机过程之如何推导得到正态分布—正态分布的理解角度</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%A6%82%E4%BD%95%E6%8E%A8%E5%AF%BC%E5%BE%97%E5%88%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E7%90%86%E8%A7%A3%E8%A7%92%E5%BA%A6/</link><pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%A6%82%E4%BD%95%E6%8E%A8%E5%AF%BC%E5%BE%97%E5%88%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E7%90%86%E8%A7%A3%E8%A7%92%E5%BA%A6/</guid><description>
&lt;h2 id="概率统计随机过程之如何推导得到正态分布正态分布的理解角度">概率统计随机过程之如何推导得到正态分布————正态分布的理解角度&lt;!-- omit in toc -->&lt;/h2>
&lt;p>原文：Kim, Kiseon, 和Georgy Shevlyakov. 《Why Gaussianity?》 IEEE Signal Processing Magazine 25, 期 2 (2008年3月): 102–13. &lt;a href="https://doi.org/10.1109/MSP.2007.913700">https://doi.org/10.1109/MSP.2007.913700&lt;/a>.&lt;/p>
&lt;p>中文部分靳志辉正态分布章节推导完善。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#正态分布表达式形式的出现">正态分布表达式形式的出现&lt;/a>&lt;/li>
&lt;li>&lt;a href="#高斯误差与正态分布">高斯——误差与正态分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#赫歇尔和麦克斯韦正态分布的pi从哪里来">赫歇尔和麦克斯韦——正态分布的&lt;span class="math">\(\pi\)&lt;/span>从哪里来？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#为什么正态分布那么普遍稳定性兰登的推导">为什么正态分布那么普遍？——稳定性，兰登的推导&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基于最大熵的推导">基于最大熵的推导&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录">附录&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附1-似然函数取对数">附1 似然函数取对数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附2-高斯积分">附2 高斯积分&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附3-高斯在推导正态分布时系数c和方差sigma2的关系">附3 高斯在推导正态分布时系数&lt;span class="math">\(c\)&lt;/span>和方差&lt;span class="math">\(\sigma^2\)&lt;/span>的关系&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="正态分布表达式形式的出现">正态分布表达式形式的出现&lt;/h2>
&lt;p>正态分布是概率论和数理统计中的一个重要的分布。在最开始的工作中（1733年），法国数学家&lt;strong>棣莫佛&lt;/strong>为了求二项分布在数值较大时的近似计算值，推导出正态分布的表达形式。但是，棣莫佛并没有把它当作是一个概率分布，只是认为这是二项分布在&lt;span class="math">\(N→∞\)&lt;/span>时的一个近似表达式。40年后，法国另一个数学家&lt;strong>拉普拉斯&lt;/strong>在推导最初版本的中心极限定理时也得出了正态分布表达式的形式，但当时他也没有意识这是一个概率的分布，也把它当作单纯的一种数学表达式。但是，这是第一次正态密度函数被数学家刻画出来，而且是以二项分布的极限分布的形式被推导出来的。熟悉基础概率统计的人都知道这个结果其实叫&lt;strong>棣莫弗-拉普拉斯中心极限定理&lt;/strong>。当然当时还没有这个称呼，直到1920年才有数学家波利亚提出中心极限定理这个名称。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>棣莫弗-拉普拉斯中心极限定理&lt;/strong>：若&lt;span class="math">\(X\thicksim B(n,p)\)&lt;/span>是n次伯努利试验中出现事件&lt;span class="math">\(A\)&lt;/span>的次数，每次试验成功的概率为&lt;span class="math">\(p\)&lt;/span>，且&lt;span class="math">\(q=1-p\)&lt;/span>，则对任意有限区间&lt;span class="math">\([a,b]\)&lt;/span>，令&lt;span class="math">\(x_k=\frac{k-np}{\sqrt{npq}}\)&lt;/span>，当&lt;span class="math">\(n→∞\)&lt;/span>时：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(P(X=k)→\frac{1}{\sqrt{npq}}\cdot\frac{1}{\sqrt{2\pi}}e^{-{1\over2}x^2_k}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(P(a≤\frac{k-np}{\sqrt{npq}}≤b)→\int_a^b\varphi(x)\mathrm{d}x\)&lt;/span>，其中&lt;span class="math">\(\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}(-∞&amp;lt;x&amp;lt;∞)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>棣莫佛-拉普拉斯中心极限定理指出：参数&lt;span class="math">\(n,p\)&lt;/span>的二项分布以&lt;span class="math">\(np\)&lt;/span>为均值，&lt;span class="math">\(np(1-p)\)&lt;/span>为方差的正态分布为极限。当&lt;span class="math">\(n\)&lt;/span>很大时，可以用作二项分布的近似值。&lt;/p>
&lt;p>总结一句：这个阶段正态分布表达式只是作为一个函数近似的结果，并没有被认为是一种概率分布。&lt;/p>
&lt;h2 id="高斯误差与正态分布">高斯——误差与正态分布&lt;/h2>
&lt;p>高斯所在的18-19世纪是天文学迅速发展的时期，积累了大量观测数据。但是，这些数据或多或少都是存在误差的。如何处理这些数据得到最准确的结果，或者这些误差有着什么样的数学规律？在当时，这是一个亟待解决的问题。&lt;/p>
&lt;p>辛普森和拉普拉斯认为误差应该服从这样一些普遍规律：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>误差分布是对称的，即&lt;span class="math">\(f(x)=f(-x)\)&lt;/span>。&lt;/li>
&lt;li>小误差应该比大误差更常见，即&lt;span class="math">\(f(x)\)&lt;/span>是&lt;span class="math">\(x\)&lt;/span>的减函数。且误差应该随&lt;span class="math">\(|x|\)&lt;/span>增大而逐渐趋于0。&lt;/li>
&lt;li>每次测量产生的误差应该是独立的。&lt;/li>
&lt;li>误差分布函数应是连续函数。&lt;/li>
&lt;/ol>
&lt;p>为此，辛普森和拉普拉斯分别设计了三角概率分布函数和重指数分布（拉普拉斯分布）作为误差的分布函数。并用这些分布证明&lt;strong>观测值取平均数比恰当选择观测值更能接近真实值&lt;/strong>。拉普拉斯在重指数分布基础上，希望使用贝叶斯法（当时拉普拉斯称之为不充分推理原则）逐渐修正后验概率得到的误差分布。但是，拉普拉斯的工作到此基本进展就很小了，在18世纪70-80年代沿着这条路径磕磕绊绊走了十几年，依旧进展甚微。&lt;/p>
&lt;p>1809年，高斯发表了其数学和天体力学的名著《绕日天体运动的理论》。在此书末尾，他写了一节有关“数据结合”的问题，实际涉及的就是这个误差分布的问题。&lt;/p>
&lt;p>高斯采用了拉普拉斯设计的函数&lt;span class="math">\(L(\theta)\)&lt;/span>，设真值为&lt;span class="math">\(\theta\)&lt;/span>，n个独立的测量值&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>。&lt;span class="math">\(L(\theta)\)&lt;/span>为出现这一现象的联合概率密度： &lt;span class="math">\[\begin{aligned}
L(\theta)=&amp;amp;L(\theta;X_1,X_2,\dotsb,X_n)\\
\stackrel{\text{独立性}}{=}&amp;amp;f(X_1-\theta)f(X_2-\theta)\dotsb f(X_n-\theta)\tag{1}
\end{aligned}\]&lt;/span> 其中，&lt;span class="math">\(f(\cdot)\)&lt;/span>为&lt;strong>待定的误差密度函数&lt;/strong>。当时接下来的处理高斯与拉普拉斯完全不同。&lt;/p>
&lt;p>一是高斯没有采取贝叶斯的推理方式，而是径直让&lt;span class="math">\((1)\)&lt;/span>达到最大值的&lt;span class="math">\(\hat{\theta}=\hat{\theta}(X_1,\dotsb,X_n)\)&lt;/span>作为&lt;span class="math">\(\theta\)&lt;/span>的估计。即 &lt;span class="math">\[L(\hat{\theta})=\max_{\theta} L(\theta)\tag{2}\]&lt;/span> 当然现在我们把这种方法叫做&lt;strong>最大似然估计&lt;/strong>（Maximum Likelihood Estimation, MLE），&lt;span class="math">\(L(\theta)\)&lt;/span>叫做似然函数（正式方法由Fisher推广于1922年）。最大似然估计的思想其实就是很直接：最合理的事情（即真值）最有可能发生。举个简化的例子，如果在一个不透明的袋子里有10个球，黑白两种颜色，但是不知道各有几个；我们袋子中有放回地取100次，其中71次是白球，那么大家认为白球、黑球各有多少个？一般人都会猜7个白球，3个黑球吧。但如果是黑白球各有5个，会不会抽样出上述结果呢？当然有可能啊，只不过概率没有7个白球，3个黑球大。这就是我们在不知不觉中直观地使用了最大似然估计。&lt;/p>
&lt;p>二是高斯把辛普森和拉普拉斯的思路倒了过来。高斯先认可算数平均&lt;span class="math">\(\bar{X}\)&lt;/span>是应取的估计值，然后去找误差密度函数&lt;span class="math">\(f(\cdot)\)&lt;/span>以迎合这一点，即找这样的&lt;span class="math">\(f(\cdot)\)&lt;/span>，使由&lt;span class="math">\((2)\)&lt;/span>式决定的&lt;span class="math">\(\hat{\theta}\)&lt;/span>就是&lt;span class="math">\(\bar{X}\)&lt;/span>。&lt;/p>
&lt;p>高斯发现，使用最大似然估计的情况下，只有当 &lt;span class="math">\[f(x)=\frac{1}{\sqrt{2\pi}h}e^{\frac{-x^2}{2h^2}}，h&amp;gt;0\]&lt;/span> 时，才有&lt;span class="math">\(\hat{\theta}=\bar{X}\)&lt;/span>。注意，高斯在原文中，并没有让&lt;span class="math">\(h=\sigma\)&lt;/span>，即方差。下面，我们来推导这个过程。&lt;/p>
&lt;p>我们从似然函数&lt;span class="math">\((1)\)&lt;/span>式开始，要求关于&lt;span class="math">\((1)\)&lt;/span>式的最小值，而其定义域&lt;span class="math">\(x\in (-∞,∞)\)&lt;/span>，由费马引理（皮埃尔·德·费马 1601-1665，法国律师和业余数学家，业余数学家之王）可知，&lt;span class="math">\((1)\)&lt;/span>式的极值点必然是驻点，即导数为0的点。但是，连乘表达式的求导难以处理，因此高斯采用了化乘为加，且不改变驻点位置（证明见附1）的取&lt;span class="math">\(\log(ln)\)&lt;/span>法，即 &lt;span class="math">\[\frac{\mathrm{d}ln L(\theta)}{\mathrm{d}\theta}=0\]&lt;/span> 展开&lt;span class="math">\(\ln L(\theta)\)&lt;/span>求导可得： &lt;span class="math">\[\sum_{i=1}^n\frac{f&amp;#39;(x_i-\theta)}{f(x_i-\theta)}=0\]&lt;/span> 为了方便计算，我们令&lt;span class="math">\(g(x)=\frac{f&amp;#39;(x)}{f(x)}\)&lt;/span>，则有 &lt;span class="math">\[\sum_{i=1}^n g(x_i-\theta)=0\]&lt;/span> 由于高斯假设极大似然估计的解就是算术平均&lt;span class="math">\(\bar{x}\)&lt;/span>，把解直接带入上式可得 &lt;span class="math">\[\sum_{i=1}^n g(x_i-\bar{x})=0 \tag{3}\]&lt;/span> 下面就是高斯精彩的数学表演，由于&lt;span class="math">\(n,x_i\)&lt;/span>的任意性，在&lt;span class="math">\((3)\)&lt;/span>式中取&lt;span class="math">\(n=2\)&lt;/span>有： &lt;span class="math">\[g(x_1-x)+g(x_2-x)=0\]&lt;/span> 又因为&lt;span class="math">\(\bar{x}=\frac{x_1+x_2}{2}\)&lt;/span>，所以&lt;span class="math">\(x_1-x=-(x_2-x)\)&lt;/span>，即&lt;span class="math">\(g(x_1-x)+g(-(x_1-x))=0\)&lt;/span>，由于&lt;span class="math">\(x_i\)&lt;/span>的任意性可得 &lt;span class="math">\[g(-x)=-g(x),g(x)是奇函数\]&lt;/span> 在&lt;span class="math">\((3)\)&lt;/span>式中再取&lt;span class="math">\(n=m+1\)&lt;/span>，并要求&lt;span class="math">\(x_1=\dotsb=x_m=-x,x_{m+1}=mx\)&lt;/span>，则有&lt;span class="math">\(\bar{x}=0\)&lt;/span>，并且 &lt;span class="math">\[\sum_{i=1}^n g(x_i-\bar{x})=mg(-x)+g(mx)=0\]&lt;/span> 又因为&lt;span class="math">\(g(x)\)&lt;/span>是奇函数，所以有 &lt;span class="math">\[g(mx)=mg(x)\]&lt;/span> 而满足上式的&lt;strong>唯一的连续奇函数解&lt;/strong>就是&lt;span class="math">\(g(x)=cx\)&lt;/span>（可用两边求导数证明），从而进一步可以解微分方程： &lt;span class="math">\[\frac{f&amp;#39;(x)}{f(x)}=cx\Rightarrow \frac{1}{y}\frac{dy}{dx}=cx\stackrel{\text{分离变量法}}{\Rightarrow}\frac{1}{y}dy=cx dx\]&lt;/span> 两边求补丁积分，且y=f(x)作为概率值恒大于等于0，可得： &lt;span class="math">\[\ln y=\frac{1}{2}cx^2+K\Rightarrow y = e^Ke^{\frac{1}{2}cx^2}\]&lt;/span> 我们令&lt;span class="math">\(e^K=M\)&lt;/span>，且&lt;span class="math">\(f(x)\)&lt;/span>作为概率密度函数，必然有&lt;span class="math">\(\int_{-∞}^∞f(x)dx=1\)&lt;/span>，所以 &lt;span class="math">\[\int_{-∞}^∞f(x) dx= \int_{-∞}^∞ Me^{\frac{1}{2}cx^2}dx =\int_{-∞}^∞\frac{M}{\sqrt{0.5|c|}} e^{-(\sqrt{0.5|c|}x)^2} d(\sqrt{0.5|c|}x)= 1\]&lt;/span> 这里需要注意，为了保证积分收敛到1，那么常数&lt;span class="math">\(c&amp;lt;0\)&lt;/span>是必然的，在&lt;span class="math">\(e\)&lt;/span>的指数部分要加上负号，同时开方时需要加绝对值符号。此外，令&lt;span class="math">\(t=\sqrt{0.5|c|}x\)&lt;/span>，则t的积分区间为&lt;span class="math">\((-∞,∞)\)&lt;/span>，上式可转换为 &lt;span class="math">\[\frac{M}{\sqrt{0.5|c|}}\int_{-∞}^∞ e^{-t^2}dt=1\]&lt;/span> 而这个广义积分正好有&lt;span class="math">\(\int_{-∞}^∞ e^{-t^2}dt=\sqrt{\pi}\)&lt;/span>（即&lt;strong>高斯积分&lt;/strong>证明附2），所以可得 &lt;span class="math">\[M=\frac{\sqrt{|c|}}{\sqrt{2\pi}}\]&lt;/span> 将&lt;span class="math">\(M\)&lt;/span>代入&lt;span class="math">\(f(x)\)&lt;/span>可得 &lt;span class="math">\[
f(x)=\frac{\sqrt{|c|}}{\sqrt{2\pi}}e^\frac{cx^2}{2}\tag{4}
\]&lt;/span> 这就是高斯推导的正态分布的表达式，它满足以算数平均作为最大似然估计的结果。如果我们用方差定义式计算一下&lt;span class="math">\(\sigma^2\)&lt;/span>，可以得到&lt;span class="math">\(\sigma^2=-\frac{1}{c}\)&lt;/span>（证明见附3），由此可得，&lt;span class="math">\(f(x)\)&lt;/span>就是&lt;span class="math">\(N(0,\sigma^2)\)&lt;/span>的表达式。正态分布在误差分布计算上有着极大的理论优势，对后世影响极大，以至于正态分布有了高斯分布的名称。&lt;/p>
&lt;p>但是，高斯的解释在逻辑上有点循环论证的味道。由于取算术平均是合理的方式，所以用MLE导出正态分布；又因为误差服从正态分布，所以算术平均能够获得更小的误差值。但是，拉普拉斯很快得知了高斯的工作，并将正态分布与中心极限定理联系起来。在1810年，拉普拉斯提出了元误差学说：如误差可以看成是大量的，有各种原因导致的微小的量的叠加，则根据中心极限定理，误差的分布应服从正态分布。拉普拉斯的补充使误差的正态分布理论有了一个更自然、更令人信服的解释。&lt;/p>
&lt;h2 id="赫歇尔和麦克斯韦正态分布的pi从哪里来">赫歇尔和麦克斯韦——正态分布的&lt;span class="math">\(\pi\)&lt;/span>从哪里来？&lt;/h2>
&lt;p>每当公式中出现&lt;span class="math">\(\pi\)&lt;/span>的时候，我都不禁会问自己？圆在哪里？而正态分布的表达式中正好有一个&lt;span class="math">\(\frac{1}{\sqrt{2\pi}}\)&lt;/span>系数，这个怎么解释呢？我们可以从赫歇尔和麦克斯韦的推导过程中一窥究竟。&lt;/p>
&lt;p>1850年，天文学家赫歇尔在对星星位置进行测量时，需要考虑二维误差的分布，为了推导这个误差的概率密度分布&lt;span class="math">\(p(x,y)\)&lt;/span>，赫歇尔设置了两个准则：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>x轴，y轴的误差时相互独立的，即随机误差在正交方向上相互独立&lt;/li>
&lt;li>误差的概率分布在空间上具有&lt;strong>旋转对称性&lt;/strong>，即误差的概率分布和角度没有关系&lt;/li>
&lt;/ol>
&lt;p>这两个准则对于进行了大量实际测量的赫歇尔都非常合理，而空间的旋转对称性，在二维空间里，那不正是让我们想到了“圆”吗？（三维空间就是球~）&lt;/p>
&lt;p>由第一条准则，我们可以把&lt;span class="math">\(x,y\)&lt;/span>联合概率分布，分离成两部分： &lt;span class="math">\[p(x,y)=f(x)\cdot f(y)\]&lt;/span> 而面对旋转对称性，和角度相关的，我们不禁就和极坐标联系起来： &lt;span class="math">\[p(x,y)=p(r\cos\theta,r\sin\theta)=g(r,\theta)\]&lt;/span> 由第二条准则，旋转不变性可知，函数&lt;span class="math">\(g(r,\theta)\)&lt;/span>实际上与角度&lt;span class="math">\(\theta\)&lt;/span>无关，即&lt;span class="math">\(g(r,\theta)=g(r)\)&lt;/span>。实际上，正是这个旋转不变性，使得&lt;span class="math">\(\theta\)&lt;/span>的积分域是&lt;span class="math">\([0,2\pi]\)&lt;/span>，也就是一个圆，即正态分布中&lt;span class="math">\(\pi\)&lt;/span>的来历。综上所述，我们可得： &lt;span class="math">\[f(x)f(y)=g(r)=g(\sqrt{x^2+y^2})\tag{5}\]&lt;/span> 由于&lt;span class="math">\(x,y\)&lt;/span>的任意性，我们取&lt;span class="math">\(y=0\)&lt;/span>，得到&lt;span class="math">\(g(x)=f(x)f(0)\)&lt;/span>，所以上式中 &lt;span class="math">\[g(\sqrt{x^2+y^2})=f(\sqrt{x^2+y^2})f(0)\tag{6}\]&lt;/span> 联合式&lt;span class="math">\((5)(6)\)&lt;/span>并在两边同时除以&lt;span class="math">\(f^2(0)\)&lt;/span>有： &lt;span class="math">\[\frac{f(x)f(y)}{f(0)f(0)}=\frac{f(\sqrt{x^2+y^2})f(0)}{f(0)f(0)}\Rightarrow \frac{f(x)}{f(0)}\frac{f(y)}{f(0)}=\frac{f(\sqrt{x^2+y^2})}{f(0)}\]&lt;/span> 两边取&lt;span class="math">\(\log\)&lt;/span>则有 &lt;span class="math">\[\log\left[\frac{f(x)}{f(0)}\right]+\log\left[\frac{f(y)}{f(0)}\right]=\log\left[\frac{f(\sqrt{x^2+y^2})}{f(0)}\right]\]&lt;/span> 令&lt;span class="math">\(h(x)=\log[\frac{f(x)}{f(0)}]\)&lt;/span>，则上式可简化为函数方程： &lt;span class="math">\[h(x)+h(y)=h(\sqrt{x^2+y^2})\tag{7}\]&lt;/span> 函数方程与代数方程、微分方程不同，并没有通用的解法。所以这个分支也没能发展起来。但是，我们不难猜出&lt;span class="math">\(h(x)=ax^2\)&lt;/span>是符合&lt;span class="math">\((7)\)&lt;/span>的一个连续函数解（我觉得可以用此函数方程的二阶导数为常数和&lt;span class="math">\(h(0)=0\)&lt;/span>证明其解的唯一性）。同时，由于概率积分的收敛性，需要&lt;span class="math">\(a&amp;lt;0\)&lt;/span>，不妨令&lt;span class="math">\(\alpha=-a\)&lt;/span>： &lt;span class="math">\[h(x)=ax^2\Rightarrow f(x)=f(0)e^{-\alpha x^2}\]&lt;/span> 对于概率密度函数定义域积分为1，则有 &lt;span class="math">\[\int_{-\infty}^\infty f(0)e^{-\alpha x^2}dx = 1\Rightarrow \frac{f(0)}{\sqrt{\alpha}}\int_{-\infty}^\infty e^{-(\sqrt{\alpha} x)^2}d\sqrt{\alpha}x=1\]&lt;/span> 不妨令&lt;span class="math">\(\sqrt{\alpha}x = t, t\in (-\infty,\infty)\)&lt;/span>，根据附2的高斯积分&lt;span class="math">\(\int_{-∞}^∞ e^{-x^2}dx=\sqrt{\pi}\)&lt;/span>，可推得&lt;span class="math">\(f(0)=\sqrt{\frac{\alpha}{\pi}}\)&lt;/span>，综上所述有： &lt;span class="math">\[f(x)=\sqrt{\frac{\alpha}{\pi}}e^{-\alpha x^2}\]&lt;/span> 这也是正态分布的表达形式。这也是在0均值正态分布，当&lt;span class="math">\(\sigma^2=\frac{1}{2\alpha}\)&lt;/span>的结果（方差证明的方式类似附3）。 而满足准则1，2的二维表达式，就是&lt;em>两个独立同分布&lt;/em>的&lt;span class="math">\(f(x)\)&lt;/span>的乘机，即&lt;span class="math">\(p(x,y)\)&lt;/span>为二维i.i.d正态分布的密度函数： &lt;span class="math">\[p(x,y)=\frac{\alpha}{\pi}e^{-\alpha(x^2+y^2)}\]&lt;/span>&lt;/p>
&lt;p>1860年麦克斯韦在考虑气体分子运动速度分布的时候，基于类似的准则推出了著名的气体分子运动速率分布的麦克斯韦-玻尔兹曼气体速率分布定率： &lt;span class="math">\[
F(v)=(\frac{m}{2\pi kT})^{3/2}e^{-\frac{mv^2}{2kT}}
\]&lt;/span> 这拆开看其实就是三维空间的i.i.d正态分布嘛，其中&lt;span class="math">\(\sigma^2=\frac{kT}{m}\)&lt;/span> &lt;span class="math">\[\begin{aligned}
F(v)&amp;amp;=(\frac{m}{2\pi kT})^{3/2}e^{-\frac{mv^2}{2kT}}\\
&amp;amp;=(\frac{m}{2\pi kT})^{1/2}e^{-\frac{mv_x^2}{2kT}}\times(\frac{m}{2\pi kT})^{1/2}e^{-\frac{mv_y^2}{2kT}}\times(\frac{m}{2\pi kT})^{1/2}e^{-\frac{mv_z^2}{2kT}}
\end{aligned}\]&lt;/span> 其中，&lt;span class="math">\(v^2=v_x^2+v_y^2+v_z^2,v_x,v_y,v_z\)&lt;/span>是三维空间正交的三个方向向量。赫歇尔和麦克斯韦的工作神奇之处在于，他们没有利用任何概率论只是，只是基于&lt;strong>空间不变性&lt;/strong>，就推导出了正态分布。&lt;/p>
&lt;h2 id="为什么正态分布那么普遍稳定性兰登的推导">为什么正态分布那么普遍？——稳定性，兰登的推导&lt;/h2>
&lt;p>1941年电子工程师兰登通过分析经验数据他发现噪声电压的分布模式很相似，不同的是分布的层级，而这个层级可以使用方差&lt;span class="math">\(σ^2\)&lt;/span>来刻画。基于这些经验，兰登提出以下两个准则：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>随机噪声具有稳定的分布模式。&lt;/li>
&lt;li>累加一个微小的随机噪声，不改变其稳定的分布模式，只改变分布的层级。(用方差度量)&lt;/li>
&lt;li>微小的随机噪声关于0对称分布。（本质上是相位(&lt;span class="math">\([0-2\pi]\)&lt;/span>)的随机性，也是正态分布中&lt;span class="math">\(\pi\)&lt;/span>的来源）&lt;/li>
&lt;/ol>
&lt;p>用数学的语言描述: 如果 &lt;span class="math">\[X \sim p(x;\sigma^2),\epsilon\sim q(e),X&amp;#39;=X+\epsilon\]&lt;/span> 则有 &lt;span class="math">\[
X&amp;#39;\sim p(x;\sigma^2+\text{var}(\epsilon))
\]&lt;/span> 按照两个随机变量和的分布的计算方式，&lt;span class="math">\(X′\)&lt;/span>的分布密度函数将是&lt;span class="math">\(X\)&lt;/span>的分布密度函数和&lt;span class="math">\(\epsilon\)&lt;/span>的分布密度函数的卷积，即有 &lt;span class="math">\[
f(x&amp;#39;)=\int_{-\infty}^\infty p(x&amp;#39;-e;\sigma^2)q(e)de
\]&lt;/span> 通过泰勒级数展开和解二阶偏微分方程（也是著名的扩散方程），我们可以解得 &lt;span class="math">\[p(x;\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{-x^2}{2\sigma^2}}
\]&lt;/span> (这里详情可以扩展写)&lt;/p>
&lt;p>杰恩斯（概率论沉思录作者）指出这个推导这基本上就是&lt;strong>中心极限定理的增量式&lt;/strong>版本，相比于中心极限定理是一次性累加所有的因素，兰登的推导是每次在原有的分布上去累加一个微小的扰动。而在这个推导中，我们看到，正态分布具有相当好的稳定性；只要数据中正态的模式已经形成，他就容易继续保持正态分布，无论外部累加的随机噪声 q(e) 是什么分布，正态分布就像一个黑洞一样把这个累加噪声吃掉。这也是稳定性也是正态分布普遍存在的原因。&lt;/p>
&lt;h2 id="基于最大熵的推导">基于最大熵的推导&lt;/h2>
&lt;p>如果&lt;strong>给定一个分布密度函数&lt;span class="math">\(p(x)\)&lt;/span>的均值&lt;span class="math">\(µ\)&lt;/span>和方差&lt;span class="math">\(σ^2\)&lt;/span>&lt;/strong>(给定均值和方差这个条件，也可以描述为给定一阶原点矩和二阶原点矩，这两个条件是等价的)，则在所有满足这两个限制的概率分布中，熵最大的概率分布&lt;span class="math">\(p(x|µ; σ^2)\)&lt;/span>就是正态分布&lt;span class="math">\(N(µ; σ^2)\)&lt;/span>&lt;/p>
&lt;p>此部分的证明用到了古典泛函分析的变分法。&lt;/p>
&lt;h2 id="附录">附录&lt;/h2>
&lt;h3 id="附1-似然函数取对数">附1 似然函数取对数&lt;/h3>
&lt;blockquote>
&lt;p>命题：对于定义域&lt;span class="math">\([x_a,x_b]\)&lt;/span>上的正函数&lt;span class="math">\(f(x)\)&lt;/span>，取对数不影响驻点位置。&lt;/p>
&lt;/blockquote>
&lt;p>证明： 驻点要求函数导数为0，即&lt;span class="math">\(f&amp;#39;(x)=0\)&lt;/span>；而&lt;span class="math">\((\log_a f(x))&amp;#39;=\frac{f&amp;#39;(x)}{f(x)\ln a}\)&lt;/span>。由于&lt;span class="math">\(f(x)&amp;gt;0\)&lt;/span>，且&lt;span class="math">\(a \neq 1\)&lt;/span>，所以只要当&lt;span class="math">\(f&amp;#39;(x)=0\)&lt;/span>时，&lt;span class="math">\((\log_a f(x))&amp;#39;=0\)&lt;/span>，即二者驻点位置一致。&lt;/p>
&lt;h3 id="附2-高斯积分">附2 高斯积分&lt;/h3>
&lt;blockquote>
&lt;p>命题：证明高斯积分&lt;span class="math">\(I=\int_{-∞}^∞ e^{-x^2}dx=\sqrt{\pi}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：令两个独立的变量&lt;span class="math">\(x，y\)&lt;/span> &lt;span class="math">\[
I^2=\int_{-∞}^∞ e^{-x^2}dx\times \int_{-∞}^∞ e^{-y^2}dy=\int_{-∞}^∞\int_{-∞}^∞ e^{-(x^2+y^2)} dx dy
\]&lt;/span> 对于含有&lt;span class="math">\(x^2+y^2\)&lt;/span>的形式，我们一般使用极坐标变换：&lt;span class="math">\(x=r\cos\theta,y=r\sin\theta\)&lt;/span>；元面积变换：&lt;span class="math">\(dxdy=rdrd\theta\)&lt;/span> &lt;span class="math">\[
\begin{aligned}
I^2 &amp;amp;= \int_0^{2\pi}\int_0^∞ e^{-r^2}rdrd\theta=\int_0^{2\pi}d\theta\int_0^∞ e^{-r^2}rdr \\
&amp;amp;=2\pi\times (-\frac{1}{2}e^{-r^2}\bigg|_0^∞)=2\pi\times{1\over2}=\pi\\
&amp;amp;\Rightarrow I=\sqrt{\pi}
\end{aligned}
\]&lt;/span> 得证。&lt;/p>
&lt;p>补充：&lt;strong>高斯积分与Gamma函数（第二类欧拉积分）的关系&lt;/strong>。在&lt;span class="math">\(I\)&lt;/span>中，因为&lt;span class="math">\(e^{-x^2}\)&lt;/span>是一个偶函数，因此积分域可以从&lt;span class="math">\(\int_{-∞}^∞dx\)&lt;/span>变成&lt;span class="math">\(2\cdot\int_0^∞dx\)&lt;/span>，此时&lt;span class="math">\(x\in(0,∞)\)&lt;/span>。如果我们做一个积分变量替换&lt;span class="math">\(x=\sqrt{t},dx={1\over 2}t^{-{1\over 2}}dt\)&lt;/span>，则&lt;span class="math">\(t\in(0,∞)\)&lt;/span>，且&lt;span class="math">\(I\)&lt;/span>有如下变换： &lt;span class="math">\[
I= \sqrt{\pi}=\int_{-∞}^∞ e^{-x^2}dx =2\int_0^∞ e^{-t}\cdot{1\over 2}t^{-{1\over 2}}dt=\int_0^∞ e^{-t}t^{-{1\over 2}}dt=\Gamma({1\over2})
\]&lt;/span> 即 &lt;span class="math">\[I=\underbrace{\int_{-∞}^∞ e^{-x^2}dx}_{\text{高斯积分}}=\underbrace{\int_0^∞ e^{-x}x^{-{1\over 2}}dx}_{\text{第二类欧拉积分}}=\Gamma({1\over2})=\sqrt{\pi}\]&lt;/span>&lt;/p>
&lt;h3 id="附3-高斯在推导正态分布时系数c和方差sigma2的关系">附3 高斯在推导正态分布时系数&lt;span class="math">\(c\)&lt;/span>和方差&lt;span class="math">\(\sigma^2\)&lt;/span>的关系&lt;/h3>
&lt;blockquote>
&lt;p>命题：式&lt;span class="math">\((4)\)&lt;/span>中的系数&lt;span class="math">\(c=-\frac{1}{\sigma^2}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：首先，从&lt;span class="math">\((4)\)&lt;/span>中，我们不难观察出分布函数是一个偶函数，所以有&lt;span class="math">\(E[x]=\mu=0\)&lt;/span>,则方差&lt;span class="math">\(E[(x-\mu)^2]=E[x^2]\)&lt;/span>，则有: &lt;span class="math">\[
\begin{aligned}
\sigma^2=&amp;amp;\frac{\sqrt{|c|}}{\sqrt{2\pi}}\int_∞^∞ x^2e^\frac{cx^2}{2}dx\\
\stackrel{\text{分部积分}}{=}&amp;amp;\frac{\sqrt{|c|}}{\sqrt{2\pi}}\left\{x\cdot\frac{1}{c}e^\frac{cx^2}{2}\bigg |_{-\infty}^\infty-\int_∞^∞ 1\cdot \frac{1}{c}e^\frac{cx^2}{2}dx\right\}\\
\stackrel{\text{等价无穷小}}{=}&amp;amp;\frac{\sqrt{|c|}}{\sqrt{2\pi}}\left\{0-\int_∞^∞ 1\cdot \frac{1}{c}e^\frac{cx^2}{2}dx\right\}\\
\stackrel{\text{第一类积分变量替换}}{=}&amp;amp;\frac{\sqrt{|c|}}{\sqrt{2\pi}}\left\{-\int_∞^∞ \frac{1}{c}\cdot\sqrt{\frac{2}{|c|}} e^{-(\sqrt{\frac{|c|}{2}}x)^2}d(\sqrt{\frac{|c|}{2}}x)\right\}\\
\stackrel{t=\sqrt{\frac{|c|}{2}}x}{=}&amp;amp;\frac{-1}{\sqrt{\pi}c}\left\{\int_∞^∞ e^{-t^2}dt\right\}\\
\stackrel{\text{附录2高斯积分}}{=}&amp;amp;\frac{-1}{\sqrt{\pi}c}\cdot \sqrt{\pi}=\frac{-1}{c}
\end{aligned}
\]&lt;/span> 即系数&lt;span class="math">\(c=-\frac{1}{\sigma^2}\)&lt;/span>，代入&lt;span class="math">\((4)\)&lt;/span>可得正态分布&lt;span class="math">\(N(0,\sigma^2)\)&lt;/span>。&lt;/p></description></item><item><title>概率统计随机过程之随机变量函数的分布</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E5%88%86%E5%B8%83/</link><pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E5%88%86%E5%B8%83/</guid><description>
&lt;h2 id="概率统计随机过程之随机变量函数的分布">概率统计随机过程之随机变量函数的分布&lt;!-- omit in toc -->&lt;/h2>
&lt;p>概率论与数理统计－－茆诗松（第二版）随机变量函数的分布内容总结&lt;/p>
&lt;p>写在最前面：在随机变量的学习中，我们一定要明确知晓，&lt;strong>概率分布函数和随机变量的定义密切相关&lt;/strong>。概率密度函数是概率分布函数的衍生结论，因此，我们在处理问题时优先考虑概率分布函数。&lt;/p>
&lt;p>现有随机变量&lt;span class="math">\(X\)&lt;/span>定义在&lt;span class="math">\((\Omega,\mathcal{F},P)\)&lt;/span>上。设存在一个定义在&lt;span class="math">\(\Omega\)&lt;/span>上的函数&lt;span class="math">\(y=g(x)\)&lt;/span>，若使用随机变量&lt;span class="math">\(X\)&lt;/span>作为函数&lt;span class="math">\(g\)&lt;/span>的自变量，则&lt;span class="math">\(Y=g(X)\)&lt;/span>显然也是一个随机变量。那么，问题来了：已知随机变量&lt;span class="math">\(X\)&lt;/span>的分布，如何求出另一个随机变量&lt;span class="math">\(Y=g(X)\)&lt;/span>的分布呢？&lt;/p>
&lt;p>多维随机变量其实就是多个随机变量的意思，这些个随机变量之间可能存在关联性（破坏了独立性），导致多维随机变量的分布有时很不直观。这里也只是介绍了一部分多维随机变量的场景（和、商、最大值最小值等），很多多维随机变量的函数没有解析的结果。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#单个随机变量函数的分布">单个随机变量函数的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单个离散型随机变量函数的分布">单个离散型随机变量函数的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单个连续型随机变量函数的分布">单个连续型随机变量函数的分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#当gx严格单调时">当&lt;span class="math">\(g(x)\)&lt;/span>严格单调时&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单个连续型随机变量函数的几个常用命题">单个连续型随机变量函数的几个常用命题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯假设的一个悖论">贝叶斯假设的一个悖论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#当gx为其他形式时">当&lt;span class="math">\(g(x)\)&lt;/span>为其他形式时&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#多维随机变量函数的分布">多维随机变量函数的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多维离散型随机变量函数的分布">多维离散型随机变量函数的分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#离散型随机变量和的分布与卷积公式">离散型随机变量和的分布与卷积公式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#思考分布可加性的本质">思考：分布可加性的本质&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#多维随机变量的最大最小值分布">多维随机变量的最大最小值分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#最大值分布">最大值分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小值分布">最小值分布&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#多维连续型随机变量函数的分布">多维连续型随机变量函数的分布&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#连续型随机变量和的分布与卷积公式">连续型随机变量和的分布与卷积公式&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#变量变换法">变量变换法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#box-muller变换">Box-muller变换&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#增补变量法">增补变量法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二维随机变量积的分布">二维随机变量积的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二维随机变量商的分布">二维随机变量商的分布&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二维随机变量积和商的分布直接推导">二维随机变量积和商的分布直接推导&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;h2 id="单个随机变量函数的分布">单个随机变量函数的分布&lt;/h2>
&lt;h3 id="单个离散型随机变量函数的分布">单个离散型随机变量函数的分布&lt;/h3>
&lt;p>离散型随机变量函数的分布时&lt;strong>比较容易&lt;/strong>的，主要是因为离散型随机变量的函数变换结果是离散固定的。其一般方法如下：&lt;/p>
&lt;p>设&lt;span class="math">\(X\)&lt;/span>是离散型随机变量，X的分布列为&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;span class="math">\(X\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(x_1\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(x_2\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(x_n\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(P\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_1)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_2)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_n)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>则&lt;span class="math">\(Y=g(X)\)&lt;/span>也是一个离散型随机变量，且此时&lt;span class="math">\(Y\)&lt;/span>的分布列相应可表示为&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;span class="math">\(Y\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(g(x_1)\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(g(x_2)\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(g(x_n)\)&lt;/span>&lt;/th>
&lt;th align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(P\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_1)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_2)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(p(x_n)\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\dotsb\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>注意&lt;/strong>：当函数值&lt;span class="math">\(g(x_1),g(x_2),\dotsb\)&lt;/span>有相等值时，则把那些相等的值分别合并，并把对应的概率相加。&lt;/p>
&lt;p>以下是几个例题：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/单个离散型随机变量函数的分布.png" alt="单个离散型随机变量函数的分布" />&lt;p class="caption">单个离散型随机变量函数的分布&lt;/p>
&lt;/div>
&lt;h3 id="单个连续型随机变量函数的分布">单个连续型随机变量函数的分布&lt;/h3>
&lt;p>找出离散型随机变量函数分布相对简单，只要按步骤求出对应随机变量值就可以了。而对连续性随机变量&lt;span class="math">\(X\)&lt;/span>，我们需要分两种情况讨论。&lt;/p>
&lt;h4 id="当gx严格单调时">当&lt;span class="math">\(g(x)\)&lt;/span>严格单调时&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>定理1&lt;/strong>:设&lt;span class="math">\(X\)&lt;/span>是连续性随机变量，其密度函数为&lt;span class="math">\(p_{_X}(x)\)&lt;/span>.&lt;span class="math">\(Y=g(X)\)&lt;/span>是另一个随机变量。若&lt;span class="math">\(y=g(x)\)&lt;/span>严格单调，其反函数&lt;span class="math">\(h(y)\)&lt;/span>有连续导函数，则&lt;span class="math">\(Y=g(X)\)&lt;/span>的密度函数为 &lt;span class="math">\[p_{_Y}(y)=\begin{cases}
p_{_X}(h(y))|h&amp;#39;(y)|, &amp;amp;a&amp;lt;y&amp;lt;b\\
0,&amp;amp;\text{otherwise}
\end{cases}\tag{1}\]&lt;/span> 其中，&lt;span class="math">\(a=\min\{g(-∞),g(∞)\},b=\max\{g(-∞),g(∞)\}\)&lt;/span>，即&lt;span class="math">\(a，b\)&lt;/span>为边界。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;blockquote>
&lt;p>不妨设&lt;span class="math">\(g(x)\)&lt;/span>为严格单调递增函数，这时它的反函数&lt;span class="math">\(h(y)\)&lt;/span>也是严格单调递增的,有&lt;span class="math">\(y\in (a,b)\)&lt;/span>，且&lt;span class="math">\(h&amp;#39;(y)&amp;gt;0\)&lt;/span>。 对于随机变量&lt;span class="math">\(Y\)&lt;/span>，由于其取值范围为&lt;span class="math">\((a,b)\)&lt;/span>,当&lt;/p>
&lt;ul>
&lt;li>当&lt;span class="math">\(y&amp;lt;a\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=0\)&lt;/span>&lt;/li>
&lt;li>当&lt;span class="math">\(y&amp;gt;b\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=1\)&lt;/span>&lt;/li>
&lt;li>当&lt;span class="math">\(a≤y≤b\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=P(g(X)≤y)=P(X≤h(y))=\int_{-\infty}^{h(y)}p_{_X}(x)\mathrm{d}x\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>由此可得&lt;span class="math">\(Y\)&lt;/span>的概率密度函数为 &lt;span class="math">\[p_{_Y}(y)=\begin{cases}
p_{_X}[h(y)]\times h&amp;#39;(y),&amp;amp;a&amp;lt;y&amp;lt;b\\
0,&amp;amp;\text{others}
\end{cases}\tag{2}\]&lt;/span> &lt;span class="math">\[\stackrel{\text{加绝对值符号无所谓}}{\Longrightarrow}式(1)\]&lt;/span> 当&lt;span class="math">\(g(x)\)&lt;/span>时严格单调递减函数时，这时它的反函数&lt;span class="math">\(h(y)\)&lt;/span>也是严格单调递减的,有&lt;span class="math">\(y\in (a,b)\)&lt;/span>，且&lt;span class="math">\(h&amp;#39;(y)&amp;lt;0\)&lt;/span>。&lt;/p>
&lt;p>对于随机变量&lt;span class="math">\(Y\)&lt;/span>，由于其取值范围为&lt;span class="math">\((a,b)\)&lt;/span>,当&lt;/p>
&lt;ul>
&lt;li>当&lt;span class="math">\(y&amp;lt;a\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=0\)&lt;/span>(这一项和单调递增函数一样)&lt;/li>
&lt;li>当&lt;span class="math">\(y&amp;gt;b\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=1\)&lt;/span>(这一项和单调递增函数一样)&lt;/li>
&lt;li>当&lt;span class="math">\(a≤y≤b\)&lt;/span>时，&lt;span class="math">\(F_Y(y)=P(Y≤y)=P(g(X)≤y){\color{red}=P(X≥h(y))}\)&lt;/span>。由于&lt;span class="math">\(g(x)\)&lt;/span>是减函数，所以自变量符号应从“≤”变成“≥”。而&lt;span class="math">\(P(X≥h(y))=1-P(X≤h(y))=1-\int_{-\infty}^{h(y)}p_{_X}(x)\mathrm{d}x\)&lt;/span>。&lt;/li>
&lt;/ul>
&lt;p>由于&lt;span class="math">\(h(y)\)&lt;/span>是单调递减函数，所以&lt;span class="math">\(h&amp;#39;(y)&amp;lt;0\)&lt;/span>。由此可得&lt;span class="math">\(Y\)&lt;/span>的概率密度函数为 &lt;span class="math">\[p_{_Y}(y)=\begin{cases}
-p_{_X}[h(y)]\times h&amp;#39;(y),&amp;amp;a&amp;lt;y&amp;lt;b\\
0,&amp;amp;\text{others}
\end{cases}\tag{3}\]&lt;/span> &lt;span class="math">\[\stackrel{\text{h&amp;#39;(y)≤0}}{\Longrightarrow}式(1)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>特别的，当&lt;span class="math">\(g(X)\)&lt;/span>的表达式为随机变量&lt;span class="math">\(X\)&lt;/span>的累积分布函数（CDF）&lt;span class="math">\(F_{_X}(X)\)&lt;/span>时，我们有以下命题：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>命题1-1&lt;/strong>：设存在一随机变量&lt;span class="math">\(X\)&lt;/span>，另一随机变量&lt;span class="math">\(Z=F(X)\)&lt;/span>，其中&lt;span class="math">\(F(\cdot)\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的分布函数，那么&lt;span class="math">\(Z\thicksim U(0,1)\)&lt;/span>&lt;/p>
&lt;p>&lt;strong>命题1-2&lt;/strong>：令&lt;span class="math">\(Z\thicksim U(0,1)\)&lt;/span>，&lt;span class="math">\(F^{-1}\)&lt;/span>是随机变量&lt;span class="math">\(X\)&lt;/span>分布函数&lt;span class="math">\(F\)&lt;/span>的反函数，那么&lt;span class="math">\(X=F^{-1}(Z)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这一对命题证明比较容易，借助&lt;code>定理1&lt;/code>同样的证明方法就能证明。&lt;strong>本质上这一对命题是&lt;code>定理1&lt;/code>的一个特例&lt;/strong>。但是一开始这个结论令我很不解，主要是我没有分清累积分布函数和随机变量函数的关系。这里的&lt;span class="math">\(F\)&lt;/span>只是随机变量的函数恰好等于累积分布函数，&lt;span class="math">\(X\)&lt;/span>经过&lt;span class="math">\(F\)&lt;/span>变换的得到随机变量&lt;span class="math">\(Z=F(X)\)&lt;/span>。而在累积分布函数CDF中，&lt;span class="math">\(F(x)\)&lt;/span>(注意这里写正确了，一定是&lt;span class="math">\(x\)&lt;/span>)求的是随机变量&lt;span class="math">\(X≤x\)&lt;/span>的概率值，是一个固定值。这二者看着相似，实有根本区别。&lt;/p>
&lt;p>此外，这对命题在计算机产生各种分布随机变量时有很大作用。因为我们只要能产生均匀分布的随机变量&lt;span class="math">\(Z\thicksim U(0,1)\)&lt;/span>就可以通过随机变量&lt;span class="math">\(X\)&lt;/span>的CDF的反函数&lt;span class="math">\(F^{-1}(Z)\)&lt;/span>，得到一个服从累计分布函数&lt;span class="math">\(F\)&lt;/span>的随机变量&lt;span class="math">\(X\)&lt;/span>。事实上，计算机中大多数常用分布随机变量都是通过这个方式产生的,这也是计算机进行随机模拟法（又称蒙特卡洛法）的基础。（有趣的是最常见的正态分布却不用这个方法产生，因为正态分布的CDF不好求，其CDF的反函数更不好求。正态分布随机变量可以以均匀分布随机变量用Box-Muller算法或其改进算法生成。）&lt;/p>
&lt;h4 id="单个连续型随机变量函数的几个常用命题">单个连续型随机变量函数的几个常用命题&lt;/h4>
&lt;blockquote>
&lt;p>命题2-1：设随机变量&lt;span class="math">\(X\)&lt;/span>服从正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>，当&lt;span class="math">\(a\neq 0\)&lt;/span>时，有&lt;span class="math">\(Y=aX+b\thicksim N(a\mu+b,a^2\sigma^2)\)&lt;/span>。&lt;/p>
&lt;p>命题2-2：对数正态分布，设随机变量&lt;span class="math">\(X\)&lt;/span>服从正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>，则&lt;span class="math">\(Y=e^X\)&lt;/span>的概率密度函数为： &lt;span class="math">\[p_Y(y)=\begin{cases}
\frac{1}{\sqrt{2\pi}y\sigma}e^{-\frac{(\ln y -\mu)^2}{2\sigma^2}},&amp;amp;y&amp;gt;0\\
0, &amp;amp;y \le 0
\end{cases}\]&lt;/span> 即这个分布为对数正态分布&lt;span class="math">\(LN(\mu,\sigma^2)\)&lt;/span>&lt;/p>
&lt;p>命题2-3：随机变量&lt;span class="math">\(X\)&lt;/span>服从伽马分布&lt;span class="math">\(Ga(\alpha,\lambda)\)&lt;/span>，则当&lt;span class="math">\(k&amp;gt;0\)&lt;/span>时，有&lt;span class="math">\(Y=kX\thicksim Ga(\alpha,\lambda/k)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>以上三个命题都是单调函数变换，可以用&lt;code>定理1&lt;/code>证明。&lt;/p>
&lt;h4 id="贝叶斯假设的一个悖论">贝叶斯假设的一个悖论&lt;/h4>
&lt;p>随机变量函数的分布还曾被Fisher（发明F分布、Fisher信息量等，频率学派大佬）用来举例反对贝叶斯学派。迫使贝叶斯学派重新找角度解释先验分布的合理性。&lt;/p>
&lt;p>事情大体是这样的，贝叶斯学派在求解概率问题时，&lt;strong>先要规定主观规定一个先验分布，然后获取样本，再通过样本和条件概率修正先验分布，得到后验分布&lt;/strong>。其中，后面两步都是有扎实的数学理论支撑的，唯有第一步选取先验分布是一个凭个人主观推测的事情。贝叶斯采取了这样一个假设（贝叶斯假设）：&lt;em>如果我们对某一个统计量没有任何了解，那么我们就不应该对任何值有偏好，所以在统计量的空间里，选取均匀分布作为先验分布&lt;/em>。&lt;/p>
&lt;p>这个解释初一听，还是非常有道理的。但是Fisher提出：按照贝叶斯假设某一随机变量&lt;span class="math">\(\theta\)&lt;/span>的先验分布属于均匀分布，因为我们对其一无所知。那么对于随机变量&lt;span class="math">\(\beta=\theta^2\)&lt;/span>，我们对&lt;span class="math">\(\beta\)&lt;/span>也同样一无所知，那么&lt;span class="math">\(\beta\)&lt;/span>是不是应该也是均匀分布？两者同时是均匀分布显然是不可能的。（注1证明）。&lt;strong>到底谁应该是取均匀分布&lt;/strong>?这一悖论被Fisher用来反对贝叶斯学派的可靠性。&lt;/p>
&lt;blockquote>
&lt;p>注1：如果一随机变量&lt;span class="math">\(X\thicksim U(0,1)\)&lt;/span>，那么它的平方&lt;span class="math">\(Y=X^2\)&lt;/span>的概率密度函数pdf并不是均匀分布。&lt;/p>
&lt;p>证明：&lt;span class="math">\(X\in [0,1]\Rightarrow Y\in [0,1]\)&lt;/span> 累积分布函数CDF:&lt;span class="math">\(F_X(x) = x\)&lt;/span>. &lt;span class="math">\(F_Y(Y)=P(Y\le y)=P(X^2\le y)=P(-\sqrt{y}\le X\le\sqrt{y})\)&lt;/span>。因为&lt;span class="math">\(X\in [0,1]\)&lt;/span>，所以&lt;span class="math">\(P(-\sqrt{y}\le X\le\sqrt{y})=P(X\le\sqrt{y})=F_X(\sqrt{y})=\sqrt{y}\Rightarrow F_Y(y)=\sqrt{y}\)&lt;/span>。因此 &lt;span class="math">\[f_Y(y)=\begin{cases}\frac{1}{2\sqrt{y}},y\in [0,1]\\0,\text{others}\end{cases}\]&lt;/span> 显然&lt;span class="math">\(Y\)&lt;/span>并不是均匀分布。&lt;/p>
&lt;/blockquote>
&lt;h4 id="当gx为其他形式时">当&lt;span class="math">\(g(x)\)&lt;/span>为其他形式时&lt;/h4>
&lt;p>当&lt;code>定理1&lt;/code>不适用时，我们可以从最基础的&lt;strong>分布函数&lt;/strong>（&lt;span class="math">\(F_Y(y)=P(g(X)≤y)\)&lt;/span>）入手，就像证明&lt;code>定理1&lt;/code>中使用的方法那样。具体可见下面例子&lt;/p>
&lt;div class="figure">
&lt;img src="../images/随机变量函数的分布例3.jpg" alt="随机变量函数的分布例3" />&lt;p class="caption">随机变量函数的分布例3&lt;/p>
&lt;/div>
&lt;p>对照&lt;span class="math">\(\chi^2\)&lt;/span>分布的密度函数，可以看出&lt;span class="math">\(Y\thicksim \chi^2(1)\)&lt;/span>。（因为&lt;span class="math">\(\chi^2(n)\)&lt;/span>正是n个服从正态分布随机变量的平方和的分布）&lt;/p>
&lt;div class="figure">
&lt;img src="../images/随机变量函数的分布例4.jpg" alt="随机变量函数的分布例4" />&lt;p class="caption">随机变量函数的分布例4&lt;/p>
&lt;/div>
&lt;h2 id="多维随机变量函数的分布">多维随机变量函数的分布&lt;/h2>
&lt;p>设&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>为n维随机变量，则&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>的函数&lt;span class="math">\(Y=g(X_1,X_2,\dotsb,X_n)\)&lt;/span>时一维随机变量。现在问题是如何由&lt;span class="math">\((X_1,X_2,\dotsb,X_n)\)&lt;/span>的联合分布，求出&lt;span class="math">\(Y\)&lt;/span>的分布。这是一类技巧性很强的工作，不仅对离散场合和连续场合由不同的方法，而且对不同形式的函数&lt;span class="math">\(g(X_1,X_2,\dotsb,X_n)\)&lt;/span>要采用不同的方法，甚至有些方法只对特殊形式的&lt;span class="math">\(g(\cdot)\)&lt;/span>适用。下面就几个常见的场景做介绍。&lt;/p>
&lt;blockquote>
&lt;p>补充定义1：n维随机变量（向量）：如果&lt;span class="math">\(X_1(\omega),X_2(\omega),\dotsb,X_n(\omega)\)&lt;/span>是定义在&lt;strong>同一个样本空间&lt;/strong>&lt;span class="math">\(\Omega=\{\omega\}\)&lt;/span>上的n个随机变量，则称 &lt;span class="math">\[X(\omega)=(X_1(\omega),X_2(\omega),\dotsb,X_n(\omega))\]&lt;/span> 为n维随机变量（向量）。&lt;/p>
&lt;/blockquote>
&lt;p>注意，多维随机变量的关键是定义在&lt;strong>同一个样本空间&lt;/strong>，对于不同样本空间&lt;span class="math">\(\Omega_1,\Omega_2\)&lt;/span>上的两个随机变量。我们只能在其乘机空间&lt;span class="math">\(\Omega_1\times\Omega_2=\{(\omega_1,\omega_2):\omega_1\in \Omega_1,\omega_2\in\Omega_2\}\)&lt;/span>及其事件域上讨论。以下多维随机变量默认遵从这一点。&lt;/p>
&lt;h3 id="多维离散型随机变量函数的分布">多维离散型随机变量函数的分布&lt;/h3>
&lt;p>首先，如果离散随机变量概率空间比较小，可将&lt;span class="math">\(Y\)&lt;/span>的取值一一求出再合并得到分布列表。这是最直观最基本的方法。见下例：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/离散型随机变量函数的分布例5.jpg" alt="多维离散型随机变量分布例5" />&lt;p class="caption">多维离散型随机变量分布例5&lt;/p>
&lt;/div>
&lt;h4 id="离散型随机变量和的分布与卷积公式">离散型随机变量和的分布与卷积公式&lt;/h4>
&lt;p>下面我们讨论一种比较常见的情景，即多维离散型随机变量和的分布。我们先讨论两维随机变量，然后在拓展到多维。&lt;/p>
&lt;p>设随机变量&lt;span class="math">\(Z=X+Y\)&lt;/span>，其中&lt;span class="math">\(X,Y\)&lt;/span>都是在同一样本空间&lt;span class="math">\(\Omega\)&lt;/span>的一维离散型随机变量，事件&lt;span class="math">\(\{Z=k\}\)&lt;/span>可以由如下诸互不相容事件 &lt;span class="math">\[\{X=i,Y=k-i\}, i\in \Omega\]&lt;/span> 的并集组成，再考虑到&lt;span class="math">\(X,Y\)&lt;/span>的独立性，则对&lt;span class="math">\(\forall k \in \Omega_Z\)&lt;/span>，有 &lt;span class="math">\[P(Z=k)=\sum_{i\in\Omega}P(X=i)P(Y=k-i)\tag{4}\]&lt;/span> 这个概率等式被称为离散场合下的&lt;strong>卷积公式&lt;/strong>。我们在这里让&lt;span class="math">\(P(X=i)\)&lt;/span>的&lt;span class="math">\(i\in\Omega\)&lt;/span>，而对&lt;span class="math">\(P(Y=k-i)\)&lt;/span>则要求所有超出样本空间的值的概率都为0。&lt;/p>
&lt;p>从二维到多维的变换可以看出是一个逐步的过程，两两逐渐相加即可。&lt;span class="math">\(N\)&lt;/span>维离散随机变量的和在一般需要&lt;span class="math">\(N-1\)&lt;/span>次求和（本质是对和的&lt;span class="math">\(N-1\)&lt;/span>次分解）。&lt;/p>
&lt;h4 id="思考分布可加性的本质">思考：分布可加性的本质&lt;/h4>
&lt;p>通过离散场合下的卷积公式，我们可以证明如下三个命题：&lt;/p>
&lt;blockquote>
&lt;p>命题3-1：&lt;strong>泊松分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\thicksim P(\lambda_1),X_2\thicksim P(\lambda_2)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\thicksim P(\lambda_1+\lambda_2)\)&lt;/span>&lt;/p>
&lt;p>命题3-2：&lt;strong>二项分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\thicksim P(n,p),X_2\thicksim P(m,p)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\thicksim P(m+n,p)\)&lt;/span>&lt;/p>
&lt;p>命题3-3：&lt;strong>负二项分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\thicksim Nb(n,p),X_2\thicksim Nb(m,p)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\thicksim Nb(m+n,p)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>卷积显然和直接相加差别很大。那为什么有些分布能满足可加性呢？我们除了用形式化的数学方法证明，有什么本质能够理解本质的方法吗？&lt;/p>
&lt;p>其实回想我们在研究最基础的伯努利分布时，将N个伯努利分布叠加后，形成了二项分布&lt;span class="math">\(B(N,p)\)&lt;/span>，换句话说二项分布就是独立同分布(i.i.d)的伯努利分布加出来的，因此多几个伯努力分布相加还是二项分布，只是二项分布的参数有所改变。这是二项分布可加性的来源。而泊松分布是二项分布在&lt;span class="math">\(\lambda=np\)&lt;/span>为定值时, &lt;span class="math">\(p\rightarrow 0, n\rightarrow\infty\)&lt;/span>的极限，因此本质也是一种二项分布，不难理解为什么泊松分布也有可加性了。此外，作为二项分布&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>的极限的正态分布，自然也是有可加性的，但是这已经拓展到连续分布了。（&lt;span class="math">\(e\)&lt;/span>是联系离散和连续的桥梁之一，这也是从离散的二项分布到正态分布表达式突然多出自然常数的一个暗示）。而其它具有可加性的离散分布，比如负二项分布是由N个独立同分布的几何分布加出来的，带有可加性也算是自然而然了。&lt;/p>
&lt;p>在连续分布中，也有一些分布是“加”出来的。最常见、应用最广泛的是&lt;strong>指数分布&lt;span class="math">\(X\thicksim \exp(\lambda)\)&lt;/span>的和&lt;/strong>，k个指数分布&lt;span class="math">\(X_i\thicksim \exp(\lambda),i\in\{1,2,\dotsb,k\}\)&lt;/span>相加是Erlang分布 &lt;span class="math">\[
X_{i}\sim \exp (\lambda ),\\
\sum_{i=1}^{k}{X_{i}}\sim \operatorname {Erlang} (k,\lambda )\Rightarrow p(x;k,\lambda )=\sum_{i=1}^{k}{X_{i}}\sim \operatorname {Erlang} (k,\lambda )\\
p(x;k,\lambda )={\lambda ^{k}x^{{k-1}}e^{{-\lambda x}} \over (k-1)!}\quad {\text{for }}x,\lambda \geq 0,k\geq 1
\]&lt;/span> 它和指数分布的参数&lt;span class="math">\(\lambda\)&lt;/span>是一样的，参数&lt;span class="math">\(k\)&lt;/span>是指由&lt;span class="math">\(k\)&lt;/span>个指数分布相加。其实际含义可以指代&lt;span class="math">\(k\)&lt;/span>个用户/物件到达所用的时间间隔等。显然，有指数分布加出来的Erlang分布也有可加性，无非就是多几个指数分布的和。而将&lt;span class="math">\(k\rightarrow \alpha\)&lt;/span>延拓到正实数域，就是Gamma分布&lt;span class="math">\(X\sim \Gamma(\alpha,\lambda)\)&lt;/span>。 &lt;span class="math">\[p(x;\alpha ,\lambda )={\frac {\lambda ^{\alpha }x^{\alpha -1}e^{-\lambda x}}{\Gamma (\alpha )}}\quad {\text{for }}x&amp;gt;0\quad \alpha ,\lambda &amp;gt;0\]&lt;/span> 其中,&lt;span class="math">\(\Gamma(\alpha)\)&lt;/span>是Gamma函数而卡方分布又是Gamma分布的特例，即&lt;span class="math">\(X\sim \chi^2(n)=\Gamma(\frac{n}{2},\frac{1}{2})\)&lt;/span>， &lt;span class="math">\[
p(x;n)={\frac {1}{2^{\frac {n}{2}}\Gamma(\frac{n}{2})}}x^{\frac {n}{2}-1}e^{\frac {-x}{2}}
\]&lt;/span> 因此这两种分布(Gamma分布、卡方分布)也不出意外的有可加性。还有一种满足可加性的分布，柯西分布，对它我了解不多，暂不描述。&lt;/p>
&lt;h3 id="多维随机变量的最大最小值分布">多维随机变量的最大最小值分布&lt;/h3>
&lt;p>最大值最小值的分布利用了分布函数和多个随机变量间的独立性，是利用定义就能搞定的多维分布。&lt;/p>
&lt;h4 id="最大值分布">最大值分布&lt;/h4>
&lt;blockquote>
&lt;p>命题4：设&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>是相互独立的n个随机变量，若&lt;span class="math">\(Y=\max\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>。则对于&lt;span class="math">\(Y\)&lt;/span>的分布有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X_i\sim F_i(x)，,i=1,2,\dotsb,n, Y\sim \prod\limits_{i=1}^n F_i(y)\)&lt;/span>;&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>i.i.d，即&lt;span class="math">\(X_i\sim F(x)\)&lt;/span>，则有&lt;span class="math">\(Y\sim [F(y)]^n\)&lt;/span>&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>为连续随机变量，且i.i.d，则&lt;span class="math">\(Y\)&lt;/span>的概率密度函数是&lt;span class="math">\(P_Y(y)=n[F(y)]^{n-1}p(y)\)&lt;/span>&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>都服从&lt;span class="math">\(X\sim \exp(\lambda)\)&lt;/span>，则&lt;span class="math">\(Y\)&lt;/span>的概率密度函数是 &lt;span class="math">\[p_Y(y)=\begin{cases}0,&amp;amp;y&amp;lt;0\\n(1-e^{-\lambda y})^{n-1}\lambda e^{-\lambda y},&amp;amp;y\ge 0\end{cases}\]&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>证明： (1) 有&lt;span class="math">\(Y=\max\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>，则&lt;span class="math">\(F_Y(y)=P(max\{X_1,X_2,\dotsb,X_n\}≤y)=P(X_1≤y,X_2≤y,\dotsb,X_n≤y)\stackrel{\text{独立性}}{=}P(X_1≤y)P(X_2≤y)\dotsb P(X_n≤y)=\prod\limits_{i=1}^n F_i(y)\)&lt;/span>。即&lt;span class="math">\(Y\sim \prod\limits_{i=1}^n F_i(y)\)&lt;/span>&lt;/p>
&lt;p>(2)因为诸&lt;span class="math">\(X_i\)&lt;/span>i.i.d，所以&lt;span class="math">\(\prod\limits_{i=1}^n F_i(y)=[F(y)]^n\)&lt;/span>，即&lt;span class="math">\(Y \sim [F(y)]^{n}\)&lt;/span> (3)将结果(2)的求导即可得证。 (4)指数分布符合(3)的前提，可以直接带入(3)的公式可证。&lt;/p>
&lt;h4 id="最小值分布">最小值分布&lt;/h4>
&lt;blockquote>
&lt;p>命题5：设&lt;span class="math">\(X_1,X_2,\dotsb,X_n\)&lt;/span>是相互独立的n个随机变量，若&lt;span class="math">\(Y=\min\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>。则对于&lt;span class="math">\(Y\)&lt;/span>的分布有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X_i\sim F_i(x)，,i=1,2,\dotsb,n, Y\sim 1-\prod\limits_{i=1}^n [1-F_i(y)]\)&lt;/span>;&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>i.i.d，即&lt;span class="math">\(X_i\sim F(x)\)&lt;/span>，则有&lt;span class="math">\(Y\sim 1-[1-F(y)]^n\)&lt;/span>&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>为连续随机变量，且i.i.d，则&lt;span class="math">\(Y\)&lt;/span>的概率密度函数是&lt;span class="math">\(P_Y(y)=n[1-F(y)]^{n-1}p(y)\)&lt;/span>&lt;/li>
&lt;li>若诸&lt;span class="math">\(X_i\)&lt;/span>都服从&lt;span class="math">\(X\sim \exp(\lambda)\)&lt;/span>，则&lt;span class="math">\(Y\)&lt;/span>的概率密度函数是 &lt;span class="math">\[p_Y(y)=\begin{cases}0,&amp;amp;y&amp;lt;0\\n\lambda e^{-n\lambda y},&amp;amp;y\ge 0\end{cases}\]&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>证明： (1)由于&lt;span class="math">\(Y=\min\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>，因此&lt;span class="math">\(F_Y(y)=P(\min\{X_1,X_2,\dotsb X_n\}≤y)=1-P(\min\{X_1,X_2,\dotsb X_n\}&amp;gt;y)=1-P(X_1&amp;gt;y,X_2&amp;gt;y,\dotsb,X_n&amp;gt;y)\stackrel{\text{独立性了}}{=}1-P(X_1&amp;gt;y)P(X_2&amp;gt;y)\dotsb P(X_n&amp;gt;y)=1-\prod\limits_{i=1}^n[1-F_i(y)]\)&lt;/span> (2)因为诸&lt;span class="math">\(X_i\)&lt;/span>i.i.d，即&lt;span class="math">\(X_i\sim F(x)\)&lt;/span>，代入(1)的结果则有&lt;span class="math">\(Y\sim 1-[1-F(y)]^n\)&lt;/span> (3)将结果(2)的求导即可得证。 (4)指数分布符合(3)的前提，可以直接带入(3)的公式可证。&lt;/p>
&lt;h3 id="多维连续型随机变量函数的分布">多维连续型随机变量函数的分布&lt;/h3>
&lt;h4 id="连续型随机变量和的分布与卷积公式">连续型随机变量和的分布与卷积公式&lt;/h4>
&lt;p>我们之前已经研究了离散型多维随机变量的卷积公式，对于连续型多维随机变量，我们采用类似的方式推导，只是把求和换成积分。&lt;/p>
&lt;blockquote>
&lt;p>连续型随机变量的卷积公式：设&lt;span class="math">\(X,Y\)&lt;/span>是两个连续且独立的随机变量，其密度函数分别是&lt;span class="math">\(p_{_X}(x),p_{_Y}(y)\)&lt;/span>，则其和&lt;span class="math">\(Z=X+Y\)&lt;/span>的概率密度函数为： &lt;span class="math">\[\begin{aligned}p_{_Z}(z)&amp;amp;=\int_{-\infty}^\infty p_{_X}(x)p_{_Y}(z-x)dx\\
&amp;amp;=\int_{-\infty}^\infty p_{_X}(z-y)p_{_Y}(y)dy
\end{aligned}\tag{5}\]&lt;/span> 上式被称为连续随机变量的卷积公式。&lt;/p>
&lt;/blockquote>
&lt;p>证明： &lt;span class="math">\(Z=X+Y\)&lt;/span>的分布函数按照定义为 &lt;span class="math">\[\begin{aligned}
F_Z(z)&amp;amp;=P(X+Y≤Z)=\iint_{x+y≤z}p_{_X}(x)p_{_Y}(y)dxdy\\
&amp;amp;=\int_{-\infty}^\infty\{\int_{-\infty}^{z-y}p_{_X}(x)dx\}p_{_Y}(y)dy\\
&amp;amp;\xlongequal{变量替换}\int_{-\infty}^\infty\int_{-\infty}^z p_{_X}(t-y)p_{_Y}(y)dtdy\\
\Rightarrow F_{Z}(z)&amp;amp;=\int_{-\infty}^z(\int_{-\infty}^∞ p_{_X}(t-y)p_{_Y}(y)dy)dt
\end{aligned}
\]&lt;/span> 对&lt;span class="math">\(F_{Z}(z)\)&lt;/span>以&lt;span class="math">\(z\)&lt;/span>求导，可得 &lt;span class="math">\[
p_{_Z}(z)=\int_{-\infty}^∞ p_{_X}(z-y)p_{_Y}(y)dy
\]&lt;/span> 令上式积分中&lt;span class="math">\(y=z-x\)&lt;/span>则可得： &lt;span class="math">\[
p_{_Z}(z)=\int_{-\infty}^∞ p_{_X}(x)p_{_Y}(z-x)dx
\]&lt;/span> 得证。&lt;/p>
&lt;p>在之前&lt;a href="#思考分布可加性的本质">思考：分布可加性的本质&lt;/a>章节中我们已经讨论了为什么有些分布具有可加性，也涉及了部分连续性随机分布，这里我们给出详细命题：&lt;/p>
&lt;blockquote>
&lt;p>命题6-1：&lt;strong>正态分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\sim N(\mu_1,\sigma_1^2),Y\sim N(\mu_2,\sigma_2^2)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)\)&lt;/span>&lt;/p>
&lt;p>命题6-2：&lt;strong>伽马分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\sim Ga(\alpha_1,\lambda),Y\sim Ga(\alpha_2,\lambda)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\sim Ga(\alpha_1+\alpha_2,\lambda)\)&lt;/span>&lt;/p>
&lt;p>命题6-3：&lt;strong>卡方分布的可加性&lt;/strong>。设随机变量&lt;span class="math">\(X\sim \chi^2(m),Y\sim \chi^2(n)\)&lt;/span>，且&lt;span class="math">\(X,Y\)&lt;/span>独立，则&lt;span class="math">\(Z=X+Y\sim \chi^2(m+n)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明： (1):正态分布的可加性。首先&lt;span class="math">\(Z=X+Y\)&lt;/span>的定义域依然是&lt;span class="math">\((-∞,∞)\)&lt;/span>,利用连续随机变量的卷积公式&lt;span class="math">\((5)\)&lt;/span>可得： &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\int_{-∞}^∞\frac{1}{2\pi\sigma_1\sigma_2}\exp\left\{ -\frac{1}{2}\left[\frac{(z-y-\mu_1)^2}{\sigma_1^2}+\frac{(y-\mu_2)^2}{\sigma_2^2} \right]\right\}dy\\
&amp;amp;\xlongequal[u=y-\mu_2]{v=z-(\mu_1+\mu_2)}\frac{1}{2\pi\sigma_1\sigma_2}\int_{-∞}^∞\exp\left\{\frac{1}{2}\left[\frac{(v-u)^2}{\sigma_1^2}+\frac{u^2}{\sigma_2^2} \right]\right\}dy\\
&amp;amp;=\frac{1}{2\pi\sigma_1\sigma_2}\int_{-∞}^∞\exp\left\{\frac{1}{2}\left[\frac{(v-u)^2}{\sigma_1^2}+\frac{u^2}{\sigma_2^2} \right]\right\}du\\
\end{aligned}
\]&lt;/span> 由于变量&lt;span class="math">\(v\)&lt;/span>与概率密度函数自变量&lt;span class="math">\(z\)&lt;/span>有关，而与积分变量&lt;span class="math">\(y\)&lt;/span>无关，因此，我们将与&lt;span class="math">\(u\)&lt;/span>无关的&lt;span class="math">\(v\)&lt;/span>从积分符号中提取出来，再把&lt;span class="math">\(u\)&lt;/span>凑成平方项，方可化腐朽为神奇 。同时，&lt;span class="math">\(u=y-\mu_1\in(-∞,∞)\)&lt;/span>，积分区间未变。 &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\frac{1}{2\pi\sigma_1\sigma_2}\int_{-∞}^∞\exp\left\{-\frac{1}{2}\left[\frac{\sigma_1^2+\sigma_2^2}{\sigma_1^2\sigma_2^2}u^2-\frac{2}{\sigma_1^2}uv+\frac{v^2}{\sigma_1^2} \right]\right\}du\\
\overset{u凑平方}{=}&amp;amp;\frac{1}{2\pi\sigma_1\sigma_2}\int_{-∞}^∞\exp\left\{-\frac{1}{2}\left[\frac{\sigma_1^2+\sigma_2^2}{\sigma_1^2\sigma_2^2}u^2-\frac{2}{\sigma_1^2}uv+\frac{\sigma_2^2}{\sigma_1^2(\sigma_1^2+\sigma_2^2)}v^2\right.\right. \\
&amp;amp;\left.\left.-\frac{\sigma_2^2}{\sigma_1^2(\sigma_1^2+\sigma_2^2)}v^2+\frac{v^2}{\sigma_1^2} \right]\right\}du\\
&amp;amp;=\frac{1}{2\pi\sigma_1\sigma_2}\exp\left\{-\frac{1}{2}\frac{v^2}{\sigma_1^2+\sigma_2^2}\right\}\\
&amp;amp;\cdot\int_{-∞}^∞\exp\left\{-\frac{1}{2}\left(\frac{\sqrt{\sigma_1^2+\sigma_2^2}}{\sigma_1\sigma_2}u-\frac{\sigma_2}{\sigma_1\sqrt{\sigma_1^2+\sigma_2^2}}v\right)^2\right\}du
\end{aligned}
\]&lt;/span> 令&lt;span class="math">\(t=\frac{\sqrt{\sigma_1^2+\sigma_2^2}}{\sigma_1\sigma_2}u-\frac{\sigma_2}{\sigma_1\sqrt{\sigma_1^2+\sigma_2^2}}v\)&lt;/span>，显然&lt;span class="math">\(v\)&lt;/span>无论取什么值，在&lt;span class="math">\(u\in(-\infty,\infty)\)&lt;/span>时，都有&lt;span class="math">\(t\in (-\infty,\infty)\)&lt;/span>。同时，&lt;span class="math">\(du=\frac{\sigma_1\sigma_2}{\sqrt{\sigma_1^2+\sigma_2^2}}dt\)&lt;/span>。因此： &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\frac{1}{2\pi\sigma_1\sigma_2}\exp\left\{-\frac{1}{2}\frac{v^2}{\sigma_1^2+\sigma_2^2}\right\}\int_{-\infty}^\infty\exp \{-\frac{1}{2}t^2\}\frac{\sigma_1\sigma_2}{\sqrt{\sigma_1^2+\sigma_2^2}}dt\\
&amp;amp;=\frac{1}{2\pi\sqrt{\sigma_1^2+\sigma_2^2}}\exp\left\{-\frac{1}{2}\frac{v^2}{\sigma_1^2+\sigma_2^2}\right\}\int_{-\infty}^\infty\exp \{-\frac{1}{2}t^2\}dt
\end{aligned}
\]&lt;/span> 根据高斯积分有&lt;span class="math">\(\int_{-\infty}^\infty\exp \{-\frac{1}{2}t^2\}dt=\sqrt{2\pi}\)&lt;/span>，代入上式，并恢复&lt;span class="math">\(v=z-(\mu_1+\mu_2)\)&lt;/span>，则有 &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\frac{1}{2\pi\sqrt{\sigma_1^2+\sigma_2^2}}\exp\left\{-\frac{1}{2}\frac{(z-\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}\right\}\cdot \sqrt{2\pi}\\
&amp;amp;=\frac{1}{2\pi\sqrt{\sigma_1^2+\sigma_2^2}}\exp\left\{-\frac{(z-\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}\right\}\\
&amp;amp;\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)
\end{aligned}
\]&lt;/span> 得证。此命题还可扩展成线性可加性：&lt;/p>
&lt;blockquote>
&lt;p>命题6-4：任意n个相互独立的正态随机变量的线性组合仍是正态随机变量。即若&lt;span class="math">\(X_i\sim N(\mu_i,\sigma_i^2),i=1,2,\dotsb,n\)&lt;/span>，则&lt;span class="math">\(Y=\sum_{i=1}^n a_i X_i +b \sim N(\sum_{i=1}^n a_i \mu_i +b,\sum_{i=1}^n a_i^2 \sigma_i^2)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>(2)：证明伽马分布的可加性。首先指出&lt;span class="math">\(Z=X+Y\)&lt;/span>的取值范围仍然在&lt;span class="math">\((0,∞)\)&lt;/span>,所以当&lt;span class="math">\(z&amp;lt;0\)&lt;/span>时，有&lt;span class="math">\(p_{_Z}(z)=0\)&lt;/span>。而当&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时，可用卷积公式，此时被积函数&lt;span class="math">\(p_{_X}(z-y)p_{_Y}(y)\)&lt;/span>的非零区域为&lt;span class="math">\(0&amp;lt;y&amp;lt;z\)&lt;/span>,故 &lt;span class="math">\[
\begin{aligned}
p_{_Z}(z)&amp;amp;=\int_{-\infty}^\infty p_{_Y}(y)p_{_X}(z-y)dy\\
&amp;amp;=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\int_0^z (z-y)^{\alpha_1-1}e^{-\lambda(z-y)}y^{\alpha_2-1}e^{-\lambda y} dy \\
&amp;amp;=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)} e^{-\lambda z} \int_0^z(z-y)^{\alpha_1-1}y^{\alpha_2-1}dy\\
\end{aligned}
\]&lt;/span> 令&lt;span class="math">\(y=zt,t\in(0,1)\)&lt;/span>，则&lt;span class="math">\(dy=zdt\)&lt;/span>带入原式可得 &lt;span class="math">\[
p_{_Z}(z)=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1)\Gamma(\alpha_2)} e^{-\lambda z} z^{\alpha_1+\alpha_2-1}\underbrace{\int_0^1(1-t)^{\alpha_1-1}t^{\alpha_2-1}dt}_{\text{贝塔函数}}
\]&lt;/span> 我们看到这个积分的式子和贝塔函数是一模一样的，贝塔函数&lt;span class="math">\(B(\alpha_1,\alpha_2)=\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)}{\Gamma(\alpha_1+\alpha_2)}\)&lt;/span>。最后，相乘我们得到： &lt;span class="math">\[
p_{_Z}(z)=\frac{\lambda^{\alpha_1+\alpha_2}}{\Gamma(\alpha_1+\alpha_2)} z^{\alpha_1+\alpha_2-1}e^{-\lambda z}\sim Ga(\alpha_1+\alpha_2,\lambda)
\]&lt;/span> 显然，这个结论可以推广到有限个尺度参数相同的独立伽马变量之和上。此外，在Gamma分布中，当我们令&lt;span class="math">\(\alpha=1\)&lt;/span>时，有&lt;span class="math">\(Ga(1,\lambda)=\exp(\lambda)\)&lt;/span>，那么我们可以做出如下命题：&lt;/p>
&lt;blockquote>
&lt;p>命题6-5：n个独立同分布的指数分布随机变量&lt;span class="math">\(X_i\sim \exp(\lambda),i=1,2,\dotsb,n\)&lt;/span>之和为Gamma分布，即&lt;span class="math">\(\sum_{i=1}^nX_i\sim \underbrace{\exp(\lambda)*\exp(\lambda)*\dotsb*\exp(\lambda)}_{n个}=Ga(\underbrace{1+1+\dotsb+1}_{n个},\lambda)=Ga(n,\lambda)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>(3)：由于卡方分布时伽马分布&lt;span class="math">\(\alpha=\frac{n}{2},\lambda=\frac{1}{2}\)&lt;/span>时的特例，因此有(2)的证明可知命题6-3也成立。&lt;/p>
&lt;h3 id="变量变换法">变量变换法&lt;/h3>
&lt;p>变量变换法是对于从&lt;span class="math">\(n\rightarrow n\)&lt;/span>个随机变量函数变换的概率分布的描述，和单个随机变量函数分布一样，也是&lt;strong>利用了反函数的特性，只不过求导变成了求多个偏导的雅各布行列式&lt;/strong>。在此我们仅介绍二维随机变量的变量变换的方法，更高维的方法也是类似的。&lt;/p>
&lt;p>设二维随机变量&lt;span class="math">\((X,Y)\)&lt;/span>的联合密度函数为&lt;span class="math">\(p(x,y)\)&lt;/span>,那么如果函数 &lt;span class="math">\[
\begin{cases} u = g_1(x,y)\\ v=g_2(x,y)\end{cases}
\]&lt;/span> 有连续偏导数，且存在唯一的反函数： &lt;span class="math">\[
\begin{cases} x = x(u,v)\\ y=y(x,y)\end{cases}
\]&lt;/span> 然后我们可以列出雅克比行列式（其中的第二项倒数也告诉我们如果一方的偏导数不好求，可以求其反函数的偏导数雅克布行列式再取倒数）： &lt;span class="math">\[
J= \frac{\partial(x,y)}{\partial(u,v)}=(\frac{\partial(u,v)}{\partial(x,y)})^{-1}=\begin{vmatrix}
\frac{\partial x}{\partial u} &amp;amp; \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} &amp;amp; \frac{\partial y}{\partial x}
\end{vmatrix}\neq 0
\]&lt;/span> 则二维随机变量&lt;span class="math">\((U,V)\)&lt;/span>的联合密度函数为 &lt;span class="math">\[
p(u,v)=p(x(u,v),y(u,v))|J|\tag{6}
\]&lt;/span> 此法为二维随机变量的变量变换法，其证明可参阅二重积分的坐标变换法。&lt;/p>
&lt;h4 id="box-muller变换">Box-muller变换&lt;/h4>
&lt;p>Box-muller变换在计算机领域是一个重要的变换，它能够用两个均匀分布生成正态分布，其数学原理就是变量变换法。我们通过证明以下命题：&lt;/p>
&lt;blockquote>
&lt;p>命题7：若两个独立的随机变量&lt;span class="math">\(U_1,U_2\)&lt;/span>都服从均匀分布&lt;span class="math">\(U(0,1)\)&lt;/span>，则其组成二维函数组的二维随机变量 &lt;span class="math">\[\begin{cases}
X=\cos(2\pi U_1)\sqrt{-2\ln U_2}\\
Y=\sin(2\pi U_1)\sqrt{-2\ln U_2}
\end{cases}\]&lt;/span> 都服从标准正态分布&lt;span class="math">\(N(0,1)\)&lt;/span>，即&lt;span class="math">\(X,Y\sim N(0,1)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>根据公式&lt;span class="math">\((6)\)&lt;/span>，根据二维均匀分布的概率密度函数有： &lt;span class="math">\[
\begin{aligned}
p(x,y)&amp;amp;=p(u_1(x,y),u_2(x,y))|J|\\
&amp;amp;= 1 \times |\begin{vmatrix}
\frac{\partial u_1}{\partial x}&amp;amp;\frac{\partial u_1}{\partial y}\\
\frac{\partial u_2}{\partial x}&amp;amp;\frac{\partial u_2}{\partial y}
\end{vmatrix}|
\end{aligned}
\]&lt;/span> 而根据&lt;span class="math">\((X,Y)\)&lt;/span>的表达式，其反函数为： &lt;span class="math">\[
\begin{cases}
U_1=\frac{1}{2\pi}\arctan(Y/X)\\
U_2=e^{-\frac{X^2+Y^2}{2}}\\
\end{cases}
\]&lt;/span> 将其代入雅可比行列式为有： &lt;span class="math">\[
\begin{aligned}
p(x,y)&amp;amp;=|\begin{vmatrix}
\frac{\partial u_1}{\partial x}&amp;amp;\frac{\partial u_1}{\partial y}\\
\frac{\partial u_2}{\partial x}&amp;amp;\frac{\partial u_2}{\partial y}
\end{vmatrix}|\\
&amp;amp;=|\begin{vmatrix}
\frac{1}{2\pi}\frac{-y}{x^2+y^2}&amp;amp;\frac{1}{2\pi}\frac{x}{x^2+y^2}\\
-xe^{-\frac{x^2+y^2}{2}}&amp;amp;-ye^{-\frac{x^2+y^2}{2}}\\
\end{vmatrix}|\\
&amp;amp;=\frac{y^2}{2\pi(x^2+y^2)}e^{-\frac{x^2+y^2}{2}}+\frac{y^2}{2\pi(x^2+y^2)}e^{-\frac{x^2+y^2}{2}}\\
&amp;amp;=\frac{1}{2\pi}e^{-\frac{x^2+y^2}{2}}=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}
\end{aligned}
\]&lt;/span> 分别求&lt;span class="math">\(x,y\)&lt;/span>的边际分布可证&lt;span class="math">\(X,Y\sim N(0,1)\)&lt;/span>。注意，本次证明省略了定义域的说明，不过由其函数关系可知&lt;span class="math">\(X,Y\in(-∞,∞)\)&lt;/span>。&lt;/p>
&lt;p>得到标准正态分布函数后，可以通过正态分布的线性变换得到其他参数的正态分布函数随机变量。&lt;/p>
&lt;h3 id="增补变量法">增补变量法&lt;/h3>
&lt;p>增补变量本质是变量变换法的一个推广：为了求出二维连续随机变量&lt;span class="math">\((X,Y)\)&lt;/span>只有一个函数&lt;span class="math">\(U=g(X,Y)\)&lt;/span>的密度函数，增补一个新的随机变量&lt;span class="math">\(V=h(X,Y)\)&lt;/span>，转化成变量变换法的场景，在通过一般的变量变换法解决。为了方便求解，我们通常令&lt;span class="math">\(V=X\)&lt;/span>或&lt;span class="math">\(V=Y\)&lt;/span>。先用变量变换法求出&lt;span class="math">\((U,V)\)&lt;/span>的联合密度函数&lt;span class="math">\(p(u,v)\)&lt;/span>，再对&lt;span class="math">\(p(u,v)\)&lt;/span>关于&lt;span class="math">\(v\)&lt;/span>积分，从而得出关于&lt;span class="math">\(U\)&lt;/span>的边际密度函数。&lt;/p>
&lt;h4 id="二维随机变量积的分布">二维随机变量积的分布&lt;/h4>
&lt;blockquote>
&lt;p>二维随机变量积的公式：设随机变量&lt;span class="math">\(X,Y\)&lt;/span>相互独立，其密度函数分别为&lt;span class="math">\(p_{_X}(x),p_{_Y}(y)\)&lt;/span>，则&lt;span class="math">\(U=XY\)&lt;/span>的密度函数为： &lt;span class="math">\[p_{_U}(u)=\int_{-∞}^∞p_{_X}(\frac{u}{v})p_{_Y}(v)\frac{1}{|v|}dv\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>第一步：增补变量，记&lt;span class="math">\(V=Y\)&lt;/span>，则&lt;span class="math">\(\begin{cases}u=xy\\v=y\end{cases}\)&lt;/span>，其反函数为&lt;span class="math">\(\begin{cases}x=u/v\\y=v\end{cases}\)&lt;/span>&lt;/p>
&lt;p>第二步：通过变量变换法求出&lt;span class="math">\((U,V)\)&lt;/span>的联合密度函数。记住雅可比行列式要取绝对值。 &lt;span class="math">\[
p(u,v)=p_{_X}(\frac{u}{v})p_{_Y}(v)|\begin{vmatrix}
\frac{1}{v}&amp;amp;\frac{-u}{v^2}\\
0&amp;amp;1
\end{vmatrix}|=p_{_X}(\frac{u}{v})p_{_Y}(v)\frac{1}{|v|}
\]&lt;/span>&lt;/p>
&lt;p>第三步：对&lt;span class="math">\(v\)&lt;/span>积分，求&lt;span class="math">\(u\)&lt;/span>的边际分布： &lt;span class="math">\[p_{_U}(u)=\int_{-∞}^∞p_{_X}(\frac{u}{v})p_{_Y}(v)\frac{1}{|v|}dv\]&lt;/span> 得证。&lt;/p>
&lt;h4 id="二维随机变量商的分布">二维随机变量商的分布&lt;/h4>
&lt;blockquote>
&lt;p>二维随机变量商的公式：设随机变量&lt;span class="math">\(X,Y\)&lt;/span>相互独立，其密度函数分别为&lt;span class="math">\(p_{_X}(x),p_{_Y}(y)\)&lt;/span>，则&lt;span class="math">\(U=X/Y\)&lt;/span>的密度函数为： &lt;span class="math">\[p_{_U}(u)=\int_{-∞}^∞p_{_X}(uv)p_{_Y}(v)|v|dv\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>第一步：增补变量，记&lt;span class="math">\(V=Y\)&lt;/span>，则&lt;span class="math">\(\begin{cases}u=x/y\\v=y\end{cases}\)&lt;/span>，其反函数为&lt;span class="math">\(\begin{cases}x=uv\\y=v\end{cases}\)&lt;/span>&lt;/p>
&lt;p>第二步：通过变量变换法求出&lt;span class="math">\((U,V)\)&lt;/span>的联合密度函数。记住雅可比行列式要取绝对值。 &lt;span class="math">\[
p(u,v)=p_{_X}(uv)p_{_Y}(v)|\begin{vmatrix}
v&amp;amp;u\\
0&amp;amp;1
\end{vmatrix}|=p_{_X}(uv)p_{_Y}(v){|v|}
\]&lt;/span>&lt;/p>
&lt;p>第三步：对&lt;span class="math">\(v\)&lt;/span>积分，求&lt;span class="math">\(u\)&lt;/span>的边际分布： &lt;span class="math">\[p_{_U}(u)=\int_{-∞}^∞p_{_X}(uv)p_{_Y}(v){|v|}dv\]&lt;/span> 得证。&lt;/p>
&lt;h4 id="二维随机变量积和商的分布直接推导">二维随机变量积和商的分布直接推导&lt;/h4>
&lt;p>上面两个说的都是两个独立的随机变量&lt;span class="math">\(X,Y\)&lt;/span>，实际上只要知道两个随机变量的联合概率分布，即使不是独立的，也有一样的结论。&lt;/p>
&lt;blockquote>
&lt;p>设二维随机变量&lt;span class="math">\((X,Y)\)&lt;/span>的联合概率密度为&lt;span class="math">\(f(x,y)\)&lt;/span>，那么二者的商的分布&lt;span class="math">\(Z=X/Y\)&lt;/span>的概率密度函数为：&lt;span class="math">\(p_{_Z}(z)=\int_{-\infty}^\infty f(zy,y)|y|\mathrm{d}y\)&lt;/span>。显然当&lt;span class="math">\(X,Y\)&lt;/span>独立时，有&lt;span class="math">\(f(zy,y)=p_{_X}(zy)p_{_Y}(y)\)&lt;/span>。&lt;/p>
&lt;p>设二维随机变量&lt;span class="math">\((X,Y)\)&lt;/span>的联合概率密度为&lt;span class="math">\(p(x,y)\)&lt;/span>，那么二者的积的分布&lt;span class="math">\(Z=XY\)&lt;/span>的概率密度函数为：&lt;span class="math">\(p_Z(z)=\int_{-\infty}^\infty f(\frac{z}{y},y)|\frac{1}{y}|\mathrm{d}y\)&lt;/span>。显然当&lt;span class="math">\(X,Y\)&lt;/span>独立时，有&lt;span class="math">\(f(\frac{z}{y},y)=p_{_X}(\frac{z}{y})p_{_Y}(y)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>我们先从随机变量商的分布开始证明。利用分布函数&lt;span class="math">\(F_{_Z}(z)\)&lt;/span>的定义，我们有： &lt;span class="math">\[
F_{_Z}(z)=P(Z\leq z)=P(\frac{X}{Y}\leq z)=\iint\limits_{\frac{x}{y}&amp;lt;z}f(x,y)\mathrm{d}x\mathrm{d}y
\]&lt;/span> 关键就是研究这个二重积分。我们先得确定积分范围。在&lt;span class="math">\(x/y\leq z\)&lt;/span>的场景中，&lt;span class="math">\(z\)&lt;/span>是一个给定的常数，因此积分的范围会被直线&lt;span class="math">\(x=zy\)&lt;/span>划分，针对&lt;span class="math">\(z\)&lt;/span>是否正负，还需要分成两种情况考虑：&lt;span class="math">\(z&amp;lt;0\)&lt;/span>和&lt;span class="math">\(z\geq 0\)&lt;/span>。&lt;/p>
&lt;p>当&lt;span class="math">\(z&amp;lt;0\)&lt;/span>时，&lt;span class="math">\(x/y\)&lt;/span>小于一个负数，那么二者必然一正一负，积分区域只能在第二、四象限；为了方便进一步确定积分范围，我们使用一个不严谨但是快速的方法。由于&lt;span class="math">\(-\infty&amp;lt; z\)&lt;/span>，那么&lt;span class="math">\(x\rightarrow -\infty,y&amp;gt;0\)&lt;/span>所在范围必然位于积分区域，即&lt;span class="math">\(x/y=z&amp;lt;0\)&lt;/span>在第二象限直线下方部分位于积分区域。同样的，&lt;span class="math">\(x\rightarrow \infty,y&amp;lt;0\)&lt;/span>所在范围必然位于积分区域，即&lt;span class="math">\(x/y=z&amp;lt;0\)&lt;/span>在第四象限直线上方部分位于积分区域。综上&lt;strong>得到&lt;span class="math">\(z&amp;lt;0\)&lt;/span>时下左图的红色阴影积分区域&lt;/strong>。&lt;/p>
&lt;p>当&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时，&lt;span class="math">\(x/y\)&lt;/span>小于一个正数，那么第二、四象限的&lt;span class="math">\(x/y\)&lt;/span>都是负数，必然属于积分区域；再看第一、三象限。采样上述类似的快速判断方法，由于&lt;span class="math">\(0&amp;lt;z\)&lt;/span>，那么&lt;span class="math">\(y\rightarrow \infty, x&amp;gt;0\)&lt;/span>所在范围必然位于积分区域，即&lt;span class="math">\(x/y=z&amp;gt;0\)&lt;/span>在第一象限直线上方部分位于积分区域。同样的，&lt;span class="math">\(y\rightarrow -\infty,x&amp;lt;0\)&lt;/span>所在范围必然位于积分区域，即&lt;span class="math">\(x/y=z&amp;gt;0\)&lt;/span>在第三象限直线下方部分位于积分区域。综上&lt;strong>得到&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时下右图的红色阴影积分区域&lt;/strong>。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/随机变量函数商的分布.jpg" alt="随机变量函数商的分布.jpg" />&lt;p class="caption">随机变量函数商的分布.jpg&lt;/p>
&lt;/div>
&lt;p>确定好了积分区域，我们下面选择积分次序。对于二重积分，可以先对&lt;span class="math">\(x\)&lt;/span>积分，也可以先对&lt;span class="math">\(y\)&lt;/span>积分，主要看那个方便。我们可以把两个次序的积分先写出来，比较一下。&lt;/p>
&lt;p>如果先对&lt;span class="math">\(y\)&lt;/span>积分，在对&lt;span class="math">\(x\)&lt;/span>积分。那么&lt;span class="math">\(z&amp;lt;0\)&lt;/span>和&lt;span class="math">\(z&amp;gt;0\)&lt;/span>的积分公式如下： &lt;span class="math">\[
\begin{aligned}
&amp;amp;z&amp;lt;0\\
&amp;amp;P(\frac{X}{Y}\leq z)=\int_{-\infty}^0\int_0^{x/z}f(x,y)\mathrm{d}y\mathrm{d}x+\int_{0}^{\infty}\int_{x/z}^0f(x,y)\mathrm{d}y\mathrm{d}x\\
&amp;amp;z&amp;gt;0\\
&amp;amp;P(\frac{X}{Y}\leq z)=\int_{0}^{\infty}\int_{x/z}^{\infty}f(x,y)\mathrm{d}y\mathrm{d}x+\int_{-\infty}^{0}\int_0^{\infty}f(x,y)\mathrm{d}y\mathrm{d}x\\
&amp;amp;\qquad\qquad\int_{-\infty}^{0}\int_{-\infty}^{z/x}f(x,y)\mathrm{d}y\mathrm{d}x+\int_{0}^{\infty}\int_{-\infty}^{0}f(x,y)\mathrm{d}y\mathrm{d}x
\end{aligned}
\]&lt;/span> 这种积分次序在&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时，被分成的四个积分区域，计算比较麻烦。我们再来尝试先对&lt;span class="math">\(x\)&lt;/span>积分，在对&lt;span class="math">\(y\)&lt;/span>积分。 &lt;span class="math">\[
\begin{aligned}
&amp;amp;z&amp;lt;0\\
&amp;amp;P(\frac{X}{Y}\leq z)=\int_{0}^{\infty}\int_{-\infty}^{yz}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^{0}\int_{yz}^{\infty}f(x,y)\mathrm{d}x\mathrm{d}y\\
&amp;amp;z&amp;gt;0\\
&amp;amp;P(\frac{X}{Y}\leq z)=\int_{0}^{\infty}\int_{-\infty}^{yz}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^{0}\int_{yz}^{\infty}f(x,y)\mathrm{d}x\mathrm{d}y\\
\end{aligned}
\]&lt;/span> 在此积分次序下，不仅都被只划分成了两个积分区域，并且在&lt;span class="math">\(z&amp;lt;0\)&lt;/span>和&lt;span class="math">\(z&amp;gt;0\)&lt;/span>时，两个积分的公式是一样的，可以合并成一个场景，不用区分&lt;span class="math">\(z\)&lt;/span>的正负，十分有利于计算，因此先对&lt;span class="math">\(x\)&lt;/span>积分，在对&lt;span class="math">\(y\)&lt;/span>积分是合适的积分方式，即 &lt;span class="math">\[
F_{_Z}(z)=P(Z\leq z)\\
=P(\frac{X}{Y}\leq z)=\int_{0}^{\infty}\int_{-\infty}^{yz}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^{0}\int_{yz }^{\infty}f(x,y)\mathrm{d}x\mathrm{d}y\\
\]&lt;/span> 联系分布函数和概率密度函数的关系，&lt;span class="math">\(F_{_Z}(z)=\int_{-\infty}^z f(t)\mathrm{d}t\)&lt;/span>，我们希望在二重积分限中去除&lt;span class="math">\(y\)&lt;/span>，只保留&lt;span class="math">\(z\)&lt;/span>，因此我们在对&lt;span class="math">\(x\)&lt;/span>的积分中采用变量代换令&lt;span class="math">\(x=yt\)&lt;/span>，那么&lt;span class="math">\(\mathrm{d}x=y\mathrm{d}t\)&lt;/span>，代入有： &lt;span class="math">\[
F_{_Z}(z)=\int_{0}^{\infty}\int_{-\infty}^{z}f(yt,y)y\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{z}^{\infty}f(yt,y)y\mathrm{d}t\mathrm{d}y
\]&lt;/span> 上式中第一项中&lt;span class="math">\(y\)&lt;/span>恒大于0，第二项中&lt;span class="math">\(y\)&lt;/span>恒小于0，所以我们加上绝对值符号，统一积分内容： &lt;span class="math">\[
\begin{aligned}
F_{_Z}(z)&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(yt,y)|y|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{z}^{\infty}-f(yt,y)|y|\mathrm{d}t\mathrm{d}y\\
&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(yt,y)|y|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\underbrace{\int_{-\infty}^{z}}_{负号改变}f(yt,y)|y|\mathrm{d}t\mathrm{d}y\\
&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(yt,y)|y|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{-\infty}^{z}f(yt,y)|y|\mathrm{d}t\mathrm{d}y\\
&amp;amp;\overset{交互积分次序}{=}\int_{-\infty}^{z}\int_{0}^{\infty}f(yt,y)|y|\mathrm{d}y\mathrm{d}t+\int_{-\infty}^{z}\int_{-\infty}^{0}f(yt,y)|y|\mathrm{d}y\mathrm{d}t\\
&amp;amp;=\int_{-\infty}^{z}\int_{-\infty}^{\infty}f(yt,y)|y|\mathrm{d}y\mathrm{d}t
\end{aligned}
\]&lt;/span> 求导得到概率密度函数。 &lt;span class="math">\[
p_{_Z}(z)=F&amp;#39;_{_Z}(z)=\int_{-\infty}^{\infty}f(yz,y)|y|\mathrm{d}y
\]&lt;/span> 得证。&lt;/p>
&lt;p>我们再讨论随机变量积的分布。利用分布函数&lt;span class="math">\(F_{_Z}(z)\)&lt;/span>的定义，我们有： &lt;span class="math">\[
F_{_Z}(z)=P(Z\leq z)=P(XY\leq z)=\iint\limits_{xy&amp;lt;z}f(x,y)\mathrm{d}x\mathrm{d}y
\]&lt;/span> 同样我们要考虑其积分区域，需要分成两种情况考虑：&lt;span class="math">\(z&amp;lt;0\)&lt;/span>和&lt;span class="math">\(z\geq 0\)&lt;/span>。具体分析过程和随机变量商的方法类似，不再具体说明。可以得到如下图所示阴影积分区域。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/随机变量函数积的分布.jpg" alt="随机变量函数积的分布.jpg" />&lt;p class="caption">随机变量函数积的分布.jpg&lt;/p>
&lt;/div>
&lt;p>在考虑积分顺序时，同样也是先对&lt;span class="math">\(x\)&lt;/span>积分，再对&lt;span class="math">\(y\)&lt;/span>积分更适合计算，所以有： &lt;span class="math">\[
F_{_Z}(z)=P(Z\leq z)\\
=P(XY\leq z)=\int_{0}^{\infty}\int_{-\infty}^{z/y}f(x,y)\mathrm{d}x\mathrm{d}y+\int_{-\infty}^{0}\int_{z/y}^{\infty}f(x,y)\mathrm{d}x\mathrm{d}y\\
\]&lt;/span> 之后变量代换（&lt;span class="math">\(x=\frac{t}{y}\)&lt;/span>）与交互积分顺序的方法也和随机变量商的分布一致，最后可以得到 &lt;span class="math">\[
\begin{aligned}
F_{_Z}(z)&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{z}^{\infty}-f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y\\
&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\underbrace{\int_{-\infty}^{z}}_{负号改变}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y\\
&amp;amp;=\int_{0}^{\infty}\int_{-\infty}^{z}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y+\int_{-\infty}^{0}\int_{-\infty}^{z}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}t\mathrm{d}y\\
&amp;amp;\overset{交互积分次序}{=}\int_{-\infty}^{z}\int_{0}^{\infty}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}y\mathrm{d}t+\int_{-\infty}^{z}\int_{-\infty}^{0}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}y\mathrm{d}t\\
&amp;amp;=\int_{-\infty}^{z}\int_{-\infty}^{\infty}f(\frac{t}{y},y)|\frac{1}{y}|\mathrm{d}y\mathrm{d}t
\end{aligned}
\]&lt;/span> 求导得到概率密度函数。 &lt;span class="math">\[
p_{_Z}(z)=F&amp;#39;_{_Z}(z)=\int_{-\infty}^{\infty}f(\frac{z}{y},y)|\frac{1}{y}|\mathrm{d}y
\]&lt;/span> 得证。&lt;/p></description></item><item><title>A-放在第一个的学习箴言</title><link>https://surprisedcat.github.io/studynotes/a-%E6%94%BE%E5%9C%A8%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AE%B4%E8%A8%80/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/a-%E6%94%BE%E5%9C%A8%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AE%B4%E8%A8%80/</guid><description>
&lt;h2 id="claims">Claims&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;p>动态规划被普遍认为是解决一般随机最优控制问题的&lt;strong>唯一可行方法&lt;/strong>。Richard, Sutton. &lt;em>Reinforcement Learning: An Introduction Second Version&lt;/em>.&lt;/p>&lt;/li>
&lt;li>&lt;p>在知乎看到Kalahari关于优化算法的评价，深有同感，记录在此： &amp;gt;真正优化算法领域，群体智能算法毫无疑问处于鄙视链的最末端。按照以前老板的说法，不就是撒豆成兵嘛。经典算法，永远是充满美感。你可以看到研究者是怎样深入挖掘问题的结构特点，设计出漂亮的算法，然后对着薄薄几页纸的论文惊叹。近似算法，伪多项式时间算法，启发式算法，元启发式算法…也有自己的精妙之处，起码你同样能够看到作者的深入思考。当然还有拉格朗日方法，某院士大牛的真爱…然后你看到了遗传，蚁群，粒子群…恩，也蛮有意思的，虽然调参是个罩门。当年身边搞智能算法的，最怕答辩时遇到大牛屡试不爽的两个问题：1. 你的近优解有上下界吗？2. 调参花了多久？最后看到…鱼群算法，鸟群算法，头脑风暴算法，菌群算法…这算误入生物系？我曾经见过一个女生，答辩时介绍自己的鱼群算法，模仿了鱼的休息，鱼觅食，鱼抱团防御天敌…就差鱼嘿咻了，估计是女生脸皮薄不好意思吧！链接：&lt;a href="https://www.zhihu.com/question/22752108/answer/2103082858">https://www.zhihu.com/question/22752108/answer/2103082858&lt;/a>&lt;/p>&lt;/li>
&lt;li>&lt;p>挣钱是不是一朝一夕的事情？是又不是。稳定的挣钱是平时生活的基础，但是想要跨越阶层的富裕，靠的是极少数的机遇，而我们的剩余财富极大概率是通过这样的机遇获得的。深耕一个不错的行业，然后等待这个机遇吧。&lt;/p>&lt;/li>
&lt;/ul></description></item><item><title>机器学习-模型评价的11个重要指标</title><link>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E7%9A%8411%E4%B8%AA%E9%87%8D%E8%A6%81%E6%8C%87%E6%A0%87/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E7%9A%8411%E4%B8%AA%E9%87%8D%E8%A6%81%E6%8C%87%E6%A0%87/</guid><description/></item><item><title>机器学习-RNN相关</title><link>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-rnn%E7%9B%B8%E5%85%B3/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-rnn%E7%9B%B8%E5%85%B3/</guid><description>
&lt;h2 id="rnn相关">RNN相关&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="simple-rnn">simple RNN&lt;/h2>
&lt;p>RNN借助循环核提取时间维度特征。&lt;/p>
&lt;p>&lt;img src="../images/simple_RNN.png" alt="simple_RNN" /> &lt;span class="math">\[
y_t= softmax(h_t w_{hy} +b_y)\\
h_t = tanh(x_t w_{xh + h_{t-1}w_{hh} + b_h})
\]&lt;/span> 三个矩阵&lt;span class="math">\(w_{hy},w_{xh},w_{hh}\)&lt;/span>在前向传播中不变，只有在反向传播中才更新。通过控制循环层在时间维度上输出的&lt;span class="math">\(h_t\)&lt;/span>数量控制输出的序列长度。&lt;/p></description></item><item><title>强化学习之价值函数近似与DQN</title><link>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E4%B8%8Edqn/</link><pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E4%B8%8Edqn/</guid><description>
&lt;h2 id="强化学习之价值函数近似与dqn">强化学习之价值函数近似与DQN&lt;!-- omit in toc -->&lt;/h2>
&lt;p>强化学习中的&lt;strong>查表法只适用于规模较小的问题&lt;/strong>。生活中有许多实际问题要复杂得多，有些是属于状态数量巨大甚至是连续的，有些行为数量较大或者是连续的。这些问题要是使用SARSA，DP，Q-learning等基本算法效率会很低，甚至会无法得到较好的解决。因此需要用价值函数近似求解那些状态数量多或者是连续状态的强化学习问题。&lt;/p></description></item><item><title>强化学习之DP,MC,TD</title><link>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8Bdpmctd/</link><pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8Bdpmctd/</guid><description>
&lt;h2 id="强化学习之-dpmctd">强化学习之 DP、MC、TD&lt;!-- omit in toc -->&lt;/h2>
&lt;p>转载自：Yunhui1998 &lt;a href="https://github.com/Yunhui1998/How-do-I-learn-RL/blob/main/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%20DP%E3%80%81MC%E3%80%81TD.md">https://github.com/Yunhui1998/How-do-I-learn-RL/blob/main/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%20DP%E3%80%81MC%E3%80%81TD.md&lt;/a>&lt;/p>
&lt;h2 id="动态规划dp蒙特卡罗mc时间差分td">动态规划（DP）、蒙特卡罗（MC）、时间差分（TD）&lt;/h2>
&lt;h3 id="dynamic-programming利用贝尔曼方程迭代">Dynamic Programming（利用贝尔曼方程迭代）&lt;/h3>
&lt;div class="figure">
&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200827193937569.png" alt="image-20200827193937569" />&lt;p class="caption">image-20200827193937569&lt;/p>
&lt;/div>
&lt;p>​ 其实可以把MC、TD都理解成DP的一种近似，只不过降低了计算复杂度以及削弱了对环境模型完备性的假设。&lt;/p>
&lt;ul>
&lt;li>&lt;p>&lt;a href="https://sites.google.com/a/chaoskey.com/algorithm/03/03">动态规划的使用条件&lt;/a>：&lt;/p>&lt;/li>
&lt;li>最优化原理：如果问题的最优解所包含的子问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。&lt;/li>
&lt;li>&lt;p>无后效性：即某阶段状态一旦确定，就不受这个状态以后决策的影响。也就是说，某状态以后的过程不会影响以前的状态，只与当前状态有关。&lt;/p>&lt;/li>
&lt;li>&lt;p>有重叠子问题：即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。（&lt;strong>该性质并不是动态规划适用的必要条件，但是如果没有这条性质，动态规划算法同其他算法相比就不具备优势&lt;/strong>）&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;a href="https://www.cnblogs.com/steven_oyj/archive/2010/05/22/1741374.html">动态规划的步骤&lt;/a>&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;strong>划分阶段&lt;/strong>：按照问题的时间或空间特征，把问题分为若干个阶段。在划分阶段时，注意划分后的阶段一定要是有序的或者是可排序的，否则问题就无法求解。&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;strong>确定状态和状态变量&lt;/strong>：将问题发展到各个阶段时所处于的各种客观情况用不同的状态表示出来。当然，状态的选择要满足无后效性。&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;strong>确定决策并写出状态转移方程&lt;/strong>：因为决策和状态转移有着天然的联系，状态转移就是根据上一阶段的状态和决策来导出本阶段的状态。所以如果确定了决策，状态转移方程也就可写出。但事实上常常是反过来做，根据相邻两个阶段的状态之间的关系来确定决策方法和状态转移方程。&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;strong>寻找边界条件&lt;/strong>：给出的状态转移方程是一个递推式，需要一个递推的终止条件或边界条件。&lt;/p>
&lt;p>一般，只要解决问题的阶段、状态和状态转移决策确定了，就可以写出状态转移方程（包括边界条件）。&lt;/p>&lt;/li>
&lt;li>&lt;p>动态规划三要素&lt;/p>&lt;/li>
&lt;li>问题的阶段&lt;/li>
&lt;li>每个阶段的状态&lt;/li>
&lt;li>&lt;p>从前一个阶段转化到后一个阶段之间的递推关系&lt;/p>&lt;/li>
&lt;/ul>
&lt;h3 id="异步的动态规划asynchronous-dynamic-programming">&lt;a href="https://zhuanlan.zhihu.com/p/30518290">异步的动态规划&lt;/a>：Asynchronous Dynamic Programming&lt;/h3>
&lt;p>在我们之前的算法中，我们每一次的迭代都会完全更新所有的，这样对于程序资源需求特别大。这样的做法叫做同步备份(synchronous backups)。异步备份的思想就是通过某种方式，使得每一次迭代不需要更新所有的，因为事实上，很多的也不需要被更新。异步备份有以下几种方案&lt;/p>
&lt;p>1.&lt;strong>In-place 动态规划所做的改进，是直接去掉了原来的副本 &lt;span class="math">\(v_{k}\)&lt;/span>, 只保留最新的副本&lt;/strong>(也就是说，在 一次更新过程中，存在着有些用的是 &lt;span class="math">\(v_{k},\)&lt;/span> 有些用的是 &lt;span class="math">\(v_{k+1}\)&lt;/span> )。具体而言，我们可以这样表示，对于所有的状态s： &lt;span class="math">\[
v(s) \leftarrow \max _{a \in A}\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v\left(s^{\prime}\right)\right)
\]&lt;/span>&lt;/p>
&lt;p>2.&lt;strong>加权扫描动态规划：Prioritized Sweeping Dynamic Programming&lt;/strong>&lt;/p>
&lt;p>Prioritized Sweeping 的思想是，根据某种方式，来确定每一个状态现在是否重要，&lt;strong>对于重要的状态进行更多的更新，对于不重要的状态更新的次数就比较少。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>更新顺序：可以使用priority queue 来有效的确定更新次序(按照优先权排队，每次把优先权最高的拿出来更新&lt;/li>
&lt;li>权重设计规则：可以使用Bellman error 来确定优先权，这个公式是通过两次的value的差异来作为state的评估标准，&lt;strong>如果某个状态上次的value和这次的value相差不大，我们有理由认为他即将达到稳定，则更新他的价值就比较小&lt;/strong>，反之则比较大。具体公式如下： &lt;span class="math">\[\max _{a \in A}\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v\left(s^{\prime}\right)-v(s) \right).\]&lt;/span>&lt;/li>
&lt;li>所以说，这个方法需要我们进行反向规划，因为我们需要知道当前状态之前的value是多少。&lt;/li>
&lt;/ul>
&lt;p>3.&lt;strong>实时动态规划：Real-Time Dynamic Programming&lt;/strong>&lt;/p>
&lt;p>实时动态规划的思想是只有Agent现在关注的状态会被更新。与当前无关的状态暂时不被更新。&lt;/p>
&lt;p>就比如我们在时间步t进入状态 &lt;span class="math">\(S_{t}\)&lt;/span> ，进行动作 &lt;span class="math">\(A_{t}\)&lt;/span>，结果是得到反馈 &lt;span class="math">\(R_{t+1}\)&lt;/span>，那么我们要做的就是，仅仅更新 &lt;span class="math">\(S_{t}\)&lt;/span> 的value function，公式如下： &lt;span class="math">\[
v\left(S_{t}\right) \leftarrow \max_{a \in A}\left(R_{S_{t}}^{a}+\gamma \sum_{s^{\prime} \in S} P_{S_{t} s^{\prime}}^{a} v\left(s^{\prime}\right)\right)
\]&lt;/span>&lt;/p>
&lt;p>4.&lt;strong>Full-Width Backups and Sample Backups&lt;/strong>&lt;/p>
&lt;p>Full-Width 和 Sample Backups的区别在于更新状态时考虑的后继状态的个数的区别，他和同步DP，异步DP思考的是两个维度的优化方向。&lt;/p>
&lt;p>Full-Width Backups 介绍的是：当我们在考虑更新某一个state 的value function的时候，我们需要遍历这个state的所有可能的action，和每一个action所可能达到的后继state，这个遍历的开销非常大，对于每一次迭代，如果有m个action和n个state，则时间复杂度为 &lt;span class="math">\(O\left(m n^{2}\right)\)&lt;/span>，也就是说，遍历次数会随着n而指数增长，这在大型的DP问题中，代价是无法接受的，所以提出了sample backups。&lt;/p>
&lt;p>sample backups 的思路是将state-to-state的过程进行采样，也就是说，我们以整个MDP&lt;span class="math">\(&amp;lt;S, A, R, S^{\prime}&amp;gt;\)&lt;/span>为单位，得到很多的样本，也就是说，对&lt;strong>于一个样本，一个state对应一个action，通过采样多个MDP过程，来估计当前的策略的优劣，而不是每个节点直接遍历所有的可能性&lt;/strong>，我们可以用下图表示：&lt;/p>
&lt;div class="figure">
&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/v2-90a0552f35411aa45017cbcd3361187d_720w.jpg" alt="img" />&lt;p class="caption">img&lt;/p>
&lt;/div>
&lt;p>​ 这样做有如下的优点：&lt;/p>
&lt;ul>
&lt;li>Model-free：一个非常重要的好处就是，由于我们不再需要计算所有的action可到达的状态，就意味着我们不需要使用状态转换概率矩阵，也就是说，我们不再需要提前完全搞明白环境的变化模型，这便是一个model-free的算法！&lt;/li>
&lt;li>假设我们采样的个数为a，那么我们一次迭代的时间复杂度就是 &lt;span class="math">\(O(a m n)\)&lt;/span>，随着state的增加，我们的时间复杂度仅仅是常数增长。一定程度避免了维度灾难。&lt;/li>
&lt;/ul>
&lt;h3 id="monte-carlo-evalution采样的一种办法">Monte-Carlo evalution（采样的一种办法）&lt;/h3>
&lt;p>​ 蒙特卡罗方法是一种基于采样的方法，也就是我们采样得到很多轨迹，然后根据采样得到的结果平均去算V（s）&lt;/p>
&lt;ul>
&lt;li>MC simulation: we can simply sample a lot of trajectories, compute the actual returns for all the trajectories, then average them&lt;/li>
&lt;li>To evaluate state &lt;span class="math">\(v(s)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;ol style="list-style-type: decimal">
&lt;li>Every time-step &lt;span class="math">\(t\)&lt;/span> that state s is visited in an episode, 2. Increment counter &lt;span class="math">\(N(s) \leftarrow N(s)+1\)&lt;/span> 3. Increment total return &lt;span class="math">\(S(s) \leftarrow S(s)+G_{t}\)&lt;/span> 4. Value is estimated by mean return &lt;span class="math">\(v(s)=S(s) / N(s)\)&lt;/span>,&lt;strong>这里计算这个均值的时候，我们其实可以用Incremental Mean的方式，也就是新估计←旧估计+步长[目标−旧估计]&lt;/strong> &lt;span class="math">\(v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\frac{1}{N\left(S_{t}\right)}\left(G_{t}-v\left(S_{t}\right)\right)\)&lt;/span> &lt;strong>By law of large numbers, &lt;span class="math">\(v(s) \rightarrow v^{\pi}(s)\)&lt;/span> as &lt;span class="math">\(N(s) \rightarrow \infty\)&lt;/span>&lt;/strong>&lt;/li>
&lt;/ol>
&lt;div class="figure">
&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200827192441184.png" alt="image-20200827192441184" />&lt;p class="caption">image-20200827192441184&lt;/p>
&lt;/div>
&lt;h3 id="tempor-difference-learning">Tempor-Difference learning&lt;/h3>
&lt;ul>
&lt;li>&lt;p>TD methods learn directly from episodes of experience TD方法从序列的经验里面进行学习&lt;/p>&lt;/li>
&lt;li>&lt;p>TD is model-free: no knowledge of MDP transitions/rewards 没有状态概率转移矩阵&lt;/p>&lt;/li>
&lt;li>&lt;p>TD learns from incomplete episodes, by bootstrapping 通过bootstrapping从不完全的轨迹学习&lt;/p>&lt;/li>
&lt;li>&lt;p>Objective: learn &lt;span class="math">\(v_{\pi}\)&lt;/span> online from experience under policy &lt;span class="math">\(\pi\)&lt;/span> Simplest TD algorithm: &lt;span class="math">\(\operatorname{TD}(0)\)&lt;/span> 也就是往前走一步进行估计 U Undate &lt;span class="math">\(v\left(S_{t}\right)\)&lt;/span> toward estimated return &lt;span class="math">\(R_{t+1}+\gamma v\left(S_{t+1}\right)\)&lt;/span> &lt;span class="math">\[
v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma v\left(S_{t+1}\right)-v\left(S_{t}\right)\right)
\]&lt;/span> &lt;span class="math">\(R_{t+1}+\gamma v\left(S_{t+1}\right)\)&lt;/span> is called &lt;strong>TD target&lt;/strong> &lt;span class="math">\(\delta_{t}=R_{t+1}+\gamma v\left(S_{t+1}\right)-v\left(S_{t}\right)\)&lt;/span> is called the &lt;strong>TD error&lt;/strong> Comparison: Incremental Monte-Carlo &lt;span class="math">\[
v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(G_{i, t}-v\left(S_{t}\right)\right)
\]&lt;/span>&lt;/p>&lt;/li>
&lt;/ul>
&lt;h3 id="n-step-td">n-step TD&lt;/h3>
&lt;p>n-step TD像是控制n的大小在TD(0)和MC中找一个平衡&lt;/p>
&lt;p>&lt;span class="math">\(\begin{array}{ll}n=1(T D) &amp;amp; G_{t}^{(1)}=R_{t+1}+\gamma v\left(S_{t+1}\right) \\ n=2 &amp;amp; G_{t}^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} v\left(S_{t+2}\right) \\ &amp;amp; \vdots \\ n=\infty(M C) &amp;amp; G_{t}^{\infty}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{T-t-1} R_{T}\end{array}\)&lt;/span>&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200829165754594.png" alt="image-20200829165754594" style="zoom: 33%;" />&lt;/p>
&lt;p>Thus the n-step return is defined as &lt;span class="math">\[
G_{t}^{n}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1} R_{t+n}+\gamma^{n} v\left(S_{t+n}\right)
\]&lt;/span> n-step &lt;span class="math">\(\mathrm{TD}: v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(G_{t}^{n}-v\left(S_{t}\right)\right)\)&lt;/span>&lt;/p>
&lt;h3 id="mcdp以及td算法的对比">MC、DP以及TD算法的对比&lt;/h3>
&lt;p>Dynamic Programming &lt;span class="math">\((\mathrm{DP})\)&lt;/span> computes &lt;span class="math">\(v_{i}\)&lt;/span> by &lt;strong>bootstrapping&lt;/strong> the rest of the expected return by the value estimate &lt;span class="math">\(v_{i-1}\)&lt;/span> Iteration on Bellman expectation backup: &lt;span class="math">\[
v_{i}(s) \leftarrow \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) v_{i-1}\left(s^{\prime}\right)\right)
\]&lt;/span> &lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200829103402838.png" alt="image-20200829103402838" style="zoom:30%;" />&lt;/p>
&lt;p>&lt;span class="math">\(\mathrm{MC}\)&lt;/span> updates the empirical mean return with one sampled episode &lt;span class="math">\[
v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(G_{i, t}-v\left(S_{t}\right)\right)
\]&lt;/span> &lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200829103510012.png" alt="image-20200829103510012" style="zoom:33%;" />&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200829170841169.png" alt="image-20200829164545152" style="zoom:40%;" />&lt;/p>
&lt;h2 id="mc相比于dp的优点">MC相比于DP的优点&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>MC works when the environment is unknown&lt;/strong> 当环境未知时，MC更管用&lt;/li>
&lt;li>Working with sample episodes has a huge advantage, even when one has complete knowledge of the environment's dynamics, for example, &lt;strong>transition probability is complex to compute&lt;/strong> 可以从采样的轨迹中进行学习总是好的，即使是知道环境的动态性，比如说，转移矩阵很难计算&lt;/li>
&lt;li>Cost of estimating a single state's value is independent of the total number of states. So you can sample episodes starting from the states of interest then average returns 不管整体的状态数量有多少，计算一个状态的价值是相对容易的。&lt;/li>
&lt;/ul>
&lt;h2 id="td相比于mc的优点">TD相比于MC的优点&lt;/h2>
&lt;p>1、TD不需要等到epsilon结束后才学习（Incomplete sequences）&lt;/p>
&lt;ul>
&lt;li>&lt;p>TD can learn online after every step &lt;span class="math">\(\mathrm{MC}\)&lt;/span> must wait until end of episode before return is known&lt;/p>&lt;/li>
&lt;li>TD can learn from incomplete sequences &lt;span class="math">\(\mathrm{M} \mathrm{C}\)&lt;/span> can only learn from complete sequences&lt;/li>
&lt;li>&lt;p>TD works in continuing (non-terminating) environments &lt;span class="math">\(\mathrm{MC}\)&lt;/span> only works for episodic (terminating) environment&lt;/p>&lt;/li>
&lt;/ul>
&lt;p>2、TD在马尔科夫的环境中更有效（因为用了bootstraping）&lt;/p>
&lt;ul>
&lt;li>TD exploits Markov property, more efficient in Markov environments &lt;span class="math">\(\mathrm{MC}\)&lt;/span> does not exploit Markov property, more effective in non-Markov environments&lt;/li>
&lt;/ul>
&lt;p>3、Lower variance&lt;/p>
&lt;p>4、Online&lt;/p>
&lt;ul>
&lt;li>总结：由于MC的高方差，无偏差的特性，有以下几个特点：&lt;/li>
&lt;/ul>
&lt;ol style="list-style-type: decimal">
&lt;li>他有更好的收敛性质。他总能够很好的拟合函数（他能够更容易接近真实的价值函数）；&lt;/li>
&lt;li>对初始化数据不敏感（因为他的标注是真实的，所以最后总会调整到正确的轨道上）；&lt;/li>
&lt;li>收敛速度比较慢&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>由于TD算法的有偏差，低方差的特性，他有以下几个特点：&lt;/li>
&lt;/ul>
&lt;ol style="list-style-type: decimal">
&lt;li>他通常速度比较快（因为数据的方差比较小，而我们一般认为收敛的准则是：当数据的波动比较小，则认为他收敛了）；&lt;/li>
&lt;li>但是对初始化数据比较敏感（如果有一个不好的初始化值，那么他虽然可以很快收敛，但不是收敛到正确的解）；&lt;/li>
&lt;/ol>
&lt;h2 id="是否有bootstrapping和sampling">是否有Bootstrapping和Sampling&lt;/h2>
&lt;ul>
&lt;li>Bootstrapping：update involves an estimate&lt;/li>
&lt;li>MC does not bootstrap&lt;/li>
&lt;li>DP bootstraps&lt;/li>
&lt;li>TD bootstraps&lt;/li>
&lt;li>Sampling:update samples an expectation&lt;/li>
&lt;li>MC samples&lt;/li>
&lt;li>DP does not sample&lt;/li>
&lt;li>TD samples&lt;/li>
&lt;/ul>
&lt;h2 id="画图理解">画图理解&lt;/h2>
&lt;p>DP：&lt;span class="math">\(v\left(S_{t}\right) \leftarrow \mathbb{E}_{\pi}\left[R_{t+1}+\gamma v\left(S_{t+1}\right)\right]\)&lt;/span>&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200829170841169.png" alt="image-20200829170841169" style="zoom:33%;" />&lt;/p>
&lt;p>MC：&lt;span class="math">\(v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(G_{t}-v\left(S_{t}\right)\right)\)&lt;/span>&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200829170920684.png" alt="image-20200829170920684" style="zoom:33%;" />&lt;/p>
&lt;p>TD(0):&lt;span class="math">\(T D(0): v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma v\left(s_{t+1}\right)-v\left(S_{t}\right)\right)\)&lt;/span>&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200829171010565.png" alt="image-20200829171010565" style="zoom:33%;" />&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Yunhui1998/markdown_image/main/RL/image-20200829171235088.png" alt="image-20200829171235088" style="zoom:33%;" />&lt;/p></description></item><item><title>概率统计随机过程之马尔可夫过程</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B/</link><pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B/</guid><description>
&lt;h2 id="概率统计随机过程之马尔可夫过程">概率统计随机过程之马尔可夫过程&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="概论">概论&lt;/h2>
&lt;p>在概率论及统计学中，&lt;strong>马尔可夫过程&lt;/strong>（英语：Markov process）是一个具备了&lt;strong>马尔可夫性质&lt;/strong>的随机过程，因为俄国数学家安德雷·马尔可夫得名。马尔可夫过程是&lt;strong>不具备记忆特质的（memorylessness）&lt;/strong>。换言之，马尔可夫过程的条件概率仅仅与系统的&lt;strong>当前状态相关&lt;/strong>，而与它的过去历史或未来状态，都是独立、不相关的。&lt;/p>
&lt;p>具备&lt;strong>离散状态的马尔可夫过程，通常被称为马尔可夫链&lt;/strong>。马尔可夫链通常使用离散的时间集合定义，又称离散时间马尔可夫链。有些学者虽然采用这个术语，但允许时间可以取连续的值。&lt;/p>
&lt;p>马尔可夫过程根据时间与状态空间的连续性可分为下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;/th>
&lt;th align="center">可数或有限的状态空间&lt;/th>
&lt;th align="center">连续或一般的状态空间&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">离散时间&lt;/td>
&lt;td align="center">在可数且有限状态空间下的马尔可夫链&lt;/td>
&lt;td align="center">Harris chain (在一般状态空间下的马尔可夫链)&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">连续时间&lt;/td>
&lt;td align="center">Continuous-time Markov process&lt;/td>
&lt;td align="center">任何具备马尔可夫性质的连续随机过程，例如维纳过程&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>注：连续时间马尔可夫过程基本就是几何分布或者指数分布过程。&lt;/p>
&lt;h2 id="马尔可夫性质">马尔可夫性质&lt;/h2>
&lt;blockquote>
&lt;p>马尔可夫性质：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。&lt;/p>
&lt;/blockquote>
&lt;p>数学上，如果&lt;span class="math">\(X(t),t&amp;gt;0\)&lt;/span>为一个随机过程，则马尔可夫性质就是指 &lt;span class="math">\[
{\mathrm{Pr}}{\big [}X(t+h)=y\,|\,X(s)=x(s),s\leq t{\big ]}={\mathrm {Pr}}{\big [}X(t+h)=y\,|\,X(t)=x(t){\big ]},\quad \forall h&amp;gt;0.
\]&lt;/span>&lt;/p></description></item><item><title>算法理论之P，NP，NPC和NP-hard</title><link>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E4%B9%8Bpnpnpc%E5%92%8Cnp-hard/</link><pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E4%B9%8Bpnpnpc%E5%92%8Cnp-hard/</guid><description>
&lt;h2 id="算法理论之pnpnpc和np-hard">算法理论之P，NP，NPC和NP-hard&lt;!-- omit in toc -->&lt;/h2>
&lt;p>个人认为写的非常好的P，NP，NPC和NP-hard解释，转自Matrix67的博客&lt;a href="http://www.matrix67.com/blog/archives/105">《什么是P问题、NP问题和NPC问题》&lt;/a>&lt;/p>
&lt;p>这或许是众多OIer最大的误区之一。&lt;/p>
&lt;p>你会经常看到网上出现“这怎么做，这不是NP问题吗”、“这个只有搜了，这已经被证明是NP问题了”之类的话。你要知道，&lt;strong>大多数人此时所说的NP问题其实都是指的NPC问题&lt;/strong>。他们没有搞清楚NP问题和NPC问题的概念。NP问题并不是那种“只有搜才行”的问题，NPC问题才是。好，行了，基本上这个误解已经被澄清了。下面的内容都是在讲&lt;strong>什么是P问题，什么是NP问题，什么是NPC问题&lt;/strong>，你如果不是很感兴趣就可以不看了。接下来你可以看到，把NP问题当成是 NPC问题是一个多大的错误。&lt;/p>
&lt;p>还是先用几句话简单说明一下时间复杂度。时间复杂度并不是表示一个程序解决问题需要花多少时间，而是当问题规模扩大后，程序需要的时间长度增长得有多快。也就是说，对于高速处理数据的计算机来说，处理某一个特定数据的效率不能衡量一个程序的好坏，而应该看当这个数据的规模变大到数百倍后，程序运行时间是否还是一样，或者也跟着慢了数百倍，或者变慢了数万倍。不管数据有多大，程序处理花的时间始终是那么多的，我们就说这个程序很好，具有&lt;span class="math">\(O(1)\)&lt;/span>的时间复杂度，也称常数级复杂度；数据规模变得有多大，花的时间也跟着变得有多长，这个程序的时间复杂度就是&lt;span class="math">\(O(n)\)&lt;/span>，比如找n个数中的最大值；而像冒泡排序、插入排序等，数据扩大2倍，时间变慢4倍的，属于&lt;span class="math">\(O(n^2)\)&lt;/span>的复杂度。还有一些穷举类的算法，所需时间长度成几何阶数上涨，这就是&lt;span class="math">\(O(a^n)\)&lt;/span>的指数级复杂度，甚至&lt;span class="math">\(O(n!)\)&lt;/span>的阶乘级复杂度。不会存在&lt;span class="math">\(O(2*n^2)\)&lt;/span>的复杂度，因为前面的那个“2”是系数，根本不会影响到整个程序的时间增长。同样地，&lt;span class="math">\(O(n^3+n^2)\)&lt;/span>的复杂度也就是&lt;span class="math">\(O(n^3)\)&lt;/span>的复杂度。因此，我们会说，一个&lt;span class="math">\(O(0.01*n^3)\)&lt;/span>的程序的效率比&lt;span class="math">\(O(100*n^2)\)&lt;/span>的效率低，尽管在n很小的时候，前者优于后者，但后者时间随数据规模增长得慢，最终&lt;span class="math">\(O(n^3)\)&lt;/span>的复杂度将远远超过&lt;span class="math">\(O(n^2)\)&lt;/span>。我们也说，&lt;span class="math">\(O(n^100)\)&lt;/span>的复杂度小于&lt;span class="math">\(O(1.01^n)\)&lt;/span>的复杂度。&lt;/p>
&lt;p>容易看出，前面的几类复杂度被分为两种级别，其中后者的复杂度无论如何都远远大于前者：一种是&lt;span class="math">\(O(1),O(log(n)),O(n^a)\)&lt;/span>等，我们把它叫做&lt;strong>多项式级的复杂度&lt;/strong>，因为它的规模n出现在底数的位置；另一种是&lt;span class="math">\(O(a^n)\)&lt;/span>和&lt;span class="math">\(O(n!)\)&lt;/span>型复杂度，它是&lt;strong>非多项式级&lt;/strong>的，其复杂度计算机往往不能承受。当我们在解决一个问题时，我们选择的算法通常都需要是多项式级的复杂度，非多项式级的复杂度需要的时间太多，往往会超时，除非是数据规模非常小。&lt;/p>
&lt;p>自然地，人们会想到一个问题：&lt;strong>会不会所有的问题都可以找到复杂度为多项式级的算法呢&lt;/strong>？很遗憾，答案是&lt;strong>否定的&lt;/strong>。有些问题甚至根本不可能找到一个正确的算法来，这称之为“&lt;strong>不可解问题&lt;/strong>”(Undecidable Decision Problem)。The Halting Problem就是一个著名的不可解问题，在我的Blog上有过专门的介绍和证明。再比如，输出从1到n这n个数的全排列。不管你用什么方法，你的复杂度都是阶乘级，因为你总得用阶乘级的时间打印出结果来。有人说，这样的“问题”不是一个“正规”的问题，正规的问题是让程序解决一个问题，输出一个“YES”或“NO”（这被称为判定性问题），或者一个什么什么的最优值（这被称为最优化问题）。那么，根据这个定义，我也能举出一个不大可能会有多项式级算法的问题来：Hamilton回路。问题是这样的：给你一个图，问你能否找到一条经过每个顶点一次且恰好一次（不遗漏也不重复）最后又走回来的路（满足这个条件的路径叫做Hamilton回路）。这个问题现在还没有找到多项式级的算法。事实上，这个问题就是我们后面要说的NPC问题。&lt;/p>
&lt;p>下面引入P类问题的概念：&lt;strong>如果一个问题可以找到一个能在多项式的时间里解决它的算法，那么这个问题就属于P问题&lt;/strong>(读者注：多项式时间内可解)。P是英文单词多项式的第一个字母。哪些问题是P类问题呢？通常NOI和NOIP（即全国青少年信息学奥林匹克竞赛）不会出不属于P类问题的题目。我们常见到的一些信息奥赛的题目都是P问题。道理很简单，一个用穷举换来的非多项式级时间的超时程序不会涵盖任何有价值的算法。&lt;/p>
&lt;p>接下来引入NP问题的概念。这个就有点难理解了，或者说容易理解错误。在这里强调（回到我竭力想澄清的误区上），&lt;strong>NP问题不是非P类问题。NP问题是指可以在多项式的时间里验证一个解的问题&lt;/strong>（读者注：即多项式时间内可验证解是不是所求解）。NP问题的另一个定义是，可以在多项式的时间里猜出一个解的问题。比方说，我RP很好，在程序中需要枚举时，我可以一猜一个准。现在某人拿到了一个求最短路径的问题，问从起点到终点是否有一条小于100个单位长度的路线。它根据数据画好了图，但怎么也算不出来，于是来问我：你看怎么选条路走得最少？我说，我RP很好，肯定能随便给你指条很短的路出来。然后我就胡乱画了几条线，说就这条吧。那人按我指的这条把权值加起来一看，嘿，神了，路径长度98，比100小。于是答案出来了，存在比100小的路径。别人会问他这题怎么做出来的，他就可以说，因为我找到了一个比100 小的解。在这个题中，找一个解很困难，但验证一个解很容易。验证一个解只需要&lt;span class="math">\(O(n)\)&lt;/span>的时间复杂度，也就是说我可以花&lt;span class="math">\(O(n)\)&lt;/span>的时间把我猜的路径的长度加出来。那么，只要我RP好，猜得准，我一定能在多项式的时间里解决这个问题。我猜到的方案总是最优的，不满足题意的方案也不会来骗我去选它。这就是NP问题。&lt;strong>当然有不是NP问题的问题，即你猜到了解但是没用，因为你不能在多项式的时间里去验证它&lt;/strong>。下面我要举的例子是一个经典的例子，它指出了一个目前还没有办法在多项式的时间里验证一个解的问题。很显然，前面所说的Hamilton回路是NP问题，因为验证一条路是否恰好经过了每一个顶点非常容易。但我要把问题换成这样：试问一个图中是否不存在Hamilton回路。这样问题就没法在多项式的时间里进行验证了，因为除非你试过所有的路，否则你不敢断定它“没有Hamilton回路”。&lt;/p>
&lt;p>之所以要定义NP问题，是因为&lt;strong>通常只有NP问题才可能找到多项式的算法&lt;/strong>。我们不会指望一个连多项式地验证一个解都不行的问题存在一个解决它的多项式级的算法。相信读者很快明白，信息学中的号称最困难的问题——&lt;strong>“NP问题”，实际上是在探讨NP问题与P类问题的关系&lt;/strong>。&lt;/p>
&lt;p>很显然，&lt;strong>所有的P类问题都是NP问题&lt;/strong>。也就是说，能多项式地解决一个问题，必然能多项式地验证一个问题的解——既然正解都出来了，验证任意给定的解也只需要比较一下就可以了。关键是，人们想知道，是否所有的NP问题都是P类问题。我们可以再用集合的观点来说明。如果把所有P类问题归为一个集合P中，把所有 NP问题划进另一个集合NP中，那么，显然有P属于NP。现在，所有对NP问题的研究都集中在一个问题上，即究竟是否有P=NP？通常所谓的“NP问题”，其实就一句话：&lt;strong>证明或推翻P=NP&lt;/strong>。&lt;/p>
&lt;p>NP问题一直都是信息学的巅峰。巅峰，意即很引人注目但难以解决。在信息学研究中，这是一个耗费了很多时间和精力也没有解决的终极问题，好比物理学中的大统一和数学中的歌德巴赫猜想等。&lt;/p>
&lt;p>目前为止这个问题还“啃不动”。但是，一个总的趋势、一个大方向是有的。&lt;strong>人们普遍认为，P=NP不成立&lt;/strong>，也就是说，多数人相信，存在至少一个不可能有多项式级复杂度的算法的NP问题。人们如此坚信P≠NP是有原因的，就是在研究NP问题的过程中找出了一类非常特殊的NP问题叫做&lt;strong>NP-完全问题，也即所谓的 NPC问题&lt;/strong>(读者注：NPC问题首先是一个NP问题，它能够在多项式时间内验证解)。C是英文单词“完全”的第一个字母。正是NPC问题的存在，使人们相信P≠NP。下文将花大量篇幅介绍NPC问题，你从中可以体会到NPC问题使P=NP变得多么不可思议。&lt;/p>
&lt;p>为了说明NPC问题，我们先引入一个概念——&lt;strong>约化&lt;/strong>(Reducibility，有的资料上叫“&lt;strong>归约&lt;/strong>”)。&lt;/p>
&lt;p>简单地说，&lt;strong>一个问题A可以约化为问题B的含义即是，可以用问题B的解法解决问题A，或者说，问题A可以“变成”问题B&lt;/strong>。《算法导论》上举了这么一个例子。比如说，现在有两个问题：求解一个一元一次方程和求解一个一元二次方程。那么我们说，前者可以约化为后者，意即知道如何解一个一元二次方程那么一定能解出一元一次方程。我们可以写出两个程序分别对应两个问题，那么我们能找到一个“规则”，按照这个规则把解一元一次方程程序的输入数据变一下，用在解一元二次方程的程序上，两个程序总能得到一样的结果。这个规则即是：两个方程的对应项系数不变，一元二次方程的二次项系数为0。按照这个规则把前一个问题转换成后一个问题，两个问题就等价了。同样地，我们可以说，Hamilton回路可以约化为TSP问题(Travelling Salesman Problem，旅行商问题)：在Hamilton回路问题中，两点相连即这两点距离为0，两点不直接相连则令其距离为1，于是问题转化为在TSP问题中，是否存在一条长为0的路径。Hamilton回路存在当且仅当TSP问题中存在长为0的回路。&lt;/p>
&lt;p>“问题A可约化为问题B”（读者注：即B问题更难，B问题包括A，解决B就能解决A）有一个重要的直观意义：&lt;strong>B的时间复杂度高于或者等于A的时间复杂度。也就是说，问题A不比问题B难&lt;/strong>。这很容易理解。既然问题A能用问题B来解决，倘若B的时间复杂度比A的时间复杂度还低了，那A的算法就可以改进为B的算法，两者的时间复杂度还是相同。正如解一元二次方程比解一元一次方程难，因为解决前者的方法可以用来解决后者。&lt;/p>
&lt;p>很显然，约化具有一项重要的性质：&lt;strong>约化具有传递性&lt;/strong>。如果问题A可约化为问题B，问题B可约化为问题C，则问题A一定可约化为问题C。这个道理非常简单，就不必阐述了。&lt;/p>
&lt;p>现在再来说一下约化的标准概念就不难理解了：如果能找到这样一个变化法则，对任意一个程序A的输入，都能按这个法则变换成程序B的输入，使两程序的输出相同，那么我们说，问题A可约化为问题B。&lt;/p>
&lt;p>当然，我们所说的“可约化”是指的可“&lt;strong>多项式地&lt;/strong>”约化(Polynomial-time Reducible)，即变&lt;strong>换输入的方法是能在多项式的时间里完成的&lt;/strong>。约化的过程只有用多项式的时间完成才有意义。&lt;/p>
&lt;p>好了，从约化的定义中我们看到，一个问题约化为另一个问题，时间复杂度增加了，问题的应用范围也增大了。通过对某些问题的不断约化，我们能够不断寻找复杂度更高，但应用范围更广的算法来代替复杂度虽然低，但只能用于很小的一类问题的算法。再回想前面讲的P和NP问题，联想起约化的传递性，自然地，我们会想问，如果不断地约化上去，不断找到能“通吃”若干小NP问题的一个稍复杂的大NP问题，&lt;strong>那么最后是否有可能找到一个时间复杂度最高，并且能“通吃”所有的 NP问题的这样一个超级NP问题&lt;/strong>？答案居然是&lt;strong>肯定的&lt;/strong>。也就是说，存在这样一个NP问题，所有的NP问题都可以约化成它。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。这种问题的存在难以置信，并且更加不可思议的是，这种问题不只一个，它有很多个，它是一类问题。&lt;strong>这一类问题就是传说中的NPC 问题，也就是NP-完全问题&lt;/strong>（读者注：NPC问题是NP问题中最难的问题，约化的终点，所有NPC问题都是等价的）。NPC问题的出现使整个NP问题的研究得到了飞跃式的发展。我们有理由相信，NPC问题是最复杂的问题。再次回到全文开头，我们可以看到，人们想表达一个问题不存在多项式的高效算法时应该说它“属于NPC问题”。此时，我的目的终于达到了，我已经把NP问题和NPC问题区别开了。到此为止，本文已经写了近5000字了，我佩服你还能看到这里来，同时也佩服一下自己能写到这里来。&lt;/p>
&lt;p>NPC问题的定义非常简单。同时满足下面两个条件的问题就是NPC问题。&lt;/p>
&lt;ul>
&lt;li>首先，它得是一个NP问题；&lt;/li>
&lt;li>然后，所有的NP问题都可以约化到它。&lt;/li>
&lt;/ul>
&lt;p>证明一个问题是 NPC问题也很简单。&lt;/p>
&lt;ul>
&lt;li>先证明它至少是一个NP问题，&lt;/li>
&lt;li>再证明其中一个已知的NPC问题能约化到它（由约化的传递性，则NPC问题定义的第二条也得以满足；至于第一个NPC问题是怎么来的，下文将介绍），这样就可以说它是NPC问题了。&lt;/li>
&lt;/ul>
&lt;p>既然所有的NP问题都能约化成NPC问题，那么只要任意一个NPC问题找到了一个多项式的算法，那么所有的NP问题都能用这个算法解决了，NP也就等于P了。因此，给NPC找一个多项式算法太不可思议了。因此，前文才说，“&lt;strong>正是NPC问题的存在，使人们相信P≠NP&lt;/strong>”。我们可以就此直观地理解，NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。&lt;/p>
&lt;p>顺便讲一下NP-Hard问题。NP-Hard问题是这样一种问题，它&lt;strong>满足NPC问题定义的第二条但不一定要满足第一条&lt;/strong>（就是说，NP-Hard问题要比NPC问题的范围广）（读者注：NP-hard问题不一定是NP问题，我们甚至不一定能在多项式时间内验证解正确与否，可直观理解为&lt;strong>至少与NP完全问题一样难&lt;/strong>）。NP-Hard问题同样难以找到多项式的算法，但它不列入我们的研究范围，因为它不一定是NP问题。即使NPC问题发现了多项式级的算法，NP-Hard问题有可能仍然无法得到多项式级的算法。事实上，由于NP-Hard放宽了限定条件，它将有可能比所有的NPC问题的时间复杂度更高从而更难以解决。&lt;/p>
&lt;p>不要以为NPC问题是一纸空谈。NPC问题是存在的。确实有这么一个非常具体的问题属于NPC问题。下文即将介绍它。 下文即将介绍&lt;strong>逻辑电路问题。这是第一个NPC问题&lt;/strong>。其它的NPC问题都是由这个问题约化而来的。因此，逻辑电路问题是NPC类问题的“鼻祖”。&lt;/p>
&lt;p>逻辑电路问题是指的这样一个问题：给定一个逻辑电路，问是否存在一种输入使输出为True。 什么叫做逻辑电路呢？一个逻辑电路由若干个输入，一个输出，若干“逻辑门”和密密麻麻的线组成。看下面一例，不需要解释你马上就明白了。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/PNPNPCNPhard1.png" alt="PNPNPCNPhard1.png" />&lt;p class="caption">PNPNPCNPhard1.png&lt;/p>
&lt;/div>
&lt;p>这是个较简单的逻辑电路，当输入1、输入2、输入3分别为True、True、False或False、True、False时，输出为True。&lt;/p>
&lt;p>有输出无论如何都不可能为True的逻辑电路吗？有。下面就是一个简单的例子。 &lt;img src="../images/PNPNPCNPhard2.png" alt="PNPNPCNPhard2.png" />&lt;/p>
&lt;p>上面这个逻辑电路中，无论输入是什么，输出都是False。我们就说，这个逻辑电路不存在使输出为True的一组输入。 回到上文，给定一个逻辑电路，问是否存在一种输入使输出为True，这即逻辑电路问题。&lt;/p>
&lt;p>逻辑电路问题属于NPC问题。这是有严格证明的。它显然属于NP问题，并且可以直接证明所有的NP问题都可以约化到它（不要以为NP问题有无穷多个将给证明造成不可逾越的困难）。证明过程相当复杂，其大概意思是说&lt;strong>任意一个NP问题的输入和输出都可以转换成逻辑电路的输入和输出&lt;/strong>（想想计算机内部也不过是一些 0和1的运算），因此对于一个NP问题来说，问题转化为了求出满足结果为True的一个输入（即一个可行解）。&lt;/p>
&lt;p>有了第一个NPC问题后，一大堆NPC问题就出现了，因为再证明一个新的NPC问题只需要将一个已知的NPC问题约化到它就行了。后来，Hamilton 回路成了NPC问题，TSP问题也成了NPC问题。现在被证明是NPC问题的有很多，任何一个找到了多项式算法的话所有的NP问题都可以完美解决了。因此说，&lt;strong>正是因为NPC问题的存在，P=NP变得难以置信&lt;/strong>。P=NP问题还有许多有趣的东西，有待大家自己进一步的挖掘。攀登这个信息学的巅峰是我们这一代的终极目标。现在我们需要做的，至少是不要把概念弄混淆了。&lt;/p>
&lt;p>Matrix67原创&lt;/p>
&lt;p>补充：&lt;/p>
&lt;img src="../images/P_np_np-complete_np-hard.svg" alt="P_np_np-complete_np-hard.svg" />
&lt;center>
P、NP、NPC、NP-hard关系图
&lt;/center></description></item><item><title>算法导论-贪心</title><link>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA-%E8%B4%AA%E5%BF%83/</link><pubDate>Sun, 18 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA-%E8%B4%AA%E5%BF%83/</guid><description>
&lt;h2 id="贪心算法">贪心算法&lt;!-- omit in toc -->&lt;/h2>
&lt;p>贪心算法通常是自顶向下设计的。也需要具有最优子结构。贪心算法也是一种启发式算法，并不能一定保证是最优解。&lt;/p>
&lt;p>从动态规划到贪心算法的转化中，我们通常经理如下步骤：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>确定问题的最优子结构&lt;/li>
&lt;li>设计一个递归算法，可以用动态规划做&lt;/li>
&lt;li>证明如果做出一个贪心选择，则只剩一个子问题&lt;/li>
&lt;li>证明贪心选择总是安全的（3，4可以交换顺序）&lt;/li>
&lt;li>设计一个递归的贪心算法&lt;/li>
&lt;li>将递归贪心算法通过尾递归改成迭代算法&lt;/li>
&lt;/ol>
&lt;p>简化来看，我们可以根据以下步骤来设计贪心算法：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>将最优化问题转换为这样的形式：对其做出一次选择后，只剩下一个子问题需要求解&lt;/li>
&lt;li>证明做出贪心选择后，原问题总是存在最优解，即贪心选择是安全的&lt;/li>
&lt;li>证明做出贪心选择后，剩下的子问题满足性质：其最优解与贪心选择的组合可以得到原问题的最优解，这样就得到了最优子结构。&lt;/li>
&lt;/ol>
&lt;p>对于每一个贪心算法，都有一个更加繁琐的DP算法。关于如何证明贪心算法是最优算法，没有适用于所有情况的方法，但&lt;strong>贪心选择性质和最优子结构&lt;/strong>是两个关键因素。&lt;/p>
&lt;h2 id="拟阵">拟阵&lt;/h2></description></item><item><title>算法导论-渐进记号</title><link>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA-%E6%B8%90%E8%BF%9B%E8%AE%B0%E5%8F%B7/</link><pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA-%E6%B8%90%E8%BF%9B%E8%AE%B0%E5%8F%B7/</guid><description>
&lt;h2 id="算法导论-渐进记号">算法导论-渐进记号&lt;!-- omit in toc -->&lt;/h2>
&lt;blockquote>
&lt;p>来自：&lt;a href="https://blog.csdn.net/so_geili/article/details/53353593">https://blog.csdn.net/so_geili/article/details/53353593&lt;/a> 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;a href="#渐近紧确界记号θbig-theta">渐近紧确界记号：&lt;span class="math">\(Θ\)&lt;/span>(big-theta)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#渐进上界记号obig-oh">渐进上界记号：&lt;span class="math">\(O\)&lt;/span>(big-oh)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几种常见的复杂度关系">几种常见的复杂度关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#符号用法测试素数测试">符号用法测试：素数测试&lt;/a>&lt;/li>
&lt;li>&lt;a href="#渐进下界记号omegabig-omega">渐进下界记号：&lt;span class="math">\(\Omega\)&lt;/span>(big-omega)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非渐进紧确上界osmall-oh">非渐进紧确上界：&lt;span class="math">\(o\)&lt;/span>(small-oh)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非渐进紧确下界omegasmall-omega">非渐进紧确下界：&lt;span class="math">\(\omega\)&lt;/span>(small-omega)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#渐近记号θοoωω关系">渐近记号&lt;span class="math">\(Θ、Ο、o、Ω、ω\)&lt;/span>关系&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="渐近紧确界记号θbig-theta">渐近紧确界记号：&lt;span class="math">\(Θ\)&lt;/span>(big-theta)&lt;/h2>
&lt;p>假设算法A的运行时间表达式&lt;span class="math">\(T_1(n)\)&lt;/span>为：&lt;span class="math">\(T_1(n)=30n^4+20n^3+40n^2+46n+100\)&lt;/span>&lt;/p>
&lt;p>假设算法B的运行时间表达式&lt;span class="math">\(T_2(n)\)&lt;/span>为：&lt;span class="math">\(T_2(n)=1000n^3+50n^2+78n+10\)&lt;/span>&lt;/p>
&lt;p>当问题规模足够大的时候，例如&lt;span class="math">\(n=1,000,000\)&lt;/span>，算法的运行时间将主要取决于时间表达式的第一项，其它项的执行时间只有它的几十万分之一，可以忽略不计。第一项的常数系数，随着&lt;span class="math">\(n\)&lt;/span>的增大，对算法的执行时间也变得不重要了。&lt;/p>
&lt;p>于是，算法A的运行时间可以记为&lt;span class="math">\(T_1(n)≈n^4\)&lt;/span>，记为&lt;span class="math">\(T_1(n)=Θ(n^4)\)&lt;/span>；算法B的运行时间可以记为：&lt;span class="math">\(T_2(n)≈n^3\)&lt;/span>，记为&lt;span class="math">\(T_2(n)=Θ(n^3)\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>Θ的数学含义&lt;/p>
&lt;p>方式一：设&lt;span class="math">\(f(n)\)&lt;/span>和&lt;span class="math">\(g(n)\)&lt;/span>是定义域为自然数集合的函数。如果&lt;span class="math">\(\lim\limits_{n\rightarrow ∞}\dfrac{f(n)}{g(n)}\)&lt;/span>存在，并且等于某个常数&lt;span class="math">\(c(c&amp;gt;0)\)&lt;/span>，那么&lt;span class="math">\(f(n)=Θ(g(n))\)&lt;/span>。通俗理解为&lt;span class="math">\(f(n)和g(n)\)&lt;/span>同阶，&lt;span class="math">\(Θ\)&lt;/span>用来表示算法的精确阶。&lt;/p>
&lt;p>方式二：&lt;span class="math">\(Θ(g(n))=\{f(n)\)&lt;/span>：存在正常量&lt;span class="math">\(c_1、c_2和n_0\)&lt;/span>，使得对所有&lt;span class="math">\(n≥n_0\)&lt;/span>，有&lt;span class="math">\(0≤c_1g(n)≤f(n)≤c_2g(n)\)&lt;/span>，若存在正常量&lt;span class="math">\(c_1、c_2\)&lt;/span>，使得对于足够大的&lt;span class="math">\(n\)&lt;/span>，函数&lt;span class="math">\(f(n)\)&lt;/span>能“夹入”&lt;span class="math">\(c_1g(n)与c_2g(n)\)&lt;/span>之间，则&lt;span class="math">\(f(n)\)&lt;/span>属于集合&lt;span class="math">\(Θ(g(n))\)&lt;/span>，记作&lt;span class="math">\(f(n)∈Θ(g(n))\)&lt;/span>。作为代替，我们通常记“&lt;span class="math">\(f(n)=Θ(g(n))\)&lt;/span>”。&lt;/p>
&lt;/blockquote>
&lt;p>由下图中左侧&lt;span class="math">\(f(n)=Θ(g(n))\)&lt;/span>图可以看出，对所有&lt;span class="math">\(n&amp;gt;n_0\)&lt;/span>时，函数&lt;span class="math">\(f(n)\)&lt;/span>乘一个常量因子可等于&lt;span class="math">\(g(n)\)&lt;/span>，我们称&lt;span class="math">\(g(n)\)&lt;/span>是&lt;span class="math">\(f(n)\)&lt;/span>的一个&lt;strong>渐近紧确界&lt;/strong>。&lt;span class="math">\(Θ\)&lt;/span>记号在五个记号中，&lt;strong>要求是最严格的&lt;/strong>，因为&lt;span class="math">\(g(n)\)&lt;/span>即可以表示上界也可以表示下界。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/渐进紧确界.jpg" alt="渐进紧确界" />&lt;p class="caption">渐进紧确界&lt;/p>
&lt;/div>
&lt;p>需要注意的是：&lt;span class="math">\(Θ(g(n)\)&lt;/span>的定义要求每个成员&lt;span class="math">\(f(n)∈Θ(g(n))\)&lt;/span>均&lt;strong>渐近非负&lt;/strong>，即当&lt;span class="math">\(n\)&lt;/span>足够大时，&lt;span class="math">\(f(n)\)&lt;/span>非负。&lt;strong>渐近正函数&lt;/strong>就是对所有足够大的&lt;span class="math">\(n\)&lt;/span>均为正的函数。&lt;/p>
&lt;h2 id="渐进上界记号obig-oh">渐进上界记号：&lt;span class="math">\(O\)&lt;/span>(big-oh)&lt;/h2>
&lt;blockquote>
&lt;p>定义：设&lt;span class="math">\(f(n)和g(n)\)&lt;/span>是定义域为自然数集&lt;span class="math">\(N\)&lt;/span>上的函数。若存在正数&lt;span class="math">\(c\)&lt;/span>和&lt;span class="math">\(n_0\)&lt;/span>使得对一切&lt;span class="math">\(n≥n_0\)&lt;/span>都有&lt;span class="math">\(0≤f(n)≤cg(n)\)&lt;/span>成立，则称&lt;span class="math">\(f(n)\)&lt;/span>的渐进的上界是&lt;span class="math">\(g(n)\)&lt;/span>，记作&lt;span class="math">\(f(n)=O(g(n))\)&lt;/span>。通俗的说&lt;span class="math">\(n\)&lt;/span>满足一定条件范围内，函数&lt;span class="math">\(f(n)\)&lt;/span>的阶&lt;strong>不高于&lt;/strong>函数&lt;span class="math">\(g(n)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>根据符号&lt;span class="math">\(O\)&lt;/span>的定义，用它评估算法的复杂度得到的只是问题规模&lt;strong>充分大时的一个上界&lt;/strong>。这个上界的阶越低，评估越精确，越有价值。&lt;/p>
&lt;blockquote>
&lt;p>例如：设&lt;span class="math">\(f(n)=n^2+n\)&lt;/span>，则&lt;/p>
&lt;p>&lt;span class="math">\(f(n)=O(n^2)\)&lt;/span>,取&lt;span class="math">\(c=2,n_0=1\)&lt;/span>即可&lt;/p>
&lt;p>&lt;span class="math">\(f(n)=O(n^3)\)&lt;/span>,取&lt;span class="math">\(c=2,n_0=2\)&lt;/span>即可。显然，&lt;span class="math">\(O(n^2)\)&lt;/span>作为上界更为精确。&lt;/p>
&lt;/blockquote>
&lt;h3 id="几种常见的复杂度关系">几种常见的复杂度关系&lt;/h3>
&lt;p>&lt;span class="math">\[O(1)&amp;lt;O(log(n))&amp;lt;O(n)&amp;lt;O(nlogn)&amp;lt;O(n^2)&amp;lt;O(2^n)&amp;lt;O(n!)&amp;lt;O(n^n)\]&lt;/span> 需要注意的是：对数函数在没有底数时，默认底数为2；如&lt;span class="math">\(\lg n=\log n=\log_2 n\)&lt;/span>。因为计算机中很多程序是用二分法实现的。&lt;/p>
&lt;h3 id="符号用法测试素数测试">符号用法测试：素数测试&lt;/h3>
&lt;pre class="sourceCode C">&lt;code class="sourceCode c">&lt;span class="dt">int&lt;/span> isprime(&lt;span class="dt">int&lt;/span> n) {
&lt;span class="kw">for&lt;/span>(&lt;span class="dt">int&lt;/span> i=&lt;span class="dv">2&lt;/span>; i&amp;lt;=(&lt;span class="dt">int&lt;/span>)sqrt(n); i++) {
&lt;span class="kw">if&lt;/span>(n%i==&lt;span class="dv">0&lt;/span>) {
&lt;span class="kw">return&lt;/span> &lt;span class="dv">0&lt;/span>;
}
}
&lt;span class="kw">return&lt;/span> &lt;span class="dv">1&lt;/span>;
}&lt;/code>&lt;/pre>
&lt;p>在上面这个素数测试的例子中，基本运算是整除；时间复杂度&lt;span class="math">\(T(n)=O(n^{\frac{1}{2}})\)&lt;/span>是正确的。当被测的数&lt;span class="math">\(n\)&lt;/span>为偶数时，基本运算一次也没执行，所以&lt;span class="math">\(T(n)=Θ(n^{\frac{1}{2}})\)&lt;/span>是错误的，因为没有办法证明&lt;span class="math">\(T(n)\)&lt;/span>的下界是&lt;span class="math">\(Ω(n^{\frac{1}{2}})\)&lt;/span>。&lt;/p>
&lt;h2 id="渐进下界记号omegabig-omega">渐进下界记号：&lt;span class="math">\(\Omega\)&lt;/span>(big-omega)&lt;/h2>
&lt;p>定义：设&lt;span class="math">\(f(n)和g(n)\)&lt;/span>是定义域为自然数集&lt;span class="math">\(N\)&lt;/span>上的函数。若存在正数&lt;span class="math">\(c和n_0\)&lt;/span>，使得对一切&lt;span class="math">\(n≥n_0\)&lt;/span>都有&lt;span class="math">\(0≤cg(n)≤f(n)\)&lt;/span>成立，则称&lt;span class="math">\(f(n)\)&lt;/span>的渐进的下界是&lt;span class="math">\(g(n)\)&lt;/span>，记作&lt;span class="math">\(f(n)=Ω(g(n))\)&lt;/span>。通俗的说&lt;span class="math">\(n\)&lt;/span>满足一定条件范围内，函数&lt;span class="math">\(f(n)\)&lt;/span>的阶&lt;strong>不低于&lt;/strong>函数&lt;span class="math">\(g(n)\)&lt;/span>。&lt;/p>
&lt;p>据符号&lt;span class="math">\(Ω\)&lt;/span>的定义，用它评估算法的复杂度得到的只是问题规模充分大时的一个下界。这个下界的阶越高，评估越精确，越有价值。&lt;/p>
&lt;blockquote>
&lt;p>例如：设&lt;span class="math">\(f(n)=n^2+n\)&lt;/span>,则&lt;/p>
&lt;p>&lt;span class="math">\(f(n)=Ω(n^2)，取c=1n_0=1即可\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(f(n)=Ω(100n)，取c=1/100n_0=1即可\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>显然，&lt;span class="math">\(Ω(n^2)\)&lt;/span>作为下界更为精确。&lt;/p>
&lt;h2 id="非渐进紧确上界osmall-oh">非渐进紧确上界：&lt;span class="math">\(o\)&lt;/span>(small-oh)&lt;/h2>
&lt;blockquote>
&lt;p>定义1：设&lt;span class="math">\(f(n)和g(n)\)&lt;/span>是定义域为自然数集&lt;span class="math">\(N\)&lt;/span>上的函数。若对于任意正数&lt;span class="math">\(c，都存在n_0\)&lt;/span>，使得对一切&lt;span class="math">\(n≥n_0\)&lt;/span>都有&lt;span class="math">\(0≤f(n)&amp;lt;cg(n)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>由&lt;span class="math">\(O\)&lt;/span>记号提供的渐近上界&lt;strong>可能是渐近紧确的，也可能是非紧确的&lt;/strong>。（如：&lt;span class="math">\(n^2=O(n^2)\)&lt;/span>是渐近紧确的，而&lt;span class="math">\(2n=O(n^2)\)&lt;/span>是非紧确上界。）&lt;/p>
&lt;blockquote>
&lt;p>例子：&lt;span class="math">\(f(n)=n^2+n\)&lt;/span>，则&lt;span class="math">\(f(n)=o(n^3)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="非渐进紧确下界omegasmall-omega">非渐进紧确下界：&lt;span class="math">\(\omega\)&lt;/span>(small-omega)&lt;/h2>
&lt;blockquote>
&lt;p>定义1：设&lt;span class="math">\(f(n)和g(n)\)&lt;/span>是定义域为自然数集&lt;span class="math">\(N\)&lt;/span>上的函数。若对于任意正数&lt;span class="math">\(c，都存在n_0\)&lt;/span>，使得对一切&lt;span class="math">\(n≥n_0\)&lt;/span>都有&lt;span class="math">\(0≤cg(n)&amp;lt;f(n)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;span class="math">\(ω\)&lt;/span>记号与&lt;span class="math">\(Ω\)&lt;/span>的关系类似于&lt;span class="math">\(o和O\)&lt;/span>记号的关系。我们用&lt;span class="math">\(ω\)&lt;/span>表示一个&lt;strong>非渐近紧确的下界&lt;/strong>。&lt;/p>
&lt;p>例子：&lt;span class="math">\(f(n)=n^2+n\)&lt;/span>，则&lt;span class="math">\(f(n)=ω(n)\)&lt;/span>是正确的。&lt;span class="math">\(f(n)=ω(n^2)\)&lt;/span>则是错误的，&lt;span class="math">\(f(n)=Ω(n^2)\)&lt;/span>是正确的。&lt;/p>
&lt;h2 id="渐近记号θοoωω关系">渐近记号&lt;span class="math">\(Θ、Ο、o、Ω、ω\)&lt;/span>关系&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">记号&lt;/th>
&lt;th align="center">含义&lt;/th>
&lt;th align="center">通俗理解&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">(1)Θ（西塔）&lt;/td>
&lt;td align="center">紧确界&lt;/td>
&lt;td align="center">相当于&amp;quot;=&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">(2)O （大欧）&lt;/td>
&lt;td align="center">上界&lt;/td>
&lt;td align="center">相当于&amp;quot;&amp;lt;=&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">(3)o（小欧）&lt;/td>
&lt;td align="center">非紧的上界&lt;/td>
&lt;td align="center">相当于&amp;quot;&amp;lt;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">(4)Ω（大欧米伽）&lt;/td>
&lt;td align="center">下界&lt;/td>
&lt;td align="center">相当于&amp;quot;&amp;gt;=&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">(5)ω（小欧米伽）&lt;/td>
&lt;td align="center">非紧的下界&lt;/td>
&lt;td align="center">相当于&amp;quot;&amp;gt;&amp;quot;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;div class="figure">
&lt;img src="../images/渐进记号关系.png" alt="渐进记号关系.png" />&lt;p class="caption">渐进记号关系.png&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之假设检验</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</guid><description>
&lt;h2 id="概率统计随机过程之假设检验">概率统计随机过程之假设检验&lt;!-- omit in toc -->&lt;/h2>
&lt;p>统计推断两大内容：&lt;strong>参数估计（点估计、区间估计）与假设检验&lt;/strong>。与参数估计不同，假设检验不需要去计算具体的数值或范围，只需要回答“Yes Or No”的问题。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#假设检验">假设检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#假设检验研究的问题">假设检验研究的问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#假设">“假设”&lt;/a>&lt;/li>
&lt;li>&lt;a href="#如何选择原假设h_0与备择假设h_1">如何选择原假设&lt;span class="math">\(H_0\)&lt;/span>与备择假设&lt;span class="math">\(H_1\)&lt;/span>？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基本方法与步骤">基本方法与步骤&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两类错误">两类错误&lt;/a>&lt;/li>
&lt;li>&lt;a href="#显著性水平">显著性水平&lt;/a>&lt;/li>
&lt;li>&lt;a href="#p值">P值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#显著性水平和p值的区别">显著性水平和P值的区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一个总体的参数假设检验">一个总体的参数假设检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总体均值的检验">总体均值的检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总体比例的检验">总体比例的检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总体方差的检验">总体方差的检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个总体的参数假设检验">两个总体的参数假设检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均值差异性检验">均值差异性检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个总体比例差的检验">两个总体比例差的检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两个总体方差之比的检验">两个总体方差之比的检验&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="假设检验">假设检验&lt;/h2>
&lt;p>假设检验的基本思想是对&lt;strong>总体参数&lt;/strong>的具体数值进行假设性陈述（如使用=，≥，≤等符号），再利用样本或实验结果来推断此假设的可信度。通常，逻辑上采用&lt;strong>反证法&lt;/strong>，但只是概率性证伪，依据是统计上的&lt;strong>小概率原理&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>小概率原理:一个事件如果发生的概率很小的话，那么它在一次试验中是几乎不可能发生的，但在多次重复试验中几乎是必然发生的。&lt;/p>
&lt;/blockquote>
&lt;p>多小能称之为小概率呢？统计学上，一般认为地认为等于或小于0.05或0.01的概率为小概率。&lt;/p>
&lt;h3 id="假设检验研究的问题">假设检验研究的问题&lt;/h3>
&lt;p>总体分布存在未知内容，对总体分布的“某种推断”做出某些“假设”，再通过&lt;strong>抽样的样本&lt;/strong>进行对假设进行检验。主要分为以下两种：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>分布类型未知：非参数假设，例如假设服从正态分布，均匀分布……这类一般很难。针对分布类型未知的假设检验为非参数的假设检验。&lt;/li>
&lt;li>参数未知：参数假设。针对分布类型已知而其中某些参数未知的假设建议称为&lt;strong>参数的假设检验&lt;/strong>（重点）。&lt;/li>
&lt;/ol>
&lt;h3 id="假设">“假设”&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">&lt;/th>
&lt;th align="center">原假设(Null hypothesis)&lt;/th>
&lt;th align="center">备择假设(Alternative hypothesis)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">表示记号&lt;/td>
&lt;td align="center">&lt;span class="math">\(H_0\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(H_1\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">别称&lt;/td>
&lt;td align="center">零假设&lt;/td>
&lt;td align="center">研究假设&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">含义&lt;/td>
&lt;td align="center">待检验的假设&lt;/td>
&lt;td align="center">与原假设对立的假设&lt;br>二者相互对立，有且只有一个成立&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">目的&lt;/td>
&lt;td align="center">研究者想收集证据予以&lt;strong>反对&lt;/strong>的假设&lt;/td>
&lt;td align="center">研究者想收集证据予以&lt;strong>支持&lt;/strong>的假设&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">等号&lt;/td>
&lt;td align="center">带有，=，≥，≤&lt;/td>
&lt;td align="center">不带有，≠，&amp;lt;,&amp;gt;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>原假设与备择假设&lt;strong>二选一&lt;/strong>，接受&lt;span class="math">\(H_0\)&lt;/span>则拒绝&lt;span class="math">\(H_1\)&lt;/span>，反则反之。&lt;/p>
&lt;p>假设设计步骤：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>先确定备择假设，在确定原假设&lt;/li>
&lt;li>=，≥，≤放在原假设中&lt;/li>
&lt;li>检验目的是收集证据拒绝原假设&lt;/li>
&lt;/ol>
&lt;h3 id="如何选择原假设h_0与备择假设h_1">如何选择原假设&lt;span class="math">\(H_0\)&lt;/span>与备择假设&lt;span class="math">\(H_1\)&lt;/span>？&lt;/h3>
&lt;p>&lt;strong>科学的审慎原则与原假设的优势性&lt;/strong>。在科学研究中，通常要求保守主义，遵守习俗、惯例和延续性。比如新的工艺或技术默认无效、新的要没有疗效、变量无关联等。&lt;/p>
&lt;p>原假设是具有先天优势的，因为必须拿出充分的证据才可以推翻，具备先天受保护性，相对于备择假设更有优势。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>一般不能轻易推翻（否定）的假设为原假设，原假设不能轻易拒绝，除非有足够的证据。&lt;/li>
&lt;li>保守性的作为原假设。&lt;/li>
&lt;li>分析人员想证明正确的命题作为备择假设，把分析人员努力证明他不正确的命题作为原假设。&lt;/li>
&lt;li>如果命题成立，但是误判为不成立时会造成严重后果的；命题为原假设。&lt;/li>
&lt;/ol>
&lt;p>个人总结：惰性，严谨性（悲观性）&lt;/p>
&lt;h2 id="基本方法与步骤">基本方法与步骤&lt;/h2>
&lt;p>&lt;strong>基本方法&lt;/strong>：&lt;/p>
&lt;p>用&lt;strong>样本&lt;/strong>构造统计量&lt;span class="math">\(T\)&lt;/span>，在原假设&lt;span class="math">\(H_0\)&lt;/span>情况下，&lt;span class="math">\(T\)&lt;/span>的分布已知，&lt;span class="math">\(T\)&lt;/span>发生的概率与此次抽样相关，&lt;strong>关注小概率事件在一次抽样中是否发生&lt;/strong>。&lt;/p>
&lt;p>&lt;em>假设检验和区间估计方法是类似的&lt;/em>。&lt;/p>
&lt;p>&lt;strong>步骤&lt;/strong>：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>提出原假设&lt;span class="math">\(H_0\)&lt;/span>与备择假设&lt;span class="math">\(H_1\)&lt;/span>&lt;/li>
&lt;li>假定&lt;span class="math">\(H_0\)&lt;/span>成立，构造统计量&lt;span class="math">\(T\)&lt;/span>，其分布已知&lt;/li>
&lt;li>对于给定的小概率&lt;span class="math">\(\alpha\)&lt;/span>，找到对应的小概率区间&lt;span class="math">\(W\)&lt;/span>，使得&lt;span class="math">\(P(\{X_1\dotsb,X_n\}\in W)=\alpha\)&lt;/span>。我们也称&lt;span class="math">\(W\)&lt;/span>为拒绝域，对应的大概率&lt;span class="math">\(1-\alpha\)&lt;/span>对应的区间&lt;span class="math">\(\bar W\)&lt;/span>为接受域。&lt;/li>
&lt;li>由样本数据值&lt;span class="math">\(x_1,\dotsb,x_n\)&lt;/span>，求出统计量&lt;span class="math">\(T\)&lt;/span>的值&lt;/li>
&lt;li>若样本数据构造的&lt;span class="math">\(\hat T\in W\Rightarrow\)&lt;/span> 在拒绝域，拒绝&lt;span class="math">\(H_0\)&lt;/span>；若样本数据构造的&lt;span class="math">\(\hat T\in \bar W\Rightarrow\)&lt;/span>在接受域，接受&lt;span class="math">\(H_0\)&lt;/span>；&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>补充&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>单侧检验，备择假设有方向性，拒绝域只在左侧（&amp;lt;）或右侧（&amp;gt;）。&lt;/li>
&lt;li>双侧检验，备择假设没有方向性，拒绝域在两侧（=）&lt;/li>
&lt;/ul>
&lt;h2 id="两类错误">两类错误&lt;/h2>
&lt;p>我们根据概率做的决策未必是对的。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>第一类错误：弃真。原假设为真，但是推翻原假设。&lt;span class="math">\(P(拒绝H_0|H_0为真)=\alpha\)&lt;/span>。此&lt;span class="math">\(\alpha\)&lt;/span>就是上文中的&lt;span class="math">\(\alpha\)&lt;/span>，称为显著性水平。&lt;span class="math">\(1-\alpha\)&lt;/span>成为置信水平。&lt;/li>
&lt;li>第二类错误：纳伪。原假设为假，但是不推翻原假设。&lt;span class="math">\(P(接受H_0|H_0为假)=\beta\)&lt;/span>。&lt;span class="math">\(1-\beta\)&lt;/span>称为检测效率。&lt;/li>
&lt;/ol>
&lt;p>两类错误通常在样本一定时无法兼顾，第一类错误低，必然导致第二类错误升高。&lt;/p>
&lt;p>N-P原则：尽量保证不犯第一类错误的前提下，尽量减少第二类错误。&lt;/p>
&lt;p>注意，我们只能说不能拒绝原假设，而不能轻易说接受原假设。一般文中的接受域只是术语。&lt;/p>
&lt;h3 id="显著性水平">显著性水平&lt;/h3>
&lt;p>显著性水平&lt;span class="math">\(\alpha\)&lt;/span>是一个概率值，代表着&lt;strong>拒绝原假设$H_0率，其概率范围叫拒绝域&lt;/strong>。通常显著性水平的值比较小，如0.01，0.05，0.10，这种小概率也表明要拒绝原假设的概率应该很小，除非证据充分。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/拒绝域.png" alt="拒绝域.png" />&lt;p class="caption">拒绝域.png&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>双侧检验：|检测统计量值|&amp;gt;临界值，拒绝&lt;span class="math">\(H_0\)&lt;/span>.&lt;/li>
&lt;li>左侧检验：检测统计量值 &amp;lt; 临界值，拒绝&lt;span class="math">\(H_0\)&lt;/span>.&lt;/li>
&lt;li>右侧检验：检测统计量值 &amp;gt; 临界值，拒绝&lt;span class="math">\(H_0\)&lt;/span>.&lt;/li>
&lt;/ul>
&lt;p>显著性水平是人为规定的，其概率大小与犯第一类错误的大小有关，显著性水平&lt;span class="math">\(\alpha\)&lt;/span>越大，拒绝原假设的概率越大，那么发生第一类错误的概率&lt;span class="math">\(P(拒绝H_0|H_0为真)\)&lt;/span>也就越大。&lt;/p>
&lt;h2 id="p值">P值&lt;/h2>
&lt;p>在上一小结使用显著性性水平和置信度的体系中，我们人为地规定了置信度，然后根据置信度确定临界值划分接受域和拒绝域。这实际上只回答了Yes Or NO的问题，具体抽样出来的样本发生概率有多小，并没有计算出来。为了近似的计算这个样本发生的概率，我们使用&lt;strong>P值&lt;/strong>这个指标。&lt;/p>
&lt;blockquote>
&lt;p>P值（P value）就是&lt;strong>当原假设为真时，比所得到的样本观察结果更极端的结果出现的概率&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>需要注意P值不是说所得样本发生的概率，而是所有比样本更加离谱发生的概率的&lt;strong>总和&lt;/strong>，它是一组事件集合的概率。对于不同的检验，离谱/极端的定义是不一样的，比如：&lt;/p>
&lt;ul>
&lt;li>左侧检验：检验统计量小于或等于根据实际观测样本数据计算得到的检验统计量值的概率。&lt;/li>
&lt;li>右侧检验：检验统计量大于或等于根据实际观测样本数据计算得到的检验统计量值的概率。&lt;/li>
&lt;li>双侧检验：既要考虑小于等于又要考虑大于等于的情形。&lt;/li>
&lt;/ul>
&lt;p>如果P值很小，说明原假设情况的发生的概率很小，而如果出现了，根据小概率原理，我们就有理由拒绝原假设，P值越小，我们拒绝原假设的理由越充分。总之，P值越小，表明结果越显著。用公式表示为： &lt;span class="math">\[
P\leq P_{\alpha}，则拒绝H_0
\]&lt;/span> 其中，&lt;span class="math">\(P_{\alpha}\)&lt;/span>是我们选定的概率。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/P值.png" alt="P值.png" />&lt;p class="caption">P值.png&lt;/p>
&lt;/div>
&lt;h3 id="显著性水平和p值的区别">显著性水平和P值的区别&lt;/h3>
&lt;p>显著性水平是人为定的，我们根据显著性水平得到临界值，用临界值和统计量比较。&lt;/p>
&lt;p>P值是设计一个统计量，然后计算该统计量所有极端情况的概率和，再和一个人为定义的概率做比较。此外，P值能够得到极端情形的概率，能够提供更多的信息。&lt;/p>
&lt;p>二者的思路正好是互补的，只不过显著性水平和人为定义的小概率值通常取得一致，比如都是0.1，0.05，所以才容易搞混。&lt;/p>
&lt;p>通常在研究报告中，作者会求出P值，然后让读者去自行选择拒绝的小概率，而非使用显著性水平，直接拒绝原假设。&lt;/p>
&lt;h2 id="一个总体的参数假设检验">一个总体的参数假设检验&lt;/h2>
&lt;p>在总体分布类型已知的情况下，如果只有一个参数需要检验，会有以下常见情况。&lt;/p>
&lt;h3 id="总体均值的检验">总体均值的检验&lt;/h3>
&lt;p>根据中心极限定理，当样本数量足够多时，样本的均值服从渐进正态分布&lt;span class="math">\(N(\mu_0,\sigma^2/n)\)&lt;/span>，其中&lt;span class="math">\(\mu_0\)&lt;/span>是总体真正的均值，&lt;span class="math">\(\sigma\)&lt;/span>是总体的标准差，&lt;span class="math">\(n\)&lt;/span>是样本容量。通常，当样本容量&lt;span class="math">\(n&amp;gt;30\)&lt;/span>时，我们就可以使用正态分布作为样本均值的分布。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/一个参数均值检验.png" alt="一个参数均值检验.png" />&lt;p class="caption">一个参数均值检验.png&lt;/p>
&lt;/div>
&lt;p>当总体的标准差&lt;span class="math">\(\sigma\)&lt;/span>未知时，我们可以用样本的标准差近似代替总体的标准差。需要注意，样本方差公式中分母是&lt;span class="math">\(n-1\)&lt;/span> &lt;span class="math">\[
s^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-{\bar {X}})^2
\]&lt;/span> 其中&lt;span class="math">\(\bar{X}\)&lt;/span>为样本均值。&lt;/p>
&lt;p>z检验就是正态检验，t检验是student检验。&lt;/p>
&lt;h3 id="总体比例的检验">总体比例的检验&lt;/h3>
&lt;p>总体比例的检验多见于使用二项分布的场景。比如，总体中具有某一个特征的个体可能比例为&lt;span class="math">\(\epsilon\)&lt;/span>，现在抽样的样本容量为&lt;span class="math">\(n\)&lt;/span>，其中具有该特征的样本数量为&lt;span class="math">\(x\)&lt;/span>，要求检验比例&lt;span class="math">\(\epsilon\)&lt;/span>的准确性。本质上是检验&lt;span class="math">\(\epsilon\)&lt;/span>与&lt;span class="math">\(p=\frac{x}{n}\)&lt;/span>的关系。&lt;/p>
&lt;p>我们知道当&lt;span class="math">\(n\)&lt;/span>较大时，二项分布是不太好求的，因为存在计算量很大的阶乘项，但是根据二项分布的中心极限定理，当&lt;span class="math">\(n\)&lt;/span>较大，且&lt;span class="math">\(n\epsilon\)&lt;/span>的值不是很小（&lt;span class="math">\(n\epsilon&amp;gt;5, n(1-\epsilon)&amp;gt;5\)&lt;/span>）时，则可用正态分布去近似二项分布。&lt;/p>
&lt;p>根据抽样要求可知，具有某一个特征的个体数量服从二项分布&lt;span class="math">\(B(n,\epsilon)\)&lt;/span>，即 &lt;span class="math">\[
P(x=k)=C_n^{k}\epsilon^k(1-\epsilon)^{n-k}
\]&lt;/span> &lt;strong>当&lt;span class="math">\(n\)&lt;/span>很大时&lt;/strong>，我们可以用正态分布近似二项分布，即&lt;span class="math">\(x\sim N(n\epsilon,n\epsilon(1-\epsilon))\)&lt;/span>，那么根据随机变量的关系有&lt;span class="math">\(p=\frac{x}{n}\sim N(\epsilon,{\epsilon(1-\epsilon)\over{n}})\)&lt;/span>，将&lt;span class="math">\(p\)&lt;/span>标准化后可得 &lt;span class="math">\[
\frac{p-\epsilon}{\sqrt{\frac{\epsilon(1-\epsilon)}{n}}}\sim N(0,1)
\]&lt;/span> 这样我们可以通过正态（Z）检验来对比例进行假设检验。&lt;/p>
&lt;h3 id="总体方差的检验">总体方差的检验&lt;/h3>
&lt;p>根据笔记&lt;a href="概率统计随机过程之抽样的分布.md">概率统计随机过程之抽样的分布&lt;/a>中内容可知，样本方差&lt;span class="math">\(S^2\)&lt;/span>与总体方差&lt;span class="math">\(\sigma^2\)&lt;/span>有如下关系： &lt;span class="math">\[
\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)
\]&lt;/span> 因此，可以对总体的方差进行卡方检验，方法与Z检验类似，只是卡方检验多用于单侧检验，且分布需要使用卡方分布。&lt;/p>
&lt;h2 id="两个总体的参数假设检验">两个总体的参数假设检验&lt;/h2>
&lt;p>两个总体的检验和单个总体的检验分类一样，也是分为均值、比例和方差的检验，如下图所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/两个总体的检验.png" alt="两个总体的检验" />&lt;p class="caption">两个总体的检验&lt;/p>
&lt;/div>
&lt;p>通常我们研究的两个总体之间的差距，因此，一般研究的是&lt;strong>两个总体的均值之差、比例之差与方差之比&lt;/strong>。&lt;/p>
&lt;h3 id="均值差异性检验">均值差异性检验&lt;/h3>
&lt;div class="figure">
&lt;img src="../images/两个总体均值差的检验.png" alt="两个总体均值差的检验" />&lt;p class="caption">两个总体均值差的检验&lt;/p>
&lt;/div>
&lt;div class="figure">
&lt;img src="../images/两个正态总体的参数假设检验1.png" alt="两个正态总体的参数假设检验1.png" />&lt;p class="caption">两个正态总体的参数假设检验1.png&lt;/p>
&lt;/div>
&lt;h3 id="两个总体比例差的检验">两个总体比例差的检验&lt;/h3>
&lt;div class="figure">
&lt;img src="../images/两个总体比例差的检验.png" alt="两个总体比例差的检验" />&lt;p class="caption">两个总体比例差的检验&lt;/p>
&lt;/div>
&lt;h3 id="两个总体方差之比的检验">两个总体方差之比的检验&lt;/h3>
&lt;div class="figure">
&lt;img src="../images/两个总体方差之比的检验.png" alt="两个总体方差之比的检验" />&lt;p class="caption">两个总体方差之比的检验&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之参数估计</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</guid><description>
&lt;h2 id="概率统计随机过程之参数估计">概率统计随机过程之参数估计&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#点估计">点估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#点估计的优良性准则">点估计的优良性准则&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩估计">矩估计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#矩估计存在的要求">矩估计存在的要求&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩估计的一般步骤">矩估计的一般步骤&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩估的特点">矩估的特点&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#极大似然估计">极大似然估计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#最大似然估计的不变原理">最大似然估计的不变原理&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#点估计的有效性详解">点估计的有效性详解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一致最小方差无偏估计">一致最小方差无偏估计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#改进一个无偏估计">改进一个无偏估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#零无偏估计法">零无偏估计法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#充分完全统计量法">充分完全统计量法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#cramer-raoc-r不等式与界">Cramer-Rao（C-R）不等式与界&lt;/a>&lt;/li>
&lt;li>&lt;a href="#区间估计">区间估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#置信区间与枢轴变量">置信区间与枢轴变量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一个正态总体的期望和方差的区间估计">一个正态总体的期望和方差的区间估计&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>统计推断三大内容：&lt;strong>抽样分布、参数估计（点估计、区间估计）与假设检验&lt;/strong>。&lt;/p>
&lt;p>参数估计的核心思想：用抽样出来的样本构造函数（统计量）来尝试近似实际分布的参数。&lt;/p>
&lt;h2 id="点估计">点估计&lt;/h2>
&lt;blockquote>
&lt;p>估计量：在参数估计大类的点估计中，那么用于估计未知参数的&lt;strong>统计量&lt;/strong>称为&lt;strong>点估计(量)&lt;/strong>，简称为&lt;strong>估计(量)&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>估计一个&lt;strong>具体的数值&lt;/strong>，实际比较困难。我们用样本来构造一个函数 &lt;span class="math">\[
\hat\theta = \Theta(X),X=(x_1,\dotsb,x_n)
\]&lt;/span> 用以计算参数&lt;span class="math">\(\theta\)&lt;/span>，由于是估计值所以用&lt;span class="math">\(\hat\theta\)&lt;/span>表示。其中&lt;span class="math">\(X=(x_1,\dotsb,x_n)\)&lt;/span>为容量为n的样本。有时也用&lt;span class="math">\(\hat g(\theta)\)&lt;/span>表示，因为有时要估计的不是&lt;span class="math">\(\theta\)&lt;/span>，而是&lt;span class="math">\(\theta\)&lt;/span>的某个函数。&lt;/p>
&lt;p>具体可参见笔记《概率统计随机过程之数理统计常用概念》中统计量与估计量那一小节。&lt;/p>
&lt;p>具体方法：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>矩估计&lt;/li>
&lt;li>最大似然估计（MLE）&lt;/li>
&lt;li>贝叶斯估计（MAP等）&lt;/li>
&lt;li>LSE（MMSE）&lt;/li>
&lt;li>……&lt;/li>
&lt;/ol>
&lt;h3 id="点估计的优良性准则">点估计的优良性准则&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>无偏性。&lt;span class="math">\(E(\hat\theta)=\theta\)&lt;/span>（样本方差的系数&lt;span class="math">\(n-1\)&lt;/span>就是这里的无偏性得出来的）&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;span class="math">\(\hat\theta\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，但是&lt;span class="math">\(g(\hat\theta)\)&lt;/span>不一定是&lt;span class="math">\(g(\theta)\)&lt;/span>的无偏估计。例如说明1。&lt;/li>
&lt;/ul>
&lt;ol start="2" style="list-style-type: decimal">
&lt;li>有效性。估计值的方差越小越好。&lt;span class="math">\(D(\hat\theta_1)\leq D(\hat\theta_2)\)&lt;/span>则&lt;span class="math">\(\hat\theta_1\)&lt;/span>更好。波动性小，无偏情况下，更可能接近于真实值。&lt;/li>
&lt;li>相合性（一致性）。&lt;span class="math">\(\lim_{n\rightarrow \infty} P(|\hat\theta-\theta|&amp;lt;\varepsilon)=1\)&lt;/span>，取的样本越多，越趋近于真实值。&lt;/li>
&lt;li>渐进正态性（大样本性质）。体现了估计量随样本数量趋近于真实值的速度。&lt;/li>
&lt;/ol>
&lt;p>说明1：样本方差&lt;span class="math">\(S^2\)&lt;/span>是&lt;span class="math">\(\sigma^2\)&lt;/span>的无偏估计，但是&lt;span class="math">\(\sqrt{(S^2)}\)&lt;/span>不是&lt;span class="math">\(\sqrt{\sigma^2}\)&lt;/span>的无偏估计。&lt;/p>
&lt;p>说明2：无偏性+有效性（尽量小方差）=一致最小方差无偏估计（UMVUE）&lt;/p>
&lt;p>说明3：无偏性和相合性是两个方面的性质，无偏性是概率性质，相合性是统计性质，无偏不一定相合。&lt;/p>
&lt;p>具体可参见笔记&lt;a href="概率统计随机过程之数理统计常用概念.md">《概率统计随机过程之数理统计常用概念》&lt;/a>中估计量的评价指标那一小节。&lt;/p>
&lt;h3 id="矩估计">矩估计&lt;/h3>
&lt;p>核心思想：近似替代。&lt;strong>用样本的矩代替总体的矩，用样本矩的函数估计总体矩的相应函数&lt;/strong>。就把样本当总体。&lt;/p>
&lt;p>理论基础是&lt;strong>格涅坚科大数定理&lt;/strong>（原始数据用原点矩估计）&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">总体的矩&lt;/th>
&lt;th align="center">&amp;lt;-----&amp;gt;&lt;/th>
&lt;th align="center">样本的矩&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">一阶 &lt;span class="math">\(E(X)\)&lt;/span>&lt;/td>
&lt;td align="center">&amp;lt;-----&amp;gt;&lt;/td>
&lt;td align="center">一阶 &lt;span class="math">\(\bar X = {1\over n}\sum X_i\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">二阶 &lt;span class="math">\(E(X^2)\)&lt;/span>&lt;/td>
&lt;td align="center">&amp;lt;-----&amp;gt;&lt;/td>
&lt;td align="center">二阶 &lt;span class="math">\(A_2 = {1\over n}\sum X_i^2\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\(\vdots\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>同时有： &lt;span class="math">\[\begin{aligned}
&amp;amp;\hat \mu = \bar X\\
&amp;amp;\hat \sigma^2 = A_2 - \hat \mu^2 =B_2(二阶中心矩)
\end{aligned}
\]&lt;/span> 不难发现，中心矩可以用原点矩来表示。&lt;/p>
&lt;h4 id="矩估计存在的要求">矩估计存在的要求&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>该分布的k阶矩是存在的&lt;/li>
&lt;li>估计量或其函数&lt;span class="math">\(g(\theta)\)&lt;/span>可以写成各阶矩组成的函数，即&lt;span class="math">\(g(\theta)=G(\underbrace{\alpha_1,\alpha_2,\dotsb,\alpha_k}_{k原点矩},\underbrace{\mu_2,\mu_3,\dotsb,\mu_s}_{s阶中心矩})\)&lt;/span>，则&lt;span class="math">\(g(\theta)\)&lt;/span>可以用样本矩的函数&lt;span class="math">\(\hat g(X)=G(a_1,a_2,\dotsb,a_k,m_2,\dotsb,m_s)\)&lt;/span>进行矩估计，其中&lt;span class="math">\(a_i,s_i\)&lt;/span>分别是&lt;span class="math">\(i\)&lt;/span>阶样本原点矩和中心矩。&lt;/li>
&lt;/ol>
&lt;h4 id="矩估计的一般步骤">矩估计的一般步骤&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>求出一些矩。一般情况下，原分布中有几个未知参数，就需要几阶矩估计。（有时也确定需要哪些矩）&lt;/li>
&lt;li>将&lt;span class="math">\(\theta\)&lt;/span>或&lt;span class="math">\(g(\theta)\)&lt;/span>用矩表示（表示方法不唯一，可以根据UMVUE准则判断优劣，即要无偏，方差要小）。&lt;/li>
&lt;li>替换矩（样本矩→总体矩，&lt;span class="math">\(\hat\theta = \varTheta(X)→\theta\)&lt;/span>或&lt;span class="math">\(\hat g(X)→g(\theta)\)&lt;/span>）。&lt;/li>
&lt;/ol>
&lt;h4 id="矩估的特点">矩估的特点&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>简单。&lt;/li>
&lt;li>一般矩估计是渐进无偏的。由于原点矩都是无偏的，中心距是渐进无偏的；只有当估计量是原点矩的线性组合时，才是无偏估计。&lt;/li>
&lt;li>没有运用到总体的分布信息，准确性稍差。而且要求总体的矩一定存在。&lt;/li>
&lt;li>矩估计不唯一，取决于&lt;span class="math">\(\theta\)&lt;/span>或&lt;span class="math">\(g(\theta)\)&lt;/span>如何被各阶矩表示。&lt;/li>
&lt;li>相合性：略&lt;/li>
&lt;/ol>
&lt;h3 id="极大似然估计">极大似然估计&lt;/h3>
&lt;p>核心思想：概率大的事件比概率小的事件更容易发生。&lt;strong>要估计的参数能够使产生这个样本的概率最大&lt;/strong>。&lt;/p>
&lt;p>步骤：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>总体的概率/密度函数。&lt;/li>
&lt;li>写出似然估计函数&lt;span class="math">\(L(\lambda)\)&lt;/span>。&lt;/li>
&lt;li>两边取&lt;span class="math">\(\ln\)&lt;/span>，即为&lt;span class="math">\(\ln(L(\lambda))\)&lt;/span>。&lt;/li>
&lt;li>对&lt;span class="math">\(\lambda\)&lt;/span>求导（多参数估计就是偏导），令导数为0。&lt;/li>
&lt;/ol>
&lt;p>TIPS：对于有些分布的极大似然估计没法直接求，比如均匀分布。&lt;/p>
&lt;p>&lt;strong>似然函数取对数的原因&lt;/strong>:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>减少计算量。乘法变成加法，从而减少了计算量；同时，如果概率中含有指数项，如高斯分布，能把指数项也化为求和形式，进一步减少计算量；另外，在对联合概率求导时，和的形式会比积的形式更方便。&lt;/li>
&lt;li>计算时更准确。为概率值都在[0,1]之间，因此，概率的连乘将会变成一s个很小的值，可能会引起浮点数下溢，尤其是当数据集很大的时候，联合概率会趋向于0，非常不利于之后的计算。&lt;/li>
&lt;/ol>
&lt;p>需要指出的是：&lt;strong>取对数不影响单调性&lt;/strong>。 &lt;span class="math">\[
p(x|\theta_1)&amp;gt;p(x|\theta_2)\Leftrightarrow \ln(p(x|\theta_1))&amp;gt;\ln(p(x|\theta_2))
\]&lt;/span> 因为相同的单调性，它确保了概率的最大对数值出现在与原始概率函数相同的点上。因此，可以用更简单的对数似然来代替原来的似然。&lt;/p>
&lt;h4 id="最大似然估计的不变原理">最大似然估计的不变原理&lt;/h4>
&lt;p>我们介绍一个致使最大似然估计得到广泛应用的定理。&lt;/p>
&lt;blockquote>
&lt;p>定理1（不变定理）：设&lt;span class="math">\(X\sim p(x;\theta),\;\theta\in\mathcal{\Theta}\)&lt;/span>，若&lt;span class="math">\(\theta\)&lt;/span>的最大似然估计为&lt;span class="math">\(\hat\theta\)&lt;/span>，则对任意函数&lt;span class="math">\(\gamma=g(\theta)\)&lt;/span>，&lt;span class="math">\(\gamma\)&lt;/span>的最大似然估计为&lt;span class="math">\(\hat\gamma=g(\hat\theta)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>这个定理的条件很宽，致使最大似然估计的应用也会广泛。在函数&lt;span class="math">\(g(\theta)\)&lt;/span>非单调的时候，它的证明需要测度论的内容，暂时不予证明。&lt;/p>
&lt;h3 id="点估计的有效性详解">点估计的有效性详解&lt;/h3>
&lt;p>我们在前面提到过可以用估计值的方差来代表估计的有效性，但是这有一个前提条件：&lt;strong>需要该估计为无偏估计&lt;/strong>。这样估计值才能紧密散布在真值周围，如果是估计值偏差较大，方差很小，那只会猜到一个错误的位置。如果用图像表示无偏与方差的关系，如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/点估计偏差与方差.jpg" alt="点估计偏差与方差" />&lt;p class="caption">点估计偏差与方差&lt;/p>
&lt;/div>
&lt;p>从上图中，我们可以看出：第一幅子图中，当无偏估计且方差很小时，估计点密布在真值周围，我们可以用取平均的方法贴近真实值；如果像第二个子图中，是有偏估计，而方差又很小，那么我们有很大可能会得到一个错误的估计。第二行的两个子图是方差较大的情况，我们可以观察到，无偏估计在方差较大时，点比较散，因此会增大估计误差；而有偏估计在方差较大时，反而可能比小方差时表现的更好。&lt;/p>
&lt;p>下面我们给出无偏估计有效性的精确定义：&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(\hat\theta_1=\hat\theta_1(x_1,x_2,\dotsb,x_n)\)&lt;/span>与&lt;span class="math">\(\hat\theta_2=\hat\theta_2(x_1,x_2,\dotsb,x_n)\)&lt;/span>都是参数&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，如果： &lt;span class="math">\[\mathrm{Var}(\hat\theta_1)\leq \mathrm{Var}(\hat\theta_2),\forall \theta\in \varTheta\]&lt;/span> 且至少对一个&lt;span class="math">\(\theta_0\in\varTheta\)&lt;/span>，有严格不等号成立，则称&lt;span class="math">\(\hat\theta_1\)&lt;/span>比&lt;span class="math">\(\hat\theta_2\)&lt;/span>有效。&lt;/p>
&lt;/blockquote>
&lt;p>那么对于&lt;strong>有偏估计&lt;/strong>，我们要如何评价它的优劣呢？有偏估计与无偏估计相比，除了随机散布造成的方差时存在的，还有与真实值之间的系统性&lt;strong>偏差&lt;/strong>。由于随机造成的误差有正有负，因此我们用&lt;strong>平方的方式&lt;/strong>来去除正负的影响（用绝对值也可以叫平均绝对误差，不过平方有更好的运算性质）。&lt;/p>
&lt;p>在此，我们定义有偏估计的有效性：&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(\hat\theta_1=\hat\theta_1(x_1,x_2,\dotsb,x_n)\)&lt;/span>与&lt;span class="math">\(\hat\theta_2=\hat\theta_2(x_1,x_2,\dotsb,x_n)\)&lt;/span>都是参数&lt;span class="math">\(\theta\)&lt;/span>的估计量，如果： &lt;span class="math">\[E(\hat\theta_1-\theta)^2\leq E(\hat\theta_2-\theta)^2,\forall \theta\in \varTheta\]&lt;/span> 且至少对一个&lt;span class="math">\(\theta_0\in\varTheta\)&lt;/span>，有严格不等号成立，则称在&lt;strong>均方误差&lt;/strong>意义下&lt;span class="math">\(\hat\theta_1\)&lt;/span>优于&lt;span class="math">\(\hat\theta_2\)&lt;/span>。其中，&lt;span class="math">\(E(\hat\theta_i-\theta)^2\)&lt;/span>称为&lt;span class="math">\(\theta_i\)&lt;/span>d的&lt;strong>均方误差&lt;/strong>，常记为&lt;span class="math">\(MSE(\hat\theta_i)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>根据定义不难发现，如果&lt;span class="math">\(\hat\theta_i\)&lt;/span>是无偏估计，那么均方误差等于方差，即&lt;span class="math">\(MSE(\hat\theta)=Var(\hat\theta)\)&lt;/span>。我们可以将均方误差的式子做如下变换： &lt;span class="math">\[\begin{aligned}
MSE(\hat\theta)&amp;amp;=E(\hat\theta-\theta)^2=E\{[\hat\theta-E(\hat\theta)]+[E(\hat\theta)-\theta]\}^2\\
&amp;amp;=E[(\hat\theta-E(\hat\theta))^2]+[E(\hat\theta)-\theta]^2\\
&amp;amp;=\mathrm{Var}(\hat\theta)+\delta^2
\end{aligned}
\]&lt;/span> 其中，我们将&lt;span class="math">\(\delta=|E(\hat\theta)-\theta|\)&lt;/span>称为（绝对）偏差，它体现了估计&lt;span class="math">\(\hat\theta\)&lt;/span>与真实值&lt;span class="math">\(\theta\)&lt;/span>之间的系统误差。由此可见，均方误差&lt;span class="math">\(MSE(\hat\theta_i)\)&lt;/span>可以分解成&lt;strong>系统误差和随机误差两部分&lt;/strong>两部分组成。无偏性可以让偏差&lt;span class="math">\(\delta\)&lt;/span>为0，有效性指标等同于要求方差最小化，而有偏估计则要求二者之和越小越好。假如有一个有偏估计其均方误差比任一个无偏估计的方差还小，则此种有偏估计应予以肯定。如下例子所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/有偏估计MSE例子.png" alt="有偏估计MSE例子.png" />&lt;p class="caption">有偏估计MSE例子.png&lt;/p>
&lt;/div>
&lt;p>可惜的是，参数的一切可能估计组成的估计类中一致最小均方误差估计不存在。&lt;/p>
&lt;p>思想实验：为什么参数的一切可能估计组成的估计类中一致最小均方误差估计不存在？&lt;/p>
&lt;blockquote>
&lt;p>如果一个估计&lt;span class="math">\(\theta^*\)&lt;/span>为一致最小均方误差，那么对于其他任意对于&lt;span class="math">\(\theta\)&lt;/span>的估计方法&lt;span class="math">\(\tilde{\theta}\)&lt;/span>在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上都有&lt;span class="math">\(MSE(\theta^*)\leq MSE(\tilde{\theta})\)&lt;/span>。问题就出自两个任意上，一是任意估计方法，二是&lt;span class="math">\(\forall\theta\in\varTheta\)&lt;/span>；这两个要求太严格了。我们来设计这样一个场景：参数的真值为&lt;span class="math">\(\theta_0\)&lt;/span>，有一个奇葩的估计方式&lt;span class="math">\(\tilde{\theta}_0\)&lt;/span>，无论给出什么样本，其估计结果都是&lt;span class="math">\(\theta_0\)&lt;/span>（固定值完全消除了样本随机性带来的随机性，导致方差为0）。尽管这个估计方式对于&lt;span class="math">\(\varTheta\)&lt;/span>中除了真实值为&lt;span class="math">\(\theta_0\)&lt;/span>的情况，一无是处，但是我们不能否认在真实值为&lt;span class="math">\(\theta_0\)&lt;/span>，这个估计很完美（绝对误差为0）。此时，&lt;span class="math">\(MSE(\tilde{\theta}_0)=0\)&lt;/span>。而一致最小均方误差&lt;span class="math">\(\theta^*\)&lt;/span>要满足&lt;span class="math">\(MSE(\theta^*)\leq MSE(\tilde{\theta})\)&lt;/span>，那么&lt;span class="math">\(\theta^*\)&lt;/span>必须是方差为0，偏差为0，那么只能让&lt;span class="math">\(\theta^*=\tilde{\theta}_0\)&lt;/span>。那么对于&lt;span class="math">\(\varTheta\)&lt;/span>中的任意&lt;span class="math">\(\theta_i\)&lt;/span>都可以构造类似的奇葩估计：无论给出什么样本，其估计结果都是&lt;span class="math">\(\theta_i\)&lt;/span>。根据&lt;span class="math">\(MSE(\theta^*)\leq MSE(\tilde{\theta})\)&lt;/span>，又必须让&lt;span class="math">\(\theta^*=\tilde\theta_i\)&lt;/span>。那么对于不同&lt;span class="math">\(\theta_i\)&lt;/span>，一致最小均方误差估计&lt;span class="math">\(\theta^*\)&lt;/span>根本不是同一个估计方式，所以一致最小均方误差估计不存在。&lt;/p>
&lt;/blockquote>
&lt;p>对此，我们的处理方式是将需要将估计方式范围缩小一些，做出一些限制，例如要求估计方法都必须是无偏的。此时产生一类非常重要的估计类：一致最小方差无偏估计，简称UMVUE。&lt;/p>
&lt;h3 id="一致最小方差无偏估计">一致最小方差无偏估计&lt;/h3>
&lt;p>我们前面已经分析过，均方误差在无偏估计中会简化为方差，此时一致最小均方误差估计简化为一致最小方差无偏估计。仿照前面的形式，我们给出一致最小方差无偏估计的定义：&lt;/p>
&lt;blockquote>
&lt;p>一致最小方差无偏估计：在参数估计&lt;span class="math">\(\mathcal{F}=\{f(x,\theta),\theta\in\varTheta\}\)&lt;/span>中，如果&lt;span class="math">\(\hat\theta\)&lt;/span>是参数&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>无偏估计&lt;/strong>，如果对另外任意一个&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>无偏估计&lt;/strong>&lt;span class="math">\(\tilde{\theta}\)&lt;/span>，在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>上都有： &lt;span class="math">\[\mathrm{Var}(\hat\theta)\leq \mathrm{Var}(\tilde{\theta})\]&lt;/span> 则称&lt;span class="math">\(\hat{\theta}\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>一致最小方差无偏估计，简记为UMVUE(Uniformly minimum variance unbiased estimation)&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>需要指出，有些参数可能不存在无偏估计，即UMVUE可能也不存在。如果参数的无偏估计存在，我们称此参数为&lt;strong>可估参数&lt;/strong>。为什么加上无偏之后，一致最小方差估计就可以存在了呢？前面我们提到两个任意上，一是任意估计方法，二是&lt;span class="math">\(\forall\theta\in\varTheta\)&lt;/span>，这两个要求太严格。无偏其实是对第一个任意的限制，缩小了估计类范围，把很多奇葩的估计方法（如上文提到的&lt;span class="math">\(\tilde{\theta}_0\)&lt;/span>方法）排除在外。&lt;/p>
&lt;p>显然，在给定样本数量后，从无偏性和有效性两个角度，UMVUE是最优解，下面我们给出三个求UMVUE的方法。第一个是零无偏估计法，第二个是充分完全统计量法，第三个用了C-R不等式（单拎出来介绍）。下面我们分别介绍。&lt;/p>
&lt;h4 id="改进一个无偏估计">改进一个无偏估计&lt;/h4>
&lt;p>假设我们已经知道了一个无偏估计，有没有办法能够优化它的方差呢？我们先介绍一种改进无偏估计的方法——Rao–Blackwell定理。&lt;/p>
&lt;blockquote>
&lt;p>定理2（Rao–Blackwell定理）：设&lt;span class="math">\(T=T(x)\)&lt;/span>是样本&lt;span class="math">\(x\)&lt;/span>关于参数&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>充分统计量&lt;/strong>，&lt;span class="math">\(\hat\theta(x)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个&lt;strong>无偏估计&lt;/strong>，即&lt;span class="math">\(E[\hat\theta(x)]=\theta\)&lt;/span>，则 &lt;span class="math">\[h(T)=E[\hat\theta(x)|T]\]&lt;/span> 是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，并且 &lt;span class="math">\[D[h(T)]\leq D[\hat\theta(x)]\]&lt;/span> 其中当且仅当&lt;span class="math">\(P(\hat\theta(x))=h(T)=1\)&lt;/span>，即&lt;span class="math">\(h(T)=\hat\theta(x)\)&lt;/span>，a.s. P成立。&lt;/p>
&lt;/blockquote>
&lt;p>我们先解释下&lt;span class="math">\(h(T)=E[\hat\theta(x)|T]\)&lt;/span>，从条件期望的&lt;span class="math">\(E(X|Y)\)&lt;/span>可知，我们是对&lt;span class="math">\(X\)&lt;/span>求期望，会将&lt;span class="math">\(X\)&lt;/span>的随机性抹去，&lt;span class="math">\(E(X|Y)\)&lt;/span>实际上是关于&lt;span class="math">\(Y\)&lt;/span>的随机变量函数，即&lt;span class="math">\(f(Y)=E(X|Y)\)&lt;/span>，当&lt;span class="math">\(Y=y\)&lt;/span>时，函数的值就确定了。因此&lt;span class="math">\(h(T)=E[\hat\theta(x)|T]\)&lt;/span>就是一个关于&lt;span class="math">\(T\)&lt;/span>的随机变量函数&lt;span class="math">\(h(T)\)&lt;/span>。这个定理的关键就是说，这样的复合函数&lt;span class="math">\(h\cdot T\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，且方差比原来的无偏估计&lt;span class="math">\(\hat\theta(x)\)&lt;/span>小。&lt;/p>
&lt;p>&lt;span class="math">\(h\cdot T\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计这点用重期望公式可以证明： &lt;span class="math">\[
E[h(T)]=E[E(\hat\theta(x)|T)]=E[\hat\theta(x)]
\]&lt;/span> 由于&lt;span class="math">\(\hat\theta(x)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计，所以&lt;span class="math">\(E[h(T)]=E[\hat\theta(x)]=\theta\)&lt;/span>，即&lt;span class="math">\(E[h(T)]\)&lt;/span>也是&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计。&lt;/p>
&lt;p>对于方差的证明，我们需要用一个小技巧： &lt;span class="math">\[
\begin{aligned}
D[\hat\theta(x)]&amp;amp;=E\{\hat\theta(x)-E[\hat\theta(x)]\}^2\\
&amp;amp;=E\{\hat\theta(x)\underbrace{-h(T)+h(T)}_{引入h(T)}-\underbrace{E[\hat\theta(x)]}_\theta\}^2\\
&amp;amp;=E[\hat\theta(x)-h(T)]^2+\underbrace{E[h(T)-\theta]^2}_{D[h(T)]}+2\{E[h(T)-\theta][\hat\theta(x)-h(T)]\}
\end{aligned}
\]&lt;/span> 前面两项好理解，需要处理一下交叉相乘的最后一项。我们可以根据重期望公式将其换种写法： &lt;span class="math">\[
E\{[h(T)-\theta][\hat\theta(x)-h(T)]\}=E_T\{E[(h(T)-\theta)(\hat\theta(x)-h(T))|T]\}\\
当T=t为给定的条件时，h(T)为一常数\\
原式=E_T\{(h(T)-\theta)E[\hat\theta(x)-h(T)|T]\}=E_T\{(h(T)-\theta)\{E[\hat\theta(x)|T]-h(T)\}\}\\
\]&lt;/span> 在式子的最后，我们发现有一项&lt;span class="math">\(E[\hat\theta(x)|T]\)&lt;/span>，而这正是&lt;span class="math">\(h(T)\)&lt;/span>的定义啊，所以必有 &lt;span class="math">\[
E_T\{(h(T)-\theta)\underbrace{\{E[\hat\theta(x)|T]-h(T)\}}_{E[\hat\theta(x)|T]=h(T)}\}=0
\]&lt;/span> 因此，&lt;span class="math">\(2\{E[h(T)-\theta][\hat\theta(x)-h(T)]\}=0\)&lt;/span>，&lt;span class="math">\(D[\hat\theta(x)]\)&lt;/span>可以写成： &lt;span class="math">\[
D[\hat\theta(x)]=E[\hat\theta(x)-h(T)]^2+D[h(T)]\geq D[h(T)],\forall \theta \in \varTheta
\]&lt;/span> 并且等号成立的条件是&lt;span class="math">\(E[\hat\theta(x)-h(T)]^2=0\)&lt;/span>，即&lt;span class="math">\(\hat\theta(x)=h(T)\)&lt;/span>。这里再多解释一句，充分统计量&lt;span class="math">\(T\)&lt;/span>也是&lt;span class="math">\(x\)&lt;/span>的函数，所以前式具体应写为&lt;span class="math">\(\hat\theta(x)=h(T(x))\)&lt;/span>或&lt;span class="math">\(\hat\theta=h\cdot T\)&lt;/span>。&lt;/p>
&lt;p>Rao–Blackwell定理的意义在于，&lt;strong>如果我们能够找到一个充分统计量，那么就用&lt;span class="math">\(E[\hat\theta(x)|T]\)&lt;/span>可以改进任何&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，得到&lt;span class="math">\(h(T(x))\)&lt;/span>&lt;/strong>。且在&lt;strong>充分统计量存在&lt;/strong>的情况下，UMVUE一定是充分统计量的函数（除非相等），否则我们可以通过&lt;span class="math">\(h(T)\)&lt;/span>构造一个方差更小的无偏估计。&lt;/p>
&lt;p>举个例子：设&lt;span class="math">\(X=(X_1,\dotsb,X_n)\)&lt;/span>是从两点分布族&lt;span class="math">\(\{b(1,p):0&amp;lt;p&amp;lt;1\}\)&lt;/span>中抽取的简单样本。显然&lt;span class="math">\(X_1\)&lt;/span>是&lt;span class="math">\(p\)&lt;/span>的一个无偏估计，从前文可知&lt;span class="math">\(T(X)=\sum_{i=1}^n X_i\)&lt;/span>是&lt;span class="math">\(p\)&lt;/span>的充分统计量，试利用&lt;span class="math">\(T\)&lt;/span>构造一个比&lt;span class="math">\(X_1\)&lt;/span>方差更小的无偏估计。&lt;/p>
&lt;p>利用Rao–Blackwell定理可知，通过条件期望构造的无偏估计如下： &lt;span class="math">\[
h(t)=E[X_1|T=t]=1\times P(X_1=1|T=t)+0\times P(X_1=0|T=t)\\
=E[X_1|T=t]=1\times P(X_1=1|T=t)=\frac{P(X_1=1,T=t)}{P(T=t)}\\
=\frac{P(X_1=1,X_2+\dots+X_n={t-1})}{P(T=t)}=\frac{p\cdot{n-1\choose{t-1}}p^{t-1}(1-p)^{n-t}}{{n\choose{t}}p^{t}(1-p)^{n-t}}\\
=\frac{t}{n}=\frac{\sum_{i=1}^n X_i}{n}=\bar{X}
\]&lt;/span> 显然，样本均值&lt;span class="math">\(h(T)=\bar{X}\)&lt;/span>也是&lt;span class="math">\(p\)&lt;/span>的无偏估计，且方差为&lt;span class="math">\(p(1-p)/n\)&lt;/span>，而&lt;span class="math">\(X_1\)&lt;/span>的方差为&lt;span class="math">\(p(1-p)\)&lt;/span>，只要当&lt;span class="math">\(n\geq 2\)&lt;/span>时，&lt;span class="math">\(\bar{X}\)&lt;/span>的方差就会小于&lt;span class="math">\(X_1\)&lt;/span>。然而，Rao-Blackwell定理能够通过充分统计量改进无偏估计统计量，却并没有告诉我们改进后的估计是否为UMVUE或者如何改进成UMVUE。为了判断一个统计量是否为UMVUE，我们先介绍一些判别方法。&lt;/p>
&lt;h4 id="零无偏估计法">零无偏估计法&lt;/h4>
&lt;blockquote>
&lt;p>定理3：设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自某总体的一个样本，&lt;span class="math">\(\hat\theta(X)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计，&lt;span class="math">\(D[\hat\theta(X)]&amp;lt;\infty\)&lt;/span>，则&lt;span class="math">\(\hat\theta\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE的&lt;strong>充要条件&lt;/strong>是：&lt;/p>
&lt;p>对任意一个满足&lt;span class="math">\(E[\varphi(X)]=0\)&lt;/span>且&lt;span class="math">\(\mathrm{Var}[\varphi(X)]&amp;lt;\infty\)&lt;/span>的&lt;span class="math">\(\varphi(X)\)&lt;/span>，都有 &lt;span class="math">\[\mathrm{Cov}(\hat\theta,\varphi)=0,\forall\theta\in\varTheta\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>其中，&lt;span class="math">\(\varphi(X)\)&lt;/span>是一个统计量，其期望为0，可称为零的无偏估计，这也是这个方法名字的由来。这个定理的常用解释为：&lt;span class="math">\(\theta\)&lt;/span>的UMVUE必与任何零的无偏估计&lt;strong>线性不相关&lt;/strong>。反之，与任何零的无偏估计&lt;strong>线性不相关&lt;/strong>的估计量，必然是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE。&lt;/p>
&lt;blockquote>
&lt;p>我们先证明这个定理的&lt;strong>充分性&lt;/strong>，即&lt;span class="math">\(\Rightarrow\)&lt;/span> UMVUE&lt;/p>
&lt;p>设有另一个&lt;span class="math">\(\theta\)&lt;/span>的无偏估计&lt;span class="math">\(\tilde{\theta}(X)\)&lt;/span>，那么构造函数&lt;span class="math">\(\varphi(X)=\hat\theta(X)-\tilde{\theta}(X)\)&lt;/span>，其期望为 &lt;span class="math">\[E[\varphi(X)]=E[\hat\theta(X)-\tilde{\theta}(X)]=E[\hat\theta(X)]-E[\tilde{\theta}(X)]=\theta-\theta=0\]&lt;/span> 方差为： &lt;span class="math">\[\mathrm{Var}[\tilde\theta(X)]=E[\tilde\theta(X)-\theta]^2=E[(\tilde{\theta}(X)-\hat\theta(X))+(\hat\theta(X)-\theta)]^2\\
=\underbrace{E[(\tilde{\theta}(X)-\hat\theta(X))^2]}_{E(\varphi^2(X))}+\underbrace{E[(\hat\theta(X)-\theta)^2]}_{\mathrm{Var}(\hat\theta)}+2\underbrace{\mathrm{Cov}(\varphi(X),\hat\theta(X))}_{定理充分性体现：=0}\\
=\underbrace{E(\varphi^2(X))}_{\geq 0}+\mathrm{Var}(\hat\theta)\geq \mathrm{Var}(\hat\theta)\]&lt;/span> 这表明，&lt;span class="math">\(\hat\theta\)&lt;/span>在&lt;span class="math">\(\theta\)&lt;/span>的无偏估计中方差最小，即为UMVUE。&lt;/p>
&lt;p>我们再证明定理的&lt;strong>必要性&lt;/strong>，即UMVUE&lt;span class="math">\(\Rightarrow \mathrm{Cov}(\hat\theta,\varphi)=0,\forall\theta\in\varTheta\)&lt;/span>。我们采用反证法证明必要性。&lt;/p>
&lt;p>设&lt;span class="math">\(\hat\theta(X)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE，存在一个&lt;span class="math">\(\varphi(X)\)&lt;/span>满足&lt;span class="math">\(E[\varphi(X)]=0\)&lt;/span>且&lt;span class="math">\(\mathrm{Var}[\varphi(X)]&amp;lt;\infty\)&lt;/span>。假设，在参数空间&lt;span class="math">\(\varTheta\)&lt;/span>中存在一个&lt;span class="math">\(\theta_0\)&lt;/span>使得&lt;span class="math">\(\mathrm{Cov}(\varphi(X),\hat\theta(X))=a\neq 0\)&lt;/span>。下面就是一系列神仙操作，令： &lt;span class="math">\[
b=-\frac{\mathrm{Cov}(\varphi(X),\hat\theta(X))}{\mathrm{Var}[\varphi(X)])}=-\frac{a}{\mathrm{Var}[\varphi(X)])}\neq 0
\]&lt;/span> 现在，构造一个&lt;span class="math">\(X\)&lt;/span>的估计&lt;span class="math">\(\tilde{\theta}=\hat{\theta}+b\varphi(X)\)&lt;/span>，对其求期望可得&lt;span class="math">\(E[\tilde{\theta}]=E[\hat{\theta}+b\varphi(X)]=E[\hat{\theta}]+bE[\varphi(X)]\)&lt;/span>，由于&lt;span class="math">\(\varphi(X)\)&lt;/span>期望为0，所以&lt;span class="math">\(E[\tilde{\theta}]=\theta\)&lt;/span>，也是一个无偏估计，然而其方差为： &lt;span class="math">\[\begin{aligned}\mathrm{Var}(\tilde{\theta})&amp;amp;=E[\hat{\theta}+b\varphi(X)-\theta]^2\\
&amp;amp;=E(\hat{\theta}-\theta)^2+b^2E[\varphi^2(X)]+2bE[(\hat{\theta}-\theta)\varphi(X)]\\
\because E(\hat{\theta}-\theta)^2&amp;amp;=\mathrm{Var}(\hat{\theta});E[\varphi^2(X)]=\mathrm{Var}[\varphi(X)]\\
\because E[(\hat{\theta}-\theta)\varphi(X)]&amp;amp;=E[(\hat{\theta}-\theta)(\varphi(X)-0)]= \mathrm{Cov}[\varphi(X),\hat\theta(X)]=a\\
原式&amp;amp;=\mathrm{Var}(\hat{\theta})+b^2\mathrm{Var}[\varphi(X)]+2ab\end{aligned}\]&lt;/span> 由于&lt;span class="math">\(b=-\frac{a}{\mathrm{Var}[\varphi(X)])}\)&lt;/span>，因此&lt;span class="math">\(b^2\mathrm{Var}[\varphi(x)]=\frac{a^2}{\mathrm{Var}[\varphi(x)]},\quad 2ab=\frac{-2a^2}{\mathrm{Var}[\varphi(x)]}\)&lt;/span>。因此上式可化简为： &lt;span class="math">\[
原式=\mathrm{Var}(\hat{\theta})-\frac{a^2}{\mathrm{Var}[\varphi(X)]}&amp;lt;\mathrm{Var}(\hat{\theta})
\]&lt;/span> 显然，此时&lt;span class="math">\(\mathrm{Var}(\tilde{\theta})&amp;lt;\mathrm{Var}(\hat{\theta})\)&lt;/span>，即&lt;span class="math">\(\hat\theta(X)\)&lt;/span>并不是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE，与前提矛盾。所以必有&lt;span class="math">\(\mathrm{Cov}(\varphi(X),\hat\theta(X))=0\)&lt;/span>。&lt;/p>
&lt;p>得证。&lt;/p>
&lt;/blockquote>
&lt;p>对于统计量&lt;span class="math">\(\hat\theta(X)\)&lt;/span>，我们必须对任意的零无偏估计统计量&lt;span class="math">\(\varphi(x)\)&lt;/span>都要有&lt;span class="math">\(\mathrm{Cov}(\hat{\theta},\varphi)=0\)&lt;/span>，这在证明时需要很多技巧，而且很多时候很难证明。但如果，我们能够证明这个统计量是个&lt;strong>充分统计量&lt;/strong>，那么就可以有以下推论：&lt;/p>
&lt;blockquote>
&lt;p>推论3-1：设&lt;span class="math">\(T=T(X)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的充分统计量，&lt;span class="math">\(h(T(X))\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计，且方差&lt;span class="math">\(D[h(T(X))]&amp;lt;\infty\)&lt;/span>。对任何充分统计量&lt;span class="math">\(T\)&lt;/span>的函数&lt;span class="math">\(\delta(T)\)&lt;/span>，如果&lt;span class="math">\(E[\delta(T)]=0\)&lt;/span>，必有 &lt;span class="math">\[\mathrm{Cov}[h(T),\delta(T)]=E[h(T)\times\delta(T)]=0\]&lt;/span> 那么，&lt;span class="math">\(h(T(X))\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的UMVUE。&lt;/p>
&lt;/blockquote>
&lt;p>此推论在充分统计量的条件下，将所有的零无偏估计缩小到零无偏的充分统计函数的函数。&lt;/p>
&lt;h4 id="充分完全统计量法">充分完全统计量法&lt;/h4>
&lt;p>充分完全统计量法的核心是LS定理，它不仅给出了UMVUE的充分条件，还给出了唯一性和如何构造UMVUE的线索。&lt;/p>
&lt;blockquote>
&lt;p>定理4（Lehmann-Scheff定理，简称LS定理）：设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自总体&lt;span class="math">\(\{f(x,\theta),\theta\in\varTheta\}\)&lt;/span>的一个样本，&lt;span class="math">\(\varTheta\)&lt;/span>为参数空间，&lt;span class="math">\(T(X)\)&lt;/span>是参数&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>充分完全统计量&lt;/strong>，若&lt;span class="math">\(\hat{g}(T(X))\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的一个无偏估计，则&lt;span class="math">\(\hat{g}(T(X))\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>唯一的UMVUE&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>注意，此处唯一是依概率1的唯一，即设&lt;span class="math">\(\hat{g},\hat{g}_1\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的两个估计量，若&lt;span class="math">\(P(\hat{g}=\hat{g}_1)=1\)&lt;/span>，对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，则视&lt;span class="math">\(\hat{g},\hat{g}_1\)&lt;/span>为同一个估计量。&lt;/p>
&lt;p>定理的证明：&lt;/p>
&lt;blockquote>
&lt;p>先证利用统计量的完全性证明唯一性。&lt;/p>
&lt;p>设&lt;span class="math">\(\hat{g}_1(T(X))\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的任一无偏估计，令&lt;span class="math">\(\delta(T(X))=\hat{g}(T(X))-\hat{g}_1(T(X))\)&lt;/span>，则&lt;span class="math">\(E[\delta(T(X))]=E[\hat{g}(T(X))]-E[\hat{g}_1(T(X))]=0, \theta\in\varTheta\)&lt;/span>。由于&lt;span class="math">\(T(X)\)&lt;/span>是完全统计量，而&lt;span class="math">\(\delta(T(X))\)&lt;/span>是其函数，根据信息处理过程中的信息量不增定理，&lt;span class="math">\(\delta(T(X))\)&lt;/span>也是完全统计量。&lt;/p>
&lt;p>根据完全统计量的定义，&lt;span class="math">\(E[\delta(T(X))]=0\)&lt;/span>，几乎处处成立（a.s.）时，&lt;span class="math">\(\delta(T(X))=0\)&lt;/span>，即&lt;span class="math">\(\hat{g}(T(X))=\hat{g}_1(T(X))\)&lt;/span>几乎处处成立（a.s.）。又由于&lt;span class="math">\(\hat{g}_1(T(X))\)&lt;/span>的任意性，唯一性得证。&lt;/p>
&lt;p>在唯一性得证的基础上，我们使用充分性证明方差最小。设&lt;span class="math">\(\varphi(X)\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>任一&lt;/strong>无偏估计，令&lt;span class="math">\(h(T(X))=E[\varphi(X)|T]\)&lt;/span>，由于&lt;span class="math">\(T(X)\)&lt;/span>时充分统计量，因此&lt;span class="math">\(p(\varphi(X)|T)\)&lt;/span>的概率分布与&lt;span class="math">\(\theta\)&lt;/span>无关，那么其期望&lt;span class="math">\(h(T(X))=E[\varphi(X)|T]\)&lt;/span>也与待估计参数无关，因此&lt;span class="math">\(h(T(X))\)&lt;/span>也是一个统计量。而由定理2，Rao–Blackwell定理可知： &lt;span class="math">\[
E[h(T(X))]=E[E[\varphi(X)|T]]=E[\varphi(X)]=\theta,\forall \theta\in\varTheta\\
D[h(T(X))]\leq D[\varphi(X)],\forall \theta\in\varTheta
\]&lt;/span> 又因为由充分完全统计量构造出来的无偏估计是唯一的（唯一性），因此对任一&lt;span class="math">\(\varphi(X)\)&lt;/span>构造出来的&lt;span class="math">\(h(T(X))=E[\varphi(X)|T]\)&lt;/span>，都等于&lt;span class="math">\(\hat{g}(T(X))\)&lt;/span>。综上可知有： &lt;span class="math">\[\hat{g}(T(X)) \leq D[\varphi(X)],\forall \theta\in\varTheta\]&lt;/span> 最小方差特性得证。&lt;/p>
&lt;/blockquote>
&lt;p>这个方法的&lt;strong>核心是先找到个一个充分完全统计量&lt;/strong>，比如可以通过因子分解定理，指数族概率函数特性，定义等方式获得。当有了充分完全统计量后，可以有两个方法求UMVUE。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>使用Rao–Blackwell定理。首先找个一个无偏估计&lt;span class="math">\(\varphi(X)\)&lt;/span>，然后通过条件期望&lt;span class="math">\(h(T)=E[\varphi(x)|T]\)&lt;/span>，得到充分统计量&lt;span class="math">\(T\)&lt;/span>的函数，且根据Rao–Blackwell定理，&lt;span class="math">\(h(T)\)&lt;/span>是无偏的。&lt;/li>
&lt;li>直接使用充分统计量，构造&lt;span class="math">\(\theta\)&lt;/span>无偏估计的函数。适用于充分统计量与无偏估计关系比较简单的情形。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>推论4-1：设&lt;span class="math">\(X=(x_1,x_2,\dotsb,x_n)\)&lt;/span>是来自指数族总体 &lt;span class="math">\[f(\bm{x},\bm{\theta})=c(\bm\theta)\exp\left\{\sum_{j=1}^k \theta_j T_j(x)\right\}h(x),\bm\theta=(\theta_1,\dotsb,\theta_k)\in\varTheta^\star\]&lt;/span> 令&lt;span class="math">\(T(x)=(T_1(x),\dotsb,T_k(x))\)&lt;/span>，且自然参数空间&lt;span class="math">\(\varTheta^\star\)&lt;/span>作为&lt;span class="math">\(R_k\)&lt;/span>的自己有内点，且&lt;span class="math">\(g(T(X))\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的无偏估计，则&lt;span class="math">\(g(T(X))\)&lt;/span>为&lt;span class="math">\(\theta\)&lt;/span>的唯一的UMVUE。&lt;/p>
&lt;/blockquote>
&lt;p>其实，根据指数族分布的性质可知，&lt;span class="math">\(T(X)\)&lt;/span>实际上就是充分完全统计量，因此只有找到一个函数&lt;span class="math">\(g\)&lt;/span>使&lt;span class="math">\(g(T(X))\)&lt;/span>无偏，那么根据定理4，它就是参数&lt;span class="math">\(\theta\)&lt;/span>的唯一的UMVUE。&lt;/p>
&lt;h3 id="cramer-raoc-r不等式与界">Cramer-Rao（C-R）不等式与界&lt;/h3>
&lt;p>Cramer-Rao不等式是另一个判别无偏估计是否为UMVUE的方法，但是Cramer-Rao不等式有更深层的含义。&lt;/p>
&lt;p>我们知道估计量始终会是一个随机变量，有自己的概率分布，而不是一个准确的值。Cramer-Rao除了给出了Cramer-Rao正则分布族这种费雪信息的存在条件，还有另一个更重要的贡献：&lt;strong>C-R不等式&lt;/strong>，可以说给了统计学理论上的绝望。&lt;/p>
&lt;p>C-R不等式，其实就是在说：统计，对真实的概率分布参数估计能力是有限的。举个不太恰当的类比，有点像量子理论中的测不准原理 （二者证明有相似之处哦）。C-R不等式告诉我们，无论我们如何抽样充足，无论我们统计方法如何科学，我们对参数的估计值，永远不可能无限逼近是逻辑上的真实值！&lt;/p>
&lt;p>回到C-R不等式和UMVUE的关系上来，其思想如下：设&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一切无偏估计构成的集合，所有的这些&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>中的无偏估计的方差必有一个下界（一定非负），这个下界称为C-R下界。如果&lt;span class="math">\(\mathcal{U}_g\)&lt;/span>中某一个估计量&lt;span class="math">\(\hat g\)&lt;/span>的方差达到了这个下界，则&lt;span class="math">\(\hat{g}\)&lt;/span>就一定是参数的UMVUE，当然会对样本分布族和&lt;span class="math">\(\hat{g}\)&lt;/span>有一些正则条件。当时，使用这种下界的方法，都一个缺点，即&lt;strong>C-R不等式给出的下界经常比实际的下界更小一些&lt;/strong>。这一情况下，C-R不等式就无法判断UMVUE的存在性。此外，C-R不等式还有其他一些用处，比如计算估计的效率、有效估计等等。&lt;/p>
&lt;p>具体C-R不等式的细节，见笔记&lt;a href="概率统计随机过程之C-R不等式.md">《概率统计随机过程之C-R不等式》&lt;/a>&lt;/p>
&lt;h2 id="区间估计">区间估计&lt;/h2>
&lt;p>估计一个&lt;strong>数值范围&lt;/strong>，核心要求：&lt;strong>希望以尽可能大的概率落在尽可能小的区间内&lt;/strong>。&lt;/p>
&lt;h3 id="置信区间与枢轴变量">置信区间与枢轴变量&lt;/h3>
&lt;ul>
&lt;li>区间长度：越长概率越大，越不精确&lt;/li>
&lt;li>以多大概率落在区间中&lt;/li>
&lt;li>希望以尽可能大的概率落在尽可能小的区间内&lt;/li>
&lt;/ul>
&lt;p>需要指出的是，置信区间是指其能包括待估计参数的区间，而不是待估计参数落入区间的意思。待估计值是客观存在的。&lt;/p>
&lt;blockquote>
&lt;p>枢轴变量：枢轴变量一般满足特定的分布，枢轴变量和待估计参数之间存在确定的函数关系，因此通过枢轴变量可以求出置信区间。&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../images/枢轴变量.jpg" alt="枢轴变量.jpg" />&lt;p class="caption">枢轴变量.jpg&lt;/p>
&lt;/div>
&lt;h3 id="一个正态总体的期望和方差的区间估计">一个正态总体的期望和方差的区间估计&lt;/h3>
&lt;p>当我们从一个含有未知参数的正态分布&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>抽样时，可以通过抽样的样本对参数&lt;span class="math">\(\mu,\sigma^2\)&lt;/span>进行区间估计，分为以下四种情况。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/一个正态总体的期望和方差的区间估计.jpg" alt="一个正态总体的期望和方差的区间估计" />&lt;p class="caption">一个正态总体的期望和方差的区间估计&lt;/p>
&lt;/div></description></item><item><title>概率统计随机过程之最大似然估计拓展</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E6%8B%93%E5%B1%95/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E6%8B%93%E5%B1%95/</guid><description>
&lt;h2 id="概率统计随机过程之最大似然估计拓展">概率统计随机过程之最大似然估计拓展&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#最大似然估计概述">最大似然估计概述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计的不变原理">最大似然估计的不变原理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计服从渐进正态的条件">最大似然估计服从渐进正态的条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#渐近正态性">渐近正态性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计具有正态性定理">最大似然估计具有正态性定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#cramer-rao正则分布族下费雪信息量的另一种表示">Cramer-Rao正则分布族下费雪信息量的另一种表示&lt;/a>&lt;/li>
&lt;li>&lt;a href="#cramer-rao不等式与界">Cramer-Rao不等式与界&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计与相对熵kl散度交叉熵的等价性">最大似然估计与相对熵（KL散度）、交叉熵的等价性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大似然估计不具备的性质">最大似然估计不具备的性质&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>最大似然估计是参数点估计中非常常用且有效的估计方法，本文对其相关性质进行拓展介绍，包括渐进正态性，一致最小方差无偏估计，C-R界以及存在隐变量时采用的EM算法。&lt;/p>
&lt;p>本文阅读需建立在&lt;strong>已经理解最大似然估计基础上&lt;/strong>，首先我们精简的概述最大似然估计。&lt;/p>
&lt;h2 id="最大似然估计概述">最大似然估计概述&lt;/h2>
&lt;p>核心思想：概率大的事件比概率小的事件更容易发生。&lt;strong>要估计的参数能够使产生这个样本的概率最大&lt;/strong>。&lt;/p>
&lt;p>步骤：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>总体的概率/密度函数。&lt;/li>
&lt;li>写出似然估计函数&lt;span class="math">\(L(\theta)\)&lt;/span>，其中&lt;span class="math">\(\theta\)&lt;/span>是待估计参数。&lt;/li>
&lt;li>两边取&lt;span class="math">\(\ln\)&lt;/span>，即为&lt;span class="math">\(l(\theta)=\ln(L(\theta))\)&lt;/span>。(注意，对数似然函数是小写&lt;span class="math">\(l\)&lt;/span>)&lt;/li>
&lt;li>对&lt;span class="math">\(l(\theta)\)&lt;/span>的&lt;span class="math">\(\theta\)&lt;/span>求导（多参数估计就是偏导），令导数为0。&lt;/li>
&lt;li>求出使导数为0的&lt;span class="math">\(\theta\)&lt;/span>即为最大似然估计参数&lt;span class="math">\(\hat\theta_{MLE}\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;p>TIPS：对于有些分布的极大似然估计没法直接求，比如均匀分布。&lt;/p>
&lt;p>&lt;strong>似然函数取对数的原因&lt;/strong>:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>减少计算量。乘法变成加法，从而减少了计算量；同时，如果概率中含有指数项，如高斯分布，能把指数项也化为求和形式，进一步减少计算量；另外，在对联合概率求导时，和的形式会比积的形式更方便。&lt;/li>
&lt;li>计算时更准确。为概率值都在[0,1]之间，因此，概率的连乘将会变成一个很小的值，可能会引起浮点数下溢，尤其是当数据集很大的时候，联合概率会趋向于0，非常不利于之后的计算。&lt;/li>
&lt;li>取对数后，可以是一个上凸函数，更有利于求取最大值。&lt;/li>
&lt;/ol>
&lt;p>需要指出的是：&lt;strong>取对数不影响单调性&lt;/strong>。 &lt;span class="math">\[
p(x|\theta_1)&amp;gt;p(x|\theta_2)\Leftrightarrow \ln(p(x|\theta_1))&amp;gt;\ln(p(x|\theta_2))
\]&lt;/span> 因为相同的单调性，它确保了概率的最大对数值出现在与原始概率函数相同的点上。因此，可以用更简单的对数似然来代替原来的似然。&lt;/p>
&lt;h3 id="最大似然估计的不变原理">最大似然估计的不变原理&lt;/h3>
&lt;p>我们介绍一个致使最大似然估计得到广泛应用的定理：&lt;/p>
&lt;blockquote>
&lt;p>不变定理：设&lt;span class="math">\(X\sim p(x;\theta),\;\theta\in\mathcal{\Theta}\)&lt;/span>，若&lt;span class="math">\(\theta\)&lt;/span>的最大似然估计为&lt;span class="math">\(\hat\theta\)&lt;/span>，则对任意函数&lt;span class="math">\(\gamma=g(\theta)\)&lt;/span>，&lt;span class="math">\(\gamma\)&lt;/span>的最大似然估计为&lt;span class="math">\(\hat\gamma=g(\hat\theta)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h2 id="最大似然估计服从渐进正态的条件">最大似然估计服从渐进正态的条件&lt;/h2>
&lt;p>最大似然估计除了不变原理，还有另一个非常好的性质就是在一定条件下满足渐进正态性，从而保证了样本增加时的参数估计的收敛速度（通常为&lt;span class="math">\(\frac{1}{\sqrt{n}}\)&lt;/span>）。&lt;/p>
&lt;h3 id="渐近正态性">渐近正态性&lt;/h3>
&lt;p>渐进正态性（大样本性质）：估计量的渐进正态性来源于&lt;strong>中心极限定理&lt;/strong>，若统计量在样本容量&lt;span class="math">\(n\rightarrow \infty\)&lt;/span>时，统计量的分布也渐近于正态分布，称为渐进正态性。具体可定义为：如果存在一序列&lt;span class="math">\(\{\sigma_n^2\}\)&lt;/span>，满足&lt;span class="math">\((\hat\theta_n-\theta)/\sigma_n(\theta)\overset{L}{\rightarrow}N(0,1)\)&lt;/span>，则称&lt;span class="math">\(\hat\theta_n\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的渐进正态估计，&lt;span class="math">\(\sigma_n^2\)&lt;/span>称为&lt;span class="math">\(\hat\theta_n\)&lt;/span>的渐进方差。&lt;/p>
&lt;p>严格定义如下：&lt;/p>
&lt;blockquote>
&lt;p>渐近正态性：设&lt;span class="math">\(\hat\theta_n=\hat\theta(x_1,x_2,\dotsb,x_n)\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的一个相合估计序列，若存在一个趋于零的正数列&lt;span class="math">\(\sigma_n(\theta)\)&lt;/span>，使得规范变量&lt;span class="math">\(y_n=\frac{\hat\theta_n-\theta}{\sigma_n(\theta)}\)&lt;/span>的概率分布函数&lt;span class="math">\(F_n(y)\)&lt;/span>收敛于标准正态分布函数&lt;span class="math">\(\varPhi(y)\)&lt;/span>，即： &lt;span class="math">\[
F_n(y)=P(\frac{\hat\theta_n-\theta}{\sigma_n(\theta)}\leq y)\rightarrow \varPhi(y)(n\rightarrow \infty)
\]&lt;/span> 如果用依分布收敛符号&lt;span class="math">\(L\)&lt;/span>可表示为： &lt;span class="math">\[
\frac{\hat\theta_n-\theta}{\sigma_n(\theta)}\overset{L}{\rightarrow} N(0,1)(n\rightarrow\infty)
\]&lt;/span> 则称&lt;span class="math">\(\hat\theta_n\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的渐进正态估计，或称&lt;span class="math">\(\hat\theta_n\)&lt;/span>具有渐进正态性，即： &lt;span class="math">\[
\hat\theta_n\sim AN(\theta,\sigma^2_n(\theta))
\]&lt;/span> 其中，&lt;span class="math">\(\sigma^2_n(\theta)\)&lt;/span>称为&lt;span class="math">\(\hat\theta_n\)&lt;/span>的渐进方差。&lt;/p>
&lt;/blockquote>
&lt;p>从&lt;span class="math">\((\hat\theta_n-\theta)/\sigma_n(\theta)\)&lt;/span>来看，分子项依概率收敛于&lt;span class="math">\(\theta\)&lt;/span>的速度与分母项&lt;span class="math">\(\sigma_n(\theta)\)&lt;/span>趋近于0的速度相同时，其比值才会稳定与正态分布。因此，&lt;span class="math">\(\hat\theta_n\)&lt;/span>收敛速度与渐近方差直接相关，渐近方差越小，收敛越快。&lt;/p>
&lt;p>还需要指出，渐进方差并不是唯一的，如果存在另一&lt;span class="math">\(\tau_n(\theta)\)&lt;/span>有&lt;span class="math">\(\frac{\sigma_n(\theta)}{\tau_n(\theta)}\rightarrow 1(n\rightarrow\infty)\)&lt;/span>，根据依概率收敛定义可知&lt;span class="math">\(\tau_n(\theta)\)&lt;/span>也是&lt;span class="math">\(\theta\)&lt;/span>的渐近方差。&lt;/p>
&lt;p>渐进正态性和相合性的关系类似于中心极限定理和大数定律。相合性是对估计的一种较低要求，它只要求估计序列&lt;span class="math">\(\{\hat\theta_n\}\)&lt;/span>在样本数量&lt;span class="math">\(n\)&lt;/span>增加的时候也趋近于&lt;span class="math">\(\theta\)&lt;/span>，但是并没有指出趋近的速度（例如是&lt;span class="math">\(1/n,1/\sqrt{n}\)&lt;/span>或&lt;span class="math">\(1/\ln n\)&lt;/span>）。而渐进正态性补充了这一点，收敛速度与渐进方差相关。&lt;strong>经验来看，大多数渐进正态估计都是以&lt;span class="math">\(1/\sqrt{n}\)&lt;/span>的速度收敛于被估参数的&lt;/strong>。&lt;/p>
&lt;h3 id="最大似然估计具有正态性定理">最大似然估计具有正态性定理&lt;/h3>
&lt;p>在一定条件下，最大似然估计具有渐进正态性。我们将通过如下定理阐释。需要指出的是，定理是以连续分布的形式给出，但是对于离散场景也是适用的。&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(p(x;\theta)\)&lt;/span>是某密度函数，其参数空间&lt;span class="math">\(\varTheta=\{\theta\}\)&lt;/span>是直线上的非退化区间（即不是一个点），假如：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，&lt;span class="math">\(p(x;\theta)\)&lt;/span>对&lt;span class="math">\(\theta\)&lt;/span>如下偏导都存在：&lt;span class="math">\(\frac{\partial\ln p}{\partial\theta},\frac{\partial^2\ln p}{\partial\theta^2},\frac{\partial^3\ln p}{\partial\theta^3}\)&lt;/span>&lt;/li>
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，有&lt;span class="math">\(|\frac{\partial\ln p}{\partial\theta}|&amp;lt;F_1(x),|\frac{\partial^2\ln p}{\partial\theta^2}|&amp;lt;F_2(x),\frac{\partial^3\ln p}{\partial\theta^3}&amp;lt;H(x)\)&lt;/span>成立，其中&lt;span class="math">\(F_1(x)\)&lt;/span>与&lt;span class="math">\(F_2(x)\)&lt;/span>在实数轴上可积，而&lt;span class="math">\(H(x)\)&lt;/span>满足：&lt;span class="math">\(\int_{-\infty}^\infty H(x)p(x;\theta)&amp;lt;M\)&lt;/span>，这里&lt;span class="math">\(M\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关。&lt;/li>
&lt;li>对一切&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，有&lt;span class="math">\(0&amp;lt;I(\theta)=E[(\frac{\partial\ln p}{\partial \theta})^2]&amp;lt;+\infty\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>则在参数真值&lt;span class="math">\(\theta\)&lt;/span>为参数空间&lt;span class="math">\(\varTheta\)&lt;/span>内点的情况下，其似然方程有一个解存在，且此解&lt;span class="math">\(\hat\theta_n=\theta(x_1,x_2,\dotsb,x_n)\)&lt;/span>依概率收敛于&lt;span class="math">\(\theta\)&lt;/span>，且： &lt;span class="math">\[
\hat\theta_n\sim AN(\theta,[nI(\theta)]^{-1})
\]&lt;/span> 其中，&lt;span class="math">\(I(\theta)\)&lt;/span>为分布&lt;span class="math">\(p(x;\theta)\)&lt;/span>中含有&lt;span class="math">\(\theta\)&lt;/span>的&lt;strong>费雪信息量&lt;/strong>，简称信息量。&lt;/p>
&lt;/blockquote>
&lt;p>这个定理的意义在于给定了最大似然分布有渐进正态性的条件，其中渐进方差（体现大样本效率）完全由&lt;strong>样本数量&lt;span class="math">\(n\)&lt;/span>和分布的费雪信息量&lt;span class="math">\(I(\theta)\)&lt;/span>决定&lt;/strong>，且费雪信息量越大（分布中含有&lt;span class="math">\(\theta\)&lt;/span>）的信息越多，渐进方差在同等样本数量下越小，从而最大似然估计效果越好。&lt;/p>
&lt;p>我们在这个定理中引入了费雪信息量这一概念，因此还要&lt;strong>注意费雪信息量是否存在&lt;/strong>。对此，我们给出一个简单的概要：&lt;strong>Cramer-Rao正则分布族中的分布费雪信息都是存在的&lt;/strong>。Cramer-Rao正则分布族定义如下：&lt;/p>
&lt;blockquote>
&lt;p>Cramer-Rao正则分布族：分布&lt;span class="math">\(p(x;\theta)\)&lt;/span>，&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>属于Cramer-Rao正则分布族，则需要满足以下五个条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>参数空间&lt;span class="math">\(\varTheta\)&lt;/span>是直线上的开区间；&lt;/li>
&lt;li>&lt;span class="math">\(\frac{\partial\ln p}{\partial\theta}\)&lt;/span>对所有&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>都存在；&lt;/li>
&lt;li>分布的支撑&lt;span class="math">\(\{x:p(x;\theta)&amp;gt;0\}\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关；&lt;/li>
&lt;li>&lt;span class="math">\(p(x;\theta)\)&lt;/span>的微分与积分运算可交换；&lt;/li>
&lt;li>对所有&lt;span class="math">\(\theta\in\varTheta\)&lt;/span>，期望&lt;span class="math">\(0&amp;lt;E[(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2]&amp;lt;+\infty\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>常用的分布大多数都属于Cramer-Rao正则分布族，但是均匀分布&lt;span class="math">\(U(0,\theta)\)&lt;/span>不是，因为其分布的支撑与&lt;span class="math">\(\theta\)&lt;/span>相关。&lt;/p>
&lt;h3 id="cramer-rao正则分布族下费雪信息量的另一种表示">Cramer-Rao正则分布族下费雪信息量的另一种表示&lt;/h3>
&lt;p>若&lt;span class="math">\(p(x;\theta)\)&lt;/span>为Cramer-Rao正则分布族，若其二节偏导数&lt;span class="math">\(\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}\)&lt;/span>对一切&lt;span class="math">\(\theta\)&lt;/span>存在，其费雪信息量还可以写为： &lt;span class="math">\[
I(\theta)=-E[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}]
\]&lt;/span> 这种方式可以帮助我们简便计算费雪信息量。此外，这是理解费雪信息量的另一个角度。我们知道二阶导数衡量一个函数的“凸程度”，绝对值越大，越陡峭，说明真值集中性越强。意味着，费雪信息量越大，我们更可能在凸峰出取到真实值附近的值。下面我们证明这两种表示是相等的。&lt;/p>
&lt;p>&lt;strong>证明&lt;/strong>：由于&lt;span class="math">\(p(x;\theta)\)&lt;/span>为Cramer-Rao正则分布族，且其二阶偏导数总是存在，对于定积分 &lt;span class="math">\[
1=\int_{-\infty}^{+\infty}p(x;\theta)\mathrm{d}x
\]&lt;/span> 两侧对&lt;span class="math">\(\theta\)&lt;/span>求偏导，由于定积分是个常数，因此导数必为0： &lt;span class="math">\[
0=\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}p(x;\theta)\mathrm{d}x
\]&lt;/span> 根据Cramer-Rao正则分布族的第4条定义，我们交换上式中积分与微分的顺序： &lt;span class="math">\[
0=\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}p(x;\theta)\mathrm{d}x=\int_{-\infty}^{+\infty}\frac{\partial p(x;\theta)}{\partial\theta}\mathrm{d}x
\]&lt;/span> 下面用到了一个计算技巧：&lt;span class="math">\(\frac{\partial p(x;\theta)}{\partial\theta}=\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\)&lt;/span>，代入其中有： &lt;span class="math">\[
0=\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x=E[\frac{\partial\ln p(x;\theta)}{\partial\theta}]
\]&lt;/span> 我们得到一个副结论：Log似然函数的一阶导期望为0。但我们还没有得到的结论，结论中需要二阶导，所以我们再对&lt;span class="math">\(\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x\)&lt;/span>求&lt;span class="math">\(\theta\)&lt;/span>的偏导有： &lt;span class="math">\[
\begin{aligned}
0&amp;amp;=\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\mathrm{d}x(交互积分微分顺序)\\
&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)+\frac{\partial\ln p(x;\theta)}{\partial\theta}\frac{\partial p(x;\theta)}{\partial\theta}]\mathrm{d}x\\
&amp;amp;\because \frac{\partial p(x;\theta)}{\partial\theta}=\frac{\partial\ln p(x;\theta)}{\partial\theta}p(x;\theta)\\
0&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)+(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2p(x;\theta)]\mathrm{d}x\\
0&amp;amp;=\int_{-\infty}^{+\infty}[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x+\underbrace{\int_{-\infty}^{+\infty}(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2p(x;\theta)]\mathrm{d}x}_{I(\theta)=E[(\frac{\partial\ln p(x;\theta)}{\partial\theta})^2]}\\
\end{aligned}
\]&lt;/span> 从而有： &lt;span class="math">\[
\begin{aligned}
I(\theta)&amp;amp;=0-\int_{-\infty}^{+\infty}\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}p(x;\theta)\mathrm{d}x\\
&amp;amp;=-E[\frac{\partial^2\ln p(x;\theta)}{\partial\theta^2}]
\end{aligned}
\]&lt;/span> 得证。&lt;/p>
&lt;p>关于副结论：Log似然函数的一阶导期望为0。我们有时候也把&lt;span class="math">\(p\)&lt;/span>的Log似然函数称为分数函数，即分数函数的期望为0，而费雪信息量正好是分数函数的二阶原点矩，同时由于其期望为0，二阶原点矩等于二阶中心矩，即&lt;strong>费雪信息量也正好是分数函数的方差&lt;/strong>。&lt;/p>
&lt;h2 id="cramer-rao不等式与界">Cramer-Rao不等式与界&lt;/h2>
&lt;p>从最大似然估计的渐进正态性，我们知道估计量始终会是一个随机变量，有自己的概率分布，而不是一个准确的值。Cramer-Rao除了给出了Cramer-Rao正则分布族这种费雪信息的存在条件，还有另一个更重要的贡献：&lt;strong>C-R不等式&lt;/strong>，可以说给了统计学理论上的绝望。&lt;/p>
&lt;p>C-R不等式，其实就是在说：统计，对真实的概率分布参数估计能力是有限的。举个不太恰当的类比，有点像量子理论中的测不准原理 （二者证明有相似之处哦）。C-R不等式告诉我们，无论我们如何抽样充足，无论我们统计方法如何科学，我们对参数的估计值，永远不可能无限逼近是逻辑上的真实值！它有自己估计的上限！&lt;/p>
&lt;h2 id="最大似然估计与相对熵kl散度交叉熵的等价性">最大似然估计与相对熵（KL散度）、交叉熵的等价性&lt;/h2>
&lt;p>在机器学习、广义线性模型的应用场景中，常常使用最小（大）化交叉熵或相对熵，又叫KL散度来替换一些最大似然估计的操作，从理论上来看它们是具有等价性的。关于相对熵的概念，可以先看笔记《&lt;a href="无线通信之信息熵回忆总结.md">无线通信之信息熵回忆总结&lt;/a>》。最大似然估计的目标是最大化似然函数，我们将其写为： &lt;span class="math">\[
\argmax_{\theta} \ln(L(\theta))=\argmax_{\theta} \ln(\prod_{i=1}^n p(x_i;\theta))
\]&lt;/span> 显然，似然函数&lt;span class="math">\(L(\theta)\)&lt;/span>是由抽样出来的已知样本&lt;span class="math">\(x_i\)&lt;/span>的概率密度函数（pdf）连乘得来的。当参数&lt;span class="math">\(\theta\)&lt;/span>已知时，&lt;span class="math">\(\prod_{i=1}^n p(x_i;\theta)\)&lt;/span>就是&lt;span class="math">\(x=(x_1,x_2,\dotsb,x_n)\)&lt;/span>的联合概率密度值；如果&lt;span class="math">\(\theta\)&lt;/span>未知，那么就会得到一个关于&lt;span class="math">\(\theta\)&lt;/span>的函数&lt;span class="math">\(L(\theta)\)&lt;/span>，而找到让函数&lt;span class="math">\(L(\theta)\)&lt;/span>最大的&lt;span class="math">\(\theta^\star\)&lt;/span>就是最大似然估计的核心。&lt;/p>
&lt;p>如果将&lt;span class="math">\(\ln\)&lt;/span>符号和&lt;span class="math">\(\prod\)&lt;/span>符号交换，上式可改写成： &lt;span class="math">\[
\argmax_{\theta} \ln(\prod_{i=1}^n p(x_i;\theta))=\argmax_{\theta}\sum_{i=1}^n \ln(p(x_i;\theta))
\]&lt;/span> 简单地，我 们可以对似然函数加个负号，然后取最大值就变成了取最小值： &lt;span class="math">\[
\argmax_{\theta}\sum_{i=1}^n \ln(p(x_i;\theta))=\argmin_{\theta}\sum_{i=1}^n -\ln(p(x_i;\theta))
\]&lt;/span> 由于&lt;span class="math">\(n\)&lt;/span>是一个固定的数，因此在上式前面乘以一个常数&lt;span class="math">\(\frac{1}{n}\)&lt;/span>不改变&lt;span class="math">\(\argmin\limits_{\theta}\)&lt;/span>的结果，即&lt;span class="math">\(\argmin\limits_{\theta} \frac{1}{n}\sum\limits_{i=1}^n -\ln(p(x_i;\theta))\)&lt;/span>。&lt;/p>
&lt;p>接下来是体现抽样与概率关系的一步。我们知道&lt;span class="math">\(x_i\)&lt;/span>都是从总体抽出来的样本，概率密度大的地方抽样出来的样本多，概率密度小的地方抽出来的样本就少。那么，想想我们如何使用蒙特卡洛法求一个随机变量函数的期望&lt;span class="math">\(E[f(Y)]\)&lt;/span>的？其中&lt;span class="math">\(Y\)&lt;/span>是一个随机变量。理论上来说： &lt;span class="math">\[
E[f(Y)]=\int_{-\infty}^\infty p(y)f(y)\mathrm{d}y
\]&lt;/span> 但是，我们常常不知道&lt;span class="math">\(Y\)&lt;/span>精确的概率密度函数&lt;span class="math">\(p(y)\)&lt;/span>，因此我们会从总体&lt;span class="math">\(Y\)&lt;/span>中抽取一定量的&lt;span class="math">\(y_i\)&lt;/span>，然后用 &lt;span class="math">\[
\tilde{E}[f(Y)]=\frac{1}{n}\sum_{i=1}^n f(y_i)
\]&lt;/span> 来近似计算期望，我们抽样的数量越多，这个值越准确。当抽样的数量趋近于无穷时，&lt;span class="math">\(\tilde{E}[f(y)]{\rightarrow}E[f(y)](P=1)\)&lt;/span>。这个表述是不是看得眼熟，没错这就是强大数定律！由此我们可以得到一个结论：&lt;/p>
&lt;blockquote>
&lt;p>结论1：当样本数足够大时，可以使用&lt;span class="math">\(\frac{1}{n}\sum_{i=1}^n f(y_i)\)&lt;/span>替代期望&lt;span class="math">\(E[f(Y)]\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>回到原来&lt;span class="math">\(\argmin\limits_{\theta} \frac{1}{n}\sum\limits_{i=1}^n -\ln(p(x_i;\theta))\)&lt;/span>的场景，我们令&lt;span class="math">\(f(x)=-\ln(p(x_i;\theta))\)&lt;/span>，并且&lt;span class="math">\(x_i\)&lt;/span>都是从总体的抽样出来的，那么当样本数量较大时，我们同样可以用求期望的方式去近似函数均值，即： &lt;span class="math">\[
\argmin\limits_{\theta} \frac{1}{n}\sum\limits_{i=1}^n -\ln(p(x;\theta))\approx\begin{cases}
\argmin\limits_{\theta} \int\limits_x p(x) [(-\ln(p(x;\theta))]\mathrm{d}x,\quad x\text{为连续随机变量}\\
\argmin\limits_{\theta} \sum\limits_x p(x) [(-\ln(p(x;\theta))],\quad x\text{为离散随机变量}
\end{cases}\\
=\argmin_\theta E[-\ln(p(x;\theta))]=\argmin_\theta H(x;\theta)
\]&lt;/span> 需要注意的是，作为概率的&lt;span class="math">\(p(x)\)&lt;/span>是与&lt;span class="math">\(\theta\)&lt;/span>无关的事实概率密度函数。因为抽样是从真实分布中抽样的，抽样结果只和&lt;span class="math">\(x\)&lt;/span>的真实分布有关，或者说抽样结果已经是包含参数&lt;span class="math">\(\theta\)&lt;/span>信息之后，体现出来的结果。到这里，我们又发现了一个重要的结论：&lt;/p>
&lt;blockquote>
&lt;p>结论2：最大似然估计的效果等同于调节参数让信息熵最小。&lt;/p>
&lt;/blockquote>
&lt;p>这个直观上很好理解，最大似然估计是找出让事件发生概率最大的参数&lt;span class="math">\(\theta\)&lt;/span>，而熵是衡量事件不确定性的度量，越可能发生的事情，其信息含量越小，熵也越小。因此，最大似然估计会使得&lt;span class="math">\(\argmin\limits_\theta H(x;\theta)\)&lt;/span>。&lt;/p>
&lt;p>到这里，我们距离交叉熵与相对熵（KL散度）貌似还有一些距离。因为上式中，我们使用的是一般的熵，而不是交叉熵、相对熵，那么区别在哪里呢？&lt;/p>
&lt;p>答案就是似然函数&lt;span class="math">\(-\ln(p(x;\theta))\)&lt;/span>。我们在上文中都默认使用了真实的概率分布&lt;span class="math">\(p(x;\theta)\)&lt;/span>，然而我们实际上，我们并不知道！并且真实分布是客观存在的，实际上也不存在参数&lt;span class="math">\(\theta\)&lt;/span>。我们只是选取了一种带有参数&lt;span class="math">\(\theta\)&lt;/span>概率分布族&lt;span class="math">\(q(x;\theta)\)&lt;/span>的去近似真实的概率&lt;span class="math">\(p(x)\)&lt;/span>；或者假定数据&lt;span class="math">\(x_i\)&lt;/span>服从某一带有参数&lt;span class="math">\(\theta\)&lt;/span>概率分布族&lt;span class="math">\(q(x;\theta)\)&lt;/span>。然而，实际情况是&lt;span class="math">\(q(x;\theta)\)&lt;/span>与真实分布&lt;span class="math">\(p(x)\)&lt;/span>是有差距的。因此，我们写出实际情况: &lt;span class="math">\[
\argmin\limits_{\theta} \frac{1}{n}\sum\limits_{i=1}^n -\ln(p(x;\theta))\approx\begin{cases}
\argmin\limits_{\theta} \int\limits_x p(x) [(-\ln(q(x;\theta))]\mathrm{d}x,\quad x\text{为连续随机变量}\\
\argmin\limits_{\theta} \sum\limits_x p(x) [(-\ln(q(x;\theta))],\quad x\text{为离散随机变量}
\end{cases}\\
=\argmin_\theta E[-\ln(q(x;\theta))]
\]&lt;/span> 注意只是将&lt;span class="math">\(p(x;\theta)\)&lt;/span>换成了&lt;span class="math">\(q(x;\theta)\)&lt;/span>，抽样的&lt;span class="math">\(x_i\)&lt;/span>反应的是真实的概率分布，因此就是约等于&lt;span class="math">\(p(x)\)&lt;/span>，且根据大数定律，样本数越大，越接近真实分布。根据笔记《&lt;a href="无线通信之信息熵回忆总结.md">无线通信之信息熵回忆总结&lt;/a>》的定义，我们知道 &lt;span class="math">\[
E[-\ln(q(x;\theta))]=\sum_{x}p(x)(-\ln q(x;\theta))=H(p,q)
\]&lt;/span> 其中，&lt;span class="math">\(H(p,q)\)&lt;/span>成为交叉熵，只有我们设定的参数分布族&lt;span class="math">\(q(x;\theta)\)&lt;/span>才与&lt;span class="math">\(\theta\)&lt;/span>相关，真实分布作为客观存在与&lt;span class="math">\(\theta\)&lt;/span>无关。&lt;/p>
&lt;p>根据交叉熵和相对熵的关系 &lt;span class="math">\[
D_{KL}(p||q)=H(p,q)-H(p)
\]&lt;/span> 可知，相对熵和交叉熵相差一个真实熵&lt;span class="math">\(H(p)\)&lt;/span>。因为&lt;span class="math">\(p(x)\)&lt;/span>是客观存在的，因此&lt;span class="math">\(H(p)\)&lt;/span>也是一个客观存在的常数，虽然我们不知道具体是多少，但是它确实只是个常数，与&lt;span class="math">\(\theta\)&lt;/span>无关，因此 &lt;span class="math">\[
\argmin_\theta E[-\ln(q(x;\theta))]=\argmin_\theta H(p,q)=\argmin_\theta D_{KL}(p||q)
\]&lt;/span> 其中，&lt;span class="math">\(q\)&lt;/span>是与&lt;span class="math">\(\theta\)&lt;/span>相关的分布族，&lt;span class="math">\(p\)&lt;/span>与&lt;span class="math">\(\theta\)&lt;/span>无关。&lt;/p>
&lt;p>综上可知，最大似然估计等同于最小化交叉熵或相对熵（KL散度）。&lt;/p>
&lt;h2 id="最大似然估计不具备的性质">最大似然估计不具备的性质&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>最大似然估计不一定是无偏的。&lt;/li>
&lt;li>当待估计参数与自变量&lt;span class="math">\(x\)&lt;/span>相关时，无法用求导的方式求解。我们需要回到最大似然估计的基本定义，既让似然函数概率最大，来考虑问题。&lt;/li>
&lt;/ol></description></item><item><title>概率统计随机过程之蒙特卡洛方法</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</guid><description>
&lt;ul>
&lt;li>&lt;a href="#蒙特卡洛求定积分的基本方法">蒙特卡洛求定积分的基本方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#随机投点法">随机投点法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本平均值法">样本平均值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重要性采样">重要性采样&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分层抽样法">分层抽样法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最大期望算法expectation-maximum-em算法">最大期望算法（Expectation Maximum, EM）算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#马尔科夫链蒙特卡洛mcmc">马尔科夫链蒙特卡洛MCMC&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概率统计随机过程之蒙特卡洛方法">概率统计随机过程之蒙特卡洛方法&lt;!-- omit in toc -->&lt;/h2>
&lt;p>蒙特卡方法洛（Monte Carlo Method）简称MC，又叫随机模拟方法，本质是随机抽样。MC方法的使用框架一般为&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>针对实际问题建立一个便于实现的概率统计模型，使所求解恰好是所建模型的概率分布或其某个数字特征，比如某个事件的概率或期望值。&lt;/li>
&lt;li>对模型中随机变量建立抽样，在计算机上进行随机试验，抽取足够的随机数，并对有关事件进行统计；&lt;/li>
&lt;li>对模拟实验结果加以分析，&lt;strong>给出所求解的估计及精度（方差）的估计&lt;/strong>；&lt;/li>
&lt;li>必要时，还应改进模型以降低估计方差和减少实验复杂度，提供模拟计算效率。&lt;/li>
&lt;/ol>
&lt;h2 id="蒙特卡洛求定积分的基本方法">蒙特卡洛求定积分的基本方法&lt;/h2>
&lt;p>蒙特卡洛方法近年来受到了广泛运用，不少统计问题到最后到可以归结为定积分的计算，如计算概率、各阶矩、贝叶斯机器学习等，因此我们首先介绍定积分的蒙特卡洛计算。考虑定积分的一个通用形式：&lt;/p>
&lt;p>&lt;span class="math">\[
\theta=\int_a^b f(x) \mathrm{d}x\tag{1}
\]&lt;/span>&lt;/p>
&lt;p>我们先考虑两种基础的蒙特卡洛积分方法，然后再给出一些给复杂而有效的办法。&lt;/p>
&lt;h3 id="随机投点法">随机投点法&lt;/h3>
&lt;p>随机投点法底层原理是伯努利大数定理（用频率接近概率），可以简单理解成几何概型的应用。简单起见，假设&lt;span class="math">\(a,b\)&lt;/span>有限，式（1）中的函数满足&lt;span class="math">\(0\leq f(x)\leq M\)&lt;/span>，令&lt;span class="math">\(\Omega=\{(x,y):a\leq x\leq b,0\leq y\leq M\}\)&lt;/span>。那么根据几何概型，在&lt;span class="math">\(\Omega\)&lt;/span>内取均匀分布的点，该点落在&lt;span class="math">\(f(x)\)&lt;/span>下方的概率为&lt;span class="math">\(p=\frac{S(\Omega)}{\theta}\)&lt;/span>，其中&lt;span class="math">\(S(\Omega)\)&lt;/span>是整体的面积，值为&lt;span class="math">\(M(b-a)\)&lt;/span>，&lt;span class="math">\(\theta\)&lt;/span>是&lt;span class="math">\(f(x)\)&lt;/span>函数下方的面积，根据积分的定义可知&lt;span class="math">\(\theta=\int_a^b f(x) \mathrm{d}x\)&lt;/span>。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/随机投点法.png" alt="随机投点法" />&lt;p class="caption">随机投点法&lt;/p>
&lt;/div>
&lt;p>显然，这就是高中学过的几何概型，概率等于面积的比值，由此，我们也可以反推&lt;span class="math">\(f(x)\)&lt;/span>下方的面积，即积分的值为： &lt;span class="math">\[
\theta=\int_a^b f(x) \mathrm{d}x=M(b-a)p\tag{2}
\]&lt;/span> 现在不知道的就是概率&lt;span class="math">\(p\)&lt;/span>。根据大数定律，如果我们往&lt;span class="math">\(\Omega\)&lt;/span>中投很多点&lt;span class="math">\(n\)&lt;/span>，若有&lt;span class="math">\(n_0\)&lt;/span>个点落在&lt;span class="math">\(f(x)\)&lt;/span>下方，那么可以通过频率逼近概率，即&lt;span class="math">\(\hat p=\frac{n_0}{n}\rightarrow p (a.s.\; n\rightarrow\infty)\)&lt;/span>，代入式（2）中有 &lt;span class="math">\[
\hat{\theta}=M(b-a)\frac{n_0}{n}\rightarrow \theta(a.s.\; n\rightarrow\infty) \tag{3}
\]&lt;/span> 上述思想是容易实现的，步骤如下&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>独立地产生&lt;span class="math">\(n\)&lt;/span>对服从&lt;span class="math">\(U(0,1)\)&lt;/span>的独立随机数&lt;span class="math">\((u_i,v_i),i=1,2,\dotsb,n\)&lt;/span>；&lt;/li>
&lt;li>根据随机变量的函数关系有&lt;span class="math">\(x_i=a+u_i(b-a),y_i=Mv_i\)&lt;/span>；&lt;/li>
&lt;li>统计&lt;span class="math">\(y_i\leq f(x_i)\)&lt;/span>的个数&lt;span class="math">\(n_0\)&lt;/span>(该点落在&lt;span class="math">\(f(x)\)&lt;/span>下方)；&lt;/li>
&lt;li>使用式（3）估算积分值。&lt;/li>
&lt;/ol>
&lt;p>随机投点法用了类似于舍选法的做法， 在非随机问题中引入随机性时用了二维均匀分布和二项分布，靠求二项分布概率来估计积分，随机投点法容易理解，但是效率较低。&lt;/p>
&lt;p>那么，&lt;strong>随机投点法精度提高一位数需要多大的代价呢&lt;/strong>？&lt;/p>
&lt;p>我们看每次投点的结果其实都是服从伯努利分布（也叫0-1分布），&lt;span class="math">\(b(1,p)\)&lt;/span>，而&lt;span class="math">\(n\)&lt;/span>次投点结果&lt;span class="math">\(n_0\)&lt;/span>则服从二项分布&lt;span class="math">\(b(n,p)\)&lt;/span>。显然，&lt;span class="math">\(\hat\theta\)&lt;/span>是&lt;span class="math">\(\theta\)&lt;/span>的无偏估计: &lt;span class="math">\[
E(\hat\theta)=E[M(b-a)\frac{n_0}{n}]=\frac{M(b-a)}{n}E[n_0]\\
=\frac{M(b-a)}{n}\times np=M(b-a)p=\theta\tag{4}
\]&lt;/span> 在无偏的情况下，我们可以用方差衡量精度： &lt;span class="math">\[
\begin{aligned}
Var(\hat\theta)&amp;amp;=Var[M(b-a)\frac{n_0}{n}]=\frac{M^2(b-a)^2}{n^2}Var(n_0)\\
&amp;amp;=\frac{M^2(b-a)^2}{n^2}\times np(1-p),其中p=\frac{\theta}{M(b-a)}\\
&amp;amp;=\frac{1}{n}\theta[M(b-a)-\theta]
\end{aligned}\tag{5}
\]&lt;/span> 其中，&lt;span class="math">\(\theta, M, b, a\)&lt;/span>都是定值，只有&lt;span class="math">\(n\)&lt;/span>是我们能够影响的数。也就是说，模拟次数&lt;span class="math">\(n\)&lt;/span>提高到&lt;span class="math">\(n\)&lt;/span>倍，方差缩小到&lt;span class="math">\(1/n\)&lt;/span>，如果看和精度直接相关的标准差形如（&lt;span class="math">\(E[x]\plusmn \mathrm{std}(x)\)&lt;/span>），那么精度每增加一位小数，实验量需要增加100倍。&lt;/p>
&lt;p>&lt;strong>随机模拟积分的精度一般都服从这样的规律&lt;/strong>。&lt;/p>
&lt;h3 id="样本平均值法">样本平均值法&lt;/h3>
&lt;p>随机投点法是一种很直觉，但是效率不高的方法，另一种效率更高的方法是利用&lt;strong>期望值(矩)的估计&lt;/strong>。取随机变量&lt;span class="math">\(U\sim U(a,b)\)&lt;/span>，则将&lt;span class="math">\(U\)&lt;/span>代入&lt;span class="math">\(f(x)\)&lt;/span>，得到一个随机数&lt;span class="math">\(Y=f(U)\)&lt;/span>： &lt;span class="math">\[
E[Y]=E[f(U)]=\int_a^b f(u) \frac{1}{b-a} \mathrm{d}u=\frac{\theta}{b-a}\\
\Rightarrow\theta=(b-a)E[Y]=(b-a)E[f(U)]\tag{6}
\]&lt;/span> 在范围&lt;span class="math">\(b-a\)&lt;/span>已知的情况下，我们需要知道&lt;span class="math">\(f(U)\)&lt;/span>的期望即可。我们知道&lt;strong>一个随机变量的期望即为一阶原点矩（一阶矩）&lt;/strong>，因此我们可以用&lt;strong>矩估计&lt;/strong>的方法来近似计算。一阶矩的估计方式很简单，&lt;strong>就是采样多个点，求函数均值，用样本均值代替一阶矩（总体期望）&lt;/strong>。&lt;/p>
&lt;p>我们取服从均匀分布&lt;span class="math">\(U(a,b)\)&lt;/span>的独立的随机变量&lt;span class="math">\(\{U_i,i=1,2,\dotsb,n\}\)&lt;/span>，则&lt;span class="math">\(Y=f(U)\)&lt;/span>的n个采样值为&lt;span class="math">\(\{Y_i=f(U_i),i=1,2,\dotsb,n\}\)&lt;/span>，根据矩估计方法， &lt;span class="math">\[
\hat Y={1\over n}\sum_{i=1}^nf(U_i) \rightarrow E[Y]=E[f(U)]\ a.s. n\rightarrow \infty\tag{7}
\]&lt;/span> 代入式（6）可得： &lt;span class="math">\[
\hat\theta=(b-a)\hat Y=\frac{b-a}{n}\sum_{i=1}^nf(U_i) \rightarrow (b-a)E[f(U)]=\theta\ a.s. n\rightarrow\infty\tag{8}
\]&lt;/span>&lt;/p>
&lt;p>简单来说，就是从&lt;span class="math">\(f(x)\)&lt;/span>上取足够多的点，用采样点的平均值代替函数值的均值，再乘以定义域长度，得到面积（积分值）。用一个图描述了蒙特卡洛求定积分的思想：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/蒙特卡洛求定积分.png" alt="蒙特卡洛求定积分.png" />&lt;p class="caption">蒙特卡洛求定积分.png&lt;/p>
&lt;/div>
&lt;p>显然，样本平均值法也是无偏估计。我们也可以用方差计算其精度。为了方便比较，我们令随机投点法的积分估计值为&lt;span class="math">\(\hat\theta_1\)&lt;/span>，样本平均值法的估计值为&lt;span class="math">\(\hat\theta_2\)&lt;/span>。那么可以计算&lt;span class="math">\(\mathrm{Var}(\hat\theta_2)\)&lt;/span>，根据式（8），有： &lt;span class="math">\[
\begin{aligned}
\mathrm{Var}(\hat\theta_2)&amp;amp;=\mathrm{Var}[\frac{b-a}{n}\sum_{i=1}^nf(U_i)]=(\frac{b-a}{n})^2\mathrm{Var}[\sum_{i=1}^nf(U_i)]\\
&amp;amp;\because U_i\quad i.i.d\quad \therefore 根据独立随机变量和的方差性质有：\\
&amp;amp;=(\frac{b-a}{n})^2\times n \times \mathrm{Var}[f(U)]\\
&amp;amp;=\frac{(b-a)^2}{n}\times \int_a^b (f(U)-E[f(U)])^2 \frac{1}{b-a} \mathrm{d}U\\
&amp;amp;\because E[f(U)]=\frac{\theta}{b-a}\\
&amp;amp;=\frac{1}{n}[(b-a)^2\int_a^b f^2(U)\frac{1}{b-a}\mathrm{d}U-\theta^2]\\
&amp;amp;=\frac{1}{n}[(b-a)\int_a^b f^2(U)\mathrm{d}U-\theta^2]
\end{aligned}\tag{9}
\]&lt;/span>&lt;/p>
&lt;p>直观来看无法比较随机投点法和样本均值法的差异，我们将&lt;span class="math">\(\hat\theta_1,\hat\theta_2\)&lt;/span>做差，进行比较，在&lt;span class="math">\(0\leq f(x) \leq M\)&lt;/span>时（随机投点法要求），可以证明&lt;span class="math">\(\mathrm{Var}(\hat\theta_1)\geq \mathrm{Var}(\hat\theta_2)\)&lt;/span>，二者相减结果如下：&lt;/p>
&lt;p>&lt;span class="math">\[
\begin{aligned}
\mathrm{Var}(\hat\theta_1)-\mathrm{Var}(\hat\theta_2)&amp;amp;=\frac{1}{n}\theta[M(b-a)]-\\
&amp;amp;=\frac{1}{n}[(b-a)\int_a^bf^2(U)\mathrm{d}U-\theta^2]\\
&amp;amp;=\frac{M(b-a)}{n}[\theta-\int_a^b \frac{f^2(U)}{M}\mathrm{d}U]\\
&amp;amp;\because 0\leq f(x) \leq M \therefore 0&amp;lt;\frac{f(U)}{M}\leq 1\\
&amp;amp;\geq \frac{M(b-a)}{n}[\theta-\int_a^b f(U)\mathrm{d}U]=0\\
\end{aligned}
\]&lt;/span> 由此可见，只要&lt;span class="math">\(\{U:f(U)&amp;lt;M\}\)&lt;/span>不是零测集，就有&lt;span class="math">\(\mathrm{Var}(\hat\theta_1)&amp;gt;\mathrm{Var}(\hat\theta_2)\)&lt;/span>。由此，样本均值法比随机投点法方差更小，更有效。此外，样本均值法不要求&lt;span class="math">\(f(x)\)&lt;/span>有上界&lt;span class="math">\(M\)&lt;/span>，可以推广至一般情形。&lt;/p>
&lt;h2 id="重要性采样">重要性采样&lt;/h2>
&lt;p>我们根据随机投点法和样本均值法已经能够进行蒙特卡洛积分的无偏估计，并且随着采样次数的增加最终能以&lt;span class="math">\(O(n^{-1/2})\)&lt;/span>收敛。我们现在需要的是优化蒙特卡洛积分的计算方法，使&lt;strong>估计的方差尽量小&lt;/strong>。&lt;/p>
&lt;p>我们再回看式（1），对他进行适当变形： &lt;span class="math">\[
\theta=\int_a^b f(x) \mathrm{d}x=\int_a^b \frac{f(x)}{g(x)} g(x) \mathrm{d}x=E[\frac{f(X)}{g(X)}]\\
g(x)\neq 0;x\in[a,b]\tag{10}
\]&lt;/span> 其中，&lt;span class="math">\(g(x)\)&lt;/span>是一个概率密度函数。那么原来的积分问题就变成了求期望，若有来自&lt;span class="math">\(g(x)\)&lt;/span>的样本（随机数）&lt;span class="math">\(\{x_1,x_2,\dotsb,x_n\}\)&lt;/span>（服从&lt;span class="math">\(g(x)\)&lt;/span>的分布），则&lt;span class="math">\(\theta\)&lt;/span>的估计可以用一阶矩估计来实现： &lt;span class="math">\[
\hat\theta=\frac{1}{n}\sum_{i=1}^n \frac{f(x_i)}{g(x_i)}\tag{11}
\]&lt;/span> 显然，如果当&lt;span class="math">\(g(x)=\frac{1}{b-a}\)&lt;/span>时，这种方法就是样本均值法，也可以说&lt;strong>样本均值法是此类方法的一个特例&lt;/strong>。&lt;/p>
&lt;p>估计量&lt;span class="math">\(\hat{\theta}\)&lt;/span>的&lt;strong>构造是很简单的，说白了就是一阶原点矩估计&lt;/strong>。我们对&lt;span class="math">\(\hat\theta\)&lt;/span>求期望易知此估计式无偏的，即&lt;span class="math">\(E[\hat{\theta}]=\theta\)&lt;/span>。而其方差为： &lt;span class="math">\[
\mathrm{Var}(\hat\theta)=\frac{1}{n}[E(\frac{f(x)}{g(x)})^2-E^2(\frac{f(x)}{g(x)})]=\frac{1}{n}[E(\frac{f(x)}{g(x)})^2-\theta^2]\tag{12}
\]&lt;/span> 那么根据式（12），&lt;strong>除了增加采样的次数，我们又多了一个可以用来降低方差的工具&lt;/strong>&lt;span class="math">\(g(x)\)&lt;/span>，我们可以通过合理设计&lt;span class="math">\(g(x)\)&lt;/span>使蒙特卡洛积分的效率更高。理论上来看，当&lt;span class="math">\(g(x)=\frac{f(x)}{\int_a^b f(x) \mathrm{d}x}\)&lt;/span>时，有&lt;span class="math">\(\mathrm{Var}(\hat\theta)=0\)&lt;/span>，此时结果最优，但是这个&lt;span class="math">\(g(x)\)&lt;/span>中必须已知式（1）&lt;span class="math">\(\int_a^b f(x) \mathrm{d}x\)&lt;/span>的结果，显然是无法直接使用的。不过，他也给我们一个启示，即&lt;span class="math">\(g(x)\)&lt;/span>与&lt;span class="math">\(f(x)\)&lt;/span>应该在形状相近的情况下，估计的方差更小。下面我们举一个例子： &lt;span class="math">\[
f(x)=-x^2+2x\quad x\in[0,2]\\
g_1(x)=\frac{1}{2};g_2(x)=e^{-x};g_3(x)=\frac{1}{\sqrt{2\pi}\times 0.5}e^{-\frac{(x-1)^2}{2\times 0.5^2}}
\]&lt;/span> 分别用3个&lt;span class="math">\(g(x)\)&lt;/span>函数，作为重要性采用的概率密度函数。首先，我们易算出&lt;span class="math">\(f(x)=-x^2+2x\quad x\in[0,2]\)&lt;/span>的积分结果为&lt;span class="math">\(\frac{4}{3}\)&lt;/span>，我们画出各个函数的图像以及&lt;span class="math">\(\frac{f(x)}{g(x)}\)&lt;/span>的图像。&lt;/p>
&lt;p>&lt;img src="../images/importance_sampling1.png" alt="importance sampling" /> &lt;img src="../images/importance_sampling2.png" alt="importance sampling" />&lt;/p>
&lt;p>我们使用matlab分别计算三个重要性采样的方差有（取&lt;span class="math">\(n=10\)&lt;/span>）： &lt;span class="math">\[
\mathrm{Var}(\frac{1}{10}\sum_{i=1}^{10} \frac{f(x)}{g_1(x)})=0.03556\\
\mathrm{Var}(\frac{1}{10}\sum_{i=1}^{10} \frac{f(x)}{g_2(x)})=0.1335\\
\mathrm{Var}(\frac{1}{10}\sum_{i=1}^{10} \frac{f(x)}{g_3(x)})=0.01205
\]&lt;/span> matlab计算结果显示&lt;span class="math">\(g_3(x)\)&lt;/span>能够缩减方差，而与&lt;span class="math">\(f(x)\)&lt;/span>形状差异大的&lt;span class="math">\(g_2(x)\)&lt;/span>反而使方差更大了。&lt;/p>
&lt;p>可以从第二个图中看出，如果&lt;span class="math">\(g(x)\)&lt;/span>与&lt;span class="math">\(f(x)\)&lt;/span>的形状相似，相除之后的函数图像更加贴近积分结果&lt;span class="math">\(\theta=\frac{4}{3}(\frac{f(x)}{g_3(x)})\)&lt;/span>，这样我们在抽样时结果接近&lt;span class="math">\(\theta\)&lt;/span>的概率也更大；如果形状相差太大，如&lt;span class="math">\(g_2(x)\)&lt;/span>积分的结果反而会变得更加不好统计。&lt;/p>
&lt;p>综合来说，&lt;strong>重要性采样让新的函数值&lt;span class="math">\(h(x)=\frac{f(x)}{g(x)}\)&lt;/span>取到接近于积分结果的概率变大了&lt;/strong>。我们现在的问题就转变成了&lt;strong>如何找到一个和&lt;span class="math">\(f(x)\)&lt;/span>形状相似的函数&lt;/strong>。&lt;/p>
&lt;h3 id="分层抽样法">分层抽样法&lt;/h3>
&lt;p>在重要性采样的结论中，我们知道当&lt;span class="math">\(f(x)\)&lt;/span>与&lt;span class="math">\(g(x)\)&lt;/span>比值为常数时，可以得到理解最小的方差0，那么我们可不可以近似的构造一个这样的函数呢？基于这个思想，我们有了分层抽样法。&lt;/p>
&lt;p>分层抽样法首先把样本空间&lt;span class="math">\(D\)&lt;/span>分成一些小区间&lt;span class="math">\(D_1,\dotsb,D_m\)&lt;/span>，且诸&lt;span class="math">\(D_i\)&lt;/span>不交，&lt;span class="math">\(\cup D_i =D\)&lt;/span>，然后在各小区间内的抽样数由其“贡献”大小决定。这里的“贡献”我们定义为在小区间&lt;span class="math">\(D_i\)&lt;/span>内的积分值，即 &lt;span class="math">\[p_i=\int_{D_i}f(x)\mathrm{d}x\tag{13}\]&lt;/span> 在区间&lt;span class="math">\(D_i\)&lt;/span>抽样数与&lt;span class="math">\(p_i\)&lt;/span>成正比。显然，分层抽样法是利用小区间构造一个离散的形状类似于原函数&lt;span class="math">\(f(x)\)&lt;/span>的&lt;strong>分段函数&lt;/strong>。&lt;/p>
&lt;p>我们继续以上个例子&lt;span class="math">\(f(x)=-x^2+2x\)&lt;/span>来说明，我们把积分区间&lt;span class="math">\([0,2]\)&lt;/span>划分成十等分，根据式（13）有以下表格： |区间|积分值&lt;span class="math">\(p_i\)&lt;/span>|归一化比例| |:-:|:-:|:-:| |[0,0.2)|0.0373|0.0280| |[0.2,0.4)|0.1013|0.0760| |[0.4,0.6)|0.1493|0.1120| |[0.6,0.8)|0.1813|0.1360| |[0.8,1.0)|0.1973|0.1480| |[1.0,1.2)|0.1973|0.1480| |[1.2,1.4)|0.1813|0.1360| |[1.4,1.6)|0.1493|0.1120| |[1.6,1.8)|0.1013|0.0760| |[1.8,2.0]|0.0373|0.0280&lt;/p>
&lt;p>我们将归一化&lt;span class="math">\(p_i\)&lt;/span>值与&lt;span class="math">\(f(x)\)&lt;/span>画到一张图上(图中黄线与红线)。此外，为了表现构造的分段函数与原函数形状是相似的，我们将分段函数线性放大6倍（蓝线）。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/分层抽样1.png" alt="分层抽样1" />&lt;p class="caption">分层抽样1&lt;/p>
&lt;/div>
&lt;p>可以看出，分层抽样构造出的函数与目标积分函数是类似的，我们区间划分的越细，分段函数越相似。根据分段函数比例分配抽样次数，可以大大降低方差。理论上，给定分层抽样函数后，每个区间的分配的抽样数&lt;strong>最优方式&lt;/strong>为: &lt;span class="math">\[
n_i=nl_i\sigma_i/(\sum_{i=1}^m l_i\sigma_i)\tag{14}
\]&lt;/span> 其中，&lt;span class="math">\(n_i\)&lt;/span>是区间&lt;span class="math">\(D_i\)&lt;/span>的抽样数，&lt;span class="math">\(n\)&lt;/span>为总抽样数，&lt;span class="math">\(m\)&lt;/span>为总区间数目，&lt;span class="math">\(l_i\)&lt;/span>是区间&lt;span class="math">\(D_i\)&lt;/span>的长度，&lt;span class="math">\(\sigma_i\)&lt;/span>是区间&lt;span class="math">\(D_i\)&lt;/span>的标准差，此时方差最小，为&lt;span class="math">\(\frac{1}{n}(\sum_{i=1}^m l_i\sigma_i)^2\)&lt;/span>。然而实际计算中，每个区间的标准差&lt;span class="math">\(\sigma_i\)&lt;/span>总是未知的，式(14)无法直接使用。即使如此，最简单的分配方案&lt;span class="math">\(n_i=nl_i/\sum l_i\)&lt;/span>方差也比样本均值法低。&lt;/p>
&lt;p>此外，其他缩减方差技术还有关联抽样法、控制变量法、对立变量法、条件期望法等等。&lt;/p>
&lt;h2 id="最大期望算法expectation-maximum-em算法">最大期望算法（Expectation Maximum, EM）算法&lt;/h2>
&lt;p>我们在参数估计中常用最大似然估计，不过实际问题中，除了需要估计的未知参数，还有别的&lt;/p>
&lt;h2 id="马尔科夫链蒙特卡洛mcmc">马尔科夫链蒙特卡洛MCMC&lt;/h2>
&lt;p>马尔可夫链蒙特卡洛（英语：Markov chain Monte Carlo，MCMC）方法（含随机游走蒙特卡洛方法）是一组用&lt;strong>马氏链从随机分布取样的算法&lt;/strong>，之前步骤的作为底本。&lt;strong>步数越多，结果越好&lt;/strong>。&lt;/p></description></item><item><title>算法导论-分治</title><link>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA-%E5%88%86%E6%B2%BB/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA-%E5%88%86%E6%B2%BB/</guid><description>
&lt;h2 id="分治divide-and-conquer">分治（divide and conquer）&lt;!-- omit in toc -->&lt;/h2>
&lt;p>分治算法的三个步骤：&lt;/p>
&lt;p>分开-解决-合并&lt;/p>
&lt;h2 id="递归式复杂度分析">递归式复杂度分析&lt;/h2>
&lt;h3 id="用代入法求解递归式">用代入法求解递归式&lt;/h3>
&lt;p>核心思想是首先猜测算法的复杂度大概是多少，然后通过数学归纳法求解其中的常数，证明算法是正确的。&lt;/p>
&lt;h3 id="递归树">递归树&lt;/h3>
&lt;p>通过画图估计生成一个好的猜测，然后再通过代入法进行验证&lt;/p>
&lt;h3 id="主方法">主方法&lt;/h3>
&lt;p>类似于母函数笔记中的递推公式。&lt;/p></description></item><item><title>算法导论-DP</title><link>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA-dp/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA-dp/</guid><description>
&lt;h2 id="动态规划">动态规划&lt;!-- omit in toc -->&lt;/h2>
&lt;p>动态规划和分治算法的区别在于，分治算法将问题分成不重叠的几个子问题，分别求解；而动态规划的子问题是重叠的，每次求解子问题后会把子问题保存起来（例如表格），避免了重复计算。&lt;/p>
&lt;h2 id="动态规划的四个步骤">动态规划的四个步骤&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>刻画一个最优解的结构特征&lt;/li>
&lt;li>递归的定义最优解的值&lt;/li>
&lt;li>计算最优解的值，通常采用自底向上的方法&lt;/li>
&lt;li>利用计算出的信息构造一个最优解&lt;/li>
&lt;/ol>
&lt;p>步骤1-3是动态规划的基础，如果我们仅仅需要最优解的值，第四步可以省略。第四步的作用在于维护一个最优解构造信息。&lt;/p>
&lt;h2 id="动态规划模式">动态规划模式&lt;/h2>
&lt;p>如果只用正常的递归求解，会反复求一些相同的子问题，造成指数级别的增长。DP有两种等价的实现方法：带备忘的自顶向下法（top-down with memoization）和自底向上法（bottom-up method）。&lt;/p>
&lt;h3 id="带备忘的自顶向下法top-down-with-memoization">带备忘的自顶向下法（top-down with memoization）&lt;/h3>
&lt;ul>
&lt;li>此方法仍然按照自然的递归形式编写过程，但是过程会保存每个子问题的解（通常保存在一个数组或散列表中）。&lt;/li>
&lt;li>当需要一个子问题的解时，过程首先检查是否已经保存过此解，&lt;/li>
&lt;li>如果是，则直接返回保存的值，从而节省了计算时间；&lt;/li>
&lt;li>否则，按照通常方式计算这个子问题。&lt;/li>
&lt;/ul>
&lt;h3 id="自底向上法bottom-up-method">自底向上法（bottom-up method）&lt;/h3>
&lt;ul>
&lt;li>方法一般需要恰当定义子问题“规模”的概念，使得任何子问题的求解都只依赖于“更小的”子问题的求解。&lt;/li>
&lt;li>因而可以将子问题按规模排序，按由小到大的顺序进行求解。&lt;/li>
&lt;li>当求解某个子问题时，所依赖的那些更小的子问题都已经求解完毕，结果已经保存。&lt;/li>
&lt;li>每个子问题只求解一次，当求解它时（也是第一次遇到它），所有前提子问题都已经求解完成。&lt;/li>
&lt;/ul>
&lt;p>两种方法得到的算法具有相同的渐进运行时间，仅有的差异是在某些特殊情况下，自顶向下方法并未真正递归地考察所有可能的子问题。由于没有频繁的递归调用开销，自底向上的复杂度函数通常具有更小的系数。&lt;/p>
&lt;h2 id="动态规划的两个关键要素">动态规划的两个关键要素&lt;/h2>
&lt;ul>
&lt;li>最优子结构，一个问题的最优解包含其子问题的最优解&lt;/li>
&lt;li>子问题重叠&lt;/li>
&lt;/ul>
&lt;p>某个问题是否适合应用动态规划，它是否具有最优子结构是一个好线索。在DP中，我们通常自底向上的使用最优子结构。同时，DP要求子问题是无关的，同一个原问题的一个子问题的解不影响另一个子问题的解。&lt;/p>
&lt;p>适合动态规划的另一个方面是子问题空间必须足够的&amp;quot;小&amp;quot;，即问题的递归算法会反复地求解相同的子问题，而不是一直生成新的子问题。&lt;/p></description></item><item><title>博弈论之完全信息静态博弈(贝叶斯博弈)</title><link>https://surprisedcat.github.io/studynotes/%E5%8D%9A%E5%BC%88%E8%AE%BA%E4%B9%8B%E5%AE%8C%E5%85%A8%E4%BF%A1%E6%81%AF%E9%9D%99%E6%80%81%E5%8D%9A%E5%BC%88%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8D%9A%E5%BC%88/</link><pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%8D%9A%E5%BC%88%E8%AE%BA%E4%B9%8B%E5%AE%8C%E5%85%A8%E4%BF%A1%E6%81%AF%E9%9D%99%E6%80%81%E5%8D%9A%E5%BC%88%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8D%9A%E5%BC%88/</guid><description>
&lt;h2 id="完全信息静态博弈贝叶斯博弈">完全信息静态博弈（贝叶斯博弈）&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#信息分割和信息函数">信息分割和信息函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#补充知识函数">补充：知识函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#奥曼模型">奥曼模型&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯纯策略">贝叶斯纯策略&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯行为策略类似于混合策略">贝叶斯行为策略（类似于混合策略）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯博弈的收益计算">贝叶斯博弈的收益计算&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯行为策略均衡">贝叶斯行为策略均衡&lt;/a>&lt;/li>
&lt;li>&lt;a href="#代理人博弈">代理人博弈&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯混合策略">贝叶斯混合策略&lt;/a>&lt;/li>
&lt;li>&lt;a href="#贝叶斯混合均衡">贝叶斯混合均衡&lt;/a>&lt;/li>
&lt;li>&lt;a href="#哈萨尼模型">哈萨尼模型&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="信息分割和信息函数">信息分割和信息函数&lt;/h2>
&lt;blockquote>
&lt;p>信息分割：对于全集&lt;span class="math">\(\Omega\)&lt;/span>,集系&lt;span class="math">\(\mathcal{H}\subseteq 2^{\Omega}\setminus \varnothing\)&lt;/span>是&lt;span class="math">\(\Omega\)&lt;/span>的一个分割，如果它是&lt;span class="math">\(\Omega\)&lt;/span>互不相交的子集的集合（即为集系）且这些子集的并是&lt;span class="math">\(\Omega\)&lt;/span>。&lt;/p>
&lt;p>形式化定义：&lt;span class="math">\(\bigcup_{h_i\in\mathcal{H}}h_i=\Omega;\forall h_i,h_j\in \mathcal{H},h_i\cap h_j=\varnothing(i\neq j)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>直观的看，信息分割就是把全集&lt;span class="math">\(\Omega\)&lt;/span>分成互不相交的几份。&lt;/p>
&lt;p>信息分割出来的每一个片段称之为&lt;strong>信息集&lt;/strong>。对每一局中人来说，&lt;strong>其无法区分同一信息集中的元素&lt;/strong>，即局中人只能直到是哪个信息集，但是不知道是该信息集中的哪一个元素。&lt;/p>
&lt;blockquote>
&lt;p>信息分割&lt;span class="math">\(\xlongequal{等价于}\)&lt;/span>信息函数。&lt;br />信息函数：映射&lt;span class="math">\(H:\Omega→2^\Omega\)&lt;/span>满足：&lt;br />(i)&lt;span class="math">\(\forall \omega\in\Omega, \omega\in H(\omega)\)&lt;/span>&lt;br />(ii)&lt;span class="math">\(\forall \omega\in\Omega,\forall \omega&amp;#39;\in H(\omega), H(\omega&amp;#39;)=H(\omega)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>信息函数意义在于给出任一个元素&lt;span class="math">\(\omega\)&lt;/span>得出它在哪一个信息集（分割）中。由于信息集两两不互相交，因此两个元素要么都在同一个信息集中要么不在同一个信息集中。信息函数是构成贝叶斯博弈的重要组成部分。&lt;/p>
&lt;h3 id="补充知识函数">补充：知识函数&lt;/h3>
&lt;blockquote>
&lt;p>知识函数：映射&lt;span class="math">\(K:2^\Omega→2^\Omega\)&lt;/span>使得&lt;span class="math">\(\forall E\in 2^\Omega, K(E)=\{\omega∈ \Omega|H(\omega)\subseteq E\}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>若&lt;span class="math">\(\omega∈ K(E)\Rightarrow \omega\)&lt;/span>发生，那么事件E也发生了。知识函数可以通过一个线索&lt;span class="math">\(\omega\)&lt;/span>，推断事件E是否发生。&lt;/p>
&lt;h2 id="奥曼模型">奥曼模型&lt;/h2>
&lt;p>使用奥曼模型表示贝叶斯博弈为&lt;span class="math">\(G=(N,A,{\color{red}{\Omega,H,P}},u)\)&lt;/span>。其中：&lt;/p>
&lt;ul>
&lt;li>局中人集合&lt;span class="math">\(N=\{1,2,3,\dotsb,n\}\)&lt;/span>和静态博弈中一样&lt;/li>
&lt;li>策略/动作空间：&lt;span class="math">\(A=A_1×A_2×\dotsb×A_n\)&lt;/span>和和静态博弈中一样&lt;/li>
&lt;li>&lt;strong>状态空间&lt;span class="math">\({\color{red}{\Omega}}\)&lt;/span>&lt;/strong>：非空有限集合。状态空间描述了贝叶斯博弈中的状态，即存在多少种的博弈，每一种博弈以一定的概率出现。&lt;/li>
&lt;li>&lt;strong>信息函数&lt;span class="math">\({\color{red}{H}}\)&lt;/span>&lt;/strong>：这是一个关键概念。它描述的是每一位局中人对此贝叶斯博弈的了解程度。首先，每一个局中人都有自己的信息函数即&lt;span class="math">\(H=(H_1,H_2,\dotsb,H_n)\)&lt;/span>；若局中人观察到某一元素发生，那他可以推到该元素属于的信息集发生了。局中人对博弈了解越清晰，其信息集越小，不确定也就越小。（对于完美信息博弈，信息集都是单元素集）&lt;/li>
&lt;li>&lt;strong>共同先验信息&lt;span class="math">\({\color{red}{P}}\)&lt;/span>&lt;/strong>：首先&lt;span class="math">\(P\)&lt;/span>描述了状态空间的概率分布，即&lt;span class="math">\(\Omega→[0,1]\)&lt;/span>。显然，有&lt;span class="math">\(\sum_{\omega \in \Omega}P(\omega)=1\)&lt;/span>。同时要求每一局中人的信息集也大于0，即&lt;span class="math">\(\forall i∈ N,\forall \omega\in\Omega,P(H_i(\omega))&amp;gt;0\)&lt;/span>&lt;/li>
&lt;li>收益&lt;span class="math">\(u=(u_1,u_2,\dotsb,u_n)\)&lt;/span>：定义和完全信息静态博弈一样，但是其&lt;strong>原像空间多了状态可供选择&lt;/strong>：&lt;span class="math">\(u_i→A×\Omega→\mathbb{R}\)&lt;/span>，即动作确定后，还需要知道目前处于哪个状态（那一个博弈）。&lt;/li>
&lt;/ul>
&lt;p>对于信息函数有：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\forall E\subseteq \Omega,P(E)=\sum_{\omega\in E}P(\omega)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\forall \omega\in \Omega,\forall \omega&amp;#39;\in H_i(\omega)有P(\omega&amp;#39;|H_i(\omega))=\frac{P(\omega&amp;#39;)}{P(H_i(\omega))}\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="贝叶斯纯策略">贝叶斯纯策略&lt;/h3>
&lt;p>贝叶斯纯策略不再是单纯的一个动作，而是&lt;strong>一个与状态相关的函数&lt;/strong>。对于局中人&lt;span class="math">\(i\)&lt;/span>其纯策略空间也不再是&lt;span class="math">\(A_i\)&lt;/span>，而是一个&lt;strong>函数的集合&lt;/strong>： &lt;span class="math">\[
S_i=\{s_i:\Omega→A_i |\;\forall\omega,\omega&amp;#39;\in\Omega[H_i(\omega)=H_i(\omega&amp;#39;)]\Rightarrow [s_i(\omega)=s_i(\omega&amp;#39;)]\}
\]&lt;/span> 其中，每一个函数&lt;span class="math">\(s_i\)&lt;/span>要做的就是当知道现在状态&lt;span class="math">\(\omega\)&lt;/span>的时候，给出对应的动作&lt;span class="math">\(a_i\)&lt;/span>（在状态和动作都有限的情况下，这种映射应该是有限的，个人意见）。在贝叶斯纯策略中，&lt;strong>如果状态&lt;span class="math">\(\omega\)&lt;/span>给定，那么局中人的动作也是确定的&lt;/strong>。但是这类函数需要满足一个限制条件，&lt;strong>由于局中人无法分辨其同一信息集中的不同元素，因此对于同一信息集下的所有元素，在特定状态&lt;span class="math">\(\omega\)&lt;/span>下，其给出的动作是一样的&lt;/strong>。&lt;/p>
&lt;h3 id="贝叶斯行为策略类似于混合策略">贝叶斯行为策略（类似于混合策略）&lt;/h3>
&lt;p>和贝叶斯纯策略集类似，贝叶斯行为策略也是一个&lt;strong>函数&lt;/strong>，定义域还是状态空间，但是至于不再限于单个动作集&lt;span class="math">\(A\)&lt;/span>，可以是混合策略，记局中人的混合策略空间为&lt;span class="math">\(M=M_1×M_2×\dotsb×M_n\)&lt;/span>，其中局中人&lt;span class="math">\(i\)&lt;/span>的混合策略空间为&lt;span class="math">\(M_i\)&lt;/span>，则局中人&lt;span class="math">\(i\)&lt;/span>的一个贝叶斯行为策略为 &lt;span class="math">\[
b_i:\Omega→M_i |\;\forall\omega,\omega&amp;#39;\in\Omega[H_i(\omega)=H_i(\omega&amp;#39;)]\Rightarrow [s_i(\omega)=s_i(\omega&amp;#39;)]
\]&lt;/span> 其中，每一个函数&lt;span class="math">\(b_i\)&lt;/span>要做的就是当知道现在状态&lt;span class="math">\(\omega\)&lt;/span>的时候，给出对应的混合策略&lt;span class="math">\(m_i\)&lt;/span>（&lt;span class="math">\(m_i\)&lt;/span>是在&lt;span class="math">\(A_i\)&lt;/span>是的一个概率组合，由于混合策略集是无限多，这种映射当然也是无穷的）。在贝叶斯行为策略中，&lt;strong>如果状态&lt;span class="math">\(\omega\)&lt;/span>给定，那么局中人的混合策略（动作的概率分布）也是确定的&lt;/strong>。同样，也需要满足和贝叶斯纯策略中一样的信息集限制，这里不再赘述。&lt;/p>
&lt;p>综合所有这样的函数，得到局中人&lt;span class="math">\(i\)&lt;/span>的贝叶斯行为策略空间 &lt;span class="math">\[
B_i=\{b_i:\Omega→M_i |\;\forall\omega,\omega&amp;#39;\in\Omega[H_i(\omega)=H_i(\omega&amp;#39;)]\Rightarrow [s_i(\omega)=s_i(\omega&amp;#39;)]\}
\]&lt;/span> 那么整体贝叶斯行为策略描述为&lt;span class="math">\(B=B_1×B_2×\dotsb×B_n\)&lt;/span>&lt;/p>
&lt;h3 id="贝叶斯博弈的收益计算">贝叶斯博弈的收益计算&lt;/h3>
&lt;p>在纯策略中，每一个贝叶斯纯策略&lt;span class="math">\(s_i\)&lt;/span>的收益是该纯策略在&lt;strong>不同状态&lt;span class="math">\(\omega\)&lt;/span>下，所给定的每一个动作收益的期望&lt;/strong>。有两点需要注意：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>局中人在应对不同信息集时，给出的动作可能是不一样的&lt;/li>
&lt;li>在同一状态&lt;span class="math">\(\omega\)&lt;/span>下，局中人&lt;span class="math">\(i\)&lt;/span>的收益不仅仅和自己的动作&lt;span class="math">\(s_i(\omega)\)&lt;/span>有关，也和其他人在状态&lt;span class="math">\(\omega\)&lt;/span>下的动作&lt;span class="math">\(s_{-i}(\omega)\)&lt;/span>有关（这和完全信息静态博弈一个道理）。需要指出，在状态&lt;span class="math">\(\omega\)&lt;/span>给定（即信息集给定）时，其他局中人&lt;span class="math">\({-i}\)&lt;/span>的动作也是确定的，即&lt;span class="math">\(s_{-i}(\omega)\)&lt;/span>是一个唯一确定的&lt;span class="math">\(n-1\)&lt;/span>维向量。&lt;/li>
&lt;/ol>
&lt;p>总结，收益由三个要素决定：状态，状态下局中人的动作，状态下其他局中人的动作，可记为： &lt;span class="math">\[
u_i(s)=\sum_{\omega\in\Omega}P(\omega)u_i(s_i(\omega),s_{-i}(\omega),\omega)
\]&lt;/span>&lt;/p>
&lt;p>在贝叶斯行为策略中，我们可以仿照纯策略写出贝叶斯行为策略的收益表达式： &lt;span class="math">\[
u_i(b)=\sum_{\omega\in\Omega}P(\omega)u_i(b_i(\omega),b_{-i}(\omega),\omega),\forall b\in B
\]&lt;/span> 到这一步与纯策略的收益只有&lt;span class="math">\(s\rightarrow b\)&lt;/span>这点不同。对于某一具体的状态&lt;span class="math">\(\omega\)&lt;/span>，由于&lt;span class="math">\(b_i(\omega)\)&lt;/span>是混合策略，因此该状态下的收益是混合策略中各个动作收益的期望，即 &lt;span class="math">\[
u_i(b_i(\omega),b_{-i}(\omega),\omega)=\sum_{a\in A}\prod_{j\in N}b_j(\omega)(a_j) u_i(a,\omega)
\]&lt;/span> 需指出，&lt;span class="math">\(a\in A\)&lt;/span>是一个N维向量，表示N个局中人各自的动作（纯策略）。 &lt;span class="math">\(u_i(a,\omega)\)&lt;/span>就是在状态&lt;span class="math">\(\omega\)&lt;/span>下执行纯策略&lt;span class="math">\(a\)&lt;/span>的收益。那么每一个纯策略发生的概率是多少呢？就是这一项&lt;span class="math">\(\prod_{j\in N}b_j(\omega)(a_j)\)&lt;/span>其中&lt;span class="math">\(b_j(\omega)(a_j)\)&lt;/span>是在状态&lt;span class="math">\(\omega\)&lt;/span>下，局中人&lt;span class="math">\(j\)&lt;/span>执行&lt;span class="math">\(a\)&lt;/span>中自身动作&lt;span class="math">\(a_j\)&lt;/span>的概率，N个局中人连乘就是纯策略&lt;span class="math">\(a\)&lt;/span>的发生概率，在对所有的&lt;span class="math">\(a\in A\)&lt;/span>求和，即为求混合策略的收益期望。在对所有的状态的概率求期望： &lt;span class="math">\[
u_i(b)=\sum_{\omega\in\Omega}P(\omega)\sum_{a\in A}\prod_{j\in N}b_j(\omega)(a_j) u_i(a,\omega)
\]&lt;/span> 就是混合策略&lt;span class="math">\(b\in B\)&lt;/span>的收益。我们可以发现，者本质上是求 两次期望，一次是状态概率下的期望，一次是动作发生概率下的期望。&lt;/p>
&lt;h3 id="贝叶斯行为策略均衡">贝叶斯行为策略均衡&lt;/h3>
&lt;p>其定义和完美信息博弈是类似的，都是用最优反应来定义。 &amp;gt;贝叶斯行动策略均衡：对于一个贝叶斯行为策略&lt;span class="math">\(b\in B\)&lt;/span>，如果&lt;span class="math">\(\forall i \in N,\forall b&amp;#39;_i\in B_i\)&lt;/span>都有 &amp;gt;&lt;span class="math">\[
&amp;gt;u_i(b_i,b_-i{})\geq u_i(b_i&amp;#39;,b_{-i})
&amp;gt;\]&lt;/span> &amp;gt;则贝叶斯行为策略&lt;span class="math">\(b\in B\)&lt;/span>是一个&lt;strong>贝叶斯行为均衡&lt;/strong>。&lt;/p>
&lt;p>为了计算方便，我们有以下定理：&lt;/p>
&lt;blockquote>
&lt;p>定理1：贝叶斯混合策略是一个贝叶斯行为均衡当且仅当&lt;span class="math">\(\forall i\in N,\forall \omega\in \Omega,\forall m_i\in M_i\)&lt;/span>有 &lt;span class="math">\[
\sum_{\omega&amp;#39;\in H_i(\omega)}P(\omega&amp;#39;)u_i(b_i(\omega&amp;#39;),b_{-i}(\omega&amp;#39;),\omega&amp;#39;)\geq\sum_{\omega&amp;#39;\in H_i(\omega)}P(\omega&amp;#39;)u_i(m_i,b_{-i}(\omega&amp;#39;),\omega&amp;#39;)\]&lt;/span> 且仅当&lt;span class="math">\(\forall i\in N,\forall \omega\in \Omega,\forall m_i\in M_i\)&lt;/span>有 &lt;span class="math">\[
\sum_{\omega&amp;#39;\in H_i(\omega)}P(\omega&amp;#39;|H_i(\omega))u_i(b_i(\omega&amp;#39;),b_{-i}(\omega&amp;#39;),\omega&amp;#39;)\geq\sum_{\omega&amp;#39;\in H_i(\omega)}P(\omega&amp;#39;|H_i(\omega))u_i(m_i,b_{-i}(\omega&amp;#39;),\omega&amp;#39;)
\]&lt;/span> 多个信息集的不等式可以求出每个信息集中作为最佳相应的贝叶斯行为策略的在纯策略上的概率分布。&lt;/p>
&lt;/blockquote>
&lt;p>我们来理解一下这个定理。先看这个定理的第一部分，它和贝叶斯行为均衡的区别仅在于概率求和的范围不同。贝叶斯行为均衡的定义是对所有状态下收益函数求期望即&lt;span class="math">\(\sum_{\omega\in\Omega}P(\omega)\dotsb\)&lt;/span>而定理中缩减了范围，由于我们知道，贝叶斯博弈中的行为策略是针对每一信息集的，信息集中的元素无法区分，因此只要&lt;strong>针对每一信息集其贝叶斯行为策略是最优的，那么整体的行为策略也会是最有的&lt;/strong>，所以就有&lt;span class="math">\(\forall \omega\in\Omega,\sum_{\omega&amp;#39;\in H(\omega)}P(\omega&amp;#39;)\dotsb\)&lt;/span>&lt;/p>
&lt;p>定理的第2部分是第1部分的扩展，实际上就是两边同时乘以了一个大于0的数:&lt;span class="math">\(\frac{P(H_i(\omega)|\omega&amp;#39;)}{P(H_i(\omega))}\)&lt;/span>。由于&lt;span class="math">\(\omega&amp;#39;\in H(\omega)\)&lt;/span>，所以分子&lt;span class="math">\(P(H_i(\omega)|\omega&amp;#39;)=1\)&lt;/span>，只要信息集&lt;span class="math">\(P(H_i(\omega))&amp;gt;0\)&lt;/span>，定理的第一部分和第二部分就是等价的。第二部分定理给出了我们一个考虑贝叶斯博弈的另一个思路，&lt;strong>即在每一个信息集中，可以独立考虑如何进行动作/策略选择&lt;/strong>，信息集之间的策略选择是没有互相影响的。接下来我们介绍基于这个思想的代理人博弈。&lt;/p>
&lt;h3 id="代理人博弈">代理人博弈&lt;/h3>
&lt;p>可以将每一个局中人在每一个信息集下的决策是独立的，即在此信息集下设置一个&lt;strong>代理人&lt;/strong>，由这个代理人来实现此信息集的&lt;strong>条件期望最大化&lt;/strong>。将贝叶斯博弈转换为代理人博弈的原理是就是&lt;code>定理1&lt;/code>的第二部分。&lt;/p>
&lt;blockquote>
&lt;p>定义贝叶斯代理人模式博弈：&lt;/p>
&lt;/blockquote>
&lt;h2 id="贝叶斯混合策略">贝叶斯混合策略&lt;/h2>
&lt;h2 id="贝叶斯混合均衡">贝叶斯混合均衡&lt;/h2>
&lt;p>在贝叶斯博弈下，贝叶斯混合策略均衡和贝叶斯行为策略均衡是等价。&lt;/p>
&lt;h2 id="哈萨尼模型">哈萨尼模型&lt;/h2>
&lt;p>哈萨尼模型的描述结构与奥曼模型类似，使用的是类型而不是信息集与状态；二者分析的结果是一样。&lt;/p></description></item><item><title>博弈论之合作博弈</title><link>https://surprisedcat.github.io/studynotes/%E5%8D%9A%E5%BC%88%E8%AE%BA%E4%B9%8B%E5%90%88%E4%BD%9C%E5%8D%9A%E5%BC%88/</link><pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%8D%9A%E5%BC%88%E8%AE%BA%E4%B9%8B%E5%90%88%E4%BD%9C%E5%8D%9A%E5%BC%88/</guid><description>
&lt;h2 id="博弈论之合作博弈">博弈论之合作博弈&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#合作博弈的一般表示形式">合作博弈的一般表示形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大联盟博弈">大联盟博弈&lt;/a>&lt;/li>
&lt;li>&lt;a href="#shapley值">Shapley值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#班扎夫权力系数">班扎夫权力系数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#存在小团体的合作博弈">存在小团体的合作博弈&lt;/a>&lt;/li>
&lt;li>&lt;a href="#核">核&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#凸博弈">凸博弈&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;h2 id="合作博弈的一般表示形式">合作博弈的一般表示形式&lt;/h2>
&lt;p>在&lt;span class="math">\(n\)&lt;/span>人博弈中，参与人的集合用&lt;span class="math">\(I=\{1, 2,...,n\}\)&lt;/span>表示，&lt;span class="math">\(I\)&lt;/span>的任意子集&lt;span class="math">\(S\)&lt;/span>称为一个联盟。&lt;/p>
&lt;p>下面给出&lt;span class="math">\(n\)&lt;/span>人博弈的特征函数式：&lt;/p>
&lt;p>设有&lt;span class="math">\(n\)&lt;/span>个人参与人的集合&lt;span class="math">\(I=\{1, 2,...,n\}\)&lt;/span>，对任一子集&lt;span class="math">\(S\subseteq I\)&lt;/span>，定义一个集合到实数的映射（集合函数）&lt;span class="math">\(V(S)\)&lt;/span>满足条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(V(\varnothing)=0\)&lt;/span>&lt;/li>
&lt;li>当&lt;span class="math">\(S_1\cap S_2=\varnothing,S_1\subset I, S_2 \subset I\)&lt;/span>时，&lt;span class="math">\(V(S_1\cup S_2)≥V(S_1)+V(S_2)\)&lt;/span>（称为超可加性，在经济学上称之为协同效应）&lt;/li>
&lt;/ol>
&lt;p>我们把&lt;span class="math">\([I,V]\)&lt;/span>称为一个&lt;span class="math">\(n\)&lt;/span>人合作博弈，称&lt;span class="math">\(V(S)\)&lt;/span>为这个&lt;span class="math">\(n\)&lt;/span>人合作博弈的&lt;font color=red>特征函数&lt;/font>，其中&lt;span class="math">\(S\)&lt;/span>是&lt;span class="math">\(I\)&lt;/span>的任意子集（联盟），&lt;span class="math">\(V(S)\)&lt;/span>描述了联盟的&lt;strong>效益&lt;/strong>。特征函数式对&lt;span class="math">\(n\)&lt;/span>人合作博弈的&lt;strong>每一种可能联盟&lt;/strong>都给出了相应的联盟收益，也就是给出了一种集合函数。&lt;/p>
&lt;h2 id="大联盟博弈">大联盟博弈&lt;/h2>
&lt;p>参与博弈的&lt;span class="math">\(n\)&lt;/span>个人形成一个合作联盟，称此联盟对应的博弈为&lt;strong>n人大联盟合作博弈&lt;/strong>。&lt;span class="math">\(n\)&lt;/span>人大联盟合作博弈的解是指对大结盟所获利益&lt;span class="math">\(V (I)\)&lt;/span>的一个&lt;strong>分配方案&lt;/strong>。即大联盟主要研究&lt;font color=red>分配方案&lt;/font>（前提是允许局中人之间&lt;font color=red>转移支付&lt;/font>）。&lt;/p>
&lt;p>若用&lt;span class="math">\(\varphi_i(V(I)),i\in I\)&lt;/span>表示参与人&lt;span class="math">\(i\)&lt;/span>从&lt;span class="math">\(n\)&lt;/span>人大联盟合作博弈中获得的收益，则&lt;span class="math">\(\varphi_i(V(I))\)&lt;/span>至少应该满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;strong>个体合理性&lt;/strong>：&lt;span class="math">\(\varphi_i(V(I))≥V(\{i\}),i\in I\)&lt;/span>，即合作至少不比单干差；&lt;/li>
&lt;li>&lt;strong>总体合理性&lt;/strong>：&lt;span class="math">\(\sum\limits_{i\in I}\varphi_i(V(I))=V(I)\)&lt;/span>，即将合作博弈&lt;span class="math">\([I,V]\)&lt;/span>中获得的收益&lt;span class="math">\(V(I)\)&lt;/span>分光。&lt;/li>
&lt;/ol>
&lt;p>因此，解决&lt;span class="math">\(n\)&lt;/span>人合作博弈问题的任务是如何获得一个合理的分配方案 &lt;span class="math">\[
\varPhi(V(I))=(\varphi_1(V(I)),\varphi_2(V(I)),\dotsb,\varphi_n(V(I)))
\]&lt;/span>&lt;/p>
&lt;h3 id="shapley值">Shapley值&lt;/h3>
&lt;p>如果说纳什均衡是非合作博弈中的核心概念，那么 Shapley 值是合作博弈中的最重要的概念，&lt;strong>Shapley 值是合作性博弈的一种解&lt;/strong>。&lt;/p>
&lt;p>1953年，美国运筹学家罗伊德·夏普利（Lloyd S. Shapley）采用&lt;strong>逻辑建模方法&lt;/strong>归纳出了三条合理分配原则，即在&lt;span class="math">\(n\)&lt;/span>人合作博弈&lt;span class="math">\([I,V]\)&lt;/span>中，参与人&lt;span class="math">\(i\)&lt;/span>从&lt;span class="math">\(n\)&lt;/span>人大联盟博弈所获得的收益&lt;span class="math">\(\varphi_i(V)\)&lt;/span>应当满足的基本性质（用公理形式表示），进而证明满足这些基本性质的合作博弈解是&lt;font color=red>惟一存在的&lt;/font>，从而妥善地解决了某类合作博弈的合理分配问题。&lt;/p>
&lt;p>这三条分配原则是：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;strong>对称性原则&lt;/strong>。每个参与人获得的分配与他在集合&lt;span class="math">\(I=\{1,2,\dotsb,n\}\)&lt;/span>中的排列位置无关。&lt;/li>
&lt;li>&lt;strong>有效性原则&lt;/strong>。若参与人&lt;span class="math">\(i\)&lt;/span>对他所参加的&lt;strong>任一合作&lt;/strong>都无贡献,则给他的分配应为&lt;strong>0&lt;/strong>。数学表达式为：任意&lt;span class="math">\(i\in S\)&lt;/span>，若&lt;span class="math">\(V(S)=V(S\setminus \{i\})\)&lt;/span>，则&lt;span class="math">\(\varphi_i(V)=0\)&lt;/span>。完全分配：&lt;span class="math">\(\sum\limits_{i\in I}\varphi_i(V)=V(I)\)&lt;/span>&lt;/li>
&lt;li>可加性原则。对&lt;span class="math">\(I\)&lt;/span>上任意两个特征函数&lt;span class="math">\(U\)&lt;/span>与&lt;span class="math">\(V\)&lt;/span>，&lt;span class="math">\(\varPhi(U+V)=\varPhi(U)+\varPhi(V)\)&lt;/span>。可加性原则表明：&lt;span class="math">\(n\)&lt;/span>个人同时&lt;strong>进行两项互不影响的合作，则两项合作的分配也应互不影响&lt;/strong>，每人的分配额是两项合作单独进行时应分配数的和。&lt;/li>
&lt;/ol>
&lt;p>满足上述三条分配原则的&lt;span class="math">\(\varphi_i(V),i\in I\)&lt;/span>,称为&lt;font color=red>Shapley值&lt;/font>。夏普利不仅证明了 Shapley 值的&lt;strong>存在惟一性&lt;/strong>，而且给出了Shapley值的&lt;strong>计算公式&lt;/strong>。下面给出这个重要结果：&lt;/p>
&lt;p>对任一&lt;span class="math">\(n\)&lt;/span>人合作博弈&lt;span class="math">\([I,V]\)&lt;/span>，Shapley值是惟一存在的，且 &lt;span class="math">\[
\varphi_i(V)=\sum_{S \subseteq I\setminus \{i\}}W(|S|) [V(S)\cup \{i\}-V(S)],i=1,2,\dotsb,n
\]&lt;/span> 其中，&lt;span class="math">\(W(|S|)=\frac{|S|!(n-|S|-1)!}{n!},|S|\)&lt;/span>为集合&lt;span class="math">\(S\)&lt;/span>的基数。&lt;/p>
&lt;p>Shapley公式解释：首先Shapley值是用来在&lt;strong>联盟内部&lt;/strong>进行收益公平分配的，不考虑联盟以外的成员。分配原则本质上是对共同收益的&lt;strong>加权平均&lt;/strong>。&lt;/p>
&lt;p>先看第二项&lt;span class="math">\(V(S)\cup \{i\}-V(S)\)&lt;/span>这个说的是有没有用户&lt;span class="math">\(i\)&lt;/span>对联盟&lt;span class="math">\(S\)&lt;/span>收益（特征函数）的影响，即&lt;span class="math">\(i\)&lt;/span>在加入联盟&lt;span class="math">\(S\)&lt;/span>的边际收益或者说&lt;span class="math">\(i\)&lt;/span>对联盟&lt;span class="math">\(S\cup \{i\}\)&lt;/span>的&lt;strong>贡献值&lt;/strong>。而系数&lt;span class="math">\(W(|S|)\)&lt;/span>是指联盟&lt;span class="math">\(S\)&lt;/span>的加权平均系数，以联盟人数&lt;span class="math">\(|S|\)&lt;/span>为分组标准，分为组内平均和组间平均。组建平均很好理解，依照联盟的人数，可以分为&lt;span class="math">\(0\rightarrow N-1\)&lt;/span>共&lt;span class="math">\(N\)&lt;/span>个组，因此平均为&lt;span class="math">\(\frac{1}{N}\)&lt;/span>。而从&lt;span class="math">\(N-1\)&lt;/span>个剩余成员中挑选人数为&lt;span class="math">\(|S|\)&lt;/span>的联盟共有&lt;span class="math">\(\frac{(n-1)!}{|S|!(n-|S|-1)!}\)&lt;/span>个，我们简单用其倒数来平均&lt;span class="math">\(i\)&lt;/span>的贡献值，组内组间平均综合即为&lt;span class="math">\(W(|S|)\)&lt;/span>。&lt;/p>
&lt;h3 id="班扎夫权力系数">班扎夫权力系数&lt;/h3>
&lt;p>称为关键参与者的次数紧性归一化即为班扎夫权力指数。&lt;/p>
&lt;h2 id="存在小团体的合作博弈">存在小团体的合作博弈&lt;/h2>
&lt;p>在参与人多于两个的情况下，就可能出现部分参与人联合起来追求小团体利益的行为，但其前提条件是&lt;strong>参与人在小团体中得到的利益大或等于在大联盟中得到的利益&lt;/strong>，即存在子集&lt;span class="math">\(S=\{i_1,i_2,\dotsb,i_k\}\subset I\)&lt;/span>，相应的总收益为&lt;span class="math">\(V(S)\)&lt;/span>，分配方案 &lt;span class="math">\[
\varPhi(V(S))=(\varphi_{i_1}(V(S)),\varphi_{i_2}(V(S)),\dotsb,\varphi_{i_n}(V(S)))
\]&lt;/span> 满足 &lt;span class="math">\[
\begin{aligned}
&amp;amp;\varphi_{i_1}(V(S))≥\varphi_{i_1}(V(I))\\
&amp;amp;\varphi_{i_2}(V(S))≥\varphi_{i_2}(V(I))\\
&amp;amp;\dots\dots\\
&amp;amp;\varphi_{i_k}(V(S))≥\varphi_{i_k}(V(I))\\
\end{aligned}
\]&lt;/span> 且其中至少有一个&lt;strong>严格不等式&lt;/strong>成立。&lt;/p>
&lt;p>为使记号简便，我们将&lt;span class="math">\(V(\{i\})\)&lt;/span>简记为&lt;span class="math">\(V(i)\)&lt;/span>，将&lt;span class="math">\(n\)&lt;/span>人合作博弈问题的分配方案简记为&lt;span class="math">\(\varPhi(V)=(\varphi_1(V),\varphi_2(V),\dotsb,\varphi_n(V))\)&lt;/span>。&lt;/p>
&lt;h3 id="核">核&lt;/h3>
&lt;p>在一个合作博弈中，满足&lt;strong>帕累托标准&lt;/strong>（除非损人才能利己）的联盟结构称为博弈的一个&lt;strong>有效解&lt;/strong>，&lt;strong>所有有效解&lt;/strong>的集合称为该博弈的&lt;strong>解集&lt;/strong>。&lt;/p>
&lt;p>但是解集中的有效解并不都是稳定的。通俗的来说，如果在某一联盟&lt;span class="math">\(V(S)\)&lt;/span>中，一部分成员通过组成另一个联盟&lt;span class="math">\(V(S&amp;#39;)\)&lt;/span>，并且&lt;span class="math">\(S&amp;#39;\)&lt;/span>的所有成员按照分配规则&lt;span class="math">\(Φ(V(S&amp;#39;))\)&lt;/span>（比如shapley值）能够比在按原来联盟中分配&lt;span class="math">\(Φ(V(S))\)&lt;/span>获得更高的收益，那么这些原来联盟的成员就有理由背离联盟&lt;span class="math">\(S\)&lt;/span>，使得这个联盟不稳定，从而这个有效解也是不稳定的。&lt;/p>
&lt;p>如果&lt;strong>一个联盟结构能使得所有参与者都不能从联盟重组中获益&lt;/strong>，这个联盟结构就是这个合作博弈的&lt;strong>核&lt;/strong>。也就是说在这种联盟结构中所有的参与者都没有改变目前结构的动力。&lt;/p>
&lt;blockquote>
&lt;p>形式化定义：对于合作博弈中&lt;span class="math">\((I,V)\)&lt;/span>的核，核内的成员收益应当且仅当满足 &lt;span class="math">\[
\forall S\subseteq I, \sum_{i\in S} x_i\geq V(S)
\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>核不一定存在，如果核存在，则其一定是博弈的有效解&lt;/strong>。&lt;/p>
&lt;h4 id="凸博弈">凸博弈&lt;/h4>
&lt;blockquote>
&lt;p>定义：对于一个博弈&lt;span class="math">\(G=(I,V)\)&lt;/span>如果是凸博弈，那么对于&lt;span class="math">\(\forall S,T\subset N,V(S\cup T)\geq V(S)+V(T)-V(S\cap T)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>凸博弈的要求比超可加性更强。&lt;/p>
&lt;blockquote>
&lt;p>定理：每一个凸博弈都有非空的核，并且Shapley值在核中。&lt;/p>
&lt;/blockquote></description></item><item><title>数值计算-多项式算法与伪多项式算法</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BC%AA%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%AE%97%E6%B3%95/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BC%AA%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%AE%97%E6%B3%95/</guid><description>
&lt;h2 id="多项式算法与伪多项式算法">多项式算法与伪多项式算法&lt;!-- omit in toc -->&lt;/h2>
&lt;p>为了理解多项式算法和为伪多项式算法的区别，我们先要明确什么是&lt;strong>多项式算法&lt;/strong>。&lt;/p>
&lt;p>多项式时间算法的一个常见的直觉就是对于某一&lt;span class="math">\(k\)&lt;/span>，时间复杂度伪&lt;span class="math">\(O(n^k)\)&lt;/span>。例如，&lt;code>选择排序算法&lt;/code>的时间运行时间为&lt;span class="math">\(O(n^2)\)&lt;/span>，就是多项式时间的。然而，使用暴力计算&lt;code>TSP&lt;/code>问题需要的时间为&lt;span class="math">\(O(n\cdot n!)\)&lt;/span>，就不是多项式时间的。&lt;/p>
&lt;p>这些运行时间都和一个变量，表示数据的输入规模的&lt;span class="math">\(n\)&lt;/span>有关。例如在选择排序算法中，n表示输入数组中元素的数量，而在TSP问题中，n表示图中节点的个数。但是，这些所谓的输入规模，仅仅是直观的定义，并不足够严谨。为了标准化这些&lt;span class="math">\(n\)&lt;/span>，在计算标准时间复杂度时，我们给出了输入规模的标准定义：&lt;/p>
&lt;blockquote>
&lt;p>一个问题的输入规模是保存输入数据所需要的bit位数。&lt;/p>
&lt;/blockquote>
&lt;p>比如，如果排序算法的输入是一个32-bit整数数组，那么输入规模就是&lt;span class="math">\(32n\)&lt;/span>，&lt;span class="math">\(n\)&lt;/span>是指数组中元素的个数。对于一个带有n个节点、m条边的图，,假设每一个点或边用32-bit来存储，需要的bit位数就是&lt;span class="math">\(\Omega(32(n+m))\)&lt;/span>。&lt;/p>
&lt;p>了解了输入规模的定义，我们来看“多项式时间”的标准定义：&lt;/p>
&lt;blockquote>
&lt;p>对于一个问题，在输入规模为&lt;span class="math">\(x\)&lt;/span>的情况下(即需要&lt;span class="math">\(x\)&lt;/span> bits来表示问题的输入)，对于某一常数&lt;span class="math">\(k\)&lt;/span>，如果一个算法能够在&lt;span class="math">\(O(x^k)\)&lt;/span>时间内解决此问题，则我们称此算法是多项式时间的。&lt;/p>
&lt;/blockquote>
&lt;p>当我们处理一些图论、链表、数组、树等问题时，这个&lt;strong>标准定义下的多项式时间和我们传统的多项式时间&lt;/strong>相差无几。还是以选择算法为例，对元素个数为&lt;span class="math">\(n\)&lt;/span>的数组进行排序时，传统时间复杂度为&lt;span class="math">\(O(n^2)\)&lt;/span>。那么如何将数组中的元素个数和输入规模的比特数联系起来呢？之前我们提到过，如果用32-bit表示一个元素，那么有&lt;span class="math">\(x=32n\)&lt;/span>，那么用&lt;span class="math">\(x\)&lt;/span>替换&lt;span class="math">\(n\)&lt;/span>就有&lt;span class="math">\(O((32x)^2)=O(x^2)\)&lt;/span>，仍然是多项式时间。&lt;/p>
&lt;p>类似的，假设在带有&lt;span class="math">\(n\)&lt;/span>个节点、&lt;span class="math">\(m\)&lt;/span>条边的图中做&lt;code>DFS(深度优先搜索)&lt;/code>，传统时间复杂度为&lt;span class="math">\(O(m+n)\)&lt;/span>。数据规模&lt;span class="math">\(x=\Theta(m+n)\)&lt;/span>，因此，标准时间复杂度是&lt;span class="math">\(O(x)\)&lt;/span>，仍是多项式时间的。&lt;/p>
&lt;p>然而，当我们处理一些与&lt;strong>数有关的问题&lt;/strong>时，事情就不太乐观了。现在我们来讨论判断一个整数是否为素数的算法，下面是一个简单的判断素数算法：&lt;/p>
&lt;pre class="sourceCode javascript">&lt;code class="sourceCode javascript">&lt;span class="kw">function&lt;/span> &lt;span class="fu">isPrime&lt;/span>(n):
&lt;span class="kw">for&lt;/span> i from &lt;span class="dv">2&lt;/span> to n - &lt;span class="dv">1&lt;/span>:
&lt;span class="kw">if&lt;/span> (n mod i) = &lt;span class="dv">0&lt;/span>, &lt;span class="kw">return&lt;/span> &lt;span class="kw">false&lt;/span>
&lt;span class="kw">return&lt;/span> &lt;span class="kw">true&lt;/span>&lt;/code>&lt;/pre>
&lt;p>这段代码的时间复杂度是什么？首先看函数显然循环了&lt;span class="math">\(O(n)\)&lt;/span>次，内部的&lt;code>n mod i&lt;/code>复杂度，我们不妨设&lt;em>一个很保守的上界&lt;/em>&lt;span class="math">\(O(n^3)\)&lt;/span>，因此这个算法的整体复杂度为&lt;span class="math">\(O(n^4)\)&lt;/span>。（由于求模运算会比&lt;span class="math">\(O(n^3)\)&lt;/span>更快，所以实际上没有&lt;span class="math">\(n^4\)&lt;/span>那么高的复杂度）。&lt;/p>
&lt;p>有意思的是，直到2004年，三位计算机科学才发表了一篇具有&lt;strong>里程碑意义的文章&lt;/strong>:&lt;a href="https://annals.math.princeton.edu/2004/160-2/p12">PRIMES is in p&lt;/a>，&lt;em>这篇文章给出了一个测定任意一个数是否是质数的多项式算法&lt;/em>。&lt;/p>
&lt;p>可能有读者在想，这算什么？我们刚刚不是很轻松的给出了一个多项式时间内判断一个数为素数的算法了吗？&lt;/p>
&lt;p>不幸的是，&lt;strong>上面那个简单的算法并没有在多项式时间内得出结果&lt;/strong>。回忆一下，我们正式定义时间复杂度的时候用的算法输入规模是如何衡量的？是&lt;strong>输入的比特数&lt;/strong>。我们的简单的判断素数的算法关于n的复杂度是&lt;span class="math">\(O(n^4)\)&lt;/span>，但是它的输入规模是多少呢？显然，输入大小为n的数需要&lt;span class="math">\(x=O(\log_2 n)\)&lt;/span>比特，即&lt;span class="math">\(n=\Theta(2^x)\)&lt;/span>。所以用标准的输入规模&lt;span class="math">\(x\)&lt;/span>表示，我们的那个简单算法的实际算法复杂度变成了&lt;span class="math">\(O(2^{4x})\)&lt;/span>，这已经成了指数时间复杂度了！&lt;/p>
&lt;p>&lt;strong>这里就是多项式时间算法和伪多项式时间算法的核心区别&lt;/strong>。一方面，我们的简单算法看上去是多项式复杂度的&lt;span class="math">\(O(n^4)\)&lt;/span>，另一方面，在标准输入规模的定义下，却不是多项式时间复杂度。&lt;/p>
&lt;p>我们可以从下面这个例子中直观感受一下简单判断素数算法的指数时间的增长速度。对于一个二进制串： &lt;span class="math">\[10001010101011\]&lt;/span> 我们记指数时间复杂度算法运行时间为&lt;span class="math">\(T\)&lt;/span>。然后，我们在二进制串后面仅仅增加一位： &lt;span class="math">\[100010101010111\]&lt;/span> 这时，算法运行时间会变为&lt;span class="math">\(2T\)&lt;/span>(至少)！因此，我们仅仅增加几个bit 就会使得算法运行时间成倍成倍的增长。(相当于数字&lt;span class="math">\(\times 2\)&lt;/span>，上面 提到的简单素数判断算法的计算量也&lt;span class="math">\(\times 2\)&lt;/span>)&lt;/p>
&lt;p>最后我们来说伪多项式时间的定义：&lt;/p>
&lt;blockquote>
&lt;p>如果一个算法的复杂度以&lt;strong>数字大小&lt;/strong>记是&lt;strong>多项式时间&lt;/strong>的，而&lt;strong>标准时间复杂度不是多项式时间&lt;/strong>的，则我们称这个算法是&lt;strong>伪多项式时间&lt;/strong>的。&lt;/p>
&lt;p>也可以理解为：算法的复杂度与&lt;strong>输入规模&lt;/strong>呈&lt;strong>指数&lt;/strong>关系，与输入的&lt;strong>数值大小&lt;/strong>呈&lt;strong>多项式&lt;/strong>关系。&lt;/p>
&lt;/blockquote>
&lt;p>一个具有伪多项式时间复杂度的NP完全问题称之为&lt;strong>弱NP完全问题&lt;/strong>，而在&lt;span class="math">\(P!=NP\)&lt;/span>的情况下，若一个NP完全问题被证明没有伪多项式时间复杂度的解，则称之为&lt;strong>强NP完全问题&lt;/strong>。&lt;/p>
&lt;h2 id="附言">附言&lt;/h2>
&lt;p>&lt;a href="https://annals.math.princeton.edu/2004/160-2/p12">PRIMES is in p&lt;/a>这篇文章证明了可以使用&lt;span class="math">\(O(\log^{12} n)即O(x^{12})\)&lt;/span>的算法来判断一个数是不是素数，因此是一个多项式时间复杂度算法。&lt;/p></description></item><item><title>数值计算-多项式插值方法</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8F%92%E5%80%BC%E6%96%B9%E6%B3%95/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8F%92%E5%80%BC%E6%96%B9%E6%B3%95/</guid><description>
&lt;h2 id="多项式插值方法">多项式插值方法&lt;!-- omit in toc -->&lt;/h2>
&lt;blockquote>
&lt;p>本笔记框架主要来自于知乎：&lt;a href="https://www.zhihu.com/question/22320408">https://www.zhihu.com/question/22320408&lt;/a>,作者：最爱麦丽素&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;a href="#多项式插值综述">多项式插值综述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lagrange插值法">Lagrange插值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重心拉格朗日插值法">重心拉格朗日插值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#newton插值法">Newton插值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#newton插值法余项与泰勒公式">Newton插值法余项与泰勒公式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫节点与勒贝格常数">切比雪夫节点与勒贝格常数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫节点">切比雪夫节点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#勒贝格常数">勒贝格常数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#hermite插值">Hermite插值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#插值法的变形">插值法的变形&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录">附录&lt;/a>&lt;/li>
&lt;li>&lt;a href="#克莱姆法则证明">克莱姆法则证明&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="多项式插值综述">多项式插值综述&lt;/h2>
&lt;p>给定（n+1）个数据点&lt;span class="math">\((x_i,y_i),i\in\{0,...,n\}\)&lt;/span>，且&lt;strong>任意两个&lt;/strong>&lt;span class="math">\(x_i\)&lt;/span>都不相等。可以用一个p阶多项式&lt;span class="math">\((0\leq p \leq n)\)&lt;/span>进行插值，使： &lt;span class="math">\[p(x_i)=y_i,0 \leq i \leq n\]&lt;/span> &lt;span class="math">\(p(x)\)&lt;/span>具体可以写为： &lt;span class="math">\[p(x)=a_nx^n+a_{n-1}x^{n-1}+\dotsb+a_1x+a_0\]&lt;/span> 在&lt;span class="math">\(p(x)\)&lt;/span>中最多会有n阶多项式，n+1个未知参数，而给定的n+1个数据点&lt;span class="math">\((x_i,y_i)\)&lt;/span>，和这些未知数恰好可以构成n+1元一次方程组。如果用矩阵的形式可以写成： &lt;span class="math">\[\begin{bmatrix}x_0^n&amp;amp;x_0^{n-1}&amp;amp;\dotsb&amp;amp;x_0&amp;amp;1\\
x_1^n&amp;amp;x_1^{n-1}&amp;amp;\dotsb&amp;amp;x_1&amp;amp;1\\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots&amp;amp;\vdots\\
x_n^n&amp;amp;x_n^{n-1}&amp;amp;\dotsb&amp;amp;x_n&amp;amp;1\\
\end{bmatrix}
\begin{bmatrix}a_n\\
a_{n-1}\\
\vdots\\
a_0\\
\end{bmatrix}=
\begin{bmatrix}y_0\\
y_1\\
\vdots\\
y_n\\
\end{bmatrix}
\]&lt;/span> 简写为&lt;span class="math">\(X\vec{a}=\vec{y}\)&lt;/span>。我们所要求的就是多项式的系数向量&lt;span class="math">\(\vec{a}\)&lt;/span>，最直观的方式当然是求逆矩阵，从而得到&lt;span class="math">\(\vec{a}=X^{-1}\vec{y}\)&lt;/span>。由于X为范德蒙矩阵，且&lt;span class="math">\(x_i,x_j\)&lt;/span>两两不相等，这就是的范德蒙矩阵&lt;span class="math">\(X\)&lt;/span>必然可逆且唯一（行列式不为0）。&lt;/p>
&lt;p>但是计算大矩阵的逆是一项艰巨的工作，因此我们可以使用克莱姆法则(Cramer‘s Rule) （证明见附录），直接求出每一项系数，即： &lt;span class="math">\[a_i=\frac
{\begin{vmatrix}
x_0^n&amp;amp;x_0^{n-1}&amp;amp;\dotsb&amp;amp;y_0&amp;amp;\dotsb&amp;amp;x_0&amp;amp;1\\
x_1^n&amp;amp;x_1^{n-1}&amp;amp;\dotsb&amp;amp;y_1&amp;amp;\dotsb&amp;amp;x_1&amp;amp;1\\
\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\vdots\\
x_n^n&amp;amp;x_n^{n-1}&amp;amp;\dotsb&amp;amp;y_n&amp;amp;\dotsb&amp;amp;x_n&amp;amp;1\\
\end{vmatrix}}
{\begin{vmatrix}
x_0^n&amp;amp;x_0^{n-1}&amp;amp;\dotsb&amp;amp;x_0^i&amp;amp;\dotsb&amp;amp;x_0&amp;amp;1\\
x_1^n&amp;amp;x_1^{n-1}&amp;amp;\dotsb&amp;amp;x_1^i&amp;amp;\dotsb&amp;amp;x_1&amp;amp;1\\
\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\vdots\\
x_n^n&amp;amp;x_n^{n-1}&amp;amp;\dotsb&amp;amp;x_n^i&amp;amp;\dotsb&amp;amp;x_n&amp;amp;1\\
\end{vmatrix}}(0 \leq i \leq n)\]&lt;/span> 分子中被替换的是范德蒙行列式的第i列。且由于范德蒙矩阵的性质，x只要两两不相同，则行列式不等于0。&lt;/p>
&lt;p>从这个式子中，我们也可以发现无论是求矩阵的逆，还是用克莱姆法则，他们的结果是一样且唯一的，只是计算量复杂。&lt;strong>因此后来改进的Newton法，Lagrange法等等都只是降低了计算量，求出来的多项式系数都是唯一的&lt;/strong>。&lt;/p>
&lt;h2 id="lagrange插值法">Lagrange插值法&lt;/h2>
&lt;p>Lagrange插值法利用了基函数构造的特点，用基函数的线性组合来组成插值多项式。从这个角度来看，Lagrange插值法可以用以下式子表示： &lt;span class="math">\[L(x)=a_0l_0(x)+a_1l_1(x)+a_2l_2(x)+\dotsb+a_nl_n(x)\]&lt;/span> 其中&lt;span class="math">\(l_i(x)\)&lt;/span>就是基函数，&lt;span class="math">\(a_i\)&lt;/span>为基函数线性组合的系数。&lt;/p>
&lt;p>因为&lt;span class="math">\(L(x)\)&lt;/span>需要经过每一个点&lt;span class="math">\((x_i,y_i)\)&lt;/span>，即&lt;span class="math">\(y_i=L(x_i)\)&lt;/span>。我们可以让&lt;span class="math">\(a_i\)&lt;/span>直接等于&lt;span class="math">\(y_i\)&lt;/span>, 那我们就只需要构造对应的&lt;span class="math">\(l_i(x_i)=1,l_i(x_j)=0\)&lt;/span>就好了。Lagrange精巧第构造了这些基函数： &lt;span class="math">\[l_i(x)=\prod_{j=0,j\neq i}^n\frac{(x-x_j)}{(x_i-x_j)}\]&lt;/span> &lt;span class="math">\[L(x)=\sum_{i=0}^n{y_i \prod_{j=0,j\neq i}^n\frac{(x-x_j)}{(x_i-x_j)}}\]&lt;/span> 从表达式中，我们不难得到以下性质：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(i \neq j\)&lt;/span>，则&lt;span class="math">\(l_i(x_j)=0\)&lt;/span>，因为&lt;span class="math">\(\exist x_j-x_j=0,\)&lt;/span>使得分子为0&lt;/li>
&lt;li>&lt;span class="math">\(i=j\)&lt;/span>，则&lt;span class="math">\(l_i(x_j)=1\)&lt;/span>，因为分子和分母完全相同。&lt;/li>
&lt;li>&lt;span class="math">\(L(x)\)&lt;/span>的阶数最大为n。&lt;span class="math">\(\prod_{0 \leq j \leq n,j\neq i}\frac{x-x_j}{x_i-x_j}\)&lt;/span>一共有n个x相乘。&lt;/li>
&lt;li>插值多项式是唯一的，和范德蒙矩阵求出来的是一样的。&lt;/li>
&lt;/ol>
&lt;p>拉格朗日插值法的公式结构整齐紧凑，在理论分析中十分方便，然而在计算中，&lt;strong>当插值点增加或减少一个时，所对应的基本多项式就需要全部重新计算&lt;/strong>，于是整个公式都会变化，非常繁琐。这时可以用&lt;strong>重心拉格朗日插值法或牛顿插值法&lt;/strong>来代替。此外，当插值点比较多的时候，拉格朗日插值多项式的次数可能会很高，因此具有数值不稳定的特点，也就是说尽管在已知的几个点取到给定的数值，但在附近却会和“实际上”的值之间有很大的偏差（如图1）。这类现象也被称为龙格现象，解决的办法是&lt;strong>分段用较低次数的插值多项式&lt;/strong>。&lt;/p>
&lt;img src="../images/Lagrange_polynomial_Divergence.jpg" alt="龙格现象" />
&lt;center>
图1 拉格朗日插值法的数值稳定性：如图，用于模拟一个十分平稳的函数时，插值多项式的取值可能会突然出现一个大的偏差（图中的14至15中间）
&lt;/center>
&lt;h3 id="重心拉格朗日插值法">重心拉格朗日插值法&lt;/h3>
&lt;p>来源维基百科：&lt;a href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC%E6%B3%95">https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC%E6%B3%95&lt;/a>&lt;/p>
&lt;p>重心拉格朗日插值法是拉格朗日插值法的改进。在拉格朗日插值多项式中，运用多项式： &lt;span class="math">\[l(x)=(x-x_0)(x-x_1)\dotsb (x-x_k)\]&lt;/span> 可以将原拉格朗日多项式的项改写： &lt;span class="math">\[l_j(x)=\frac{l(x)}{x-x_j}\frac{1}{\prod_{i=0,i\neq j}^k(x_j-x_i)}\]&lt;/span> 如果我们将&lt;strong>重心权&lt;/strong>定义为： &lt;span class="math">\[w_j=\frac{1}{\prod_{i=0,i\neq j}^k(x_j-x_i)}\]&lt;/span> 拉格朗日多项式的项可以变为： &lt;span class="math">\[l_j(x)=l(x)\frac{w_j}{x-x_j}\]&lt;/span> 同样，原拉格朗日多项式可以把&lt;span class="math">\(l(x)\)&lt;/span>提取出来： &lt;span class="math">\[L(x)=l(x)\sum_{j=0}^k\frac{w_j}{x-x_j}y_j\]&lt;/span> 即所谓的&lt;strong>重心拉格朗日插值公式第一型&lt;/strong>或改进拉格朗日插值公式。它的优点是当插值点的个数增加一个时，将每个&lt;span class="math">\(w_{j}\)&lt;/span>都除以&lt;span class="math">\((x_{j}-x_{k+1})\)&lt;/span>，就可以得到新的重心权&lt;span class="math">\(w_{k+1}\)&lt;/span>，计算复杂度为&lt;span class="math">\(O(n)\)&lt;/span>，比重新计算每个基本多项式所需要的复杂度&lt;span class="math">\(O(n^{2})\)&lt;/span>降了一个量级。&lt;/p>
&lt;p>将以上的拉格朗日插值多项式用来对函数&lt;span class="math">\(g(x)\equiv 1\)&lt;/span>插值，可以得到： &lt;span class="math">\[\because y_i \equiv 1 \\
\therefore \forall x,g(x)=l(x)\sum_{j=0}^k\frac{w_j}{(x-x_j)} \]&lt;/span> 我们用&lt;span class="math">\(L(x)\)&lt;/span>除以&lt;span class="math">\(g(x) \equiv 1\)&lt;/span>，可以得到： &lt;span class="math">\[L(x)=\frac{\sum_{j=0}^k\frac{w_j}{x-x_j}y_j}{\sum_{j=0}^k\frac{w_j}{(x-x_j)}}\]&lt;/span> 这个公式被称为&lt;strong>重心拉格朗日插值公式（第二型）或真正的重心拉格朗日插值公式&lt;/strong>。它继承了第一型容易计算的特点，并且在代入&lt;span class="math">\(x\)&lt;/span>值计算&lt;span class="math">\(L(x)\)&lt;/span>的时候不必计算多项式&lt;span class="math">\(l(x)\)&lt;/span>。它的另一个优点是，结合&lt;strong>切比雪夫节点进行插值的话，可以很好地模拟给定的函数，使得插值点个数趋于无穷时，最大偏差趋于零&lt;/strong>。同时，重心拉格朗日插值结合切比雪夫节点进行插值可以达到极佳的数值稳定性。第一型拉格朗日插值是&lt;strong>向后稳定&lt;/strong>的，而第二型拉格朗日插值是&lt;strong>向前稳定&lt;/strong>的，并且&lt;strong>勒贝格常数很小&lt;/strong>。&lt;/p>
&lt;p>前向稳定：算法的前向误差是结果与真解之间的差别，即&lt;span class="math">\(\Delta y=y^{*}-y\)&lt;/span>。&lt;/p>
&lt;p>后向稳定：后向误差是满足&lt;span class="math">\(f(x+\Delta x)=y^{*}\)&lt;/span>的最小&lt;span class="math">\(\Delta x\)&lt;/span>，也就是说后向误差说明算法的所解决的问题。如果对于任意的输入&lt;span class="math">\(x\)&lt;/span>来说后向误差都很小，那么算法就是后向稳定的。&lt;/p>
&lt;p>前向误差和后向误差通过条件数发生关系：前向误差的幅度最多是条件数乘以后向误差的幅度。&lt;/p>
&lt;h2 id="newton插值法">Newton插值法&lt;/h2>
&lt;p>回头看看Lagrange的基函数，如果我们能找到一组新的基函数，使得新增的基函数对原有函数进行修正，就能防止有新节点插入时Lagrange的重复计算。考虑以下&lt;strong>基函数&lt;/strong>。 &lt;span class="math">\[\begin{aligned}
\phi_0(x)&amp;amp;=1\\
\phi_1(x)&amp;amp;=(x-x_0)\\
\phi_2(x)&amp;amp;=(x-x_0)(x-x_1)\\
\dotsb&amp;amp;=\dotsb\\
\phi_{n+1}(x)&amp;amp;=\prod_{i=0}^n(x-x_i)\\
\end{aligned}
\]&lt;/span> 根据线性无关的基函数我们可以用其线性组合得到&lt;strong>待定系数&lt;/strong>的插值多项式： &lt;span class="math">\[p(x)=a_0\phi_0(x)+a_1\phi_1(x)\dotsb+a_n\phi_n(x)\]&lt;/span> 首先，对于1个点&lt;span class="math">\((x_0,y_0),p_0(x)=a_0=y_0\)&lt;/span>，显然。&lt;/p>
&lt;p>对于2个点&lt;span class="math">\((x_0,y_0),(x_1,y_1)\)&lt;/span>。 &lt;span class="math">\[p_1(x)=y_0+a_1(x-x_0)\\
a_1=\frac{y_1-y_0}{x_1-x_0}\]&lt;/span>&lt;/p>
&lt;p>对于三个点&lt;span class="math">\((x_0,y_0),(x_1,y_1),(x_2,y_2)\)&lt;/span>。 &lt;span class="math">\[p_2(x)=y_0+\frac{y_1-y_0}{x_1-x_0}+a_2\phi_2(x)\\
a_2=\frac{\frac{y_2-y_1}{x_2-x_1}-\frac{y_1-y_0}{x_1-x_0}}{x_2-x_0}\]&lt;/span>&lt;/p>
&lt;p>根据系数的特定，我们定义&lt;strong>均差（或差商）&lt;/strong>： &amp;gt;一阶均差： &amp;gt;&lt;span class="math">\[f[x_i,x_j]=\frac{f(x_i)-f(x_j)}{x_i-x_j}\]&lt;/span> &amp;gt;二阶均差：一阶均差的均差。 &amp;gt;&lt;span class="math">\[f[x_i,x_j,x_k]=\frac{f[x_i,x_j]-f[x_j,x_k]}{x_i-x_k}\]&lt;/span> &amp;gt;N阶均差：N-1阶均差的均差。&lt;/p>
&lt;p>均差具有对称性：均差&lt;span class="math">\(f[x_0,x_1,\dotsb,x_k]\)&lt;/span>与插值节点的顺序无关，即 &lt;span class="math">\[f[x_0,x_1,\dotsb,x_k]=f[x_{i_0},x_{i_1},\dotsb,x_{i_k}]\]&lt;/span> 其中，&lt;span class="math">\(x_{i_0},x_{i_1},\dotsb,x_{i_k}\)&lt;/span>是&lt;span class="math">\(x_0,x_1,\dotsb,x_k\)&lt;/span>的任意一个排列。&lt;/p>
&lt;p>根据上述定义和规律，我们知道对于n+1个节点，其插值多项式为： &lt;span class="math">\[\begin{aligned}
p_n(x)=&amp;amp;f(x_0)+\\
&amp;amp;f[x_1,x_0](x-x_0)+\\
&amp;amp;f[x_2,x_1,x_0](x-x_0)(x-x_1)+\\
&amp;amp;f[x_3,x_2,x_1,x_0](x-x_0)(x-x_1)+(x-x_2)+\\
&amp;amp;\dotsb
\end{aligned}\]&lt;/span> 每新增一个节点&lt;span class="math">\((x_{n+1},y_{n+1})\)&lt;/span>时，只需要增加一项新的基函数&lt;span class="math">\(\phi_{n+1}=(x-x_0)(x-x_1)\dotsb(x-x_n)\)&lt;/span>和新的系数&lt;span class="math">\(f[x_0,x_1,\dotsb,x_{n+1}]\)&lt;/span>即可。其中， &lt;span class="math">\[f[x_0,x_1,\dotsb,x_{n+1}]=\frac{f[x_0,x_1,\dotsb,x_n]-f[x_1,x_2,\dotsb,x_{n+1}]}{x_0-x_{n+1}}\]&lt;/span> 下面这个图显示了均差表的计算，由于实际运算中，这些都是数值，因此计算并不困难。也是O(n)的计算复杂度。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/均差计算1.jpg" alt="均差计算1" />&lt;p class="caption">均差计算1&lt;/p>
&lt;/div>
&lt;p>新增一个插值点，只需要计算相关的差分就可以了。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/均差计算2.jpg" alt="均差计算2" />&lt;p class="caption">均差计算2&lt;/p>
&lt;/div>
&lt;h3 id="newton插值法余项与泰勒公式">Newton插值法余项与泰勒公式&lt;/h3>
&lt;p>泰勒把牛顿插值法做了一些改造。 首先，设&lt;span class="math">\(f(x)\)&lt;/span>是一个函数，它在&lt;span class="math">\(x_0,x_0+\Delta x,x_0+2\Delta x,x_0+3\Delta x,\cdots,x_0+n\Delta x\)&lt;/span>的值已知（和之前的相比，相当于每个点都是等距离间隔的，间隔&lt;span class="math">\(\Delta x\)&lt;/span>），令： &lt;span class="math">\[
\Delta f(x_0)=f(x_0+\Delta x)-f(x_0)，也称为一阶差分，\]&lt;/span> &lt;span class="math">\[\Delta f(x_0+\Delta x)=f(x_0+2\Delta x)-f(x_0+\Delta x),
\]&lt;/span> 进一步令： &lt;span class="math">\[\Delta^2 f(x_0)=\Delta f(x_0+\Delta x)-\Delta f(x_0)，也称为二阶差分（为一阶差分的差分）\\
\Delta^3 f(x_0)=\Delta^2 f(x_0+\Delta x)-\Delta^2 f(x_0)，也称为三阶差分。\]&lt;/span> 做了这些假设之后我们来看看之前提到的&lt;span class="math">\(a_1\)&lt;/span>会变成什么样子： &lt;span class="math">\[a_1=\frac{f(x_1)-f(x_0)}{x_1-x_0}\implies a_1=\frac{\Delta f(x_0)}{\Delta x}。\]&lt;/span> 而&lt;span class="math">\(f_1(x)\)&lt;/span>会变成： &lt;span class="math">\[f_1(x)=f(x_0)+\frac{f(x_1)-f(x_0)}{x_1-x_0}(x-x_0)\\
\implies f_1(x)=f(x_0)+\frac{\Delta f(x_0)}{\Delta x}(x-x_0)。\]&lt;/span> 同样的&lt;span class="math">\(f_2(x)\)&lt;/span>就变成了： &lt;span class="math">\[f_2(x)=f(x_0)+\frac{\Delta f(x_0)}{\Delta x}(x-x_0)+\frac{\Delta^2 f(x_0)}{2\Delta x}(x-x_0)(x-x_1)。\]&lt;/span> 泰勒断言，当&lt;span class="math">\(\Delta x=0\)&lt;/span>时： &lt;span class="math">\[f_1(x)=f(x_0)+f&amp;#39;(x_0)(x-x_0)\\
f_1(x)=f(x_0)+f&amp;#39;(x_0)(x-x_0)+\frac{f&amp;#39;&amp;#39;(x_0)}{2!}(x-x_0)^2\\
（\Delta x=0时有x-x_1=x-x_0）\]&lt;/span> 以此类推泰勒就得到了大名鼎鼎的泰勒公式： &lt;span class="math">\[f(x)=f(x_0)+f&amp;#39;(x_0)(x-x_0)+\frac{f&amp;#39;&amp;#39;(x_0)}{2!}(x-x_0)^2+\cdots\]&lt;/span>&lt;/p>
&lt;h2 id="切比雪夫节点与勒贝格常数">切比雪夫节点与勒贝格常数&lt;/h2>
&lt;p>&lt;a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8F%92%E5%80%BC">https://zh.wikipedia.org/wiki/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8F%92%E5%80%BC&lt;/a>&lt;/p>
&lt;p>切比雪夫节点与勒贝格常数都是在没有给定插值节点时，自己选择插值节点的准则。&lt;/p>
&lt;h3 id="切比雪夫节点">切比雪夫节点&lt;/h3>
&lt;p>对于一个插值区间&lt;span class="math">\([a, b]\)&lt;/span> 如果要在这个插值区间上选取&lt;span class="math">\(n\)&lt;/span>个点作为插值基点，使得上面的最大误差最小，则基点的选法如下: &lt;span class="math">\[x_i=\frac{b+a}{2}+\frac{b-a}{2}\cos\frac{(2i-1)\pi}{2n}\\
(i=1,2,\dotsb,n)\]&lt;/span> 这些节点称为切比雪夫（插值）节点。&lt;/p>
&lt;h3 id="勒贝格常数">勒贝格常数&lt;/h3>
&lt;p>在插值节点&lt;span class="math">\(x_0、\dotsb、x_n\)&lt;/span>以及包含所有插值节点的区间&lt;span class="math">\([a, b]\)&lt;/span>确定下来，插值的过程就是将函数&lt;span class="math">\(f\)&lt;/span>映射到多项式&lt;span class="math">\(p\)&lt;/span>。这定义了一个&lt;span class="math">\([a, b]\)&lt;/span>区间上所有连续函数从空间&lt;span class="math">\(C([a, b])\)&lt;/span>到其自身的映射。映射&lt;span class="math">\(X\)&lt;/span>是线性的，并且它是函数&lt;span class="math">\(f\)&lt;/span>在子空间&lt;span class="math">\(Π_n\)&lt;/span>上的投影。勒贝格常数&lt;span class="math">\(L\)&lt;/span>定义为&lt;span class="math">\(X\)&lt;/span>的算子范数，它满足勒贝格引理： &lt;span class="math">\[\|f-X(f)\|\leq (L+1)\|f-p^{*}\|.\]&lt;/span> 换句话说，插值多项式的误差最多是最优逼近的&lt;span class="math">\(L+1\)&lt;/span>倍，这表明我们要找使&lt;span class="math">\(L\)&lt;/span>最小的插值节点。尤其是选择切比雪夫节点时： &lt;span class="math">\[L \ge \frac{2}{\pi}log(n+1)+C \quad for\quad some\quad costant\quad C \]&lt;/span> 我们再次证明切比雪夫节点是多项式插值中比较好的选择，但是这些节点并不是最优的。&lt;/p>
&lt;h2 id="hermite插值">Hermite插值&lt;/h2>
&lt;p>&lt;strong>埃尔米特插值&lt;/strong>是另一类插值问题，这类插值在给定的节点处，不但要求插值多项式的函数值与原函数值相同。同时还要求在节点处，插值多项式的一阶直至指定阶的导数值，也与被插函数的相应阶导数值相等，这样的插值称为埃尔米特(Hermite)插值。 Hermite插值在不同的节点，提出的插值条件个数可以不同，若在某节点&lt;span class="math">\(x_{i}\)&lt;/span>，要求插值函数多项式的函数值，一阶导数值，直至&lt;span class="math">\(m_{i}-1\)&lt;/span>阶导数值均与被插函数的函数值相同及相应的导数值相等。我们称&lt;span class="math">\(x_{i}\)&lt;/span>为&lt;span class="math">\(m_{i}\)&lt;/span>重插值点节,因此，Hermite插值应给出两组数，一组为插值点&lt;span class="math">\(\{x_{i}\}_{i=0}^{n}\)&lt;/span>节点，另一组为相应的重数标号&lt;span class="math">\(\{m_{i}\}_{i=0}^{n}\)&lt;/span>。&lt;/p>
&lt;h2 id="插值法的变形">插值法的变形&lt;/h2>
&lt;p>讲到这里基本上所有插值都是上面这些方法的结合体或者扩展了，根据不同的应用我们会使用不同的算法来解决我们的问题，像已知很多点的一次导数和他的位置，一般只有在应用物体位置和速度都知道的情况下，而这也只有在方程次数很高的情况下才会应用高次Hermite插值。通常我们接触的插值应用也只在3次方程进行。比如当我们仅仅知道点的位置而想让曲线更加平滑，不考虑误差，用最普通的Newton来算就好。为了增加视觉上的平滑或者说和周围点的连贯行，我们可以根据已知相邻点的位置来计算当前点的导数，进而使用Hermite算法。&lt;/p>
&lt;p>如果我们将所有的点分段进行计算，则变为&lt;strong>样条插值&lt;/strong>。&lt;/p>
&lt;p>对于样条插值的连续性，如果我们选择3次样条插值来满足一阶导数连续的话，实际上是满足不了2阶导数连续的，举个例子，&lt;span class="math">\(x^3\)&lt;/span>在y轴左侧，&lt;span class="math">\(x^2\)&lt;/span>在y轴右侧构成的曲线，在原点处的一阶导数都为0，而&lt;span class="math">\(f_+&amp;#39;&amp;#39;(0)=2,f_-&amp;#39;&amp;#39;(0)=0\)&lt;/span>就会导致看起来略微有些别扭的情况，但是实际应用中，这种不合适经常被忽略。&lt;/p>
&lt;p>在多段插值中，已知点的位置，对于点上的导数的计算的选择的不同则变为不同的插值。 &lt;img src="../images/插值法变形.svg" alt="插值法的变形" />&lt;/p>
&lt;h2 id="附录">附录&lt;/h2>
&lt;h3 id="克莱姆法则证明">克莱姆法则证明&lt;/h3>
&lt;p>来源：维基百科&lt;a href="https://zh.wikipedia.org/wiki/%E5%85%8B%E8%90%8A%E5%A7%86%E6%B3%95%E5%89%87">https://zh.wikipedia.org/wiki/%E5%85%8B%E8%90%8A%E5%A7%86%E6%B3%95%E5%89%87&lt;/a>&lt;/p>
&lt;p>克莱姆法则：一个线性方程组可以用矩阵与向量的方程来表示： &lt;span class="math">\[Ax=c \quad(1)\]&lt;/span> 其中的&lt;span class="math">\(A\)&lt;/span>是一个&lt;span class="math">\(n\times n\)&lt;/span>的方块矩阵，而向量&lt;span class="math">\(x=( x_1, x_2, \cdots x_n )^T\)&lt;/span>是一个长度为n的列向量&lt;span class="math">\(c=( c_1, c_2, \cdots c_n )^T\)&lt;/span>也一样。&lt;/p>
&lt;p>克莱姆法则说明：如果&lt;span class="math">\(A\)&lt;/span>是一个可逆矩阵（&lt;span class="math">\(\det{A} \neq 0\)&lt;/span>），那么方程(1)有解&lt;span class="math">\(x=( x_1, x_2, \cdots x_n )^T\)&lt;/span>，其中&lt;/p>
&lt;p>&lt;span class="math">\[x_i = { \det(A_i) \over \det(A)} \quad(1)\]&lt;/span>&lt;/p>
&lt;p>当中&lt;span class="math">\(A_{i}\)&lt;/span>是被列向量&lt;span class="math">\(c\)&lt;/span>取代了&lt;span class="math">\(A\)&lt;/span>的第i列的列向量后得到的矩阵。为了方便，我们通常使用&lt;span class="math">\(\Delta\)&lt;/span>来表示&lt;span class="math">\(\det(A)\)&lt;/span>，用&lt;span class="math">\(\Delta_i\)&lt;/span>来表示&lt;span class="math">\(\det(A_i)\)&lt;/span>。所以等式(1)可以写成为： &lt;span class="math">\[x_i = { \Delta_i \over \Delta }。\]&lt;/span>&lt;/p>
&lt;p>证明：&lt;/p>
&lt;p>对于n元线性方程组&lt;span class="math">\(Ax=c\)&lt;/span>&lt;/p>
&lt;p>把系数矩阵&lt;span class="math">\(A\)&lt;/span>表示成列向量的形式 &lt;span class="math">\[A=\left(u_{1},u_{2},\cdots ,u_{n}\right)\]&lt;/span> 由于系数矩阵可逆，故方程组一定有解&lt;span class="math">\(x^{*}=A^{{-1}}c\)&lt;/span>.&lt;/p>
&lt;p>设&lt;span class="math">\(x^{*}=(x_{1},x_{2},\cdots ,x_{n})^{T}\)&lt;/span>，即 &lt;span class="math">\[Ax^{*}=\sum _{{k=1}}^{n}x_{k}u_{k}=c\]&lt;/span>&lt;/p>
&lt;p>考虑&lt;span class="math">\(\Delta_i\)&lt;/span>的值，利用行列式的线性和交替性质，有&lt;/p>
&lt;p>&lt;span class="math">\[{\begin{aligned}
\Delta _{i}&amp;amp;=det\left(\cdots ,u_{{i-1}},c,u_{{i+1}},\cdots \right) \\&amp;amp;=det\left(\cdots ,u_{{i-1}},\sum _{{k=1}}^{n}x_{k}u_{k},u_{{i+1}},\cdots \right)
\\&amp;amp;=\sum _{{k=1}}^{n}x_{k}\cdot det\left(\cdots ,u_{{i-1}},u_{k},u_{{i+1}},\cdots \right)
\\&amp;amp;=x_{i}\cdot det\left(\cdots ,u_{{i-1}},u_{i},u_{{i+1}},\cdots \right)
\\&amp;amp;=x_{i}\Delta
\end{aligned}}\]&lt;/span>&lt;/p>
&lt;p>于是&lt;span class="math">\(x_{i}=\frac{\Delta_{i}}{\Delta}\)&lt;/span>。&lt;/p></description></item><item><title>数值计算-插值与回归</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97-%E6%8F%92%E5%80%BC%E4%B8%8E%E5%9B%9E%E5%BD%92/</link><pubDate>Mon, 30 Nov 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97-%E6%8F%92%E5%80%BC%E4%B8%8E%E5%9B%9E%E5%BD%92/</guid><description>
&lt;h2 id="插值与回归">插值与回归&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#引言">引言&lt;/a>&lt;/li>
&lt;li>&lt;a href="#魏尔施特拉斯逼近定理">魏尔施特拉斯逼近定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#逼近-回归-插值">逼近-回归-插值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#插值理论">插值理论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性插值最简单">线性插值——最简单&lt;/a>&lt;/li>
&lt;li>&lt;a href="#双线性插值">双线性插值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多项式插值">多项式插值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lagrange插值">Lagrange插值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归算法">回归算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性回归">线性回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多项式回归">多项式回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#其他">其他&lt;/a>&lt;/li>
&lt;li>&lt;a href="#内插法外推法曲线拟合及回归">内插法、外推法、曲线拟合及回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#参考文献">参考文献&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="引言">引言&lt;/h2>
&lt;h3 id="魏尔施特拉斯逼近定理">魏尔施特拉斯逼近定理&lt;/h3>
&lt;p>魏尔施特拉斯逼近定理（Stone–Weierstrass theorem、Weierstrass theorem）共有两个：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>闭区间上的连续函数可用多项式级数一致逼近。(泰勒级数特例)&lt;/li>
&lt;li>闭区间上周期为&lt;span class="math">\(2\pi\)&lt;/span>的连续函数可用三角函数级数一致逼近。（傅里叶级数特例）&lt;/li>
&lt;/ol>
&lt;h3 id="逼近-回归-插值">逼近-回归-插值&lt;/h3>
&lt;p>在一个闭区间内，多项式可以逼近任何一个连续函数。在这种情况下，研究如何使用&lt;strong>多项式来进行数据的拟合&lt;/strong>就成为了一个关键的问题（最小误差）。这就是所谓的多项式的&lt;strong>回归算法&lt;/strong>。&lt;/p>
&lt;p>在数值分析中，多项式插值是使用多项式对一组给定的数据来进行插值的过程，也就是说，在给定一组数据的情况下，多项式&lt;strong>插值&lt;/strong>的目的就是找到一个多项式，使得它&lt;strong>恰好通过&lt;/strong>这些数据点。拉格朗日插值算法可以对实践中的某些物理量进行观测，然后得到一个多项式，从而表示各个结果之间的内在联系。多项式插值算法也是数值积分和数值常微分方程的算法基础。&lt;/p>
&lt;h2 id="插值理论">插值理论&lt;/h2>
&lt;h3 id="线性插值最简单">线性插值——最简单&lt;/h3>
&lt;p>给定两个点&lt;span class="math">\((x_0,y_0),(x_1,y_1)\)&lt;/span>，线性插值是一条连接两个点的直线。对于&lt;span class="math">\(x\in(x_0,x_1)\)&lt;/span>，y可以通过斜截式给出： &lt;span class="math">\[\frac{y-y_0}{x-x_0}=\frac{y_1-y_0}{x_1-x_0}\implies y=y_0+\frac{y_1-y_0}{x_1-x_0}(x-x_0),x\in(x_0,x_1)\]&lt;/span>&lt;/p>
&lt;p>对于多个点的线性插值来说，x的值之和最近的两点相关，每个采样点都是用折现连接的（如图1）。其公式可以表达为： &lt;span class="math">\[y=f(x)=y_i+\frac{y_i-y_{i-1}}{x_i-x_{i-1}}(x-x_i),x\in(x_{i-1},x_i)\]&lt;/span>&lt;/p>
&lt;img src="../images/linear_interpolation.png" alt="线性插值" />
&lt;center>
图1 线性插值
&lt;/center>
&lt;h3 id="双线性插值">双线性插值&lt;/h3>
&lt;p>双线性插值一般用于&lt;span class="math">\(z=f(x,y)\)&lt;/span>的2+1维模式，在&lt;strong>平面图形&lt;/strong>处理中有广泛应用。其本质上是做&lt;strong>2+1次线性插值&lt;/strong>。&lt;/p>
&lt;p>典型的示意图如图2：&lt;/p>
&lt;img src="../images/bilinear_interpolation.png" alt="双线性插值" />
&lt;center>
图2 双线性插值
&lt;/center>
&lt;p>在2D网格上，我们给出四个点&lt;span class="math">\((x_1,y_1),(x_1,y_2),(x_2,y_1),(x_2,y_2)\)&lt;/span>和四个点的函数值，要求&lt;span class="math">\(x\in(x_1,x_2),y\in(y_1,y_2)\)&lt;/span>，双线性插值求&lt;span class="math">\(f(x,y)\)&lt;/span>则如下：&lt;/p>
&lt;p>&lt;span class="math">\[f(R_1)\approx f(Q_{11})\frac{x_2-x}{x_2-x_1}+f(Q_{21})\frac{x-x_1}{x_2-x_1}\]&lt;/span> &lt;span class="math">\[f(R_2)\approx f(Q_{21})\frac{x_2-x}{x_2-x_1}+f(Q_{22})\frac{x-x_1}{x_2-x_1}\]&lt;/span> 有点像加权平均。然后将&lt;span class="math">\(R_1,R_2\)&lt;/span>再做一次线性插值： &lt;span class="math">\[f(x,y)=f(P)=f(R_1)\frac{y_2-y}{y_2-y_1}+f(R_2)\frac{y-y_1}{y_2-y_1}\]&lt;/span> 带入&lt;span class="math">\(f(R_1),f(R_2)\)&lt;/span>可得： &lt;span class="math">\[f(x,y)=\frac{1}{(x_2-x_1)(y_2-y_1)}[f(Q_{11})(x_2-x)(y_2-y)+f(Q_{12})(x-x_1)(y_2-y) \\
+f(Q_{21})(x_2-x)(y-y_1)+f(Q_{22})(x-x_1)(y-y_1)]\]&lt;/span> 写成矩阵形式： &lt;span class="math">\[f(x,y)=\frac{1}{(x_2-x_1)(y_2-y_1)}\begin{bmatrix} x_2-x&amp;amp;x-x_1\end{bmatrix}\begin{bmatrix} Q_{11}&amp;amp;Q_{12}\\Q_{21}&amp;amp;Q_{22}\end{bmatrix}\begin{bmatrix} y_2-y\\y-y_1\end{bmatrix}\]&lt;/span> 在CV计算中，&lt;span class="math">\(x_2-x_1,y_2-y_1\)&lt;/span>经常为1（相邻像素点），因此可直接写成： &lt;span class="math">\[f(x,y)=\begin{bmatrix} x_2-x&amp;amp;x-x_1\end{bmatrix}\begin{bmatrix} Q_{11}&amp;amp;Q_{12}\\Q_{21}&amp;amp;Q_{22}\end{bmatrix}\begin{bmatrix} y_2-y\\y-y_1\end{bmatrix}\]&lt;/span>&lt;/p>
&lt;h3 id="多项式插值">多项式插值&lt;/h3>
&lt;p>给定（n+1）个数据点&lt;span class="math">\((x_i,y_i),i\in\{0,...,n\}\)&lt;/span>，且&lt;strong>任意两个&lt;/strong>&lt;span class="math">\(x_i\)&lt;/span>都不相等。可以用一个p阶多项式&lt;span class="math">\((0\leq p \leq n)\)&lt;/span>进行插值，使： &lt;span class="math">\[p(x_i)=y_i,0 \leq i \leq n\]&lt;/span> &lt;span class="math">\(p(x)\)&lt;/span>具体可以写为： &lt;span class="math">\[p(x)=a_nx^n+a_{n-1}x^{n-1}+\dotsb+a_1x+a_0\]&lt;/span> 求解系数&lt;span class="math">\(a_i\)&lt;/span>这是一个n+1元一次方程组,写成范德蒙矩阵为： &lt;span class="math">\[\begin{bmatrix}x_0^n&amp;amp;x_0^{n-1}&amp;amp;\dotsb&amp;amp;x_0&amp;amp;1\\
x_1^n&amp;amp;x_1^{n-1}&amp;amp;\dotsb&amp;amp;x_1&amp;amp;1\\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots&amp;amp;\vdots\\
x_n^n&amp;amp;x_n^{n-1}&amp;amp;\dotsb&amp;amp;x_n&amp;amp;1\\
\end{bmatrix}
\begin{bmatrix}a_n\\
a_{n-1}\\
\vdots\\
a_0\\
\end{bmatrix}=
\begin{bmatrix}y_0\\
y_1\\
\vdots\\
y_n\\
\end{bmatrix}
\]&lt;/span> 可以通过标准方式来求解系数&lt;span class="math">\(a_i\)&lt;/span>，由于在范德蒙矩阵中，如果&lt;span class="math">\(\forall x_i,x_j\)&lt;/span>两两不同，则范德蒙矩阵的行列式不为0，可以获得&lt;strong>唯一解&lt;/strong>。以后的无论是牛顿法还是Lagrange插值法得到的结果和这种方式解出来是一样的，只是加快了计算。&lt;/p>
&lt;h3 id="lagrange插值">Lagrange插值&lt;/h3>
&lt;p>同样，给点（n+1）个数据点&lt;span class="math">\((x_i,y_i),i\in\{0,...,n\}\)&lt;/span>，且&lt;strong>任意两个&lt;/strong>&lt;span class="math">\(x_i\)&lt;/span>都不相等，可以通过Lagrange插值公式得到多项式，其可以写成： &lt;span class="math">\[L(x)=\sum_{i=0}^{n}l_i(x)y_i\]&lt;/span> 其中，&lt;span class="math">\(l_i(x)\)&lt;/span>为Lagrange插值系数，表达式为： &lt;span class="math">\[l_i(x)=\prod_{0 \leq j \leq n,j\neq i}\frac{x-x_j}{x_i-x_j}\]&lt;/span> 从表达式中，我们不难得到以下性质：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(i \neq m\)&lt;/span>，则&lt;span class="math">\(l_i(x_m)=0\)&lt;/span>，因为&lt;span class="math">\(\exist x_m==x_j,\)&lt;/span>使得分子为0&lt;/li>
&lt;li>&lt;span class="math">\(i=m\)&lt;/span>，则&lt;span class="math">\(l_i(x_m)=1\)&lt;/span>，因为分子和分母完全相同。&lt;/li>
&lt;li>&lt;span class="math">\(L(x)\)&lt;/span>的阶数最大为n。&lt;span class="math">\(\prod_{0 \leq j \leq n,j\neq i}\frac{x-x_j}{x_i-x_j}\)&lt;/span>一共有n个x相乘。&lt;/li>
&lt;li>插值多项式是唯一的，和范德蒙矩阵求出来的是一样的。&lt;/li>
&lt;/ol>
&lt;h2 id="回归算法">回归算法&lt;/h2>
&lt;h3 id="线性回归">线性回归&lt;/h3>
&lt;p>线性回归使用一个线性函数（一次函数），调节各个参数，使它尽量满足数据误差最小。 其他笔记中写过，根据最小方差准则计算可逆矩阵回归参数的公式为： &lt;span class="math">\[\vec{\alpha}=(X^TX)^{-1}X^T\vec{y}\]&lt;/span>&lt;/p>
&lt;h3 id="多项式回归">多项式回归&lt;/h3>
&lt;p>给定n个数据点&lt;span class="math">\((x_i,y_i),i\in\{1,\dots,n\}\)&lt;/span>，可以建立一个m阶多项式来预测&lt;span class="math">\(y\)&lt;/span>的值，其推到公式如下： &lt;span class="math">\[y=\beta_0+\beta_1 x+\beta_2 x^2+\dotsb+\beta_m x^m+\epsilon\]&lt;/span> 对于每一个二元点有： &lt;span class="math">\[y_i=\beta_0+\beta_1 x_i+\beta_2 x_i^2+\dotsb+\beta_m x_i^m+\epsilon_i,1 \leq i \leq n\]&lt;/span> 如果把这些点组合成矩阵形式，则有： &lt;span class="math">\[\begin{bmatrix}y_0\\
y_1\\
\vdots\\
y_n\\
\end{bmatrix}=
\begin{bmatrix}1&amp;amp;x_1&amp;amp;\dotsb&amp;amp;x_1^m\\
1&amp;amp;x_2&amp;amp;\dotsb&amp;amp;x_2^m\\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\
1&amp;amp;x_n&amp;amp;\dotsb&amp;amp;x_n^m\\
\end{bmatrix}
\begin{bmatrix}\beta_0\\
\beta_1\\
\vdots\\
\beta_m\\
\end{bmatrix}+
\begin{bmatrix}\epsilon_0\\
\epsilon_1\\
\vdots\\
\epsilon_m\\
\end{bmatrix}
\]&lt;/span> 即&lt;span class="math">\(\vec{y}=X\vec{\beta}+\vec{\epsilon}\)&lt;/span>，根据最小方差估计，同线性回归可得多项式回归的系数： &lt;span class="math">\[\vec{\beta}=(X^TX)^{-1}X^T\vec{y}(m&amp;lt;n)\]&lt;/span>&lt;/p>
&lt;h3 id="其他">其他&lt;/h3>
&lt;p>其他回归算法如脊回归，Log回归等有专门笔记记录。&lt;/p>
&lt;h2 id="内插法外推法曲线拟合及回归">内插法、外推法、曲线拟合及回归&lt;/h2>
&lt;p>&lt;strong>内插法&lt;/strong>求解以下的问题：有一未知函数在一些特定位置下的值，求未知函数在已知数值的点之间某一点的值。&lt;/p>
&lt;p>&lt;strong>外推法&lt;/strong>类似内插法，但需要知道数值的点是在其他已知数值点的范围以外。一般而言外推法的误差会大于内插法。&lt;/p>
&lt;p>&lt;strong>曲线拟合&lt;/strong>是在已知一些数据的条件下，找到一条曲线完全符合现有的数据，数据可能是一些特定位置及其对应的值，也可能是其他资料，例如角度或曲率等。&lt;/p>
&lt;p>&lt;strong>回归分析&lt;/strong>类似曲线拟合，也是根据一些特定位置及其对应的值，要找到对应曲线。但回归分析考虑到数据可能有误差，因此所得的的曲线不需要和数据完全符合。一般会使用最小方差法来进行回归分析。[1]&lt;/p>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>[1] 数值分析，维基百科&lt;a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90">https://zh.wikipedia.org/wiki/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90&lt;/a>&lt;/p>
&lt;p>[2] 多项式插值算法与回归算法.张戎.知乎.&lt;a href="https://zhuanlan.zhihu.com/p/33690607">https://zhuanlan.zhihu.com/p/33690607&lt;/a>&lt;/p></description></item><item><title>概率统计随机过程之马尔可夫过程转移矩阵（随机矩阵）</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E8%BD%AC%E7%A7%BB%E7%9F%A9%E9%98%B5%E9%9A%8F%E6%9C%BA%E7%9F%A9%E9%98%B5/</link><pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E8%BD%AC%E7%A7%BB%E7%9F%A9%E9%98%B5%E9%9A%8F%E6%9C%BA%E7%9F%A9%E9%98%B5/</guid><description>
&lt;h2 id="概率统计随机过程之转移矩阵随机矩阵">概率统计随机过程之转移矩阵（随机矩阵）&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注：本文说的随机矩阵是转移矩阵，英文叫stochastic matrix不是random matrix。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#极限行为和不变概率稳态">极限行为和不变概率（稳态）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#状态分类">状态分类&lt;/a>&lt;/li>
&lt;li>&lt;a href="#周期性与非周期">周期性与非周期&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可约与不可约">可约与不可约&lt;/a>&lt;/li>
&lt;li>&lt;a href="#不可约非周期链遍历链">不可约非周期链→遍历链&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可约性非常返态和周期的影响">可约性、非常返态和周期的影响&lt;/a>&lt;/li>
&lt;li>&lt;a href="#随机矩阵的主特征1的简单证明">随机矩阵的主特征1的简单证明&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从线性代数的角度看马尔可夫矩阵">从线性代数的角度看马尔可夫矩阵&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="极限行为和不变概率稳态">极限行为和不变概率（稳态）&lt;/h2>
&lt;p>有转移矩阵&lt;span class="math">\(P\)&lt;/span>，但经过&lt;span class="math">\(N→ ∞\)&lt;/span>后，是否存在一个收敛的极限矩阵 &lt;span class="math">\[\Pi=\lim_{n→ ∞}P^n\]&lt;/span> 如果能有一个更好的性质： &amp;gt;极限矩阵每一行都是一样的。&lt;/p>
&lt;p>那么对于任一个概率向量&lt;span class="math">\(\vec v\)&lt;/span>(每一项非负且和为1)，都有： &lt;span class="math">\[\vec \pi=\vec v\lim_{n→ ∞}P^n\]&lt;/span> 则&lt;span class="math">\(\vec \pi\)&lt;/span>为不变概率分布或稳态概率分布。这个时候，&lt;span class="math">\(\Pi\)&lt;/span>的值应该是 &lt;span class="math">\[\Pi=\lim_{n→ ∞}P^n=\begin{bmatrix}
\vec \pi\\
\vec \pi\\
\vdots\\
\vec \pi
\end{bmatrix}\]&lt;/span> 因为向量左乘是行线性组合，&lt;span class="math">\(\vec \pi\)&lt;/span>之间的任何线性组合都是&lt;span class="math">\(\vec \pi\)&lt;/span>.&lt;/p>
&lt;p>那么什么样的矩阵能有稳态分布吧?&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>主特征值为1，且为单重（1）&lt;/li>
&lt;li>能选出一个非负的左特征向量（2）&lt;/li>
&lt;/ol>
&lt;p>随机矩阵的主特征值为1这点可以证明见章节&lt;a href="#随机矩阵的主特征1的简单证明">随机矩阵的主特征1的简单证明&lt;/a>。但是主特征值为单重和非负左特征向量需要更高等的数学技巧。我们用Perron–Frobenius theorem先给出一部分符合两点要求的矩阵。&lt;/p>
&lt;p>Perron–Frobenius theorem给出了一系列符合这两点的矩阵：&lt;/p>
&lt;blockquote>
&lt;p>定理1：Perron–Frobenius theorem：&lt;strong>所有元素均为正实数&lt;/strong>的正矩阵的基本性质的重要定理.该定理断言:若矩阵&lt;span class="math">\(A&amp;gt;0，ρ(A)为A\)&lt;/span>的谱半径，则:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(ρ(A)\)&lt;/span>为&lt;span class="math">\(A\)&lt;/span>的正特征值,其对应的一个特征向量为正特征向量。&lt;/li>
&lt;li>对于&lt;span class="math">\(A\)&lt;/span>的任意其他特征值，都有&lt;span class="math">\(|λ|&amp;lt;ρ(A)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(ρ(A)是A\)&lt;/span>的单重特征值。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>尽管Perron–Frobenius theorem给出了一系列矩阵，但是却不能覆盖所有具有适当极限性质的随机矩阵。因为随机矩阵的要求是非负，而不是全是正数。尽管随机矩阵&lt;span class="math">\(P\)&lt;/span>的元素不全是正数，但是我们注意到，&lt;span class="math">\(P^n\)&lt;/span>可能是全正的。而且根据矩阵的特征值分解，&lt;span class="math">\(P^n\)&lt;/span>的特征值为&lt;span class="math">\(P\)&lt;/span>特征值的n次方，且特征向量不变。而&lt;span class="math">\(P\)&lt;/span>主特征值1的任意次方都是1，所以只要&lt;span class="math">\(P^n\)&lt;/span>满足了Perron–Frobenius theorem，那么&lt;span class="math">\(P\)&lt;/span>也满足。&lt;/p>
&lt;p>鉴于此，我们给出以下结论：&lt;/p>
&lt;blockquote>
&lt;p>结论1：如果&lt;span class="math">\(P\)&lt;/span>是一个随机矩阵，且对于某一n，&lt;span class="math">\(P^n\)&lt;/span>的所有元素都是正数，则&lt;span class="math">\(P\)&lt;/span>满足条件（1）（2）。&lt;/p>
&lt;/blockquote>
&lt;h2 id="状态分类">状态分类&lt;/h2>
&lt;p>那么到底什么样的矩阵，当&lt;span class="math">\(n\)&lt;/span>充分大时，&lt;span class="math">\(P^n\)&lt;/span>的元素全都是正的呢？答案是&lt;strong>遍历的&lt;/strong>随机矩阵。遍历的随机矩阵是&lt;strong>不可约、非周期&lt;/strong>的矩阵。几个不能符合（1）（2）的矩阵包括：可约的，随机矩阵不收敛（周期性），有非常返态的（因为非常返态导致矩阵可约）。&lt;/p>
&lt;p>下面对几个概念进行介绍。&lt;/p>
&lt;h3 id="周期性与非周期">周期性与非周期&lt;/h3>
&lt;p>一种直观情况：如果有正整数&lt;span class="math">\(d,d&amp;gt;1\)&lt;/span>，只有当&lt;span class="math">\(n=d,2d,3d,…\)&lt;/span>时，有&lt;span class="math">\(p(n)_{ij}&amp;gt;0\)&lt;/span>，则状态i为&lt;strong>周期性的状态&lt;/strong>，周期为&lt;span class="math">\(d\)&lt;/span>。&lt;/p>
&lt;p>周期性所说不直观的定义：若集合&lt;span class="math">\(\{n: n≥1, p_{ii}&amp;gt; 0\}\)&lt;/span>非空，则称它的最大公约数，&lt;span class="math">\(d=d(i)\)&lt;/span>为状态&lt;span class="math">\(i\)&lt;/span>的周期。若&lt;span class="math">\(d&amp;gt;1\)&lt;/span>，称&lt;span class="math">\(i\)&lt;/span>是周期的;若&lt;span class="math">\(d=1\)&lt;/span>,称 &lt;span class="math">\(i\)&lt;/span>是非周期的。（引用：《应用随机过程（张波）》，第5章）&lt;/p>
&lt;p>我当初也纳闷，定义中取公约数和周期没啥关系，然而，这才是更加完善的周期。见下图（《应用随机过程（张波）》，图5-3）&lt;/p>
&lt;p>&lt;img src="../images/马尔可夫链周期性.jpg" alt="马尔可夫链周期性" /> 由状态1出发返回状态1的步长为&lt;span class="math">\(T=\{4,6,8,10,12,...\}\)&lt;/span>，很直观的感受到，4步之后就是两步两步的间隔了。&lt;/p>
&lt;p>定义周期：方案一是取4，但是会丢失了&lt;span class="math">\(\{6,10,14...\}\)&lt;/span>这些步长；方案二就是取公约数2，就可以防止&lt;span class="math">\(\{6,10,14...\}\)&lt;/span>步长的丢失，弊端也许就是不如&lt;span class="math">\(\{d,2d,3d...\}\)&lt;/span>这种结构来得直观。（有点像&lt;span class="math">\(y=ax+b，a\)&lt;/span>是周期）。&lt;/p>
&lt;p>补充：当马尔可夫链中&lt;strong>任意一个状态都为周期性&lt;/strong>的状态时，则称马尔可夫链是周期的。但&lt;strong>非周期性的马尔可夫链才是我们想要的&lt;/strong>，它是构成遍历的马尔可夫链的必要条件，遍历的马尔可夫有很多好的性质。&lt;/p>
&lt;blockquote>
&lt;p>定理2：如果状态&lt;span class="math">\(i\)&lt;/span>和&lt;span class="math">\(j\)&lt;/span>互通，那么&lt;span class="math">\(d(i)=d(j)\)&lt;/span>（互通类中各状态周期相等。）&lt;/p>
&lt;/blockquote>
&lt;p>非周期定义：如果链从所有任意起点返回到自己所需要的步数集的最大公约数为1，则说明此链是非周期的，即所有状态不是定期返回的且所有返回的间隔的最大公约数为1。但是并不是说，返回自己的状态只需要一步，而是说，对于某个步数&lt;span class="math">\(M\)&lt;/span>，所有步数n，满足&lt;span class="math">\(n≥M\)&lt;/span>，都能够返回原状态。&lt;/p>
&lt;blockquote>
&lt;p>如何判别一个状态是非周期的？&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>若此状态带有自环，则必为非周期的（虽然非周期的状态不一定有自环）&lt;/li>
&lt;li>若此状态与一个非周期的状态互通，则必为非周期的。（定理2应用）&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>以上&lt;strong>只是两个充分&lt;/strong>条件。&lt;/p>
&lt;h3 id="可约与不可约">可约与不可约&lt;/h3>
&lt;p>互通：如果马尔可夫链里某状态&lt;span class="math">\(i\)&lt;/span>经过&lt;span class="math">\(n\)&lt;/span>步到达状态&lt;span class="math">\(j\)&lt;/span>的概率&lt;span class="math">\(&amp;gt;0\)&lt;/span>且相反的，状态&lt;span class="math">\(j\)&lt;/span>经过&lt;span class="math">\(n\)&lt;/span>步到&lt;span class="math">\(i\)&lt;/span>的概率也&lt;span class="math">\(&amp;gt;0\)&lt;/span>，即说明这俩状态是互通的. 如果对所有&lt;span class="math">\(n\)&lt;/span>上面都不成立，则证明&lt;span class="math">\(i,j\)&lt;/span>不通。&lt;/p>
&lt;p>可约的（reducible），意思就是在状态集中至少存在一个状态&lt;span class="math">\(i\)&lt;/span>到状态&lt;span class="math">\(j\)&lt;/span>是不通的！注意有可能&lt;span class="math">\(j\)&lt;/span>到&lt;span class="math">\(i\)&lt;/span>是通的，互通是一个相互的概念。&lt;span class="math">\(i→j\)&lt;/span>同时&lt;span class="math">\(j→i\)&lt;/span>才是互通。可约就是存在不互通的状态。&lt;/p>
&lt;p>不可约（irreducible），意思是马尔可夫链链从状态空间里一个位置经过有限步跳转到任何一个位置的概率大于0，即说的是马氏链里任何两个状态都是互通的。&lt;/p>
&lt;h3 id="不可约非周期链遍历链">不可约非周期链→遍历链&lt;/h3>
&lt;p>现在我们来证明一个非周期、不可约的转移矩阵满足结论（1），即 &amp;gt;如果&lt;span class="math">\(P\)&lt;/span>是非周期、不可约的转移矩阵(遍历的)，那么存在&lt;span class="math">\(M&amp;gt;0,\forall n≥M\)&lt;/span>，有&lt;span class="math">\(P^n\)&lt;/span>&amp;gt;0(每一个元素大于0)。&lt;span class="math">\(\Leftrightarrow\)&lt;/span>存在稳态概率&lt;span class="math">\(\vec\pi\)&lt;/span>的条件。&lt;/p>
&lt;p>证明：&lt;/p>
&lt;p>取任意&lt;span class="math">\(i,j\)&lt;/span>，由于矩阵&lt;span class="math">\(P\)&lt;/span>不可约，所以&lt;span class="math">\(i,j\)&lt;/span>属于同一个连通类。因此存在一个数，记为&lt;span class="math">\(m_{ij}\)&lt;/span>，使得&lt;span class="math">\(p_{m_{ij}}(i,j)&amp;gt;0\)&lt;/span>，即&lt;span class="math">\(i\)&lt;/span>通过&lt;span class="math">\(m_{ij}\)&lt;/span>次转移后到达&lt;span class="math">\(j\)&lt;/span>。&lt;/p>
&lt;p>此外，由于&lt;span class="math">\(P\)&lt;/span>是非周期的，因此存在一个&lt;span class="math">\(M_i\)&lt;/span>，使得所有&lt;span class="math">\(n≥M_i\)&lt;/span>，有&lt;span class="math">\(p_n(i,i)&amp;gt;0\)&lt;/span>（非周期的性质）。因此对所有的n≥M_i&lt;span class="math">\(，有 \)&lt;/span>&lt;span class="math">\(p_{n+m_{ij}}(i,j)≥p_n(i,i)p_{m_{ij}}(i,j)&amp;gt;0\)&lt;/span>$ 令&lt;span class="math">\(M\)&lt;/span>为所有&lt;span class="math">\(i,j\)&lt;/span>组合中&lt;span class="math">\(M_i+m_{ij}\)&lt;/span>的最大值（现在讨论有限状态马氏链，因此最大值存在），则对于所有&lt;span class="math">\(n≥M\)&lt;/span>，以及任意的&lt;span class="math">\(i,j\)&lt;/span>都有 &lt;span class="math">\[p_{n}(i,j)&amp;gt;0.\]&lt;/span> 因此，由&lt;span class="math">\(i,j\)&lt;/span>的任意性可知：存在&lt;span class="math">\(n\)&lt;/span>，使得&lt;span class="math">\(P^n\)&lt;/span>中每一项大于0。&lt;/p>
&lt;p>如果我们用更加数学的语言描述，将其归纳成定理：&lt;/p>
&lt;blockquote>
&lt;p>定理3：如果&lt;span class="math">\(P\)&lt;/span>是非周期、不可约的马尔可夫链转移矩阵，则存在一个唯一一个不变（稳态）概率向量&lt;span class="math">\(\vec\pi\)&lt;/span>，满足 &lt;span class="math">\[\vec\pi P=\vec\pi(这不是特征向量的转置吗？)\]&lt;/span> 如果&lt;span class="math">\(\phi_0\)&lt;/span>为任一初始概率向量，则有 &lt;span class="math">\[\lim_{n→ ∞}\phi_0P^n=\vec\pi\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这也给我们求不变概率提示了一个更简单的方法，求转置矩阵&lt;span class="math">\(P^T\)&lt;/span>特征值为1时的特征向量，并要求次特征向量1-范数为1。（矩阵的特征值在转置变换中不改变，转置矩阵与原矩阵是相似的。）&lt;/p>
&lt;h3 id="可约性非常返态和周期的影响">可约性、非常返态和周期的影响&lt;/h3>
&lt;p>可约性：可以当成几个不可约的子矩阵处理。&lt;/p>
&lt;p>非常返态：初始概率会对最终稳态概率有影响。&lt;/p>
&lt;p>周期：绝对值为1的特征值不只1个，周期是几，就有几个&lt;strong>不同&lt;/strong>的绝对值为1的复根。同样，周期是多少，就有多少个不同的状态组，每个状态组都有其极限值。有平均稳态，是各个状态组极限稳态概率的平均数。&lt;/p>
&lt;h2 id="随机矩阵的主特征1的简单证明">随机矩阵的主特征1的简单证明&lt;/h2>
&lt;p>按照定义&lt;span class="math">\(A*[1,1,1,...1,1,1]^T\)&lt;/span>得到一个列向量。&lt;/p>
&lt;p>而&lt;span class="math">\(A\)&lt;/span>是一个随机矩阵(这里以行的和为1为例)，即&lt;span class="math">\(A\)&lt;/span>的每一行和为1，&lt;span class="math">\(A*[1,1,1,...1,1,1]^T\)&lt;/span>的每一个元素按照定义展开，正好是A的每一行的和，即是1，所以&lt;/p>
&lt;p>&lt;span class="math">\(A*[1,1,1,...1,1,1]^T=[1,1,1,...1,1,1]^T\)&lt;/span>。按照特征值的定义，即得到一个特征值为1。&lt;/p>
&lt;p>然后你还要证明1是最大的特征值。&lt;/p>
&lt;p>矩阵的1范数是1（行和范数），而根据范数不等式，任何范数都大于等于谱半径（=最大特征值），所以该矩阵的最大特征值不大于1，而，已知有一个特征值为1，所以最大特征值就是1。&lt;/p>
&lt;p>（其实严谨点还需要证明这个最大特征值即谱半径的重数为1）&lt;/p>
&lt;p>严谨证明请看：&lt;a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem#Non-negative_matrices">Perron–Frobenius theorem&lt;/a>&lt;/p>
&lt;h2 id="从线性代数的角度看马尔可夫矩阵">从线性代数的角度看马尔可夫矩阵&lt;/h2>
&lt;p>如果我们从线性代数的角度看马尔可夫矩阵，会有一个整体的视角，获得更直观的认识。&lt;/p>
&lt;p>我们如果通过特征基来表示马尔可夫矩阵&lt;span class="math">\(A\)&lt;/span>，有： &lt;span class="math">\[
Av=c_1\lambda_1 x_1+c_2\lambda_2 x_2+\dotsb+c_n\lambda_n x_n\\
v=c_1x_1+c_2x_2+\dotsb+c_nx_n
\]&lt;/span> 其中，&lt;span class="math">\(\lambda_i\)&lt;/span>是矩阵&lt;span class="math">\(A\)&lt;/span>的特征值，且我们将其按从大到小顺序排好，向量&lt;span class="math">\(x_i\)&lt;/span>是对应的特征矩阵，我们将任意向量&lt;span class="math">\(v\)&lt;/span>用特征基表示，其系数为&lt;span class="math">\(c_i\)&lt;/span>。&lt;/p>
&lt;p>那么，初始状态向量经过&lt;span class="math">\(k\)&lt;/span>个周期的马尔可夫矩阵为： &lt;span class="math">\[v_k=A^kv=c_1\lambda_1^k x_1+c_2\lambda_2^k x_2+\dotsb+c_n\lambda_n^k x_n\]&lt;/span> 如果想让&lt;span class="math">\(v_k\)&lt;/span>趋于稳态，那么特征值的&lt;span class="math">\(|\lambda_i|\)&lt;/span>都必须小于等于1，否则矩阵就会发散。&lt;/p>
&lt;p>我们再看行列式&lt;span class="math">\(\det(A-I)\)&lt;/span>，由于&lt;span class="math">\(A\)&lt;/span>的每一列和为1，当对角元素减1后，列和必然为0，因此&lt;span class="math">\(A-I\)&lt;/span>的行向量是线性相关的，即&lt;span class="math">\(A-I\)&lt;/span>是奇异矩阵，所以&lt;span class="math">\(\det(A-I)=0\)&lt;/span>，这使得1是&lt;span class="math">\(A\)&lt;/span>的一个特征值。&lt;/p>
&lt;p>设特征值&lt;span class="math">\(\lambda_1\sim\lambda_i\)&lt;/span>为1，剩下的小于1，那么当&lt;span class="math">\(k\rightarrow ∞\)&lt;/span>时，有&lt;span class="math">\(\lambda_1^k,\dotsb,\lambda_i^k=1,\lambda_{i+1},\dotsb,\lambda_n\rightarrow 0\)&lt;/span>，此时 &lt;span class="math">\[
v_k=A^kv=c_1 x_1+c_2 x_2+\dotsb+c_ix_i
\]&lt;/span> 即为稳态。&lt;/p></description></item><item><title>测度论-Borel集的作用？意义？它为什么重要？</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA-borel%E9%9B%86%E7%9A%84%E4%BD%9C%E7%94%A8%E6%84%8F%E4%B9%89%E5%AE%83%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%8D%E8%A6%81/</link><pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA-borel%E9%9B%86%E7%9A%84%E4%BD%9C%E7%94%A8%E6%84%8F%E4%B9%89%E5%AE%83%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%8D%E8%A6%81/</guid><description>
&lt;h2 id="borel-集的作用意义它为什么重要">Borel 集的作用？意义？它为什么重要？&lt;!-- omit in toc -->&lt;/h2>
&lt;p>转载自知乎：&lt;a href="https://www.zhihu.com/question/33991971">https://www.zhihu.com/question/33991971&lt;/a>&lt;/p>
&lt;p>我来从概率论的角度说一下Borel集的意义吧。&lt;/p>
&lt;p>首先回忆一下概率空间的定义，概率空间是一个三元组，样本空间、事件的集合、概率。&lt;/p>
&lt;p>我们的概率函数是定义在事件上的，而不是定义在样本空间上的。&lt;/p>
&lt;p>概率的定义必须满足几个性质，也就是Kolmogorov公理：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Kolmogorov公理.jpg" alt="Kolmogorov公理" />&lt;p class="caption">Kolmogorov公理&lt;/p>
&lt;/div>
&lt;p>当样本空间离散的时候，概率非常容易定义，只要分配给每一个结果一个概率值，其和等于1就好了。这个时候我们可以&lt;strong>定义概率的所有事件的集合&lt;/strong>就是样本空间的全部子集，没毛病。&lt;/p>
&lt;p>但是当样本空间不是离散的，比如说是实轴R，那么概率的定义就比较麻烦了。我们发现，&lt;strong>并不是R上的所有子集（事件）都可以被定义上概率&lt;/strong>，比如：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/勒贝格不可测集例6.png" alt="勒贝格不可测集例6" />&lt;p class="caption">勒贝格不可测集例6&lt;/p>
&lt;/div>
&lt;div class="figure">
&lt;img src="../images/勒贝格不可测集.jpg" alt="勒贝格不可测集" />&lt;p class="caption">勒贝格不可测集&lt;/p>
&lt;/div>
&lt;p>看，我们发现，如果严格按照概率函数的定义，并不是R上的所有子集（事件）都能被定义概率的。那么我们能不能把那些能定义概率的集合（事件）挑出来只对他们进行研究呢？&lt;/p>
&lt;p>当然可以。但是挑出来的这些事件的集合必须有一些要求，比如：&lt;/p>
&lt;p>空集必须在这个事件的集合里面 如果我们关心某个事件A，那么「事件A不发生」，也就是A的补集也必须在这个事件的集合里面 如果我们关心一系列事件，那么这些事件同时发生、至少有一个发生的概率也必须能被研究。 这就诞生了所谓的σ-代数的概念。&lt;/p>
&lt;p>那么我们怎么在R上定义概率呢？这个时候我们需要引入分布函数了：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/分布函数.jpg" alt="分布函数" />&lt;p class="caption">分布函数&lt;/p>
&lt;/div>
&lt;p>有了分布函数，我们可以先在区间上定义概率：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/定义概率.jpg" alt="定义概率" />&lt;p class="caption">定义概率&lt;/p>
&lt;/div>
&lt;p>然后通过所谓的内测度外测度定义出一些集合的概率函数。R上所有的区间所生成的最小σ-代数就是Borel σ-代数，Borel σ-代数的元素叫做Borel集。我们发现，刚好所有的Borel集都是可以被定义上概率的，而且是最符合我们直觉的（除非特意构造，碰到的多数集合都是Borel集），所以我们就干脆只研究Borel集的概率算了。&lt;/p>
&lt;p>所以它为什么重要？因为通过它，我们排除了那些不好的集合，限制了我们讨论的范围，把我们的问题简单化了。&lt;/p></description></item><item><title>测度论6之可测函数收敛性</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA6%E4%B9%8B%E5%8F%AF%E6%B5%8B%E5%87%BD%E6%95%B0%E6%94%B6%E6%95%9B%E6%80%A7/</link><pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA6%E4%B9%8B%E5%8F%AF%E6%B5%8B%E5%87%BD%E6%95%B0%E6%94%B6%E6%95%9B%E6%80%A7/</guid><description>
&lt;h2 id="可测函数收敛性">可测函数收敛性&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#回顾函数列逐点收敛和一致收敛">回顾函数列逐点收敛和一致收敛&lt;/a>&lt;/li>
&lt;li>&lt;a href="#逐点收敛和一致收敛定义与区别">逐点收敛和一致收敛定义与区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一个例子逐点收敛却不一致收敛">一个例子逐点收敛却不一致收敛&lt;/a>&lt;/li>
&lt;li>&lt;a href="#依测度收敛converge-in-measure">依测度收敛（Converge in measure）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#依测度收敛性质">依测度收敛性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几乎处处收敛几乎一致收敛依测度收敛的关系">几乎处处收敛、几乎一致收敛、依测度收敛的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#依测度柯西序列cauchy-sequence-in-measure">依测度柯西序列（Cauchy sequence in measure）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#证明附录">证明附录&lt;/a>&lt;/li>
&lt;li>&lt;a href="#依测度收敛即为几乎处处实值">依测度收敛即为几乎处处实值&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="回顾函数列逐点收敛和一致收敛">回顾函数列逐点收敛和一致收敛&lt;/h2>
&lt;h3 id="逐点收敛和一致收敛定义与区别">逐点收敛和一致收敛定义与区别&lt;/h3>
&lt;p>逐点收敛也称点态收敛，（英语：pointwise convergence，或称简单收敛），是数学中描述一组函数序列向一个函数趋近的一种方式（函数趋近极限有其他不同方式，个中差异请小心分辨）。详细点讲，如果这组函数叙列在定义域中每点的取值都会趋于一个极限值，这时可以&lt;strong>用每点的极限来定义这组函数序列的极限函数&lt;/strong>，被趋近的这个极限函数称作这个函数叙列的逐点极限。&lt;/p>
&lt;blockquote>
&lt;p>定义1：(逐点收敛)极限函数：设&lt;span class="math">\(\{f_{n}\}\)&lt;/span>是一组&lt;strong>有相同定义域&lt;/strong>的函数序列。序列&lt;span class="math">\(\{f_{n}\}\)&lt;/span>&lt;strong>（逐点）收敛&lt;/strong>当且仅当存在函数&lt;span class="math">\(f\)&lt;/span>，使得在定义域中的每点&lt;span class="math">\(x\)&lt;/span>，都有： &lt;span class="math">\[\lim _{{n\rightarrow \infty }}f_{n}(x)=f(x)\]&lt;/span> 这时我们就说序列&lt;span class="math">\(\{f_{n}\}\)&lt;/span>（逐点）收敛到&lt;span class="math">\(f\)&lt;/span>，或说函数&lt;span class="math">\(f\)&lt;/span>是序列&lt;span class="math">\(f_{n}\)&lt;/span>的（逐点收敛）极限函数。可记为&lt;span class="math">\(f_n→f \ pointwise\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>逐点收敛并不能保证函数列中函数的一些性质比如连续性。&lt;/p>
&lt;blockquote>
&lt;p>定义2：一致收敛极限函数：让&lt;span class="math">\(\{f_{n}\}_{n\in \mathbb {N} }\)&lt;/span>是定义在&lt;span class="math">\(S\)&lt;/span>上，值域为&lt;span class="math">\(\mathbb {R}\)&lt;/span>或&lt;span class="math">\(\mathbb {C}\)&lt;/span>的一组函数序列，若序列 &lt;span class="math">\(\{f_{n}\}_{n\in \mathbb {N}}\)&lt;/span>&lt;strong>均匀收敛&lt;/strong>至函数&lt;span class="math">\(f\)&lt;/span>在集合&lt;span class="math">\(S\)&lt;/span>上，即表示对所有&lt;span class="math">\(\epsilon &amp;gt;0\)&lt;/span>，存在&lt;span class="math">\(N∈\mathbb{N}\)&lt;/span>，使得当所有&lt;span class="math">\(n\geq N\)&lt;/span>且 &lt;span class="math">\(x∈ S\)&lt;/span>时有 &lt;span class="math">\[|f_{n}(x)-f(x)|&amp;lt;\epsilon.\]&lt;/span> 这时我们就说序列&lt;span class="math">\(\{f_{n}\}\)&lt;/span>一致收敛(或均匀收敛)到&lt;span class="math">\(f\)&lt;/span>，或说函数&lt;span class="math">\(f\)&lt;/span>是序列&lt;span class="math">\(f_{n}\)&lt;/span>的一致连续极限函数。&lt;/p>
&lt;/blockquote>
&lt;p>一致收敛大致可想成：若函数序列&lt;span class="math">\(f_n\)&lt;/span>一致收敛至函数&lt;span class="math">\(f\)&lt;/span>，代表对所有定义域中的点&lt;span class="math">\(x\)&lt;/span>，&lt;span class="math">\(f_n(x)\)&lt;/span>收敛至&lt;span class="math">\(f(x)\)&lt;/span>会&lt;strong>有（大致）相同的收敛速度&lt;/strong>（所以才会用“均匀”或“一致”来形容这种模式的收敛）。由于它对收敛要求&lt;strong>较逐点收敛更强&lt;/strong>，故能保持一些重要的分析性质。&lt;/p>
&lt;p>一致收敛可保持&lt;span class="math">\(\Rightarrow\)&lt;/span>连续性、黎曼可积性、积分可交换性、微分可交换性（这点不一定需要一定限制）。&lt;/p>
&lt;p>注意到，一致收敛和逐点收敛定义的区别在于，在一致收敛中&lt;span class="math">\(N\)&lt;/span>的选取仅与&lt;span class="math">\(\epsilon\)&lt;/span>相关，而在逐点收敛中&lt;span class="math">\(N\)&lt;/span>还多了与点&lt;span class="math">\(x\)&lt;/span>相关。所以&lt;strong>一致收敛必定逐点收敛，而反之则不然&lt;/strong>。&lt;/p>
&lt;h3 id="一个例子逐点收敛却不一致收敛">一个例子逐点收敛却不一致收敛&lt;/h3>
&lt;p>函数序列&lt;span class="math">\(f_{n}:[0,1]\rightarrow [0,1]\)&lt;/span>，让&lt;span class="math">\(f_{n}(x)=x^{n}\)&lt;/span>，则&lt;span class="math">\(\{f_{n}\}\)&lt;/span>逐点收敛到（不连续）函数 &lt;span class="math">\[
f(x)={\begin{cases}0&amp;amp;x\in [0,1)\\1&amp;amp;x=1\end{cases}},
\]&lt;/span> 但显然&lt;span class="math">\(\{f_{n}\}\)&lt;/span>并不一致收敛到该函数。因为对每个&lt;span class="math">\(n\)&lt;/span>，&lt;span class="math">\(\sup\{\,\left|f_{n}(x)-f(x)\right|:x\in [0,1]\,\}\)&lt;/span> 皆为 1，所以 &lt;span class="math">\[
\lim_{n\rightarrow \infty }\,\sup\{\,\left|f_{n}(x)-f(x)\right|:x\in [0,1]\,\}=1\neq 0
\]&lt;/span> 这说明了序列&lt;span class="math">\(\{f_{n}\}\)&lt;/span>并不一致收敛。&lt;/p>
&lt;h2 id="依测度收敛converge-in-measure">依测度收敛（Converge in measure）&lt;/h2>
&lt;blockquote>
&lt;p>定义（依测度收敛）:&lt;strong>几乎处处实值&lt;/strong>的可测函数序列&lt;span class="math">\(\{f_n\}\)&lt;/span>称为&lt;strong>依测度收敛&lt;/strong>（convergent in measure），如果存在可测函数&lt;span class="math">\(f\)&lt;/span>，使得对任意&lt;span class="math">\(\varepsilon\)&lt;/span>,有 &lt;span class="math">\[
\lim_{n→∞}\mu(\{x:|f_n(x)-f(x)|≥\varepsilon\})=0
\]&lt;/span> 此时，我们就说&lt;span class="math">\(\{f_n\}\)&lt;/span>依测度收敛&lt;span class="math">\(f\)&lt;/span>，记作&lt;span class="math">\(f_n→f\)&lt;/span> in measure.&lt;/p>
&lt;/blockquote>
&lt;p>注：几乎处处实值指取值不是&lt;span class="math">\(\{-\infty,+\infty\}\)&lt;/span>。&lt;span class="math">\(f_n→f\)&lt;/span> in measure自动蕴含着(a)每个&lt;span class="math">\(f_n\)&lt;/span>都是几乎处处实值且可测的 (b) &lt;span class="math">\(f\)&lt;/span>是可测的。&lt;/p>
&lt;p>例子1 概率论中的依概率测度收敛，是随机变量比较强的收敛，强于依分布收敛。&lt;/p>
&lt;p>例子2 (处处不收敛但依测度收敛的例子)&lt;/p>
&lt;p>在Lebesgue测度空间&lt;span class="math">\((R,\mathcal{B},m)\)&lt;/span>上，取&lt;span class="math">\(E=(0,1]\)&lt;/span>。构造函数序列如下：&lt;/p>
&lt;p>第一步：将&lt;span class="math">\((0,1]\)&lt;/span>二等分，定义两个函数： &lt;span class="math">\[f_1^{(1)}=\begin{cases}
1,\quad x\in (0,\frac{1}{2}]\\0,\quad x\in(\frac{1}{2},1]
\end{cases},
f_2^{(1)}=\begin{cases}
0,\quad x\in (0,\frac{1}{2}]\\1,\quad x\in(\frac{1}{2},1]
\end{cases}
\]&lt;/span> 第二步：将&lt;span class="math">\((0,1]\)&lt;/span>四等分、八等分、。。。依次作下去，到第&lt;span class="math">\(n\)&lt;/span>次分割时，将$(0,1]分成&lt;span class="math">\(2^n\)&lt;/span>份，并定义&lt;span class="math">\(2^n\)&lt;/span>个函数，每个函数&lt;span class="math">\(f_j^{(n)}\)&lt;/span>定义为 &lt;span class="math">\[
f_j^{(n)}=\begin{cases}
1,\quad x\in (\frac{j-1}{2^n},\frac{j}{2^n}]\\0,\quad x\notin (\frac{j-1}{2^n},\frac{j}{2^n}]
\end{cases}
\]&lt;/span> 即每个函数只有一小部分区间&lt;span class="math">\((\frac{j-1}{2^n},\frac{j}{2^n}]\)&lt;/span>为1，其他区间为0。&lt;/p>
&lt;p>第三步：把函数&lt;span class="math">\(\{f_j^{(n)}\}(j=1,2,\dotsb,2^n)\)&lt;/span>按先上标再下标的顺序排列， &lt;span class="math">\[
f_1^{(1)},f_2^{(1)},f_1^{(2)},f_2^{(2)},f_3^{(2)},f_4^{(2)},\dotsb,f_1^{(n)},f_2^{(n)},\dotsb,f_{2^n}^{(n)},\dotsb
\]&lt;/span> 其中&lt;span class="math">\(f_j^{(n)}\)&lt;/span>在这个序列里是 第&lt;span class="math">\(2^n-2+j\)&lt;/span>个函数。&lt;/p>
&lt;p>显然，函数序列&lt;span class="math">\(\{f_j^{(n)}\}\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上不收敛于0，因为序列中总有某一段区间上其值为1；但该序列趋于&lt;span class="math">\(\infty\)&lt;/span>的情况下，值为1的区间测度趋于0，所以该序列&lt;strong>依Lebesgue测度&lt;/strong>&lt;span class="math">\(m\)&lt;/span>收敛于 0。&lt;/p>
&lt;p>详细解释如下：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/不收敛与依测度收敛.png" alt="不收敛与依测度收敛" />&lt;p class="caption">不收敛与依测度收敛&lt;/p>
&lt;/div>
&lt;h3 id="依测度收敛性质">依测度收敛性质&lt;/h3>
&lt;p>在测度空间&lt;span class="math">\((X,\mathcal{F},\mu)\)&lt;/span>下有以下性质：&lt;/p>
&lt;p>（1）&lt;span class="math">\(f_n→f\)&lt;/span> in measure &lt;span class="math">\(\Leftrightarrow \ f_n-f→0\)&lt;/span> in measure。证明是Trivial的。&lt;/p>
&lt;p>（2）&lt;span class="math">\(f_n→f\)&lt;/span> in measure &lt;span class="math">\(\Rightarrow \ f\)&lt;/span> is a.e. real-valued。证明见附录&lt;a href="#依测度收敛即为几乎处处实值">依测度收敛即为几乎处处实值&lt;/a>。&lt;/p>
&lt;h2 id="几乎处处收敛几乎一致收敛依测度收敛的关系">几乎处处收敛、几乎一致收敛、依测度收敛的关系&lt;/h2>
&lt;h2 id="依测度柯西序列cauchy-sequence-in-measure">依测度柯西序列（Cauchy sequence in measure）&lt;/h2>
&lt;h2 id="证明附录">证明附录&lt;/h2>
&lt;h3 id="依测度收敛即为几乎处处实值">依测度收敛即为几乎处处实值&lt;/h3>
&lt;blockquote>
&lt;p>&lt;span class="math">\(f_n→f\)&lt;/span> in measure &lt;span class="math">\(\Rightarrow \ f\)&lt;/span> is a.e. real-valued&lt;/p>
&lt;/blockquote>
&lt;p>证明：（提示：把&lt;span class="math">\(f\)&lt;/span>取&lt;span class="math">\(\plusmn\infty\)&lt;/span>的点分两种考虑：一种是&lt;span class="math">\(f_n\)&lt;/span>在这点取&lt;span class="math">\(\plusmn\infty\)&lt;/span>，于是&lt;span class="math">\(f\)&lt;/span>自然在这点取&lt;span class="math">\(\plusmn\infty\)&lt;/span>；另一种是&lt;span class="math">\(f_n\)&lt;/span>虽然在这点取有限值，但随着&lt;span class="math">\(n→\infty\)&lt;/span>， &lt;span class="math">\(f_n→f\)&lt;/span>趋于无穷大。）&lt;/p>
&lt;p>令 [公式] ,则 &lt;a href="测度的次可加性">公式&lt;/a>。又因为每个 [公式] 都是几乎处处实值的，故 [公式] 。因此 [公式] 。&lt;/p>
&lt;p>任取 [公式] ,对所有的 [公式] 都有：[公式] 因此[公式] 对所有的 [公式] 成立。 不等式的左边与 [公式] 无关；右边因为 [公式] ，故随着 [公式] 而趋于 [公式]&lt;/p>
&lt;p>那么我们取 [公式] ,就得到 [公式] ，这就说明了 [公式] 是几乎处处实值。&lt;/p></description></item><item><title>测度论之勒贝格积分稳定性</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA%E4%B9%8B%E5%8B%92%E8%B4%9D%E6%A0%BC%E7%A7%AF%E5%88%86%E7%A8%B3%E5%AE%9A%E6%80%A7/</link><pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA%E4%B9%8B%E5%8B%92%E8%B4%9D%E6%A0%BC%E7%A7%AF%E5%88%86%E7%A8%B3%E5%AE%9A%E6%80%A7/</guid><description>
&lt;h2 id="勒贝格积分稳定性">勒贝格积分稳定性&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="单调收敛定理">单调收敛定理&lt;/h2>
&lt;h2 id="fatou引理">Fatou引理&lt;/h2>
&lt;h2 id="勒贝格控制收敛定理">勒贝格控制收敛定理&lt;/h2>
&lt;h2 id="tonelli定理">Tonelli定理&lt;/h2>
&lt;h2 id="fubini定理">Fubini定理&lt;/h2></description></item><item><title>测度论4之Lebesgue--Stieltjes测度</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA4%E4%B9%8Blebesgue-stieltjes%E6%B5%8B%E5%BA%A6/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA4%E4%B9%8Blebesgue-stieltjes%E6%B5%8B%E5%BA%A6/</guid><description>
&lt;h2 id="lebesgue-stieltjes测度">Lebesgue-Stieltjes测度&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#定义lebesgue-stieltjes测度">定义Lebesgue-Stieltjes测度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从外测度到lebesgue-stieltjes测度">从外测度到Lebesgue-Stieltjes测度&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="定义lebesgue-stieltjes测度">定义Lebesgue-Stieltjes测度&lt;/h2>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(F\)&lt;/span>为&lt;span class="math">\(R\)&lt;/span>上的&lt;strong>右连续增函数&lt;/strong>, 则在&lt;span class="math">\(B(R)\)&lt;/span>有&lt;strong>唯一&lt;/strong>的&lt;span class="math">\(σ\)&lt;/span>有限测度&lt;span class="math">\(µ_F\)&lt;/span> 满足: &lt;span class="math">\[µ_F((a, b]) = F(b) − F(a), a, b ∈ R, a ≤ b.\]&lt;/span> &lt;span class="math">\(µ_F\)&lt;/span>称为由&lt;span class="math">\(F\)&lt;/span>决定的&lt;span class="math">\(R\)&lt;/span>上的&lt;strong>Lebesgue-Stieltjes测度&lt;/strong>.&lt;/p>
&lt;p>特殊的，如果&lt;span class="math">\(F(x)=x\)&lt;/span>，则为Lebesgue测度。&lt;/p>
&lt;/blockquote>
&lt;h2 id="从外测度到lebesgue-stieltjes测度">从外测度到Lebesgue-Stieltjes测度&lt;/h2>
&lt;p>令&lt;span class="math">\(X=\mathbb{R}\)&lt;/span>，令&lt;span class="math">\(\mathcal{C}\)&lt;/span>是由&lt;strong>左开右闭&lt;/strong>区间&lt;span class="math">\((a,b]\)&lt;/span>构成的集合系 (其实就是半环)。&lt;span class="math">\(\alpha(x)\)&lt;/span>是一个&lt;strong>单调递增的右连续函数&lt;/strong> (注：这个右连续其实跟左开右闭区间是有一种默契的。 比如，若右边也是开，那么右连续的函数，跳开的地方就很难精准的测量了)；于是&lt;span class="math">\(\alpha\)&lt;/span>满足：&lt;span class="math">\(x&amp;lt;y \Rightarrow \alpha(x)≤\alpha(y)，\lim\limits_{z→x^+}\alpha(z)=\alpha(x)\)&lt;/span>。 这里并不要求&lt;span class="math">\(\alpha\)&lt;/span>严格单调递增。&lt;/p>
&lt;blockquote>
&lt;p>定义集合映射：&lt;span class="math">\(l((a,b)])=\alpha(b)-\alpha(a)\)&lt;/span>。根据通用的外测度构造方法，设定外测度为 &lt;span class="math">\[m^\ast(E)=\inf\biggl\{\sum_{i=1}^∞ l(A_i):A_i ∈ \mathcal{C}，E\subset \bigcup_{i=1}^∞ A_i\biggl\}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>由Caratheodory's定理可知，&lt;span class="math">\(m^\ast\)&lt;/span>定义在&lt;span class="math">\(m^\ast\)&lt;/span>可测集上是一个测度。注意到若&lt;span class="math">\(K=(a,b]，L=(b,c]，K\cup L =(a,c]\)&lt;/span>，且&lt;span class="math">\(l(K)+l(L)=[\alpha(b)-\alpha(a)]+[\alpha(c)-\alpha(b)]=\alpha(c)-\alpha(a)=l(K\cup L)\)&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理：任一&lt;span class="math">\(\mathbb R\)&lt;/span>上Borel-&lt;span class="math">\(\sigma\)&lt;/span>-代数中的集合都是&lt;span class="math">\(m^\ast\)&lt;/span>可测的。&lt;/p>
&lt;/blockquote></description></item><item><title>测度论4之勒贝格测度</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA4%E4%B9%8B%E5%8B%92%E8%B4%9D%E6%A0%BC%E6%B5%8B%E5%BA%A6/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA4%E4%B9%8B%E5%8B%92%E8%B4%9D%E6%A0%BC%E6%B5%8B%E5%BA%A6/</guid><description>
&lt;h2 id="勒贝格测度">勒贝格测度&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#lebesgue可测">Lebesgue可测&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可测集的性质">可测集的性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常见的可测集">常见的可测集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重要基础证明任何区间i都是可测集并且mii">重要基础证明：任何区间&lt;span class="math">\(I\)&lt;/span>都是可测集，并且&lt;span class="math">\(m(I)=|I|\)&lt;/span>&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常见可测集">常见可测集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#borel集">Borel集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#勒贝格可测集的结构">勒贝格可测集的结构&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="lebesgue可测">Lebesgue可测&lt;/h2>
&lt;blockquote>
&lt;p>定义1：可测集：在外测度的基础上，我们指出&lt;span class="math">\(E\subset R^n\)&lt;/span>只要满足以下条件，我们就称之为&lt;strong>Lebesgue可测集或&lt;span class="math">\(m^\ast\)&lt;/span>可测集，简称可测集&lt;/strong>：对&lt;span class="math">\(\forall T \subset R^n\)&lt;/span>，有 &lt;span class="math">\[m^\ast(T)=m^\ast(T\cap E)+m^\ast(T\cap E^c)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>其中，我们称&lt;span class="math">\(T\)&lt;/span>为试验集，这一条件也被称为Caratheodory条件，简称卡氏条件。可测集的全体类称为可测集类，记为&lt;span class="math">\(\mathcal{M}\)&lt;/span>。&lt;/p>
&lt;p>在这个定义下，我们下面来考虑定义在外测度上的可测集有什么性质和结构。注意到，外测度定义性质3有 &lt;span class="math">\[m^\ast(A\cup B)≤m^\ast(A)+m^\ast(B)(次可列可加性)\\
\because T=(T\cap E)\cup(T\cap E^c)\\
\therefore m^\ast(T)≤m^\ast(T\cap E)+m^\ast(T\cap E^c)恒成立。\]&lt;/span> 因此，我们证明集合可测时，&lt;strong>只需要证明&lt;/strong> &lt;span class="math">\[m^\ast(T)≥m^\ast(T\cap E)+m^\ast(T\cap E^c)\]&lt;/span> 即可。且对于&lt;span class="math">\(m^\ast(T)=∞\)&lt;/span>这个条件总是成立的。因此只需论证在&lt;span class="math">\(m^\ast(T)&amp;lt;∞\)&lt;/span>的情形，&lt;strong>这是证明卡氏条件的关键&lt;/strong>。&lt;/p>
&lt;p>需要特别指出的是，可测性不是集合本身的性质，而是依赖于外测度的定义。事实上，我们可以给出两种外测度的定义，使集合在一个外测度下可测，而另一个不可测。&lt;/p>
&lt;p>外测度为0的点集为零测集。&lt;/p>
&lt;blockquote>
&lt;p>推论1：零测集都是可测集。&lt;/p>
&lt;/blockquote>
&lt;p>证明：若&lt;span class="math">\(m^\ast(E)=0\)&lt;/span>，则&lt;span class="math">\(m^\ast(T)≤m^\ast(T\cap E)+m^\ast(T\cap E^c)≤m^\ast(E)+m^\ast(T)=m^\ast(T)，即m^\ast(T)=m^\ast(T\cap E)+m^\ast(T\cap E^c)\)&lt;/span>。&lt;/p>
&lt;h2 id="可测集的性质">可测集的性质&lt;/h2>
&lt;p>对于可测集类&lt;span class="math">\(\mathcal{M}\)&lt;/span>&lt;/p>
&lt;p>I. &lt;span class="math">\(\emptyset ∈ \mathcal{M}\)&lt;/span>&lt;/p>
&lt;ol start="2" style="list-style-type: upper-roman">
&lt;li>&lt;p>若&lt;span class="math">\(E ∈ \mathcal{M}\)&lt;/span>，则&lt;span class="math">\(E^c ∈ \mathcal{M}\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>若&lt;span class="math">\(E_1 ∈ \mathcal{M}，E_2 ∈ \mathcal{M}，\)&lt;/span>则&lt;span class="math">\(E_1\cup E_2，E_1\cap E_2，E1-E2\)&lt;/span>皆属于&lt;span class="math">\(\mathcal{M}\)&lt;/span>（可测集的任何有限次交、并所得集合仍是可测集）。&lt;/p>&lt;/li>
&lt;li>&lt;p>若&lt;span class="math">\(E_i ∈ \mathcal{M}(i=1,2,\dots)\)&lt;/span>，则其并集也属于&lt;span class="math">\(\mathcal{M}\)&lt;/span>。若进一步有&lt;span class="math">\(E_i\cap E_j=\emptyset\)&lt;/span>，则 &lt;span class="math">\[m^\ast(\bigcup_{i=1}^∞ E_i)=\sum_{i=1}^∞ m^\ast(E_i)，\]&lt;/span> 即&lt;span class="math">\(m^\ast\)&lt;/span>在&lt;span class="math">\(\mathcal{M}\)&lt;/span>上满足可数可加性（&lt;span class="math">\(\sigma\)&lt;/span>可加性）。&lt;/p>&lt;/li>
&lt;/ol>
&lt;p>证明：&lt;/p>
&lt;p>（1）由推论1可知成立。&lt;/p>
&lt;p>（2）注意到可测集卡氏条件对于补集的对称性可立即得出结论。&lt;/p>
&lt;p>（3）我们先证有限并的封闭性。对于任一试验集&lt;span class="math">\(T\subset R^n\)&lt;/span>，根据集合分解（如下图）与外测度的次可加性，我们有： &lt;span class="math">\[\begin{aligned}
m^\ast(T)&amp;amp;≤_{外测度定义3}m^\ast(T\cap(E_1\cup E_2))+m^\ast(T\cap(E_1\cup E_2)^c)\\
&amp;amp;=m^\ast(T\cap(E_1\cup E_2))+m^\ast(T\cap E_1^c\cap E_2^c)_{德摩根律}\\
&amp;amp;≤_{单调性}m^\ast(T\cap E_1\cap E_2)+m^\ast(T\cap E_1\cap E_2^c)+m^\ast(T\cap E_1^c\cap E_2)+m^\ast(T\cap E_1^c\cap E_2^c)\\
&amp;amp;(由下图可知)(T\cap E_1\cap E_2) \cup(T\cap E_1\cap E_2^c)\cup(T\cap E_1^c\cap E_2)\subset E_1\cup E_2\\
&amp;amp;=\underline{m^\ast((T\cap E_1)\cap E_2)+m^\ast((T\cap E_1)\cap E_2^c)}+\underline{m^\ast((T\cap E_1^c)\cap E_2)+m^\ast((T\cap E_1^c)\cap E_2^c)}\\
&amp;amp;(由于E_2是可测集)\\
&amp;amp;=m^\ast(T\cap E_1)+m^\ast(T\cap E_1^c)\\
&amp;amp;(由于E_1是可测集)\\
&amp;amp;=m^\ast(T)\\
&amp;amp;\therefore m^\ast(T)≥m^\ast(T\cap(E_1\cup E_2))+m^\ast(T\cap(E_1\cup E_2)^c)
\end{aligned}\]&lt;/span> &lt;img src="../images/集合分解.png" alt="集合分解" /> 因此&lt;span class="math">\(E_1\cup E_2\)&lt;/span>是可测集。接着由数学归纳法可递推，&lt;strong>可测集的有限并可测&lt;/strong>。&lt;/p>
&lt;p>其次，我们证明对于交集的封闭性。我们注意到&lt;span class="math">\(E_1 \cap E_2=(E_1^c \cup E_2^c)^c\)&lt;/span>。要证&lt;span class="math">\((E_1^c \cup E_2^c)^c\)&lt;/span>可测，根据性质2，我们只需要证明补集&lt;span class="math">\(((E_1^c \cup E_2^c)^c)^c=E_1^c \cup E_2^c\)&lt;/span>可测。因为&lt;span class="math">\(E_1，E_2\)&lt;/span>可测，所以其补集（&lt;span class="math">\(E_1^c，E_2^c\)&lt;/span>）都可测。再根据刚刚证明的可测集的有限并可测，则&lt;span class="math">\(E_1^c\cup E_2^c\)&lt;/span>可测。综上所述，&lt;span class="math">\(E_1 \cap E_2\)&lt;/span>可测。接着由数学归纳法可递推，&lt;strong>可测集的有限交可测&lt;/strong>。&lt;/p>
&lt;p>最后我们证明可测集的差集可测。由&lt;span class="math">\(E_1-E_2=E_1\cap E_2^c\)&lt;/span>，且&lt;span class="math">\(E_1,E_2^c\)&lt;/span>可测&lt;span class="math">\(\Rightarrow E_1\cup E_2^c\)&lt;/span>可测容易得出，&lt;span class="math">\(E_1-E_2\)&lt;/span>&lt;strong>差集也可测&lt;/strong>。&lt;/p>
&lt;p>（4）从有限到无穷可数。我们先证可列并的封闭性。由于&lt;span class="math">\(E_i ∈ \mathcal{M}(i=1,2,\dots)\)&lt;/span>，我们令 &lt;span class="math">\[S=\bigcup_{i=1}^∞ E_i\qquad S_k=\bigcup_{i=1}^k E_i，k=1,2,3,\dots\]&lt;/span> 由（3）可知，有限并&lt;span class="math">\(S_k\)&lt;/span>是可测的，我们可用试验集&lt;span class="math">\(T\subset R^n\)&lt;/span>，有 &lt;span class="math">\[\begin{aligned}
m^\ast(T)&amp;amp;=m^\ast(T\cap S_k)+m^\ast(T\cap S_k^c)\\
&amp;amp;=m^\ast(T\cap \bigcup_{i=1}^k E_i)+m^\ast(T\cap S_k^c)\\
&amp;amp;=_{分配律}m^\ast(\bigcup_{i=1}^k(T\cap E_i))+m^\ast(T\cap S_k^c)\\
&amp;amp;=_{有限并可测}\sum_{i=1}^k m^\ast(T\cap E_i)+m^\ast(T\cap S_k^c)\\
&amp;amp;\because T\cup S_k^c \supset T\cup S^c\\
&amp;amp;≥_{单调性}\sum_{i=1}^k m^\ast(T\cap E_i)+m^\ast(T\cap S^c)\\
当k→∞时，就有\\
m^\ast(T)&amp;amp;≥\sum_{i=1}^∞ m^\ast(T\cap E_i)+m^\ast(T\cap S^c)\\
&amp;amp;≥_{外测度定义3}m^\ast(T\cap S)+m^\ast(T\cap S^c)
\end{aligned}\]&lt;/span> 由此可知，&lt;span class="math">\(S ∈ \mathcal{M}\)&lt;/span>，即可列并&lt;span class="math">\(S=\bigcup_{i=1}^∞ E_i\)&lt;/span>是可测的。&lt;/p>
&lt;p>接下来，我们进一步证明如果&lt;span class="math">\(E_i,E_j\)&lt;/span>两两互不相交，则 &lt;span class="math">\[m^\ast(\bigcup_{i=1}^∞ E_i)=\sum_{i=1}^∞ m^\ast(E_i)\]&lt;/span> 在以上公式&lt;span class="math">\(m^\ast(T)≥\sum_{i=1}^∞ m^\ast(T\cap E_i)+m^\ast(T\cap S^c)\)&lt;/span>中，我们以&lt;span class="math">\(T\cap S\)&lt;/span>替换&lt;span class="math">\(T\)&lt;/span>： &lt;span class="math">\[m^\ast(T\cap S)≥\sum_{i=1}^∞ m^\ast(T\cap S \cap E_i)+m^\ast(T\cap S\cap S^c)\\
=\sum_{i=1}^∞ m^\ast(T\cap (S \cap E_i))=\sum_{i=1}^∞ m^\ast(T\cap E_i)\]&lt;/span> 因为反向不等式总是成立的，所以 &lt;span class="math">\[m^\ast(T\cap S)=\sum_{i=1}^∞ m^\ast(T\cap E_i)\]&lt;/span> 当我们取&lt;span class="math">\(T=R^n\)&lt;/span>时有 &lt;span class="math">\[m^\ast(S)=m^\ast(\bigcup_{i=1}^∞ E_i)=\sum_{i=1}^∞ m^\ast(E_i)\]&lt;/span> 我们从定理（1）（2）（4）发现，这些可测集合性质正好满足&lt;span class="math">\(\sigma\)&lt;/span>代数的定义，所以我们可知，所有可测集组成的集类&lt;span class="math">\(\mathcal{M}\)&lt;/span>是一个&lt;span class="math">\(\sigma\)&lt;/span>代数。而性质（3）中的内容，也正好是&lt;span class="math">\(\sigma\)&lt;/span>代数的性质。&lt;/p>
&lt;blockquote>
&lt;p>推论2：可测集的可列并也是可测集。&lt;span class="math">\(\bigcup\limits_{i=1}^∞ E_i∈ \mathcal{M}\)&lt;/span>。&lt;/p>
&lt;p>推论3：可测集的可列交也是可测集。&lt;span class="math">\(\bigcap\limits_{i=1}^∞ E_i∈ \mathcal{M}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>对于这些可测集&lt;span class="math">\(E∈ \mathcal{M}\)&lt;/span>，&lt;strong>定义在其上的外测度可以称为测度&lt;/strong>，记为&lt;span class="math">\(m(E)\)&lt;/span>，这就是通常所说的&lt;span class="math">\(R^n\)&lt;/span>上的&lt;strong>Lebesgue测度&lt;/strong>。&lt;/p>
&lt;p>下面我们在添加两个关于集合极限测度的运算性质：&lt;/p>
&lt;p>V. （递增可测集列的测度运算）若有递增可测集列&lt;span class="math">\(E_1\subset E_2\subset\dotsb\subset E_k \dotsb\)&lt;/span>，则 &lt;span class="math">\[m(\lim_{k→∞} E_k)=\lim_{k→∞} m(E_k)\]&lt;/span>&lt;/p>
&lt;ol start="6" style="list-style-type: upper-roman">
&lt;li>（递减可测集列的测度运算）若有递增可测集列&lt;span class="math">\(E_1\supset E_2\supset\dotsb\supset E_k \dotsb\)&lt;/span>且&lt;span class="math">\(m(E_1)&amp;lt;∞\)&lt;/span>，则 &lt;span class="math">\[m(\lim_{k→∞} E_k)=\lim_{k→∞} m(E_k)\]&lt;/span>&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>推论4：（Fatou引理）设&lt;span class="math">\(\{E_k\}\)&lt;/span>是可测集列，则 &lt;span class="math">\[m(\mathop{\underline{\lim}}\limits_{k→∞} E_k)≤\mathop{\underline{\lim}}\limits_{k→∞} m(E_k)\\
m(\overline{\lim_{k→∞}} E_k)≥\overline{\lim}_{k→∞} m(E_k)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="常见的可测集">常见的可测集&lt;/h2>
&lt;blockquote>
&lt;p>定义2：若&lt;span class="math">\(I\)&lt;/span>为一个区间（无论开区间还是左半开区间还是闭区间），称 &lt;span class="math">\[\prod_{i=1}^n=(b_1-a_1)\times(b_2-a_2)\times\dotsb\times(b_n-a_n)\]&lt;/span> 为区间&lt;span class="math">\(I\)&lt;/span>的体积，记为&lt;span class="math">\(|I|\)&lt;/span>，区间具有连通性。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>开集vs开区间&lt;/strong>：开集可以是可数个开区间的并集，开区间是开集的一种特殊情形，按照拓扑学观点，是连通的，而开集可以不连通。&lt;/p>
&lt;h3 id="重要基础证明任何区间i都是可测集并且mii">重要基础证明：任何区间&lt;span class="math">\(I\)&lt;/span>都是可测集，并且&lt;span class="math">\(m(I)=|I|\)&lt;/span>&lt;/h3>
&lt;blockquote>
&lt;p>定理1：&lt;span class="math">\(R^n\)&lt;/span>上任何区间&lt;span class="math">\(I\)&lt;/span>都是可测集，并且&lt;span class="math">\(m(I)=|I|\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>这是一个非常重要的定理，是所有&lt;span class="math">\(R^n\)&lt;/span>可测集的基础&lt;/strong>。我们分两步来证明，（1）区间的外测度等于体积&lt;span class="math">\(m^\ast(I)=|I|\)&lt;/span>；（2）定义在区间上的外测度是可测的，即&lt;span class="math">\(m=m^\ast|_I\)&lt;/span>。如果两个都成立，那么&lt;span class="math">\(|I|=m^\ast(I)|_I=m(I)|_I\)&lt;/span>&lt;/p>
&lt;p>（1）区间的外测度等于体积&lt;span class="math">\(m^\ast(I)=|I|\)&lt;/span>。为了简单起见，我们只证明&lt;span class="math">\(R^1\)&lt;/span>情形下。根据外测度使体积下限的定义有&lt;span class="math">\(m^\ast(I)≤|I|\)&lt;/span>。由于可列点集的测度都是零，所以取不取等号外测度不变，这里我们取闭区间&lt;span class="math">\(\bar{I}\)&lt;/span>（因为可以用有限覆盖定理）。&lt;/p>
&lt;p>对于任意的&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>，&lt;strong>存在&lt;/strong>一列开区间&lt;span class="math">\(\{I_i\}\)&lt;/span>使 &lt;span class="math">\[\bar I \subset \bigcup_{i=1}^∞ I_i且\bigcup_{i=1}^∞ |I_i|&amp;lt;m^\ast(\bar I)+\varepsilon(外测度加下限定义可得)\]&lt;/span> 根据有限覆盖定理，有界闭区间可以被有限个开区间覆盖，设这些开区间为&lt;span class="math">\(I_1,I_2,\dots,I_n\)&lt;/span>，使得&lt;span class="math">\(\bar{I}\subset \bigcup\limits_{i=1}^n I_i\)&lt;/span>。因为&lt;span class="math">\(\bar{I}=\bigcup\limits_{i=1}^n(\bar{I}\cap I_i)\)&lt;/span>，所以由这些&lt;span class="math">\(I_i\)&lt;/span>的端点与&lt;span class="math">\(\bar I\)&lt;/span>的交点就把&lt;span class="math">\(I\)&lt;/span>分解城有限多个无公共内点的小区间，&lt;span class="math">\(\bar I\)&lt;/span>的长度就是这些小区间长度的和，并且每个小区间至少包含在某一个&lt;span class="math">\(\bar I \cap I_i\)&lt;/span>中，于是 &lt;span class="math">\[|\bar I|≤\sum_{i=1}^n|\bar I \cap I_i|≤\sum_{i=1}^n|I_i|≤\sum_{i=1}^∞|I_i|≤m^\ast(\bar I)+\varepsilon\]&lt;/span> 由&lt;span class="math">\(\varepsilon\)&lt;/span>的任意性可知&lt;span class="math">\(m^\ast(\bar I)≥|\bar I|\)&lt;/span>.&lt;/p>
&lt;p>因此&lt;span class="math">\(m^\ast(I)=m^\ast(\bar I)=|I|\)&lt;/span>&lt;/p>
&lt;div class="figure">
&lt;img src="../images/区间测度.svg" alt="区间测度" />&lt;p class="caption">区间测度&lt;/p>
&lt;/div>
&lt;p>（2）定义在区间上的外测度是可测的，即&lt;span class="math">\(m=m^\ast|_I\)&lt;/span>。先证，对&lt;strong>任一开区间(这里不是任一集合)&lt;/strong>&lt;span class="math">\(I_0\)&lt;/span>，都有 &lt;span class="math">\[|I_0|=m^\ast(I_0\cap I)+m^\ast(I_0\cap I^c)\]&lt;/span> 事实上，如果&lt;span class="math">\(I_0=I\)&lt;/span>，上式必然成立。（如第（1）部分所证）。当&lt;span class="math">\(I \neq I_0\)&lt;/span>时，&lt;span class="math">\(I_0\cap I\)&lt;/span>是一个区间，而&lt;span class="math">\(I_0\cap I^c\)&lt;/span>可以分解成有限个互不相交的区间&lt;span class="math">\(I_i，(i=1,2,\dots,k)\)&lt;/span>并(在一维中很好理解，最多两个，在高维度中会有很多个互不相交区间)。从而由外测度此可列可加性知： &lt;span class="math">\[m^\ast(I_0\cap I^c)=m^\ast(\bigcup_{i=1}^k I_i)≤\sum_{i=1}^k m^\ast(I_i)\xlongequal{证明（1）}\sum_{i=1}^k|I_i|\]&lt;/span> 因此 &lt;span class="math">\[m^\ast(I_0\cap I)+m^\ast(I_0\cap I^c)≤|I\cap I_0|+\sum_{i=1}^k|I_i|\xlongequal{体积性质}|I_0|\]&lt;/span> 另一方面，由于外测度的次可列可加性有： &lt;span class="math">\[m^\ast(I_0\cap I)+m^\ast(I_0\cap I^c)≥m^\ast((I_0\cap I)\cup(I_0\cap I^c))=m^\ast(I_0)=|I_0|\]&lt;/span> 综上所述：&lt;span class="math">\(|I_0|=m^\ast(I_0\cap I)+m^\ast(I_0\cap I^c)\)&lt;/span>.&lt;/p>
&lt;p>接下来，需要证明对任一&lt;strong>点集&lt;/strong>&lt;span class="math">\(T\subset R^n\)&lt;/span>,任一&lt;strong>区间&lt;/strong>&lt;span class="math">\(I\)&lt;/span>都满足 &lt;span class="math">\[m^\ast(T)=m^\ast(T\cap I)+m^\ast(T\cap I^c)\]&lt;/span> 由外测度定义可知，对于&lt;span class="math">\(\forall \varepsilon&amp;gt;0\)&lt;/span>，有一列&lt;strong>开区间&lt;/strong>&lt;span class="math">\(\{I_i\}\)&lt;/span>，使&lt;span class="math">\(T\subset \bigcup\limits_{n=1}^∞ I_i\)&lt;/span>，且&lt;span class="math">\(\sum\limits_{i=1}^∞ |I_i|≤m^\ast(T)+\varepsilon\)&lt;/span>。由于 &lt;span class="math">\[T\cap I\subset (\bigcup_{i=1}^∞ I_i)\cap I=\bigcup_{i=1}^∞ (I_i\cap I)\\
T\cap I^c\subset (\bigcup_{i=1}^∞ I_i)\cap I^c=\bigcup_{i=1}^∞ (I_i\cap I^c)\\
\Rightarrow m^\ast(T\cap I)≤\sum_{i=1}^∞ m^\ast(I_i\cap I)\\
\Rightarrow m^\ast(T\cap I^c)≤\sum_{i=1}^∞ m^\ast(I_i\cap I^c)\\
\Rightarrow m^\ast(T\cap I)+m^\ast(T\cap I^c)≤\sum_{i=1}^∞ m^\ast(I_i\cap I)+\sum_{i=1}^∞ m^\ast(I_i\cap I^c)\\
=\sum_{i=1}^∞ [m^\ast(I_i\cap I)+m^\ast(I_i\cap I^c)]\\
\xlongequal{由于每一个I_i都是区间}\sum_{i=1}^∞|I_i|≤m^\ast(T)+\varepsilon\\
即m^\ast(T\cap I)+m^\ast(T\cap I^c)≤m^\ast(T)+\varepsilon
\]&lt;/span> 由&lt;span class="math">\(\varepsilon\)&lt;/span>的任意性可知&lt;span class="math">\(m^\ast(T\cap I)+m^\ast(T\cap I^c)≤m^\ast(T)\)&lt;/span>。而&lt;span class="math">\(≥\)&lt;/span>是外测度定义的次可列可加性，所以&lt;span class="math">\(m^\ast(T\cap I)+m^\ast(T\cap I^c)=m^\ast(T)\)&lt;/span>。&lt;/p>
&lt;p>这说明区间&lt;span class="math">\(I\)&lt;/span>满足卡氏条件，即为可测集。&lt;/p>
&lt;h3 id="常见可测集">常见可测集&lt;/h3>
&lt;blockquote>
&lt;p>定理2：欧几里得空间的Lindelof覆盖定理：任取非空开集&lt;span class="math">\(E\subset R^n\)&lt;/span>，一族开集&lt;span class="math">\(\mathcal A\)&lt;/span>是&lt;span class="math">\(E\)&lt;/span>的覆盖，那么对于&lt;span class="math">\(\mathcal A\)&lt;/span>，存在&lt;span class="math">\(S\)&lt;/span>的&lt;strong>可列&lt;/strong>子覆盖.&lt;/p>
&lt;/blockquote>
&lt;p>证明：令&lt;span class="math">\(\mathcal M=\{N(y,r)|\quad r为正有理数，y为有理点\}\)&lt;/span>，则&lt;span class="math">\(\mathcal M\)&lt;/span>是一个开集族，并且&lt;span class="math">\(\mathcal{M}\)&lt;/span>可列。（&lt;span class="math">\(R^n\)&lt;/span>中坐标为有理数的点为有理点）。&lt;/p>
&lt;p>由假设可知，对任一点&lt;span class="math">\(x ∈ E\)&lt;/span>，存在开集&lt;span class="math">\(G_x ∈ \mathcal A\)&lt;/span>(开覆盖)，使&lt;span class="math">\(x ∈ G_x\)&lt;/span>。因为&lt;span class="math">\(G_x\)&lt;/span>为开集，所以存在以&lt;span class="math">\(x\)&lt;/span>为中心的邻域&lt;span class="math">\(N(x,\delta_x)\subset G_x\)&lt;/span>.&lt;/p>
&lt;p>现在任取一有理点&lt;span class="math">\(a_x ∈ N(x,\frac{\delta_x}{4})\)&lt;/span>，再取有理数&lt;span class="math">\(r_a\)&lt;/span>，使&lt;span class="math">\(\frac{\delta_x}{4}&amp;lt;r_a&amp;lt;\frac{\delta_x}{2}\)&lt;/span>，作&lt;span class="math">\(a_x\)&lt;/span>的邻域&lt;span class="math">\(N(a_x,r_a)\)&lt;/span>。如下图&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Lindelof覆盖定理证明.svg" alt="Lindelof覆盖定理证明.svg" />&lt;p class="caption">Lindelof覆盖定理证明.svg&lt;/p>
&lt;/div>
&lt;p>由&lt;span class="math">\(a_x，r_a\)&lt;/span>的取法，不难发现有&lt;span class="math">\(x ∈ N(a_x,r_a)\subset N(x,\delta_x)\subset G_x\)&lt;/span>.&lt;/p>
&lt;p>于是&lt;span class="math">\(E\)&lt;/span>中每一个点都有对应的有理点&lt;span class="math">\(a_x\)&lt;/span>为中心，有理数&lt;span class="math">\(r_a\)&lt;/span>为半径的邻域&lt;span class="math">\(N(a_x,r_a)\)&lt;/span>。这些邻域组成集合系&lt;span class="math">\(\{N(a_x,r_a)\}_{x ∈ E}\)&lt;/span>是E的一个开覆盖（因为每个点都被一个邻域覆盖了，我理解是由于实数的基数大于有理数，必然有一些点使用相同的邻域），同时&lt;span class="math">\((a_x,r_a)\)&lt;/span>都是有理数，因此&lt;span class="math">\(\{N(a_x,r_a)\}_{x ∈ E}\subset\mathcal{M}\)&lt;/span>，所以&lt;span class="math">\(\{N(a_x,r_a)\}_{x ∈ E}\)&lt;/span>是至多可列的。&lt;/p>
&lt;p>又因为&lt;span class="math">\(N(a_x,r_a)\subset N(x,\delta_x)\subset G_x\)&lt;/span>，因此取每一个包含&lt;span class="math">\(N(a_x,r_a)\)&lt;/span>的&lt;span class="math">\(G_x\)&lt;/span>组成的集合系也是可列的，并且能够覆盖&lt;span class="math">\(E\)&lt;/span>。Lindelof覆盖定理得证。&lt;/p>
&lt;blockquote>
&lt;p>引理1：&lt;span class="math">\(R^n\)&lt;/span>中任何非空开集都可表示为至多可列个开区间的并。&lt;/p>
&lt;/blockquote>
&lt;p>这可以看成是Lindelof覆盖定理的一个推广，从开集到开区间的推广。这是成立的。因为在证明是Lindelof覆盖定理中，我们使用的是邻域，这就相当于是开区间，把这些开区间取并集，这个引理可得证。&lt;/p>
&lt;p>有了以上覆盖定理，在加上可测集的运算性质，我们可以得出如下定理：&lt;/p>
&lt;blockquote>
&lt;p>定理3：任何开集、闭集都是可测集。&lt;/p>
&lt;/blockquote>
&lt;p>由引理1可知任何非空开集都可表示为至多可列个开区间的并，而定理1告诉我们开区间是可测的，再通过推论2，任一开集的并集仍然是开集。因此开集是可测的。&lt;/p>
&lt;p>再根据测度的性质，可测集的补集都是可测的，而开集的补集是闭集，因此闭集也是可测的。&lt;/p>
&lt;p>例：康托尔集可测且测度为0.&lt;/p>
&lt;p>由康托尔集的构造可知&lt;span class="math">\(P_0=[0,1]-G_0\)&lt;/span>，其中&lt;span class="math">\(G_0=(\frac{1}{3},\frac{2}{3})\cup[(\frac{1}{9},\frac{2}{9}])\cup(\frac{7}{9},\frac{8}{9}])\cup\dotsb\)&lt;/span>为开集。&lt;/p>
&lt;p>由于&lt;span class="math">\(闭集[0,1]，开集G_0\)&lt;/span>均可测，且可测集的差集可测所以&lt;span class="math">\(P_0=[0,1]-G_0\)&lt;/span>可测。由测度的可加性可知， &lt;span class="math">\[\begin{aligned}
m(P_0)&amp;amp;=m([0,1])-m(G_0)\\
&amp;amp;=1-(1/3+2\times1/3^2+\dotsb+2^{n-1}\times 1/3^n+\dotsb)\\
&amp;amp;=1-1=0
\end{aligned}\]&lt;/span> 康托尔集给出了一个基数是&lt;span class="math">\(c/\aleph_1\)&lt;/span>，但是测度却是0的例子。&lt;/p>
&lt;h2 id="borel集">Borel集&lt;/h2>
&lt;blockquote>
&lt;p>定义3：Borel集：凡属可以从开集出发，用取补集、取有限个或可列个集合的并或交等过程而得到的集合，统称为Borel集。Borel集的集合系就是Borel（&lt;span class="math">\(\sigma\)&lt;/span>-）代数。&lt;/p>
&lt;/blockquote>
&lt;p>显然开集、闭集、&lt;span class="math">\(F_\sigma、G_\delta\)&lt;/span>都是Borel集。&lt;/p>
&lt;blockquote>
&lt;p>定理4：任何的Borel集都是可测的。&lt;/p>
&lt;/blockquote>
&lt;p>这一点可以从可测集的运算性质得出。&lt;/p>
&lt;h2 id="勒贝格可测集的结构">勒贝格可测集的结构&lt;/h2>
&lt;blockquote>
&lt;p>引理2：&lt;span class="math">\(R^n\)&lt;/span>中任何可测集&lt;span class="math">\(E\)&lt;/span>都可表为至多可列个&lt;strong>互不相交的有界可测集&lt;/strong>的并。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;span class="math">\(R^n\)&lt;/span>空间的可列化。令 &lt;span class="math">\[S=\{x| x ∈ R^n，n-1≤d(x,0)&amp;lt;n\},n=1,2,\dotsb，\]&lt;/span> 其中0表示&lt;span class="math">\(R^n\)&lt;/span>中原点的坐标，则&lt;span class="math">\(S_n(n=1,2,\dots)\)&lt;/span>可测。效果图如下，即一个个同心圆区间，且&lt;span class="math">\(R^n=\bigcup\limits_{n=1}^∞ S_n\)&lt;/span>。&lt;/p>
&lt;p>&lt;img src="../images/Rn分割.svg" width=300px height=300px>&lt;/p>
&lt;p>令&lt;span class="math">\(E_n=E\cap S_n\)&lt;/span>，则&lt;span class="math">\(E_n\)&lt;/span>是可测集&lt;span class="math">\(E和有界可测S_n\)&lt;/span>的交集，也有界可测，且有两两互不相交。那么&lt;span class="math">\(E=\bigcup\limits_{n=1}^∞ E_n\)&lt;/span>。&lt;/p>
&lt;p>这个引理的意义在于当我们讨论无界可测集的时候可以分解成有界可测集来讨论。下面我们来讨论各种集合与可测集之间的关系。&lt;/p>
&lt;blockquote>
&lt;p>定理5：（开集与可测集）可测集E&lt;span class="math">\(\Leftrightarrow\)&lt;/span> 对于&lt;span class="math">\(\forall \varepsilon&amp;gt;0\)&lt;/span>恒有开集&lt;span class="math">\(G\supset E\)&lt;/span>，使&lt;span class="math">\(m^\ast(G-E)&amp;lt;\varepsilon\)&lt;/span>&lt;/p>
&lt;p>定理6：（闭集与可测集）可测集E&lt;span class="math">\(\Leftrightarrow\)&lt;/span> 对于&lt;span class="math">\(\forall \varepsilon&amp;gt;0\)&lt;/span>恒有闭集&lt;span class="math">\(F\subset E\)&lt;/span>，使&lt;span class="math">\(m^\ast(E-F)&amp;lt;\varepsilon\)&lt;/span>&lt;/p>
&lt;p>定理7：（&lt;span class="math">\(G_\delta\)&lt;/span>与可测集）可测集E&lt;span class="math">\(\Leftrightarrow\)&lt;/span> 恒有&lt;span class="math">\(G_\delta\)&lt;/span>型集合&lt;span class="math">\(G\supset E\)&lt;/span>，使&lt;span class="math">\(m^\ast(G-E)=0\)&lt;/span>&lt;/p>
&lt;p>定理8：（&lt;span class="math">\(F_\sigma\)&lt;/span>与可测集）可测集E&lt;span class="math">\(\Leftrightarrow\)&lt;/span> 恒有&lt;span class="math">\(F_\sigma\)&lt;/span>型集合&lt;span class="math">\(F\subset E\)&lt;/span>，使&lt;span class="math">\(m^\ast(E-F)=0\)&lt;/span>&lt;/p>
&lt;p>定理9：（Borel集与可测集）任何可测集必是一个Borel集（&lt;span class="math">\(F_\sigma\)&lt;/span>）与一个测度为0的可测集的并；同时也是一个Borel集（&lt;span class="math">\(G_\delta\)&lt;/span>）与一个测度为0的可测集的差集。&lt;/p>
&lt;/blockquote></description></item><item><title>测度论5之可测函数</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA5%E4%B9%8B%E5%8F%AF%E6%B5%8B%E5%87%BD%E6%95%B0/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA5%E4%B9%8B%E5%8F%AF%E6%B5%8B%E5%87%BD%E6%95%B0/</guid><description>
&lt;h2 id="可测函数">可测函数&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#点集上的函数">点集上的函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#广义实数">广义实数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#点集上的连续函数与函数列的极限">点集上的连续函数与函数列的极限&lt;/a>&lt;/li>
&lt;li>&lt;a href="#这时我们就说序列f_n逐点收敛到f或说函数f是序列f_n的逐点收敛极限函数">&amp;gt;这时我们就说序列&lt;span class="math">\(\{f_{n}\}\)&lt;/span>（逐点）收敛到&lt;span class="math">\(f\)&lt;/span>，或说函数&lt;span class="math">\(f\)&lt;/span>是序列&lt;span class="math">\(f_{n}\)&lt;/span>的（逐点收敛）极限函数。&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几乎处处连续">“几乎处处连续”&lt;/a>&lt;/li>
&lt;li>&lt;a href="#勒贝格可测函数">勒贝格可测函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#补充稠密与疏朗">补充：稠密与疏朗&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可测函数运算性质">可测函数运算性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#简单函数">简单函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可测函数的构造">可测函数的构造&lt;/a>&lt;/li>
&lt;li>&lt;a href="#补充集合与函数">补充：集合与函数&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="点集上的函数">点集上的函数&lt;/h2>
&lt;h3 id="广义实数">广义实数&lt;/h3>
&lt;blockquote>
&lt;p>定义1：广义实数：&lt;span class="math">\(R\cup \{-∞,+∞\}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>规定&lt;span class="math">\(0\times\pm ∞=0\)&lt;/span>&lt;/p>
&lt;p>下面几种无穷运算无意义。 &lt;span class="math">\[\frac{∞}{∞}，\frac{∞}{0}，\frac{a}{0}\\
(+∞)-(+∞)，(-∞)-(-∞)，(+∞)+(-∞)，(-∞)+(+∞)\]&lt;/span>&lt;/p>
&lt;h3 id="点集上的连续函数与函数列的极限">点集上的连续函数与函数列的极限&lt;/h3>
&lt;blockquote>
&lt;p>定义2：对于在&lt;span class="math">\(E\subset R^n\)&lt;/span>上的函数&lt;span class="math">\(f\)&lt;/span>，我们用记号： &lt;span class="math">\[E[f(x)&amp;gt;a]\]&lt;/span> 表示&lt;span class="math">\(E\)&lt;/span>中满足&lt;span class="math">\(f(x)&amp;gt;a\)&lt;/span>的点&lt;span class="math">\(x\)&lt;/span>的集合全体，即 &lt;span class="math">\[E[f(x)&amp;gt;a]=\{x|x ∈ E，f(x)&amp;gt;a\}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>类似的可以定义：&lt;span class="math">\(E[f(x)&amp;gt;a]，E[f(x)&amp;lt;a]，E[f(x)≥a]，E[f(x)≤a]，E[f(x)=a]，E[a&amp;lt;f(x)≤b]\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>定义3：(逐点收敛)极限函数：设&lt;span class="math">\(\{f_{n}\}\)&lt;/span>是一组有相同定义域的函数序列。序列&lt;span class="math">\(\{f_{n}\}\)&lt;/span>&lt;strong>（逐点）收敛&lt;/strong>当且仅当存在函数&lt;span class="math">\(f\)&lt;/span>，使得在定义域中的每点&lt;span class="math">\(x\)&lt;/span>，都有： &lt;span class="math">\[\lim _{{n\rightarrow \infty }}f_{n}(x)=f(x)\]&lt;/span> 这时我们就说序列&lt;span class="math">\(\{f_{n}\}\)&lt;/span>（逐点）收敛到&lt;span class="math">\(f\)&lt;/span>，或说函数&lt;span class="math">\(f\)&lt;/span>是序列&lt;span class="math">\(f_{n}\)&lt;/span>的（逐点收敛）极限函数。 -------- 定义4：一致收敛极限函数：让&lt;span class="math">\(\{f_{n}\}_{n\in \mathbb {N} }\)&lt;/span>是定义在&lt;span class="math">\(S\)&lt;/span>上，值域为&lt;span class="math">\(\mathbb {R}\)&lt;/span>或&lt;span class="math">\(\mathbb {C}\)&lt;/span>的一组函数序列，若序列 &lt;span class="math">\(\{f_{n}\}_{n\in \mathbb {N}}\)&lt;/span>均匀收敛至函数&lt;span class="math">\(f\)&lt;/span>在集合&lt;span class="math">\(S\)&lt;/span>上，即表示对所有&lt;span class="math">\(\epsilon &amp;gt;0\)&lt;/span>，存在&lt;span class="math">\(N∈\mathbb{N}\)&lt;/span>，使得当所有&lt;span class="math">\(n\geq N\)&lt;/span>且 &lt;span class="math">\(x∈ S\)&lt;/span>时有 &lt;span class="math">\[|f_{n}(x)-f(x)|&amp;lt;\epsilon.\]&lt;/span> 这时我们就说序列&lt;span class="math">\(\{f_{n}\}\)&lt;/span>一致收敛到&lt;span class="math">\(f\)&lt;/span>，或说函数&lt;span class="math">\(f\)&lt;/span>是序列&lt;span class="math">\(f_{n}\)&lt;/span>的一致连续极限函数。&lt;/p>
&lt;/blockquote>
&lt;p>注意到，一致收敛和逐点收敛定义的区别在于，在一致收敛中&lt;span class="math">\(N\)&lt;/span>的选取仅与&lt;span class="math">\(\epsilon\)&lt;/span>相关，而在逐点收敛中&lt;span class="math">\(N\)&lt;/span>还多了与点&lt;span class="math">\(x\)&lt;/span>相关。所以一致收敛必定逐点收敛，而反之则不然。&lt;/p>
&lt;blockquote>
&lt;p>定理1：&lt;span class="math">\(\{f_n(x)\}\)&lt;/span>是点集&lt;span class="math">\(E\)&lt;/span>上的连续函数列，且一致收敛于&lt;span class="math">\(f(x)\)&lt;/span>，则&lt;span class="math">\(f(x)\)&lt;/span>是&lt;span class="math">\(E\)&lt;/span>上的连续函数。&lt;/p>
&lt;/blockquote>
&lt;p>点集上一致收敛的连续函数列，其一致连续极限函数连续。&lt;/p>
&lt;blockquote>
&lt;p>定理2：点集&lt;span class="math">\(E\)&lt;/span>上的函数列&lt;span class="math">\(\{f_n(x)\}\)&lt;/span>不收敛与&lt;span class="math">\(f(x)\)&lt;/span>的点集合为 &lt;span class="math">\[\bigcup_{k=1}^∞ \bigcap_{N=1}^∞ \bigcup_{n=N}^∞ E[|f_n(x)-f(x)|≥\frac{1}{k}]\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这里不给出严格的证明，只讲如何理解这个式子。首先我们把式子化简如下： &lt;span class="math">\[\bigcap_{N=1}^∞ \bigcup_{n=N}^∞ E_n\\
E_n=E[|f_n(x)-f(x)|≥\frac{1}{k}]\]&lt;/span> 我们取得是&lt;span class="math">\(E_n\)&lt;/span>的上极限，所谓点集序列上极限就是序列中存在于无穷多个集合的元素，即这个点一直都在集合序列中。而&lt;span class="math">\(E_n\)&lt;/span>则是表示某个项&lt;span class="math">\(f_n(x)\)&lt;/span>和极限函数&lt;span class="math">\(f(x)\)&lt;/span>有差距的点，有差距的点一直存在与序列中表示不会收敛到极限函数。最后差距的大小用&lt;span class="math">\(\frac{1}{k}\)&lt;/span>表示，对所有大于0的差距取并集，则是所有不收敛的点。&lt;/p>
&lt;h3 id="几乎处处连续">“几乎处处连续”&lt;/h3>
&lt;blockquote>
&lt;p>定义：设有一个与集合&lt;span class="math">\(E\subset R^n\)&lt;/span>中的点&lt;span class="math">\(x\)&lt;/span>有关的命题&lt;span class="math">\(P(x)\)&lt;/span>。若除了&lt;span class="math">\(E\)&lt;/span>中的&lt;strong>一个零测集以外&lt;/strong>，&lt;span class="math">\(P(x)\)&lt;/span>皆为真，则称&lt;span class="math">\(P(x)\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上&lt;strong>几乎处处&lt;/strong>是真的。简记为&lt;span class="math">\(P(x)\)&lt;/span>，&lt;span class="math">\(a.e.\quad x ∈ E\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>例如1：&lt;span class="math">\(f(x)，g(x)\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上可测，若有 &lt;span class="math">\[m(\{x ∈ E:f(x)\neq g(x)\})=0\]&lt;/span> 则称&lt;span class="math">\(f(x)与g(x)\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上&lt;strong>几乎处处相等&lt;/strong>。也称&lt;span class="math">\(f(x)与g(x)\)&lt;/span>是对等的，记为&lt;span class="math">\(f(x)=g(x)，a.e.\quad x ∈ E\)&lt;/span>&lt;/p>
&lt;p>例如1：&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上可测，若有 &lt;span class="math">\[m(\{x ∈ E:|f(x)|&amp;lt;+∞\})=0\]&lt;/span> 则称&lt;span class="math">\(f(x)与g(x)\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上是&lt;strong>几乎处处有限的&lt;/strong>。记为&lt;span class="math">\(|f(x)|&amp;lt;∞，a.e.\quad x ∈ E\)&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理：几乎处处相等的一个性质—如果&lt;span class="math">\(f(x)可测，且f(x)=g(x)，a.e. \quad x ∈ E\)&lt;/span>可测，则&lt;span class="math">\(g(x)\)&lt;/span>可测.&lt;/p>
&lt;/blockquote>
&lt;p>证明：令&lt;span class="math">\(A=\{x:f(x)\neq g(x)\}\)&lt;/span>，则&lt;span class="math">\(m(A)=0\)&lt;/span>，且&lt;span class="math">\(E-A\)&lt;/span>是可测集。 &lt;span class="math">\[\begin{aligned}
&amp;amp;\{x ∈ E:g(x)&amp;gt;t\}\\
&amp;amp;=\{x ∈ E-A:g(x)&amp;gt;t\}\cup \{x ∈ A:g(x)&amp;gt;t\}\\
&amp;amp;=\{x ∈ E-A:f(x)&amp;gt;t\}\cup \{x ∈ A:g(x)&amp;gt;t\}\\
&amp;amp;第一个因为f(x)=g(x)，第二个因为m(A)=0\\
&amp;amp;=\{x ∈ E-A:f(x)&amp;gt;t\} 为可测集
\end{aligned}\]&lt;/span> 由此可知，对于一个可测函数来说，&lt;strong>当改变它在零测集上的值时不会改变函数的可测性&lt;/strong>&lt;/p>
&lt;h2 id="勒贝格可测函数">勒贝格可测函数&lt;/h2>
&lt;blockquote>
&lt;p>定义：可测函数：&lt;span class="math">\(f(x)\)&lt;/span>是定义在可测集&lt;span class="math">\(E\subset R^n\)&lt;/span>上的广义实值函数，如果对于&lt;span class="math">\(\forall t ∈ R^1\)&lt;/span>，有点集 &lt;span class="math">\[\{x ∈ E:f(x)&amp;gt;t\}(或简写作x:f(x)&amp;gt;t)\]&lt;/span> 是可测集，则称&lt;span class="math">\(f(x)\)&lt;/span>是在&lt;span class="math">\(E\)&lt;/span>上&lt;strong>可测函数&lt;/strong>，或称&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上&lt;strong>可测&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>这一定义中虽然指的是对任意的&lt;span class="math">\(t\in R^1\)&lt;/span>，但实际上我们只需要对&lt;span class="math">\(R^1\)&lt;/span>中的一个稠密集中的元&lt;span class="math">\(r\)&lt;/span>，指出集合&lt;span class="math">\(\{x:f(x)&amp;gt;r\}\)&lt;/span>是可测集即可。&lt;/p>
&lt;h3 id="补充稠密与疏朗">补充：稠密与疏朗&lt;/h3>
&lt;blockquote>
&lt;p>补充定义：稠密。若&lt;span class="math">\(A⊂B且\bar A=B\)&lt;/span>， 则称A在B中稠密，或称A是B的稠密子集。A的闭包等于B。&lt;/p>
&lt;p>稠密的相对概念是无处稠密，又称疏朗。如果一个闭包的内核是空集，即&lt;span class="math">\(\bar E^\circ=\emptyset\)&lt;/span>，则称&lt;span class="math">\(E\)&lt;/span>在&lt;span class="math">\(R^n\)&lt;/span>上是无处稠密集（疏朗集）。&lt;/p>
&lt;/blockquote>
&lt;p>例如，&lt;span class="math">\(E\subset R^n\)&lt;/span>，若&lt;span class="math">\(\bar E =R^n\)&lt;/span>则称&lt;span class="math">\(E\)&lt;/span>为&lt;span class="math">\(R^n\)&lt;/span>中的稠密集。有理数集和无理数集都是稠密集。整数、&lt;span class="math">\(集合\{1,1/2,1/3,\dots \}\)&lt;/span>、Cantor集在实数轴&lt;span class="math">\(R\)&lt;/span>上是无处稠密集。&lt;/p>
&lt;p>无处稠密的闭集的补集是一个稠密的开集，因此无处稠密集的补集是内部为稠密的集合。一个无处稠密集并不一定就是可忽略的。例如，如果&lt;span class="math">\(X\)&lt;/span>位于单位区间&lt;span class="math">\([0,1]\)&lt;/span>，&lt;strong>不仅有可能有勒贝格测度为零的稠密集（例如有理数集），也有可能有测度为正数的无处稠密集&lt;/strong>。&lt;/p>
&lt;p>例如（一个康托尔集的变体），从&lt;span class="math">\([0,1]\)&lt;/span>内移除所有形为&lt;span class="math">\(a/2n\)&lt;/span>的最简二进分数，以及旁边的区间&lt;span class="math">\([a/2^n − 1/2^{2n+1}, a/2^n + 1/2^{2n+1}]\)&lt;/span>；由于对于每一个&lt;span class="math">\(n\)&lt;/span>，这最多移除了总和为&lt;span class="math">\(1/2^{n+1}\)&lt;/span>的区间，留下的无处稠密集的测度就至少是&lt;span class="math">\(1/2\)&lt;/span>（实际上刚刚大于0.535……，因为重叠的原因），因此在某种意义上表示了&lt;span class="math">\([0,1]\)&lt;/span>的大多数空间。&lt;/p>
&lt;blockquote>
&lt;p>定理：设&lt;span class="math">\(f(x)\)&lt;/span>是可测集&lt;span class="math">\(E\)&lt;/span>上的函数，&lt;span class="math">\(D\)&lt;/span>是&lt;span class="math">\(R^1\)&lt;/span>中的一个稠密集。若对任意的&lt;span class="math">\(r ∈ D\)&lt;/span>，点集&lt;span class="math">\(\{x:f(x)&amp;gt;r\}\)&lt;/span>都是可测集，则对任意的&lt;span class="math">\(t\in R^1\)&lt;/span>，点集&lt;span class="math">\(\{x:f(x)&amp;gt;t\}\)&lt;/span>也是可测集。&lt;/p>
&lt;/blockquote>
&lt;p>证明：任选一个实数&lt;span class="math">\(t\)&lt;/span>，因为&lt;span class="math">\(D\)&lt;/span>在&lt;span class="math">\(R^1\)&lt;/span>中稠密，所以我们能够在&lt;span class="math">\(D\)&lt;/span>中取一点列&lt;span class="math">\(\{r_k\}\)&lt;/span>，使得 &lt;span class="math">\[r_k≥t(k=1,2,\dotsb); \lim_{k→ ∞}r_k=t\]&lt;/span> 我们有 &lt;span class="math">\[\{x:f(x)&amp;gt;t\}=\bigcup_{k=1}^∞ \{x:f(x)&amp;gt;r_k\}\]&lt;/span> 因为每一个点集&lt;span class="math">\(\{x:f(x)&amp;gt;r_k\}\)&lt;/span>都是可测集，可测集的任意并集也是可测集，所以&lt;span class="math">\(\{x:f(x)&amp;gt;t\}\)&lt;/span>也是可测集。&lt;/p>
&lt;blockquote>
&lt;p>定理：等价定义：对于&lt;span class="math">\(E\)&lt;/span>上的可测函数&lt;span class="math">\(f(x)，t ∈ R^1\)&lt;/span>，则以下点集都可测：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\{x:f(x)≤t\}=E-\{x:f(x)&amp;gt;t\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\{x:f(x)≥t\}=\bigcap\limits_{k=1}^∞\{x:f(x)&amp;gt;t-\frac{1}{k}\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\{x:f(x)&amp;lt;t\}=E-\{x:f(x)≥t\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\{x:f(x)=t\}=\{x:f(x)≥t\}\cap \{x:f(x)≤t\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\{x:f(x)&amp;lt;+∞\}=\bigcup\limits_{k=1}^∞\{x:f(x)&amp;lt;k\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\{x:f(x)=+∞\}=E-\{x:f(x)&amp;lt;+∞\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\{x:f(x)&amp;gt;-∞\}=\bigcup\limits_{k=1}^∞\{x:f(x)&amp;gt;-k\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\{x:f(x)=-∞\}=E-\{x:f(x)&amp;gt;-∞\}\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h3 id="可测函数运算性质">可测函数运算性质&lt;/h3>
&lt;p>如果&lt;span class="math">\(f(x)，g(x)\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上可测：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(f(x)在E_1，E_2\)&lt;/span>上可测，那么&lt;span class="math">\(f(x)在E_1\cup E_2\)&lt;/span>上可测。&lt;/li>
&lt;li>&lt;span class="math">\(f(x)在E\)&lt;/span>的子集上可测。&lt;/li>
&lt;li>&lt;span class="math">\(cf(x)，c ∈ R^1\)&lt;/span>可测&lt;/li>
&lt;li>&lt;span class="math">\(f(x)\pm g(x)\)&lt;/span>可测&lt;/li>
&lt;li>&lt;span class="math">\(f(x)\times g(x)\)&lt;/span>可测&lt;/li>
&lt;li>&lt;span class="math">\(\sup_{k≥1}\{f_k(x)\}\)&lt;/span>可测&lt;/li>
&lt;li>&lt;span class="math">\(\inf_{k≥1}\{f_k(x)\}\)&lt;/span>可测&lt;/li>
&lt;li>&lt;span class="math">\(\overline{\lim}_{k→∞}\{f_k(x)\}\)&lt;/span>可测&lt;/li>
&lt;li>&lt;span class="math">\(\underline{\lim}_{k→∞}\{f_k(x)\}\)&lt;/span>可测&lt;/li>
&lt;li>&lt;span class="math">\({\lim}_{k→∞}\{f_k(x)\}=f(x)\)&lt;/span>，且&lt;span class="math">\(\{f_k(x)\}\)&lt;/span>是可测函数列，那么&lt;span class="math">\(f(x)\)&lt;/span>可测&lt;/li>
&lt;/ol>
&lt;h3 id="简单函数">简单函数&lt;/h3>
&lt;blockquote>
&lt;p>定义：简单函数又称单纯函数，（英语：simple function），在数学的实分析中是指&lt;strong>值域只有有限个值&lt;/strong>的实函数，类似阶梯函数。有些作者要求简单函数是可测的，因为在实际应用上，特别在讨论勒贝格积分时，必须是可测函数，要不然积分的定义没有意义。&lt;/p>
&lt;/blockquote>
&lt;p>对于取值为&lt;span class="math">\(\{c_1,c_2,\dots,c_p\}\)&lt;/span>的简单函数有： &lt;span class="math">\[E=\bigcap_{i=1}^{p}E_i;E_i\cap E_j =\emptyset,i,j=1,2,3,\dots,p\\
f(x)=c_i; x ∈ E_i;\]&lt;/span> 此外可以记&lt;span class="math">\(f\)&lt;/span>为 &lt;span class="math">\[f(x)=\sum_{i=1}^p c_i \chi_{E_i}(x),x ∈ E\]&lt;/span> 从而简单函数时&lt;strong>有限个特征函数的线性组合&lt;/strong>。特别的，当每一个&lt;span class="math">\(E_i\)&lt;/span>都是一个矩体，那么&lt;span class="math">\(f(x)\)&lt;/span>是一个阶梯函数。显然对于简单函数&lt;span class="math">\(f(x)，g(x)\)&lt;/span>，&lt;span class="math">\(f(x)\pm g(x)，f(x)\cdot g(x)\)&lt;/span>都是简单函数。&lt;/p>
&lt;p>如果简单函数中，每个值的集合&lt;span class="math">\(E_i\)&lt;/span>都是可测的，那么称&lt;span class="math">\(f(x)\)&lt;/span>是&lt;strong>可测简单函数&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>定理：简单函数逼近定理：&lt;/p>
&lt;p>（1）.非负可测函数：若&lt;span class="math">\(f(x)\)&lt;/span>是&lt;span class="math">\(E\)&lt;/span>上的非负可测简单函数，则存在&lt;strong>非负可测简单函数渐升列&lt;/strong>： &lt;span class="math">\[\varphi_k(x)≤\varphi_{k+1}(x)，k=1,2,\dots\\
\lim_{k→∞}\varphi_k(x)=f(x)，x ∈ E\]&lt;/span> （2）.可测函数：若&lt;span class="math">\(f(x)\)&lt;/span>是&lt;span class="math">\(E\)&lt;/span>上的可测简单函数，则存在&lt;strong>可测简单函数列&lt;/strong>&lt;span class="math">\(\{\varphi_k(x)\}\)&lt;/span>，使得&lt;span class="math">\(|\varphi_k(x)|≤|f(x)|\)&lt;/span>，且有 &lt;span class="math">\[\lim_{k→∞}\varphi_k(x)=f(x)，x ∈ E\]&lt;/span> 若&lt;span class="math">\(f(x)\)&lt;/span>还是有界的，则上述收敛是一致的。&lt;/p>
&lt;/blockquote>
&lt;p>证明：（1）对于任意自然数&lt;span class="math">\(k\)&lt;/span>，我们将&lt;span class="math">\([0,k]\)&lt;/span>划分称&lt;span class="math">\(k\cdot 2^k\)&lt;/span>等分，并记 &lt;span class="math">\[E_{k,j}=\biggl\{x ∈ E:\frac{j-1}{2^k}≤f(x)&amp;lt;\frac{j}{2^k}\biggl\},\\
E_k=\biggl\{x ∈ E:f(x)≥k\biggl\},\\
j=1,2,\dotsb,k\cdot 2^k,k=1,2,\dotsb.\]&lt;/span> 设函数为 &lt;span class="math">\[\varphi_k(x)=\begin{cases}
\frac{j-1}{2^k},\quad x ∈ E_{k,j},\\
k,\quad x ∈ E_{k},
\end{cases}\\
j=1,2,\dotsb,k\cdot 2^k,k=1,2,\dotsb.\]&lt;/span> 可直接写成； &lt;span class="math">\[\varphi_k(x)=k\chi_{E_k}(x)+\sum_{j=1}^{k2^k}\frac{j-1}{2^k}\chi_{E_{k,j}}(x),\chi(x)为特征函数\]&lt;/span> 其效果如下图： &lt;img src="../images/简单函数逼近.svg" alt="简单函数逼近.svg" /> 从图中很明显的看出：每个简单函数都是非负可测简单函数，并且函数列是递增的趋近于&lt;span class="math">\(f(x)\)&lt;/span>，即&lt;span class="math">\(\lim\limits_{k→∞} \varphi_k(x)=f(x)，x ∈ E\)&lt;/span>。&lt;/p>
&lt;p>（2）对于任意可测函数&lt;span class="math">\(f(x)\)&lt;/span>我们可以把它分成非负0的部分&lt;span class="math">\(f^+(x)\)&lt;/span>和负数部分&lt;span class="math">\(f^-(x)（为f(x)负数部分的绝对值）\)&lt;/span>，所以&lt;span class="math">\(f(x)=f^+(x)-f^-(x)\)&lt;/span>。有第(1)部分证明和可测函数运算法则可知，&lt;span class="math">\(f^+(x)\)&lt;/span>与&lt;span class="math">\(f^-(x)\)&lt;/span>都是非负可测函数。于是存在可测简单函数列&lt;span class="math">\(\{\varphi_k^{(1)}(x)\},\{\varphi_k^{(2)}(x)\}\)&lt;/span>满足 &lt;span class="math">\[\lim_{k→∞} \varphi_k^{(1)}(x)=f^+(x)，\lim_{k→∞} \varphi_k^{(2)}(x)=f^-(x)，x ∈ E\]&lt;/span> 显然，可测函数的差&lt;span class="math">\(\varphi_k^{(1)}(x)-\varphi_k^{(2)}(x)\)&lt;/span>是简单可测函数，且有 &lt;span class="math">\[\lim_{k→ i}[\varphi_k^{(1)}(x)-\varphi_k^{(2)}(x)]=f^+(x)-f^-(x)=f(x)， x ∈ E\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定义：支集：对于在&lt;span class="math">\(E\subset R^n\)&lt;/span>上的函数&lt;span class="math">\(f(x)\)&lt;/span>，我们称点集 &lt;span class="math">\[\overline{\{x:f(x)\neq 0\}}\]&lt;/span> 为&lt;span class="math">\(f(x)\)&lt;/span>的支集，记为&lt;span class="math">\(\mathop{supp}(f)\)&lt;/span>。若&lt;span class="math">\(f(x)\)&lt;/span>的支集是有界的，则称&lt;span class="math">\(f(x)\)&lt;/span>是&lt;strong>具有紧支集&lt;/strong>的函数。&lt;/p>
&lt;/blockquote>
&lt;h2 id="可测函数的构造">可测函数的构造&lt;/h2>
&lt;h2 id="补充集合与函数">补充：集合与函数&lt;/h2>
&lt;p>示性函数（特征函数，Characteristic function）可以代表不同的概念。最通常且多数通称为指示函数。 &lt;span class="math">\[{\mathbf {1}}_{A}:X\to \{0,1\}或者写作\\
\chi(A)=\begin{cases}
1,\quad x ∈ A\\
0,\quad x \notin A
\end{cases}\]&lt;/span> 其中在集合&lt;span class="math">\(X\)&lt;/span>中任一子集合&lt;span class="math">\(A\)&lt;/span>,皆满足于集合&lt;span class="math">\(A\)&lt;/span>内一点为值1,于集合&lt;span class="math">\(X − A\)&lt;/span>内一点为值 0。&lt;/p>
&lt;p>显然&lt;span class="math">\(\chi(A)=\chi(B)\Leftrightarrow A=B\)&lt;/span>。&lt;/p>
&lt;p>一般的，示性函数（特征函数）的运算法则如下： &lt;img src="../images/示性函数运算法则.png" alt="示性函数运算法则" />&lt;/p></description></item><item><title>变分法1-最小作用量</title><link>https://surprisedcat.github.io/studynotes/%E5%8F%98%E5%88%86%E6%B3%951-%E6%9C%80%E5%B0%8F%E4%BD%9C%E7%94%A8%E9%87%8F/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%8F%98%E5%88%86%E6%B3%951-%E6%9C%80%E5%B0%8F%E4%BD%9C%E7%94%A8%E9%87%8F/</guid><description>
&lt;h2 id="知乎转载最小作用量原理">[知乎转载]最小作用量原理&lt;!-- omit in toc -->&lt;/h2>
&lt;p>作者：Y.Galbort 北大在读，空间物理专业，太阳物理方向&lt;/p>
&lt;blockquote>
&lt;p>本文参照了朗道《力学》的写法，先开宗明义地给出最小作用量原理的概念。但我刚刚读朗道这本经典之作时，其实并不理解为什么可以给予最小作用量原理这样一个公理性的地位，毕竟它看起来既不显然、也不简明。本文后面的内容就是后来我对于这一疑问寻找到的解答，可能也有很多不完善或者不准确的地方，仅供大家参考。&lt;/p>
&lt;/blockquote>
&lt;h2 id="最小作用量原理的表述">最小作用量原理的表述&lt;/h2>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/168781828">https://zhuanlan.zhihu.com/p/168781828&lt;/a>&lt;/p>
&lt;p>变分与拉格朗日量(1)：背景，定义和途径 &lt;a href="https://zhuanlan.zhihu.com/p/296201074">https://zhuanlan.zhihu.com/p/296201074&lt;/a>&lt;/p></description></item><item><title>变分法2-欧拉-拉格朗日方程</title><link>https://surprisedcat.github.io/studynotes/%E5%8F%98%E5%88%86%E6%B3%952-%E6%AC%A7%E6%8B%89-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%96%B9%E7%A8%8B/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%8F%98%E5%88%86%E6%B3%952-%E6%AC%A7%E6%8B%89-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%96%B9%E7%A8%8B/</guid><description>
&lt;h2 id="知乎转载变分法基础欧拉-拉格朗日方程的导出">[知乎转载]：变分法基础&amp;amp;欧拉-拉格朗日方程的导出&lt;!-- omit in toc -->&lt;/h2>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/168789203">https://zhuanlan.zhihu.com/p/168789203&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/168789203">https://zhuanlan.zhihu.com/p/168789203&lt;/a>&lt;/p></description></item><item><title>变分法3-浅谈变分原理</title><link>https://surprisedcat.github.io/studynotes/%E5%8F%98%E5%88%86%E6%B3%953-%E6%B5%85%E8%B0%88%E5%8F%98%E5%88%86%E5%8E%9F%E7%90%86/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%8F%98%E5%88%86%E6%B3%953-%E6%B5%85%E8%B0%88%E5%8F%98%E5%88%86%E5%8E%9F%E7%90%86/</guid><description>
&lt;h2 id="知乎转载浅谈变分原理">[知乎转载]：浅谈变分原理&lt;!-- omit in toc -->&lt;/h2>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/139018146">https://zhuanlan.zhihu.com/p/139018146&lt;/a>&lt;/p></description></item><item><title>变分法4-运动常数、拉格朗日乘子法</title><link>https://surprisedcat.github.io/studynotes/%E5%8F%98%E5%88%86%E6%B3%954-%E8%BF%90%E5%8A%A8%E5%B8%B8%E6%95%B0%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%8F%98%E5%88%86%E6%B3%954-%E8%BF%90%E5%8A%A8%E5%B8%B8%E6%95%B0%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/</guid><description>
&lt;h2 id="知乎转载续浅谈变分原理运动常数拉格朗日乘子法">[知乎转载]：续浅谈变分原理：运动常数、拉格朗日乘子法&lt;!-- omit in toc -->&lt;/h2>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/148762023">https://zhuanlan.zhihu.com/p/148762023&lt;/a>&lt;/p></description></item><item><title>测度论3.5之内侧度</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA3.5%E4%B9%8B%E5%86%85%E4%BE%A7%E5%BA%A6/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA3.5%E4%B9%8B%E5%86%85%E4%BE%A7%E5%BA%A6/</guid><description>
&lt;h2 id="内测度">内测度&lt;!-- omit in toc -->&lt;/h2>
&lt;p>有外测度，那有没有内测度 (inner measure)呢？还真有，但是在测度的构造中，内测度不是必须的，这边简单介绍一下。外测度试图从集合的外面向内逼近，而内测度则是从集合的里面向外逼近。&lt;/p>
&lt;blockquote>
&lt;p>定义：内测度是一个对某个集合&lt;span class="math">\(X\)&lt;/span>的所有子集有定义的一个函数&lt;span class="math">\(\varphi :2^{X}\rightarrow [0,\infty ],\)&lt;/span> 满足下列条件:&lt;/p>
&lt;ul>
&lt;li>空集: 空集的内测度为0。&lt;span class="math">\(\varphi (\varnothing )=0\)&lt;/span>&lt;/li>
&lt;li>超加性：对两个交集为空的集合A和B，有&lt;span class="math">\(\varphi (A\cup B)\geq \varphi (A)+\varphi (B).\)&lt;/span>&lt;/li>
&lt;li>集合降链的极限：对一个集合序列&lt;span class="math">\(A_{j}\)&lt;/span>，若对于所有的&lt;span class="math">\(j\)&lt;/span>满足&lt;span class="math">\(A_{j}\supseteq A_{j+1}\)&lt;/span>，且&lt;span class="math">\(\varphi (A_{1})&amp;lt;\infty\)&lt;/span>，则&lt;span class="math">\(\varphi \left(\bigcap _{j=1}^{\infty }A_{j}\right)=\lim _{j\to \infty }\varphi (A_{j})\)&lt;/span>&lt;/li>
&lt;li>若集合&lt;span class="math">\(A\)&lt;/span>满足&lt;span class="math">\(\varphi (A)=\infty\)&lt;/span>，则对所有正数&lt;span class="math">\(c\)&lt;/span>, 存在&lt;span class="math">\(A\)&lt;/span>的一个子集&lt;span class="math">\(B\)&lt;/span>，使得&lt;span class="math">\(c\leq \varphi (B)&amp;lt;\infty\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>另外一个定义可以从外测度诱导出来： &amp;gt;定义2：令&lt;span class="math">\(X\)&lt;/span>为一集合，&lt;span class="math">\(\mu^\ast\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>上的一个外测度。定义内测度&lt;span class="math">\(\mu_*(A):=\mu^\ast(A)-\mu^\ast(A^c)\)&lt;/span>&lt;/p>
&lt;p>内测度可以从测度中构造出来： &amp;gt;定义3：令&lt;span class="math">\((X,\mathcal{M},\mu)\)&lt;/span>为一测度空间, 定义内侧度&lt;span class="math">\(\mu_*(E):=\sup\{\mu(S):S\in\mathcal{A}，S\subset E\}\)&lt;/span>&lt;/p>
&lt;p>还有以下定理 &amp;gt;定理：令&lt;span class="math">\(\mu\)&lt;/span>是&lt;span class="math">\(\mathbb{R}^n\)&lt;/span>上的 Lebesgue 测度，&lt;span class="math">\(\mu_*(A)=\mu^\ast(A)\)&lt;/span>当且仅当&lt;span class="math">\(A\)&lt;/span>为&lt;span class="math">\(\mu\)&lt;/span>-可测。&lt;/p></description></item><item><title>测度论3之外测度的他山之石</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA3%E4%B9%8B%E5%A4%96%E6%B5%8B%E5%BA%A6%E7%9A%84%E4%BB%96%E5%B1%B1%E4%B9%8B%E7%9F%B3/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA3%E4%B9%8B%E5%A4%96%E6%B5%8B%E5%BA%A6%E7%9A%84%E4%BB%96%E5%B1%B1%E4%B9%8B%E7%9F%B3/</guid><description>
&lt;h2 id="学习笔记外测度">学习笔记——外测度&lt;!-- omit in toc -->&lt;/h2>
&lt;p>转载自知乎&lt;https://zhuanlan.zhihu.com/p/106119812>。其中本人修正了一些小错误。&lt;/p>
&lt;p>原作者：&lt;a href="https://www.zhihu.com/people/tang-shu-43-28">路边社特约记者&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#引论">引论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#外测度的抽象定义">外测度的抽象定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#构造外测度的一般方法">构造外测度的一般方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从外测度到测度">从外测度到测度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#caratheodorys定理的应用">Caratheodory's定理的应用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#准测度外侧度测度">准测度→外侧度→测度&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="引论">引论&lt;/h2>
&lt;p>外测度的基本想法是用一些形状良好的，已经定义了类似测度概念（称为类测度）的集合去尽可能“小”的覆盖其他集合，然后用这些集合的”类测度“的和作为被覆盖集合的外测度。&lt;/p>
&lt;p>譬如在实数轴&lt;span class="math">\(R\)&lt;/span>上，定义开区间&lt;span class="math">\(I=(a,b)\)&lt;/span>的类测度为其长度&lt;span class="math">\(\rho(I)=b-a\)&lt;/span>,那么可以定义&lt;span class="math">\(R\)&lt;/span>中任意集合&lt;span class="math">\(E\)&lt;/span>的外测度为： &lt;span class="math">\[\mu^\ast(E)=\inf\biggl\{\sum\limits_{j=1}^∞ \rho(I_j):E\subset \bigcup\limits_{j=1}^∞ I_j \biggl\}\]&lt;/span> 直观的讲，就是把无数个小区间拼起来，让它们盖住原来的集合，而且要让冗余的面积尽可能的小。在进一步构造外测度之前，先给出外测度的抽象定义。&lt;/p>
&lt;h2 id="外测度的抽象定义">外测度的抽象定义&lt;/h2>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(X\)&lt;/span>为任意集合，记&lt;span class="math">\(\mathcal{P}(X)\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>所有子集的族，所谓&lt;span class="math">\(X\)&lt;/span>上的一个外测度是指一个函数&lt;span class="math">\(\mu^\ast:\mathcal{P}(X)→[0,+∞)\)&lt;/span>，满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\mu^\ast(\emptyset)=0\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(A\subset B \subset X\)&lt;/span>，则&lt;span class="math">\(\mu^\ast(A)≤\mu^\ast(B)\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(\{A_n\}_{n=1}^∞\subset \mathcal{P}(X)\)&lt;/span>，则&lt;span class="math">\(\mu^\ast(\bigcup\limits_{n=1}^∞ A_n)≤\sum\limits_{n=1}^∞\mu^\ast(A_n)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h2 id="构造外测度的一般方法">构造外测度的一般方法&lt;/h2>
&lt;p>如引论中所述，为了构造在集合&lt;span class="math">\(X\)&lt;/span>中的外测度，一般我们先在&lt;span class="math">\(X\)&lt;/span>中某个子集族&lt;span class="math">\(\mathcal{C}\)&lt;/span>上构建类似测度的概念，即定义一个函数 &lt;span class="math">\[\rho:\mathcal{C}→[0,+∞)\]&lt;/span>&lt;/p>
&lt;p>且要求：&lt;span class="math">\(\emptyset∈\mathcal{C}，X ∈ \mathcal{C}，\rho(\emptyset)=0\)&lt;/span>。接着便可以在&lt;span class="math">\(X\)&lt;/span>中定义，&lt;span class="math">\(\forall A \subset X\)&lt;/span>: &lt;span class="math">\[\mu^\ast:=\inf\biggl\{\sum\limits_{j=1}^∞ \rho(E_j): A\subset \bigcup\limits_{j=1}^∞ E_j，\{E_j\}_{j=1}^∞ \subset \mathcal{C}\biggl\}\]&lt;/span> 自然首先应该证明上面的定义满足外测度的三条性质。&lt;/p>
&lt;blockquote>
&lt;p>定理一：如上定义的&lt;span class="math">\(\mu^\ast\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>上一个外测度.&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../images/外测度构造证明.png" alt="外侧度构造证明" />&lt;p class="caption">外侧度构造证明&lt;/p>
&lt;/div>
&lt;h2 id="从外测度到测度">从外测度到测度&lt;/h2>
&lt;p>外测度的思想很简单且对于&lt;span class="math">\(X\)&lt;/span>的幂集都适用，但外测度却并不满足我们对测度所期待的全部性质。具体的讲，类比于面积和体积的概念，我们希望测度至少满足：&lt;/p>
&lt;p>（1）一个集合的测度应该等于把该集合分成任意多（至少是可数）个不相交集合的测度的和；&lt;/p>
&lt;p>（2）在一些空间中，譬如欧式空间中，对集合进行平移、旋转、反射等操作应该不改变集合的测度；&lt;/p>
&lt;p>当然我们希望测度满足的性质不止这两条，但这两条性质从某种意义上说就是互相矛盾的。譬如在三维空间中，Banach和Tarski证明了所谓的分球悖论，即仅通过平移、旋转、反射就能将一个球分成同样的两个球。那么性质（1）和（2）显然不可能同时满足。&lt;/p>
&lt;p>有两种方式处理这种情况。【一】是抛弃掉某些性质，【二】是对测度的定义域做一些限制。做法【一】并不令人满意，因为性质（1）和（2）显然是我们对三维空间物体体积的最直观且最基本的概念。所以做法【二】是比较合适的。&lt;/p>
&lt;p>一个简单的定义是， &amp;gt;对&lt;span class="math">\(X\)&lt;/span>的一个子集&lt;span class="math">\(A\)&lt;/span>，&lt;span class="math">\(A\)&lt;/span>如果满足： &amp;gt;&lt;span class="math">\[\forall E \subset X，\mu^\ast(E)=\mu^\ast(E\cap A)+\mu^\ast(E\cap A^c)\]&lt;/span> &amp;gt;则称集合&lt;span class="math">\(A\)&lt;/span>是&lt;span class="math">\(\mu^\ast\)&lt;/span>可测的（&lt;strong>我们的可测是指某个集合可测&lt;/strong>）。&lt;/p>
&lt;p>我们记&lt;span class="math">\(\mathcal{M}\)&lt;/span>为所有&lt;span class="math">\(\mu^\ast\)&lt;/span>可测的集合的族，那么：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\mathcal{M}\)&lt;/span>是一个&lt;span class="math">\(\sigma\)&lt;/span>代数。&lt;/li>
&lt;li>记&lt;span class="math">\(\mu\)&lt;/span>是&lt;span class="math">\(\mu^\ast\)&lt;/span>在&lt;span class="math">\(\mathcal{M}\)&lt;/span>上的限制，即&lt;span class="math">\(\mu=\mu^\ast|_{\mathcal{M}}\)&lt;/span>，那么&lt;span class="math">\(\mu\)&lt;/span>是一个完备的测度。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>定理二（Caratheodory's theorem):如上两条性质成立。即外测度在其可测集上是一个完备的测度。&lt;/p>
&lt;p>Proof：&lt;/p>
&lt;p>&lt;strong>首先证明&lt;span class="math">\(\mathcal{M}\)&lt;/span>是一个&lt;span class="math">\(\sigma\)&lt;/span>代数&lt;/strong>，即证明&lt;span class="math">\(\mathcal{M}\)&lt;/span>满足对补运算和可数并运算的封闭性。&lt;/p>
&lt;p>（补运算封闭性）由等式&lt;span class="math">\(\mu^\ast(E)=\mu^\ast(E\cap A)+\mu^\ast(E\cap A^c)\)&lt;/span>对&lt;span class="math">\(A,A^c\)&lt;/span>的对称性知&lt;span class="math">\(A ∈ \mathcal{M}\Leftrightarrow A^c ∈ \mathcal{M}\)&lt;/span>。&lt;/p>
&lt;p>（可数并封闭性）先证明有限并的封闭性。设&lt;span class="math">\(A,B ∈ \mathcal{M}\)&lt;/span>，且注意到 &lt;span class="math">\[A\cup B = (A\cap B)\cup(A\cap B^c)\cup(A^c \cap B)\]&lt;/span> 所以&lt;span class="math">\(\forall E\subset X\)&lt;/span> &lt;span class="math">\[\begin{aligned}
&amp;amp;\mu^\ast(E\cap(A\cup B))+\mu^\ast(E\cap(A\cup B)^c)\\
&amp;amp;=\mu^\ast(E\cap((A\cap B)\cup(A\cap B^c)\cup(A^c \cap B)))+\mu^\ast(E\cap(A\cup B)^c)\\
&amp;amp;≤\mu^\ast(E\cap A\cap B)+\mu^\ast(E\cap A\cap B^c)+\mu^\ast(E\cap A^c\cap B)+\mu^\ast(E\cap(A\cup B)^c)\\
&amp;amp;(外测度定义条件3)\\
&amp;amp;=\mu^\ast(E\cap A)+\mu^\ast(E\cap A^c)(\mu^\ast可测定义)\\
&amp;amp;=\mu^\ast(E)(\mu^\ast可测定义)\\
&amp;amp;即\mu^\ast(E\cap(A\cup B))+\mu^\ast(E\cap(A\cup B)^c)≤\mu^\ast(E)
\end{aligned}\]&lt;/span> 而由外测度定义3可知： &lt;span class="math">\[\mu^\ast(E)≤\mu^\ast(E\cap(A\cup B))+\mu^\ast(E\cap(A\cup B)^c)\]&lt;/span> 所以&lt;span class="math">\(\mu^\ast(E)=\mu^\ast(E\cap(A\cup B))+\mu^\ast(E\cap(A\cup B)^c)\)&lt;/span>&lt;/p>
&lt;p>所以&lt;span class="math">\(A\cup B ∈ \mathcal{M}\)&lt;/span>。然后由&lt;strong>数学归纳法&lt;/strong>可以得到&lt;strong>有限并&lt;/strong>的封闭性。&lt;/p>
&lt;p>再证明可数并的封闭性。令&lt;span class="math">\(F_N=\bigcup\limits_{n=1}^N A_n，F=\bigcup\limits_{n=1}^∞ A_n\)&lt;/span>，因为&lt;span class="math">\(F_N\subset F\Rightarrow F_N^c \supset F^c\)&lt;/span>，所以根据&lt;span class="math">\(\mu^\ast\)&lt;/span>可测条件，对&lt;span class="math">\(\forall E \subset X，\forall N&amp;gt;0\)&lt;/span>有 &lt;span class="math">\[\mu^\ast(E)=\mu^\ast(E\cap F_N)+\mu^\ast(E\cap F_N^c)\\
≥\mu^\ast(E\cap F_N)+\mu^\ast(E\cap F^c)\]&lt;/span> 对&lt;span class="math">\(N\)&lt;/span>取极限则有 &lt;span class="math">\[\mu^\ast(E)≥\lim_{N→∞}\mu^\ast(E\cap F_N)+\mu^\ast(E\cap F^c)\\
=\mu^\ast(E\cap F)+\mu^\ast(E\cap F^c)\]&lt;/span> 而由外测度定义3可知， &lt;span class="math">\[\mu^\ast(E)=\mu^\ast((E\cap F)\cup (E\cap F^c))≤\mu^\ast(E\cap F)+\mu^\ast(E\cap F^c)\]&lt;/span> 综上，&lt;span class="math">\(\mu^\ast(E)=\mu^\ast(E\cap F)+\mu^\ast(E\cap F^c)\)&lt;/span>。满足可列可加性。&lt;/p>
&lt;p>&lt;span class="math">\(\mathcal M\)&lt;/span>满足互补集封闭，可列可加性，所以是一个&lt;span class="math">\(\sigma\)&lt;/span>代数。&lt;/p>
&lt;p>&lt;strong>接着证明&lt;span class="math">\(\mu\)&lt;/span>是一个完备的测度&lt;/strong>。 先说明&lt;span class="math">\(\mu\)&lt;/span>是一个测度。&lt;/p>
&lt;p>（A）&lt;span class="math">\(\mu(\emptyset)=0\)&lt;/span>，显然外测度的定义。&lt;/p>
&lt;p>（B）先证明有限可加性。设&lt;span class="math">\(A,B ∈ \mathcal{M}，A\cap B = \emptyset\)&lt;/span>，因为&lt;span class="math">\(\mu=\mu^\ast|_{\mathcal{M}}\)&lt;/span>，则依据&lt;span class="math">\(\mu^\ast\)&lt;/span>可测条件： &lt;span class="math">\[\mu(A\cup B)=\mu((A\cup B)\cap A)+\mu((A\cup B)\cap A^c)\\
=\mu(A)+\mu(B)\]&lt;/span> 则再根据归纳法可得有限可加性。&lt;/p>
&lt;p>再证明对于可数可加加性。设&lt;span class="math">\(\{A_n\}_{n=1}^∞ ∈ \mathcal{M}\)&lt;/span>，且两两不相交。令&lt;span class="math">\(F_N=\bigcup\limits_{n=1}^N A_n，F=\bigcup\limits_{n=1}^∞ A_n\)&lt;/span>，有： &lt;span class="math">\[\begin{aligned}
\mu(F)&amp;amp;=\mu(F\cap F_n)+\mu(F\cap F_n^c)(\mu^\ast可测条件)\\
&amp;amp;=\mu(\bigcup\limits_{n=1}^N A_n)+\mu(\bigcup\limits_{n=N+1}^∞ A_n)\\
&amp;amp;=\sum_{n=1}^N\mu(A_n)+\mu(\bigcup\limits_{n=N+1}^∞ A_n)（有限可加性）\\
&amp;amp;≥\sum_{n=1}^N\mu(A_n)（非负性）
\end{aligned}\]&lt;/span> 当对&lt;span class="math">\(N\)&lt;/span>取极限时，&lt;span class="math">\(\mu(F)=\mu(\bigcup\limits_{n=1}^∞ A_n)≥\lim\limits_{N→∞}\sum\limits_{n=1}^N\mu(A_n)=\sum\limits_{n=1}^∞\mu(A_n)\)&lt;/span>。&lt;/p>
&lt;p>又因为根据外测度定义3可知： &lt;span class="math">\[\mu(\bigcup_{n=1}^∞ A_n)≤\sum_{n=1}^∞\mu(A_n)\]&lt;/span> 所以&lt;span class="math">\(\mu(\bigcup\limits_{n=1}^∞ A_n)=\sum\limits_{n=1}^∞\mu(A_n)\)&lt;/span>。&lt;/p>
&lt;p>因此&lt;span class="math">\(\mu=\mu^\ast|_{\mathcal{M}}\)&lt;/span>满足测度定义。&lt;/p>
&lt;p>（c）最后证明完备性。设&lt;span class="math">\(A ∈ \mathcal{M}，\mu(A)=0\)&lt;/span>，从而对&lt;span class="math">\(\forall F \subset A，需要证明0≤\mu^\ast(F)≤\mu^\ast(A)=\mu(A)=0\)&lt;/span>，即&lt;span class="math">\(\mu^\ast(F)=0\)&lt;/span>。&lt;/p>
&lt;p>根据外测度定义3：对于&lt;span class="math">\(\forall E \subset X，\mu^\ast(E)≤\mu^\ast(E\cap F)+\mu^\ast(E\cap F^c)\)&lt;/span>。因为&lt;span class="math">\(F \subset A\)&lt;/span>，所以根据外测度单调性&lt;span class="math">\(0≤\mu^\ast(E\cap F)≤\mu^\ast(E \cap A)≤\mu^\ast(A)=0\)&lt;/span>。因此&lt;span class="math">\(\mu^\ast(E\cap F)+\mu^\ast(E\cap F^c)=\mu^\ast(E\cap F^c)≤\mu^\ast(E)\)&lt;/span>。&lt;/p>
&lt;p>综上所述：&lt;span class="math">\(\mu^\ast(E)=\mu^\ast(E\cap F)+\mu^\ast(E\cap F^c)\)&lt;/span>。&lt;span class="math">\(A\)&lt;/span>的子集&lt;span class="math">\(F\)&lt;/span>可测。根据外测度单调性可知，&lt;span class="math">\(0≤\mu^\ast(F)≤\mu^\ast(A)=0\)&lt;/span>。&lt;/p>
&lt;p>所以0测度集的任意子集也可测，即A是完备的。&lt;/p>
&lt;/blockquote>
&lt;p>因此，我们得到一个重要结论，如果把外测度限制再某个&lt;span class="math">\(\sigma\)&lt;/span>代数上，那么可以得到一个完备的测度。&lt;/p>
&lt;h2 id="caratheodorys定理的应用">Caratheodory's定理的应用&lt;/h2>
&lt;h3 id="准测度外侧度测度">准测度→外侧度→测度&lt;/h3>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/106119812">https://zhuanlan.zhihu.com/p/106119812&lt;/a>&lt;/p></description></item><item><title>测度论3之测度的构造</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA3%E4%B9%8B%E6%B5%8B%E5%BA%A6%E7%9A%84%E6%9E%84%E9%80%A0/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA3%E4%B9%8B%E6%B5%8B%E5%BA%A6%E7%9A%84%E6%9E%84%E9%80%A0/</guid><description>
&lt;h2 id="测度的构造">测度的构造&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#外测度测度的基础定义">外测度——测度的基础定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#为什么建立外测度">为什么建立外测度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#外测度定义">外测度定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可测与不可测">可测与不可测&lt;/a>&lt;/li>
&lt;li>&lt;a href="#参考文献">参考文献&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>这一章中，我们给出测度的构造，这是一个比较复杂的过程。&lt;/p>
&lt;p>第1节会介绍一下外测度，第2节会引入最重要的一个测度：一维的Lebesgue测度。第3节则是介绍更多Lebesgue测度相关的结论跟例子。当然，我们会发现Lebesgue测度无法测量实数的所有子集，第4节中我们会给出不可测集。&lt;/p>
&lt;p>从外测度中构造测度的方法有很多的用途，Lebesgue测度就是它的一个应用。在第5节，我们会讨论Caratheodory扩张定理，这是一个构造一般的测度很有力的工具。而在第6节我们利用零测集再进行一次扩张，即完备化。&lt;/p>
&lt;p>让我们来简单想一下如何在实数集上构造Lebesgue测度。首先考虑实数轴上开区间的测度&lt;strong>m&lt;/strong>，直觉告诉我们，它就因该是这个区间的长度。由Lindelof 引理在实数集上的应用，&lt;strong>任意一个实数的开子集都能够表示成可数个互不相交开区间的并&lt;/strong>。那么对于开集&lt;span class="math">\(G=\bigcup\limits_{i=1}^\infty(a_i,b_i)\)&lt;/span>，&lt;span class="math">\((a_i,b_i)_{i\in \mathbb{N}}\)&lt;/span>为互不相交的开区间，那么自然的，我们要求这个测度满足： &lt;span class="math">\[m(G)=\sum\limits_{i=1}^\infty(b_i-a_i)\]&lt;/span> 接下来对更一般的子集&lt;span class="math">\(E\subset\mathbb{R}\)&lt;/span>，我们可能可以定义 &lt;span class="math">\[m(E)=\inf\{m(G):G为开集,E\subset G\}。\]&lt;/span> 这边已经开始有点外测度的意思。当然，这是一个很丰满的想法，只是现实有点骨感；因为在第4节，我们会证明测度&lt;strong>m&lt;/strong>无法测量实数的所有子集。于是我们只能考虑把测度建立在一个比实数幂集要小的&lt;span class="math">\(\sigma\)&lt;/span>-代数上——这个就是构造Lebesgue测度的基本想法。实际的构造可能跟这里的想法略有偏差，比如技术上左开右闭的区间会比开区间好处理很多，所以我们会选择&lt;span class="math">\((a，b]\)&lt;/span>的区间。&lt;/p>
&lt;h2 id="外测度测度的基础定义">外测度——测度的基础定义&lt;/h2>
&lt;h3 id="为什么建立外测度">为什么建立外测度&lt;/h3>
&lt;p>&lt;strong>从内部划分小图形以求图形面积的方式&lt;/strong>是一种比较直观的方式，比如多边形面积往往用内部所含三角形的面积来度量，又如Riemann积分中曲边梯形面积的计算也是从其内部划分小矩形出发再逐步计算的，但是这种方法对&lt;strong>只具有内点的点集&lt;/strong>才有效，对于推广到&lt;strong>一般的点集&lt;/strong>上不是很方便。&lt;/p>
&lt;p>为了对一般点集也能度量出某种“长度、面积或体积”，放弃从点集内部进行扩张的方法，而是采&lt;strong>用从外部挤压的方式&lt;/strong>，也就是用矩形去覆盖点集，然后计算这些矩形的面积总和。&lt;/p>
&lt;p>一般来说，覆盖的点集要比原来的点集的“面积大”，因此这里取所有这种覆盖所求出矩形面积总和的&lt;strong>下确界&lt;/strong>来代表它的某种度量。&lt;/p>
&lt;p>另外还有一个问题：每次覆盖所用的矩形可以有多少个？如果只允许&lt;strong>有限个&lt;/strong>，则由此所建立的度量就是所谓的Jordan容度，这种度量有严重缺陷，Lebesgue将其改造为允许有&lt;strong>可数个&lt;/strong>矩形参与覆盖。&lt;/p>
&lt;h3 id="外测度定义">外测度定义&lt;/h3>
&lt;p>回顾下测度:测度定义&lt;strong>在&lt;span class="math">\(\sigma\)&lt;/span>域&lt;/strong>上，并且满足可列可加性（&lt;span class="math">\(\Rightarrow\)&lt;/span>有限可加性、可数次可加性、有限次可加性、次可加性、单调性）。&lt;/p>
&lt;p>而外侧度定义在&lt;span class="math">\(X\)&lt;/span>的&lt;strong>幂集&lt;/strong>上，并且只需要满足&lt;strong>次可列可加性&lt;/strong>和单调性。&lt;/p>
&lt;blockquote>
&lt;p>外侧度：令&lt;span class="math">\(X\)&lt;/span>为一个集合，外测度被定义为一个映射&lt;span class="math">\(\mu^\ast:\mathcal{P}(X)→[0,+∞]\)&lt;/span>，满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\mu^\ast(\emptyset)=0\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(A \subset B\)&lt;/span>，那么&lt;span class="math">\(\mu^\ast(A)&amp;lt;\mu^\ast(B)\)&lt;/span>(单调性)&lt;/li>
&lt;li>&lt;span class="math">\(\forall A_1,A_2,\dots\subset X，\mu^\ast(\bigcup\limits_{i=1}^∞A^i)≤\sum\limits_{i=1}^∞\mu^\ast(A^i)\)&lt;/span>(次可列可加性)&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>注：外测度定义在&lt;span class="math">\(X\)&lt;/span>是所有子集上，也就是说&lt;span class="math">\(X\)&lt;/span>的任意子集都可以谈论外测度。这与测度不同，测度定义在&lt;span class="math">\(\sigma\)&lt;/span>代数上，即只有一部分&lt;span class="math">\(X\)&lt;/span>子集可以谈论测度。&lt;/p>
&lt;p>“外”测度的外字是指从外侧向内逼近测度，实际上外侧度可以比实际测量值大一点点。在这个思想下，我们可以通过覆盖的概念来构造外侧度。&lt;/p>
&lt;blockquote>
&lt;p>构造外测度：令&lt;span class="math">\(X\)&lt;/span>为一集合，&lt;span class="math">\(\mathcal C\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的包含空集的子集族，&lt;span class="math">\(l\)&lt;/span>是&lt;span class="math">\(\mathcal C\)&lt;/span>上的非负扩展实数值(&lt;span class="math">\(R^+\cup\{0,+∞\}\)&lt;/span>)函数，且&lt;span class="math">\(l\)&lt;/span>在空集处取零。那么定义 &lt;span class="math">\[\mu^\ast(E)=\inf {\biggl \{}\sum_{i=0}^∞ l(A_{i})\,{\bigg |}E\subseteq \bigcup_{i=0}^∞ A_{i},\forall i∈\mathbb{N},A_i ∈ \mathcal{C}{\biggr\}} \tag{1}\]&lt;/span> 则&lt;span class="math">\(\mu^\ast\)&lt;/span>是一个外测度。&lt;/p>
&lt;/blockquote>
&lt;p>这里未要求&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的子集，外测度被定义为&lt;span class="math">\(\mathcal C\)&lt;/span>中元素构成的覆盖的最小值。覆盖必是大于等于原集合的，所以体现了从外部包围的“外”的特点。&lt;/p>
&lt;p>该构造满足外测度的定义的三条要求。&lt;a href="https://zhuanlan.zhihu.com/p/61550686">证明链接&lt;/a>&lt;/p>
&lt;h3 id="可测与不可测">可测与不可测&lt;/h3>
&lt;p>注意到外测度只满足&lt;strong>次可加性&lt;/strong>，遗憾的是，可以证明Lebesgue外测度并不满足可加性。外测度居然不是我们所希望的&lt;span class="math">\(\mathbb{R^n}\)&lt;/span>中点集的测度，怎么办？&lt;/p>
&lt;p>补救的方法是把&lt;strong>使外测度不具有可加性的集合排除在外，并称其为不可测集&lt;/strong>，那么剩下的称为可测集，于是在可测集族上，外测度将真正成为长度、面积或体积的推广，发挥它的作用。&lt;/p>
&lt;p>万幸的是我们在&lt;strong>实际生活中能遇到的一切集合都是Lebesgue可测集&lt;/strong>。&lt;/p>
&lt;p>可以借由外测度来定义&lt;span class="math">\(X\)&lt;/span>中的可测集合： &amp;gt;如果子集合 &lt;span class="math">\(E\subseteq X\)&lt;/span>是&lt;span class="math">\(\mu^\ast\)&lt;/span>-可测的，当且仅当对&lt;span class="math">\(X\)&lt;/span>的任意子集合&lt;span class="math">\(A\)&lt;/span>有： &amp;gt;&lt;span class="math">\[\mu^\ast(A)=\mu^\ast(A\cap E)+\mu^\ast(A\cap E^{c}) \tag{2}\]&lt;/span>&lt;/p>
&lt;p>注：理解上面定义的动机，如果一个外测度要成为测度，那么它需要从次可加变成可加，上面的定义正是涉及到可加的性质。注意到&lt;span class="math">\((A\cap E)\cap(A\cap E^c)=\emptyset，(A\cap E)\cup(A\cap E^c)=A\)&lt;/span>，所以集合&lt;span class="math">\(E\)&lt;/span>就是对集合&lt;span class="math">\(A\)&lt;/span>的一个“分解“，&lt;span class="math">\(\mu^\ast\)&lt;/span>的可加性不会对任意分解成立，但是会对&lt;span class="math">\(E\)&lt;/span>的分解成立，因为&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(\sigma\)&lt;/span>-可测的。&lt;/p>
&lt;p>我们记所有的可测集类的基数为&lt;span class="math">\(\mathcal{u}\)&lt;/span>，实际上&lt;span class="math">\(\mathcal{u}=2^{\aleph_1}\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>Caratheodory条件：令&lt;span class="math">\(\mu^\ast\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>上的一个外测度，那么由所有&lt;span class="math">\(\mu^\ast\)&lt;/span>-可测的集合构成的集合系&lt;span class="math">\(\mathcal A\)&lt;/span>是一个&lt;span class="math">\(\sigma\)&lt;/span>-代数。&lt;span class="math">\(\mu=\mu^\ast|_{\mathcal A}\)&lt;/span>是一个测度，且&lt;span class="math">\(\mathcal A\)&lt;/span>包含了所有的零测集。&lt;/p>
&lt;/blockquote>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/47944282">从Lebesgue外测度到Lebesgue测度&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/61550686">外测度&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/35850693">测度 &amp;amp;外测度 &amp;amp;测度的完备化&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zh.wikipedia.org/zh-hans/%E5%A4%96%E6%B5%8B%E5%BA%A6">维基外测度&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>无线通信之信息熵回忆总结</title><link>https://surprisedcat.github.io/studynotes/%E6%97%A0%E7%BA%BF%E9%80%9A%E4%BF%A1%E4%B9%8B%E4%BF%A1%E6%81%AF%E7%86%B5%E5%9B%9E%E5%BF%86%E6%80%BB%E7%BB%93/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%97%A0%E7%BA%BF%E9%80%9A%E4%BF%A1%E4%B9%8B%E4%BF%A1%E6%81%AF%E7%86%B5%E5%9B%9E%E5%BF%86%E6%80%BB%E7%BB%93/</guid><description>
&lt;h2 id="转载-详解机器学习中的熵条件熵相对熵和交叉熵">转载： 详解机器学习中的熵、条件熵、相对熵和交叉熵&lt;!-- omit in toc -->&lt;/h2>
&lt;p>原文链接：&lt;a href="https://www.cnblogs.com/kyrieng/p/8694705.html">https://www.cnblogs.com/kyrieng/p/8694705.html&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#信息熵information-entropy">信息熵(information entropy)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#联合熵joint-entropy">联合熵(Joint entropy)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#条件熵conditional-entropy">条件熵(Conditional entropy)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相对熵-relative-entropykl散度">相对熵 (Relative entropy)（KL散度）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#交叉熵cross-entropy">交叉熵(Cross entropy)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#机器学习中的应用">机器学习中的应用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总结">总结&lt;/a>&lt;/li>
&lt;li>&lt;a href="#证明随机分布为均匀分布时熵最大">证明随机分布为均匀分布时熵最大&lt;/a>&lt;/li>
&lt;li>&lt;a href="#jenson不等式证明相对熵大于等于0">Jenson不等式证明相对熵大于等于0&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="信息熵information-entropy">信息熵(information entropy)&lt;/h2>
&lt;p>&lt;strong>熵 (entropy)&lt;/strong> 这一词最初来源于热力学。1948年，克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也被称为香农熵 (Shannon entropy)，信息熵 (information entropy)。本文只讨论信息熵。首先，我们先来理解一下信息这个概念。信息是一个很抽象的概念，百度百科将它定义为：指音讯、消息、通讯系统传输和处理的对象，泛指人类社会传播的一切内容。那信息可以被量化么？可以的！香农提出的“信息熵”概念解决了这一问题。&lt;/p>
&lt;p>熵是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、&lt;strong>平均自信息量&lt;/strong>。一条信息的信息量大小和它的&lt;strong>不确定性&lt;/strong>有直接的关系。我们需要搞清楚一件非常非常不确定的事，或者是我们一无所知的事，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们就不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，&lt;strong>信息量的度量就等于不确定性的多少&lt;/strong>。比如，有人说广东下雪了。对于这句话，我们是十分不确定的。因为广东几十年来下雪的次数寥寥无几。为了搞清楚，我们就要去看天气预报，新闻，询问在广东的朋友，而这就需要大量的信息，信息熵很高。再比如，中国男足近10年能赢世界杯决赛圈。对于这句话，因为确定性很高，几乎不需要引入信息，信息熵很低。&lt;/p>
&lt;p>考虑一个离散的随机变量&lt;span class="math">\(x\)&lt;/span>，由上面两个例子可知，信息的量度应该依赖于概率分布&lt;span class="math">\(p(x)\)&lt;/span>，因此我们想要寻找一个函数&lt;span class="math">\(I(x)\)&lt;/span>，它是概率&lt;span class="math">\(p(x)\)&lt;/span>的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件&lt;span class="math">\(x 和 y\)&lt;/span>，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：&lt;span class="math">\(I(x,y)=I(x)+I(y)\)&lt;/span>。&lt;/p>
&lt;p>因为两个事件是独立不相关的，因此&lt;span class="math">\(p(x,y)=p(x)p(y)\)&lt;/span>。根据这两个关系，&lt;strong>很容易看出&lt;span class="math">\(I(x)\)&lt;/span>一定与&lt;span class="math">\(p(x)\)&lt;/span>的对数&lt;/strong>有关 (因为对数的运算法则是&lt;span class="math">\(\log_a(mn)=\log_a m+\log_a n)\)&lt;/span>。因此，我们有 &lt;span class="math">\[
I(x)=−\log p(x)
\]&lt;/span> &lt;strong>其中负号是用来保证信息量是正数或者零。而 log 函数基的选择是任意的&lt;/strong>（信息论中基常常选择为2，因此信息的单位为比特bits；而机器学习中基常常选择为自然常数，因此单位常常被称为奈特nats）。&lt;strong>&lt;span class="math">\(I(x)\)&lt;/span>也被称为随机变量&lt;span class="math">\(x\)&lt;/span>的自信息 (self-information)，描述的是随机变量的某个事件发生所带来的信息量&lt;/strong>。图像如图：&lt;/p>
&lt;img src="../images/self_information.png" alt="self_information" />
&lt;center>
概率与自信息
&lt;/center>
&lt;p>最后，我们正式引出信息熵。 现在假设一个发送者想传送一个随机变量的值给接收者。那么在这个过程中，他们传输的&lt;strong>平均信息量&lt;/strong>可以通过求&lt;span class="math">\(I(x)=−\log p(x)\)&lt;/span>关于概率分布&lt;span class="math">\(p(x)\)&lt;/span>的期望得到，即： &lt;span class="math">\[
H(X)=-\displaystyle\sum_{x}p(x)\log p(x)=-\sum_{i=1}^{n}p(x_i)\log p(x_i)
\]&lt;/span> &lt;span class="math">\(H(X)\)&lt;/span>就被称为随机变量&lt;span class="math">\(x\)&lt;/span>的&lt;strong>熵,它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望&lt;/strong>。从公式可得，随机变量的&lt;strong>取值个数越多&lt;/strong>，&lt;strong>状态数也就越多&lt;/strong>，&lt;strong>信息熵就越大&lt;/strong>，&lt;strong>混乱程度就越大&lt;/strong>。当随机分布为&lt;strong>均匀分布&lt;/strong>时，熵最大，且&lt;span class="math">\(0≤H(X)≤\log n\)&lt;/span>。可见证明&lt;a href="#证明随机分布为均匀分布时熵最大">证明随机分布为均匀分布时熵最大&lt;/a>&lt;/p>
&lt;h3 id="联合熵joint-entropy">联合熵(Joint entropy)&lt;/h3>
&lt;p>将一维随机变量分布推广到&lt;strong>多维随机变量&lt;/strong>分布，则其联合熵 (Joint entropy) 为： &lt;span class="math">\[
\begin{aligned}
H(X,Y)&amp;amp;=-\displaystyle\sum_{x,y}p(x,y)logp(x,y)\\
&amp;amp;=-\sum_{i=1}^{n}\sum_{j=1}^{m}p(x_i,y_i)logp(x_i,y_i)
\end{aligned}
\]&lt;/span> &lt;strong>注意点：&lt;/strong>&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>熵只依赖于随机变量的分布，与随机变量取值无关，所以也可以将&lt;span class="math">\(X\)&lt;/span>的熵记作&lt;span class="math">\(H(p)\)&lt;/span>。&lt;/li>
&lt;li>令&lt;span class="math">\(0\log 0=0\)&lt;/span>(因为某个取值概率可能为0)。&lt;/li>
&lt;/ol>
&lt;p>那么这些定义有着什么样的性质呢？考虑一个随机变量&lt;span class="math">\(x\)&lt;/span>。这个随机变量有4种可能的状态，每个状态都是等可能的。为了把&lt;span class="math">\(x\)&lt;/span>的值传给接收者，我们&lt;strong>至少&lt;/strong>需要传输2比特的消息。（&lt;span class="math">\(H(X)=-4\times\dfrac{1}{4}log_2\dfrac{1}{4}=2\ bits\)&lt;/span>）&lt;/p>
&lt;p>现在考虑一个具有4种可能的状态&lt;span class="math">\(\{a,b,c,d\}\)&lt;/span> 的随机变量，每个状态各自的概率为&lt;span class="math">\((1/2,1/4,1/8,1/8)\)&lt;/span>。这种情形下的熵为： &lt;span class="math">\[
H(X)=-\displaystyle\frac{1}{2}log_2\frac{1}{2}-\frac{1}{4}log_2\frac{1}{4}-\frac{1}{8}log_2\frac{1}{8}-\frac{1}{8}log_2\frac{1}{8}=1.75\ bits
\]&lt;/span> 这个实例也佐证了&lt;strong>非均匀分布比均匀分布的熵要小&lt;/strong>。现在让我们考虑如何把变量状态的类别传递给接收者。与之前一样，我们可以使用一个2比特的数字来完成这件事情。然而，我们可以利用非均匀分布这个特点，使用&lt;strong>更短的编码来描述更可能的事件，使用更长的编码来描述不太可能的事件&lt;/strong>。我们希望这样做能够得到一个更短的平均编码长度。我们可以使用下面的编码串（哈夫曼编码）：0、10、110、111来表示状态&lt;span class="math">\(\{a,b,c,d\}\)&lt;/span>。传输的编码的平均长度就是： &lt;span class="math">\[
1\times \frac{1}{2}\times1+1\times \frac{1}{4}\times2+2\times\frac{1}{8}\times3=1.75\ bits
\]&lt;/span> 和该分布信息的熵一致。这个值与上方的随机变量的熵相等。熵和最短编码长度的这种关系是一种普遍的情形，称为&lt;strong>Shannon无损信源编码定理&lt;/strong>。定理表明熵是&lt;strong>传输一个随机变量状态值所需的比特位下界（最短平均编码长度）&lt;/strong>。因此，信息熵可以应用在数据压缩方面。这里这篇文章&lt;http://www.ruanyifeng.com/blog/2014/09/information-entropy.html>讲的很详细了，我就不赘述了。&lt;/p>
&lt;h2 id="条件熵conditional-entropy">条件熵(Conditional entropy)&lt;/h2>
&lt;p>条件熵&lt;span class="math">\(H(Y|X)\)&lt;/span>表示在已知随机变量&lt;span class="math">\(X\)&lt;/span>的条件下随机变量&lt;span class="math">\(Y\)&lt;/span>的不确定性。条件熵&lt;span class="math">\(H(Y|X)\)&lt;/span>定义为&lt;span class="math">\(X\)&lt;/span>给定条件下&lt;span class="math">\(Y\)&lt;/span>的条件概率分布的熵对&lt;span class="math">\(X\)&lt;/span>的数学期望： &lt;span class="math">\[
\begin{aligned}
H(Y|X) &amp;amp;= \sum_x p(x)H(Y|X=x)\\
&amp;amp;=-\sum_x p(x) \sum_y p(y|x)\log p(y|x)\\
&amp;amp;=-\sum_x\sum_y p(x,y) \log p(y|x)\\
&amp;amp;=-\sum_{x,y} p(x,y) \log p(y|x)
\end{aligned}
\]&lt;/span> 条件熵&lt;span class="math">\(H(Y|X)\)&lt;/span>相当于联合熵&lt;span class="math">\(H(X,Y)\)&lt;/span>减去单独的熵&lt;span class="math">\(H(X)\)&lt;/span>，即 &lt;span class="math">\[
H(Y|X)=H(X,Y)−H(X)
\]&lt;/span> 证明如下： &lt;span class="math">\[
\begin{aligned}
H(X,Y)&amp;amp;=-\sum_{x,y} p(x,y) \log p(x,y) \\
&amp;amp;=-\sum_{x,y}p(x,y)\log(p(y|x)p(x))\\
&amp;amp;=-\sum_{x,y} p(x,y) \log p(y|x) - \sum_{x,y}p(x,y)\log p(x)\\
&amp;amp;=H(Y|X)-\sum_x\sum_y p(x,y)\log p(x)\\
&amp;amp;=H(Y|X)-\sum_x p(x)\log p(x)\\
&amp;amp;=H(Y|X)+H(X)
\end{aligned}
\]&lt;/span> 举个例子，比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布&lt;span class="math">\(H(X,Y)\)&lt;/span>，因为两个事件加起来的信息量肯定是大于单一事件的信息量的。假设&lt;span class="math">\(H(X)\)&lt;/span>对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了。当已知&lt;span class="math">\(H(X)\)&lt;/span>这个信息量的时候，&lt;span class="math">\(H(X,Y)\)&lt;/span>剩下的信息量就是条件熵： &lt;span class="math">\[
H(Y|X)=H(X,Y)-H(X)
\]&lt;/span> 因此，可以这样理解，&lt;strong>描述&lt;span class="math">\(X\)&lt;/span>和&lt;span class="math">\(Y\)&lt;/span>所需的信息是描述&lt;span class="math">\(X\)&lt;/span>自己所需的信息,加上给定&lt;span class="math">\(X\)&lt;/span>的条件下具体化&lt;span class="math">\(Y\)&lt;/span>所需的额外信息&lt;/strong>。可以看这篇文章，讲得很详细。&lt;a href="https://zhuanlan.zhihu.com/p/26551798">https://zhuanlan.zhihu.com/p/26551798&lt;/a>&lt;/p>
&lt;h2 id="相对熵-relative-entropykl散度">相对熵 (Relative entropy)（KL散度）&lt;/h2>
&lt;p>设&lt;span class="math">\(p(x)、q(x)\)&lt;/span>是离散随机变量&lt;span class="math">\(X\)&lt;/span>中取值的两个概率分布，则&lt;span class="math">\(p\)&lt;/span>对&lt;span class="math">\(q\)&lt;/span>的相对熵是： &lt;span class="math">\[
D_{KL}(p||q)=\displaystyle\sum_{x}p(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}
\]&lt;/span> 性质：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>如果&lt;span class="math">\(p(x)\)&lt;/span>和&lt;span class="math">\(q(x)\)&lt;/span>两个分布相同，那么相对熵等于0&lt;/li>
&lt;li>&lt;span class="math">\(D_{KL}(p||q)≠D_{KL}(q||p)\)&lt;/span>，相对熵具有不对称性。&lt;/li>
&lt;li>&lt;span class="math">\(D_{KL}(p||q)≥0\)&lt;/span>。&lt;a href="#Jenson不等式证明相对熵大于等于0">可利用Jensen不等式证明&lt;/a>。&lt;/li>
&lt;/ol>
&lt;p>总结：&lt;strong>相对熵可以用来衡量两个概率分布之间的差异&lt;/strong>，上面公式的意义就是求&lt;span class="math">\(p\)&lt;/span>与&lt;span class="math">\(q\)&lt;/span>之间的对数差在&lt;span class="math">\(p\)&lt;/span>上的期望值。&lt;/p>
&lt;h2 id="交叉熵cross-entropy">交叉熵(Cross entropy)&lt;/h2>
&lt;p>现在有关于样本集的两个概率分布&lt;span class="math">\(p(x)\)&lt;/span>和&lt;span class="math">\(q(x)\)&lt;/span>，其中&lt;span class="math">\(p(x)\)&lt;/span>为真实分布，&lt;span class="math">\(q(x)\)&lt;/span>非真实分布。如果用真实分布&lt;span class="math">\(p(x)\)&lt;/span>来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为: &lt;span class="math">\[
H(p) =\displaystyle\sum_{x}p(x)log\frac{1}{p(x)}
\]&lt;/span> 如果使用非真实分布&lt;span class="math">\(q(x)\)&lt;/span>来表示来自真实分布&lt;span class="math">\(p(x)\)&lt;/span>的平均编码长度，则是： &lt;span class="math">\[
H(p,q) =\displaystyle\sum_{x}p(x)log\frac{1}{q(x)}
\]&lt;/span> 因为用&lt;span class="math">\(q(x)\)&lt;/span>来编码的样本来自于分布&lt;span class="math">\(p(x)\)&lt;/span>，所以&lt;span class="math">\(H(p,q)\)&lt;/span>中的概率是&lt;span class="math">\(p(x)\)&lt;/span>。此时就将&lt;span class="math">\(H(p,q)\)&lt;/span>称之为交叉熵。举个例子。考虑一个随机变量&lt;span class="math">\(x\)&lt;/span>，真实分布&lt;span class="math">\(p(x)= \left( \displaystyle\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8} \right)\)&lt;/span>，非真实分布&lt;span class="math">\(q(x)=\left( \displaystyle\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4} \right)\)&lt;/span>，则&lt;span class="math">\(H(p)=1.75bits\)&lt;/span>（最短平均码长），交叉熵&lt;span class="math">\(H(p,q)=\displaystyle\frac{1}{2}log_24+\frac{1}{4}log_24+\frac{1}{8}log_24+\frac{1}{8}log_24=2\ bits\)&lt;/span>。由此可以看出根据&lt;strong>非真实分布&lt;span class="math">\(q(x)\)&lt;/span>得到的平均码长总是大于根据真实分布&lt;span class="math">\(p(x)\)&lt;/span>得到的平均码长&lt;/strong>。&lt;/p>
&lt;p>我们再化简一下相对熵的公式。 &lt;span class="math">\[
D_{KL}(p||q)=\displaystyle\sum_{x}p(x)log\frac{p(x)}{q(x)}=\sum_{x}p(x)logp(x)-p(x)logq(x)
\]&lt;/span> 有没有发现什么？&lt;/p>
&lt;p>熵的公式 &lt;span class="math">\[
H(p)=-\displaystyle\sum_{x}p(x)logp(x)
\]&lt;/span> 交叉熵的公式 &lt;span class="math">\[
H(p,q)=\displaystyle\sum_{x}p(x)log\frac{1}{q(x)}=-\sum_{x}p(x)logq(x)
\]&lt;/span> 所以有： &lt;span class="math">\[
D_{KL}(p||q)=H(p,q)-H(p)
\]&lt;/span> 当用非真实分布 q(x) 得到的平均码长比真实分布 p(x) 得到的平均码长多出的比特数就是相对熵。又因为&lt;span class="math">\(D_{KL}(p||q)≥0\)&lt;/span>恒成立。所以： &lt;span class="math">\[
H(p,q)≥H(p),当且仅当p(x)=q(x)时，等号成立
\]&lt;/span> &lt;strong>并且当&lt;span class="math">\(H(p)\)&lt;/span>为常量时（注：在机器学习中，训练数据分布是固定的），最小化相对熵&lt;span class="math">\(D_{KL}(p||q)\)&lt;/span>等价于最小化交叉熵&lt;span class="math">\(H(p,q)\)&lt;/span>也等价于最大化似然估计&lt;/strong>。&lt;/p>
&lt;h3 id="机器学习中的应用">机器学习中的应用&lt;/h3>
&lt;p>在机器学习中，我们希望在训练数据上模型学到的分布&lt;span class="math">\(P(model)\)&lt;/span>和真实数据的分布&lt;span class="math">\(P(real)\)&lt;/span>越接近越好，所以我们可以使其相对熵最小。但是我们没有真实数据的分布，所以只能希望模型学到的分布&lt;span class="math">\(P(model)\)&lt;/span>和训练数据的分布&lt;span class="math">\(P(train)\)&lt;/span>尽量相同。假设训练数据是从总体中独立同分布采样的，那么我们可以通过最小化训练数据的经验误差来降低模型的泛化误差。即：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>希望学到的模型的分布和真实分布一致，&lt;span class="math">\(P(model)≃P(real)\)&lt;/span>&lt;/li>
&lt;li>但是真实分布不可知，&lt;strong>假设&lt;/strong>训练数据是从真实数据中独立同分布采样的，&lt;span class="math">\(P(train)≃P(real)\)&lt;/span>&lt;/li>
&lt;li>因此，我们希望学到的模型分布至少和训练数据的分布一致，&lt;span class="math">\(P(train)≃P(model)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>根据之前的描述，最小化训练数据上的分布&lt;span class="math">\(P(train)\)&lt;/span>与最小化模型分布&lt;span class="math">\(P(model)\)&lt;/span>的差异等价于&lt;strong>最小化相对熵&lt;/strong>，即&lt;span class="math">\(D_{KL}(P(train)||P(model))\)&lt;/span>。此时，&lt;span class="math">\(P(train)\)&lt;/span>就是&lt;span class="math">\(D_{KL}(p||q)\)&lt;/span>中的&lt;span class="math">\(p\)&lt;/span>，即真实分布，&lt;span class="math">\(P(model)\)&lt;/span>就是&lt;span class="math">\(q\)&lt;/span>。又因为训练数据的分布&lt;span class="math">\(p\)&lt;/span>是给定的，所以最小化&lt;span class="math">\(D_{KL}(p||q)\)&lt;/span>等价于最小化&lt;span class="math">\(H(p,q)\)&lt;/span>。得证，&lt;strong>交叉熵可以用来计算学习模型分布与训练分布之间的差异&lt;/strong>。交叉熵广泛用于逻辑回归的Sigmoid和Softmax函数中作为损失函数使用。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>信息熵是衡量随机变量分布的混乱程度，是随机分布各事件发生的信息量的期望值，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大；信息熵推广到多维领域，则可得到联合信息熵；条件熵表示的是在&lt;span class="math">\(X\)&lt;/span>给定条件下，&lt;span class="math">\(Y\)&lt;/span>的条件概率分布的熵对&lt;span class="math">\(X\)&lt;/span>的期望。&lt;/li>
&lt;li>相对熵可以用来衡量两个概率分布之间的差异。&lt;/li>
&lt;li>交叉熵可以来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。&lt;/li>
&lt;/ol>
&lt;p>或者从信息论的角度:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>信息熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）。&lt;/li>
&lt;li>相对熵是指用&lt;span class="math">\(q\)&lt;/span>来表示分布&lt;span class="math">\(p\)&lt;/span>额外需要的编码长度。&lt;/li>
&lt;li>交叉熵是指用分布&lt;span class="math">\(q\)&lt;/span>来表示分布&lt;span class="math">\(p\)&lt;/span>的平均编码长度。&lt;/li>
&lt;/ol>
&lt;h2 id="证明随机分布为均匀分布时熵最大">证明随机分布为均匀分布时熵最大&lt;/h2>
&lt;p>证明：&lt;span class="math">\(0≤H(X)≤ \log n\)&lt;/span>&lt;/p>
&lt;p>利用拉格朗日乘子法求极值，因为&lt;span class="math">\(p(x_1)+p(x_2)+\dotsb+p(x_n)=1\)&lt;/span>，同时信息熵为 &lt;span class="math">\[
目标函数：H(X)=p(x_1)\log(p(x_1))+p(x_2)\log(p(x_2))+\dotsb+p(x_n)\log(p(x_n))\\
s.t.\qquad p(x_1)+p(x_2)+\dotsb+p(x_n)-1=0
\]&lt;/span> 我们可以构造拉格朗日函数： &lt;span class="math">\[
\begin{aligned}
L(p(x_1),p(x_2),\dots,p(x_n),\lambda)=&amp;amp;-(p(x_1)\log p(x_1)+p(x_2)\log p(x_2)+\dots+p(x_n)\log p(x_n))\\&amp;amp;+\lambda(p(x_1)+p(x_2)+\dots+p(x_n)-1)
\end{aligned}
\]&lt;/span> &lt;span class="math">\(L(p(x_1),p(x_2),…,p(x_n),λ)\)&lt;/span>分别对&lt;span class="math">\(p(x_1),p(x_2),\dotsb,p(x_n),λ\)&lt;/span>求偏导数，令偏导数为&lt;span class="math">\(0\)&lt;/span>，假设&lt;span class="math">\(\log以e\)&lt;/span>为底： &lt;span class="math">\[
\lambda-log(e\cdot p(x_1))=0\\
\lambda-log(e\cdot p(x_2))=0\\
\dotsb\\
\lambda-log(e\cdot p(x_n))=0\\
p(x_1)+p(x_2)+\dots+p(x_n)-1=0
\]&lt;/span> 不难求出： &lt;span class="math">\[
p(x_1)=p(x_2)=\dots=p(x_n)=\frac{1}{n}
\]&lt;/span> 将上式代数熵&lt;span class="math">\(H(X)\)&lt;/span>的计算公式可得 &lt;span class="math">\[
H(X)=-(\frac{1}{n}\log \frac{1}{n}+\frac{1}{n}\log \frac{1}{n}+\dotsb+\frac{1}{n}\log \frac{1}{n})\\
=-\log \frac{1}{n}=\log n
\]&lt;/span> 由此可证&lt;span class="math">\(\log n\)&lt;/span>为最大值。&lt;/p>
&lt;h2 id="jenson不等式证明相对熵大于等于0">Jenson不等式证明相对熵大于等于0&lt;/h2>
&lt;p>&lt;span class="math">\[
\begin{aligned}
D_{KL}(p||q)&amp;amp;=\sum_x p(x) \log\frac{p(x)}{q(x)}\\
&amp;amp;=-\sum_x p(x) \log \frac{q(x)}{p(x)}\\
&amp;amp;=-E_{p(x)}(\log\frac{q(x)}{p(x)})\\
&amp;amp;≥-\log\sum_x p(x)\frac{q(x)}{p(x)}\\
&amp;amp;=\log \sum_x q(x)\\
&amp;amp;\because \sum_x q(x)=1\qquad q(x)≥0\\
&amp;amp;\therefore D_{KL}(p||q)≥0
\end{aligned}
\]&lt;/span>&lt;/p></description></item><item><title>机器学习-框架</title><link>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A1%86%E6%9E%B6/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A1%86%E6%9E%B6/</guid><description>
&lt;h2 id="机器学习框架">机器学习框架&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="机器学习分类">机器学习分类&lt;/h2>
&lt;h2 id="参数学习与非参数学习">参数学习与非参数学习&lt;/h2>
&lt;p>参数学习(parametric learning algorithm)是指有固定一组参数，通过监督学习算法的数据不断优化。&lt;/p>
&lt;p>非参数学习(non-parametric learning algorithm)是指参数的数量不定，会随着数据量规模的变化而&lt;strong>线性&lt;/strong>变化。&lt;/p>
&lt;p>普通的线性回归属于参数学习算法；而局部加权线性回归（LWLR）属于非参数学习算法。&lt;/p>
&lt;h3 id="三类机器学习算法">三类机器学习算法&lt;/h3>
&lt;p>监督学习：各种回归，决策树，随机森林，LNN，&lt;/p>
&lt;p>无监督学习：K-means，Apriori&lt;/p>
&lt;p>强化学习：马尔科夫决策过程（MDP）&lt;/p>
&lt;h2 id="常用机器学习算法">常用机器学习算法&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>Linear Regression&lt;/li>
&lt;li>Logistic Regression&lt;/li>
&lt;li>Decision Tree&lt;/li>
&lt;li>SVM&lt;/li>
&lt;li>Naive Bayes&lt;/li>
&lt;li>kNN&lt;/li>
&lt;li>K-Means&lt;/li>
&lt;li>Random Forest&lt;/li>
&lt;li>Dimensionality Reduction Algorithms&lt;/li>
&lt;li>Gradient Boosting algorithms
&lt;ol style="list-style-type: decimal">
&lt;li>GBM&lt;/li>
&lt;li>XGBoost&lt;/li>
&lt;li>LightGBM&lt;/li>
&lt;li>CatBoost&lt;/li>
&lt;/ol>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>线性回归算法 Linear Regression&lt;/li>
&lt;li>支持向量机算法 (Support Vector Machine,SVM)&lt;/li>
&lt;li>最近邻居/k-近邻算法 (K-Nearest Neighbors,KNN)&lt;/li>
&lt;li>逻辑回归算法 Logistic Regression&lt;/li>
&lt;li>决策树算法 Decision Tree&lt;/li>
&lt;li>k-平均算法 K-Means&lt;/li>
&lt;li>随机森林算法 Random Forest&lt;/li>
&lt;li>朴素贝叶斯算法 Naive Bayes&lt;/li>
&lt;li>降维算法 Dimensional Reduction&lt;/li>
&lt;li>梯度增强算法 Gradient Boosting&lt;/li>
&lt;/ul></description></item><item><title>测度论0之集合重新认识</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA0%E4%B9%8B%E9%9B%86%E5%90%88%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA0%E4%B9%8B%E9%9B%86%E5%90%88%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86/</guid><description>
&lt;h2 id="重新认识集合">重新认识集合&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#先导概念">先导概念&lt;/a>&lt;/li>
&lt;li>&lt;a href="#点">点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#导集内核闭包">导集内核闭包&lt;/a>&lt;/li>
&lt;li>&lt;a href="#闭集">闭集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#闭集的运算">闭集的运算&lt;/a>&lt;/li>
&lt;li>&lt;a href="#开集">开集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#开集的运算">开集的运算&lt;/a>&lt;/li>
&lt;li>&lt;a href="#完备集">完备集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#heine-borel有限覆盖定理">Heine-Borel有限覆盖定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#直线上开集闭集与完备集的构造">直线上开集、闭集与完备集的构造&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非空有界开集开集的构造是基础">非空有界开集（开集的构造是基础）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非空有界闭集">非空有界闭集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#完备集构造">完备集构造&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从测度论的角度定义连续函数">从测度论的角度定义连续函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#函数在孤立点处连续">函数在孤立点处连续&lt;/a>&lt;/li>
&lt;li>&lt;a href="#borel集">Borel集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#f_sigma与g_delta集">&lt;span class="math">\(F_{\sigma}\)&lt;/span>与&lt;span class="math">\(G_{\delta}\)&lt;/span>集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#borel集定义">Borel集定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#cantor集">Cantor集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三分康托集">三分康托集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#集列的极限">集列的极限&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="先导概念">先导概念&lt;/h2>
&lt;h3 id="点">点&lt;/h3>
&lt;blockquote>
&lt;p>聚点（极限点）：设X是数集，实数&lt;span class="math">\(a\)&lt;/span>满足，&lt;span class="math">\(\forall \delta&amp;gt;0\)&lt;/span>，满足去心邻域&lt;span class="math">\(U^\circ(a,\delta)\cap X \neq \emptyset\)&lt;/span>，则称&lt;span class="math">\(a\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>的聚点（极限点）。&lt;/p>
&lt;p>等价描述为：如果点&lt;span class="math">\(a\)&lt;/span>的任何邻域 &lt;span class="math">\[U^\circ(a,\delta)=\{x|0&amp;lt;|x-a|&amp;lt;\delta\}\]&lt;/span> 都含有X中无穷多个点，则称&lt;span class="math">\(a\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>的聚点（极限点）。&lt;/p>
&lt;/blockquote>
&lt;p>所有的内点都是聚点，聚点不需要在集合中，边界点也不需要在集合中。内点和孤立点都必须在集合中。&lt;/p>
&lt;blockquote>
&lt;p>孤立点：在数集&lt;span class="math">\(X\)&lt;/span>中，点&lt;span class="math">\(x\)&lt;/span>称为拓扑空间子集&lt;span class="math">\(S\)&lt;/span>的孤立点，如果 &lt;span class="math">\(x∈S\)&lt;/span>，且存在&lt;span class="math">\(x\)&lt;/span>的一个邻域 ，其中不含&lt;span class="math">\(S\)&lt;/span>中除了&lt;span class="math">\(x\)&lt;/span>的其他点。&lt;/p>
&lt;/blockquote>
&lt;p>在数集中所有的孤立点都是边界点，一般拓扑空间不一定时是。&lt;/p>
&lt;img src="../images/内点聚点孤立点边界点.png" alt="内点聚点孤立点边界点" />
&lt;center>
各种点之间关系
&lt;/center>
&lt;h3 id="导集内核闭包">导集内核闭包&lt;/h3>
&lt;blockquote>
&lt;p>内核：设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中的点集，由&lt;span class="math">\(E\)&lt;/span>的所有内点组成的集合称为&lt;span class="math">\(E\)&lt;/span>的内核，记为&lt;span class="math">\(E^\circ\)&lt;/span>。&lt;/p>
&lt;p>导集：设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中的点集，由&lt;span class="math">\(E\)&lt;/span>的所有聚点组成的集合称为&lt;span class="math">\(E\)&lt;/span>的导集，记为&lt;span class="math">\(E&amp;#39;\)&lt;/span>。&lt;/p>
&lt;p>闭包：设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中的点集，称&lt;span class="math">\(E\cup E&amp;#39;\)&lt;/span>为&lt;span class="math">\(E\)&lt;/span>的闭包，记为&lt;span class="math">\(\overline{E}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>闭包点的定义非常接近聚点的定义。这两个定义之间的差别非常微小但很重要——在聚点的定义中，点&lt;span class="math">\(x\)&lt;/span>的邻域必须包含“不是&lt;span class="math">\(x\)&lt;/span>自身的”这个集合的点。因此，所有聚点都是闭包点，但不是所有的闭包点都是聚点。&lt;strong>不是聚点的闭包点就是孤点&lt;/strong>。也就是说，点&lt;span class="math">\(x\)&lt;/span>是孤点，若它是&lt;span class="math">\(S\)&lt;/span>的元素，且存在&lt;span class="math">\(x\)&lt;/span>的邻域，该邻域中除了&lt;span class="math">\(x\)&lt;/span>没有其他的点属于&lt;span class="math">\(S\)&lt;/span>。&lt;/p>
&lt;p>对给定的集合&lt;span class="math">\(S\)&lt;/span>和点&lt;span class="math">\(x\)&lt;/span>，&lt;span class="math">\(x\)&lt;/span>是&lt;span class="math">\(S\)&lt;/span>的闭包点，当且仅当&lt;span class="math">\(x\)&lt;/span>属于&lt;span class="math">\(S\)&lt;/span>，或&lt;span class="math">\(x\)&lt;/span>是&lt;span class="math">\(S\)&lt;/span>的聚点，即&lt;span class="math">\(\overline{E}= E\cup E&amp;#39;\)&lt;/span>。&lt;/p>
&lt;p>关于导集、内核、闭包有以下定理： &amp;gt;如果&lt;span class="math">\(A \subset B\)&lt;/span>，则&lt;span class="math">\(A^\circ \subset B^\circ，A&amp;#39;\subset B&amp;#39;，\overline{A}\subset\overline{B}\)&lt;/span>。即导集、内核、闭包的运算具有单调性。&lt;/p>
&lt;h2 id="闭集">闭集&lt;/h2>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中的点集，如果导集&lt;span class="math">\(E&amp;#39;\subset E\)&lt;/span>，则&lt;span class="math">\(E\)&lt;/span>为闭集。所有的极限点都在&lt;span class="math">\(E\)&lt;/span>中可取。&lt;/p>
&lt;/blockquote>
&lt;p>数集中只有&lt;span class="math">\(\emptyset 与 R^n\)&lt;/span>既是开集也是闭集；有理数集、半开半闭区间既不是开集也不是闭集。&lt;/p>
&lt;blockquote>
&lt;p>定理1.设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中点集，则&lt;span class="math">\(E\)&lt;/span>的导集&lt;span class="math">\(E&amp;#39;\)&lt;/span>与闭包&lt;span class="math">\(\overline{E}\)&lt;/span>都是闭集。&lt;/p>
&lt;p>定理2.闭集的补集是开集。开集的补集是闭集。&lt;/p>
&lt;/blockquote>
&lt;h3 id="闭集的运算">闭集的运算&lt;/h3>
&lt;blockquote>
&lt;p>&lt;strong>任意多个&lt;/strong>闭集的交集是闭集。 &lt;span class="math">\[ClosedSet_n为闭集，\bigcap_{n=1}^∞ ClosedSet_n为闭集\]&lt;/span> &lt;strong>有限多个&lt;/strong>闭集的并集是闭集。 &lt;span class="math">\[ClosedSet_n为闭集，\bigcup_{n=1}^N ClosedSet_n为闭集\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>反例：无限多个闭集的并集是开集。&lt;/p>
&lt;p>&lt;strong>例&lt;/strong>：考虑一列闭区间&lt;span class="math">\(F_n=[-\frac{n}{n+1},\frac{n}{n+1}]，n=1,2,\dots\)&lt;/span>。每个&lt;span class="math">\(F_n\)&lt;/span>是闭集，但是&lt;span class="math">\(\bigcup_{n=1}^∞ F_n= (-1,1)\)&lt;/span>为开集。&lt;/p>
&lt;p>&lt;strong>例&lt;/strong>: 我们认为每一个有理数点是闭集，那么所有有理数点组成的有理数集既不是开集也不是闭集，而是&lt;span class="math">\(F_{\sigma}\)&lt;/span>集。&lt;/p>
&lt;h2 id="开集">开集&lt;/h2>
&lt;blockquote>
&lt;p>如果&lt;span class="math">\(E\)&lt;/span>中的每一个点都是内点，则&lt;span class="math">\(E\)&lt;/span>是开集，即&lt;span class="math">\(E\Leftrightarrow E^\circ\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>数集中只有&lt;span class="math">\(\emptyset 与 R^n\)&lt;/span>既是开集也是闭集；有理数集、半开半闭区间既不是开集也不是闭集。&lt;/p>
&lt;h3 id="开集的运算">开集的运算&lt;/h3>
&lt;blockquote>
&lt;p>&lt;strong>任意多个&lt;/strong>开集的并集是开集。 &lt;span class="math">\[OpenSet_n为开集，\bigcup_{n=1}^∞ OpenSet_n为开集\]&lt;/span> &lt;strong>有限多个&lt;/strong>开集的交集是开集。 &lt;span class="math">\[OpenSet_n为开集，\bigcap_{n=1}^N OpenSet_n为开集\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>反例：无限多个开集的交集是闭集。&lt;/p>
&lt;p>&lt;strong>例&lt;/strong>：考虑一列开区间&lt;span class="math">\(G_n=(-\frac{1}{n},\frac{1}{n})，n=1,2,\dots\)&lt;/span>。每个&lt;span class="math">\(G_n\)&lt;/span>是开集，但是&lt;span class="math">\(\bigcap_{n=1}^∞ G_n= \{0\}\)&lt;/span>为闭集。&lt;/p>
&lt;h2 id="完备集">完备集&lt;/h2>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中的点集，如果导集&lt;span class="math">\(E\subset E&amp;#39;\)&lt;/span>，则&lt;span class="math">\(E\)&lt;/span>为自密集（和闭集定义正好相反）。如果&lt;span class="math">\(E=E&amp;#39;\)&lt;/span>，则&lt;span class="math">\(E\)&lt;/span>为完备集（自密闭集是完备集）。&lt;/p>
&lt;/blockquote>
&lt;p>解释：孤立点和聚点的定义是对立的，也就是说自密集不含有孤立点。自密闭集是完备集，因此完备集就是&lt;strong>不含孤立点的闭集&lt;/strong>。但是在一个闭集中，去点所有的孤立点，也不一定时完备集。因为有时候去点所有孤立点会诞生新的孤立点，如下例。&lt;/p>
&lt;p>&lt;strong>例&lt;/strong>：设&lt;span class="math">\(E=\{0,1,1/2,1/3,\dots\}\)&lt;/span>，显然这是个闭集。其聚点只有一个是&lt;span class="math">\(\{0\}\)&lt;/span>，其他都是孤立点。但是去掉所有孤立点后，这个集合只剩下点&lt;span class="math">\(\{0\}\)&lt;/span>，并不是完备集。&lt;/p>
&lt;h3 id="heine-borel有限覆盖定理">Heine-Borel有限覆盖定理&lt;/h3>
&lt;blockquote>
&lt;p>推广的有限覆盖定理：Heine-Borel有限覆盖定理&lt;/p>
&lt;p>设&lt;span class="math">\(F\)&lt;/span>是&lt;strong>有界闭集&lt;/strong>&lt;span class="math">\(\Leftrightarrow\)&lt;/span>&lt;span class="math">\(\{G_\lambda\}_{\lambda ∈ I}\)&lt;/span>是&lt;span class="math">\(F\)&lt;/span>的任一开覆盖，则&lt;span class="math">\(\{G_\lambda\}_{\lambda ∈ I}\)&lt;/span>中必存在有限多个&lt;strong>开集&lt;/strong>&lt;span class="math">\(G_1,G_2,\dots,G_m\)&lt;/span>同样覆盖了&lt;span class="math">\(F\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>从开区间闭区间变成了开集闭集。&lt;/p>
&lt;blockquote>
&lt;p>进一步推广（不需要有界闭集条件）：&lt;span class="math">\(F\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>任一点集，&lt;span class="math">\(\{G_\lambda\}_{\lambda ∈ I}\)&lt;/span>是&lt;span class="math">\(F\)&lt;/span>的任一开覆盖，则&lt;span class="math">\(\{G_\lambda\}_{\lambda ∈ I}\)&lt;/span>中必存在&lt;strong>至多可列&lt;/strong>个&lt;strong>开集&lt;/strong>&lt;span class="math">\(G_1,G_2,\dots,G_m\)&lt;/span>同样覆盖了&lt;span class="math">\(F\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>去掉有界闭集限制，结论从有限个开集变成可列个开集。&lt;/p>
&lt;h2 id="直线上开集闭集与完备集的构造">直线上开集、闭集与完备集的构造&lt;/h2>
&lt;h3 id="非空有界开集开集的构造是基础">非空有界开集（开集的构造是基础）&lt;/h3>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(G\)&lt;/span>是直线上&lt;span class="math">\(R^1\)&lt;/span>上的开集，如果区间&lt;span class="math">\((\alpha,\beta) \subset G\)&lt;/span>，而且端点&lt;span class="math">\(\alpha,\beta\)&lt;/span>不属于&lt;span class="math">\(G\)&lt;/span>，那么称&lt;span class="math">\((\alpha,\beta)\)&lt;/span>为&lt;span class="math">\(G\)&lt;/span>的一个&lt;strong>构成区间&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>例如：开集&lt;span class="math">\((0,1)\cup (2,3)\)&lt;/span>的构成区间是&lt;span class="math">\((0,1)\)&lt;/span>和&lt;span class="math">\((2,3)\)&lt;/span> 。&lt;/p>
&lt;blockquote>
&lt;p>开区间构造定理：&lt;span class="math">\(R^1\)&lt;/span>上任何&lt;strong>非空有界&lt;/strong>开集都可表示成&lt;strong>有限个或可列个&lt;/strong>互不相交的构成区间的并。&lt;/p>
&lt;/blockquote>
&lt;h3 id="非空有界闭集">非空有界闭集&lt;/h3>
&lt;blockquote>
&lt;p>定理：设&lt;span class="math">\(F\)&lt;/span>是非空有界闭集，则&lt;span class="math">\(F\)&lt;/span>中必有一最大点和一最小点。&lt;/p>
&lt;/blockquote>
&lt;p>证明思路：有界→有上确界→上确界b邻域→b在导集中→导集&lt;span class="math">\(\subset\)&lt;/span>闭集。&lt;/p>
&lt;blockquote>
&lt;p>闭区间构造定理：设&lt;span class="math">\(F\)&lt;/span>是&lt;span class="math">\(R^1\)&lt;/span>上的任一非空有界闭集，则&lt;span class="math">\(F\)&lt;/span>是由一闭区间中去掉有限或者可列个开区间而成。这些开区间的端点还是属于&lt;span class="math">\(F\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h3 id="完备集构造">完备集构造&lt;/h3>
&lt;blockquote>
&lt;p>直线&lt;span class="math">\(R^1\)&lt;/span>上的&lt;span class="math">\(F\)&lt;/span>是任一非空有界完备集，则&lt;span class="math">\(F\)&lt;/span>是由一闭区间中去点有限或可列个&lt;strong>彼此没有公共端点&lt;/strong>且&lt;strong>与原来闭区间也没有公共端点&lt;/strong>的开区间而组成，这些开区间的端点还是属于&lt;span class="math">\(F\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h2 id="从测度论的角度定义连续函数">从测度论的角度定义连续函数&lt;/h2>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(f(x)\)&lt;/span>是定义在&lt;span class="math">\(E\subset R^n\)&lt;/span>上的实值函数，&lt;span class="math">\(x_0∈E\)&lt;/span>。如果对任意的&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>，存在&lt;span class="math">\(\delta&amp;gt;0\)&lt;/span>，使得当&lt;span class="math">\(x ∈ E\cap B(x_0,\delta)\)&lt;/span>时有： &lt;span class="math">\[|f(x)-f(x_0)|&amp;lt;\varepsilon，\]&lt;/span> 则称&lt;span class="math">\(f(x)在x_0\)&lt;/span>点处&lt;strong>连续&lt;/strong>。&lt;span class="math">\(x_0\)&lt;/span>是&lt;span class="math">\(f\)&lt;/span>的一个&lt;strong>连续点&lt;/strong>。（在&lt;span class="math">\(x \notin E&amp;#39;\)&lt;/span>的情形，即&lt;span class="math">\(x_0\)&lt;/span>是&lt;span class="math">\(E\)&lt;/span>的孤立点的时候，&lt;span class="math">\(f(x)\)&lt;/span>自然在&lt;span class="math">\(x=x_0\)&lt;/span>处连续）。若&lt;span class="math">\(E\)&lt;/span>中任一点皆为&lt;span class="math">\(f\)&lt;/span>的连续点，则称&lt;span class="math">\(f(x)\)&lt;/span>&lt;strong>在E上连续&lt;/strong>。我们记E上的连续函数之全体为&lt;span class="math">\(C(E)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h3 id="函数在孤立点处连续">函数在孤立点处连续&lt;/h3>
&lt;p>我们先来看孤立点的定义：&lt;/p>
&lt;p>集合&lt;span class="math">\(S\)&lt;/span>的一个点&lt;span class="math">\(x\)&lt;/span>，如果存在&lt;span class="math">\(x\)&lt;/span>的一个邻域&lt;span class="math">\(U(x,δ)\)&lt;/span>，除了点&lt;span class="math">\(x\)&lt;/span>以外，&lt;span class="math">\(U(x,δ)\)&lt;/span>不包含&lt;span class="math">\(S\)&lt;/span>中的其他点，则称点&lt;span class="math">\(x\)&lt;/span>为孤点或孤立点。&lt;/p>
&lt;p>从图像上来看，函数在孤立点处连续这个结论很难理解。&lt;/p>
&lt;p>但是从连续函数的定义和孤立点的定义上进行分析，这个结论却很明显：&lt;/p>
&lt;p>&lt;span class="math">\(x_0∈E\)&lt;/span>是&lt;span class="math">\(E\)&lt;/span>上的孤立点时，则存在一个&lt;span class="math">\(\delta&amp;gt;0，E\cap B(x_0,\delta)=\{x_0\}\)&lt;/span>。故任意&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>，都有&lt;span class="math">\(x ∈E\cap B(x_0,\delta)\)&lt;/span>，使得&lt;span class="math">\(|f(x)-f(x_0)|=|f(x_0)-f(x_0)|=0&amp;lt;\varepsilon\)&lt;/span>.这说明函数在孤立点&lt;span class="math">\(x_0\)&lt;/span>处连续。&lt;/p>
&lt;h2 id="borel集">Borel集&lt;/h2>
&lt;p>闭集与开集是&lt;span class="math">\(R^n\)&lt;/span>中最基本的集合。但是在&lt;span class="math">\(R^n\)&lt;/span>中有很多点集既不是开集也不是闭集。&lt;/p>
&lt;h3 id="f_sigma与g_delta集">&lt;span class="math">\(F_{\sigma}\)&lt;/span>与&lt;span class="math">\(G_{\delta}\)&lt;/span>集&lt;/h3>
&lt;blockquote>
&lt;p>&lt;span class="math">\(F_{\sigma}\)&lt;/span>与&lt;span class="math">\(G_{\delta}\)&lt;/span>集：若&lt;span class="math">\(E\subset R^n\)&lt;/span>是&lt;strong>可数个闭集&lt;/strong>的&lt;strong>并&lt;/strong>则称&lt;span class="math">\(E\)&lt;/span>为&lt;span class="math">\(F_{\sigma}\)&lt;/span>（型）集。若&lt;span class="math">\(E\subset R^n\)&lt;/span>是&lt;strong>可数个开集&lt;/strong>的&lt;strong>交&lt;/strong>则称&lt;span class="math">\(E\)&lt;/span>为&lt;span class="math">\(G_{\delta}\)&lt;/span>（型）集。&lt;/p>
&lt;/blockquote>
&lt;p>由定义可知&lt;span class="math">\(F_{\sigma}\)&lt;/span>集的补集是&lt;span class="math">\(G_\delta\)&lt;/span>集，&lt;span class="math">\(G_\delta\)&lt;/span>集的补集是&lt;span class="math">\(F_{\sigma}\)&lt;/span>集。&lt;/p>
&lt;p>例如：每一个有理数点&lt;span class="math">\(\{r_k\}\)&lt;/span>是闭集，&lt;span class="math">\(R^n\)&lt;/span>中的全体有理数为 &lt;span class="math">\[\bigcup_{k=1}^∞\{r_k\}\]&lt;/span> 为&lt;strong>可数个闭集&lt;/strong>的&lt;strong>并&lt;/strong>，即&lt;span class="math">\(F_{\sigma}\)&lt;/span>集。&lt;/p>
&lt;h3 id="borel集定义">Borel集定义&lt;/h3>
&lt;blockquote>
&lt;p>由&lt;span class="math">\(R^n\)&lt;/span>中一切开集构成的开集族所生成的&lt;span class="math">\(\sigma\)&lt;/span>代数称为&lt;span class="math">\(Borel- \sigma-\)&lt;/span>代数，记为&lt;span class="math">\(\mathcal{B}\)&lt;/span>。&lt;span class="math">\(\mathcal{B}\)&lt;/span>中的元称为Borel集。&lt;/p>
&lt;/blockquote>
&lt;p>显然，&lt;span class="math">\(R^n\)&lt;/span>中的闭集，开集，&lt;span class="math">\(F_{\sigma}\)&lt;/span>与&lt;span class="math">\(G_{\delta}\)&lt;/span>集皆为Borel集；任一Borel集的补集也是Borel集；Borel集的并、交、上下极限集皆为Borel集。可数个&lt;span class="math">\(F_{\sigma}\)&lt;/span>集的交集（&lt;span class="math">\(F_{\sigma\delta}\)&lt;/span>）是Borel集。&lt;/p>
&lt;p>博雷尔集是指在一个指定的拓扑空间中，可由其开集（或者等价地，可由其闭集）的可数次并运算、交运算和（或）差运算得到的一个集合。&lt;/p>
&lt;p>我们研究的集合&lt;strong>基本都是Borel集&lt;/strong>。&lt;/p>
&lt;h2 id="cantor集">Cantor集&lt;/h2>
&lt;h3 id="三分康托集">三分康托集&lt;/h3>
&lt;p>康托尔集是由不断去掉线段的中间三分之一而得出。首先从区间&lt;span class="math">\([0,1]\)&lt;/span>中去掉中间的三分之一（&lt;span class="math">\(\frac {1}{3},{\frac {2}{3}}\)&lt;/span>），留下两条线段：&lt;span class="math">\([0,{\frac {1}{3}}]\cup [{\frac {2}{3}},1]\)&lt;/span>。然后，把这两条线段的中间三分之一都去掉，留下四条线段&lt;span class="math">\([0,{\frac {1}{9}}]\cup [{\frac {2}{9}},{\frac {1}{3}}]\cup [{\frac {2}{3}},{\frac {7}{9}}]\cup [{\frac {8}{9}},1]\)&lt;/span>。把这个过程一直进行下去，其中第&lt;span class="math">\(n\)&lt;/span>个集合为：&lt;/p>
&lt;p>&lt;span class="math">\[{\frac {C_{n-1}}{3}}\cup \left({\frac {2}{3}}+{\frac {C_{n-1}}{3}}\right).\]&lt;/span> 康托尔集就是由所有过程中没有被去掉的区间&lt;span class="math">\([0,1]\)&lt;/span>中的点组成。&lt;/p>
&lt;p>下面的图显示了这个过程的最初六个步骤。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Cantor_set_in_seven_iterations.png" alt="Cantor_set_in_seven_iterations" />&lt;p class="caption">Cantor_set_in_seven_iterations&lt;/p>
&lt;/div>
&lt;p>康托集具有以下直观的性质：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>第n次，从集合&lt;span class="math">\([0,1]\)&lt;/span>中会去掉&lt;span class="math">\(2^{n-1}\)&lt;/span>个长度为&lt;span class="math">\(\frac{1}{3^n}\)&lt;/span>的开区间，余下的每个闭区间长度是&lt;span class="math">\(\frac{1}{3^n}\)&lt;/span>，共有&lt;span class="math">\(2^n\)&lt;/span>个。&lt;/li>
&lt;li>无论去掉开区间的过程进行多少次，康托尔集的点必属于每次去掉开区间后留下来的某个闭区间。&lt;/li>
&lt;li>从&lt;span class="math">\([0,1]\)&lt;/span>中每次去掉开区间后，开区间的端点都属于康托尔集。&lt;/li>
&lt;/ol>
&lt;p>此外，康托尔集还有以下结论：&lt;/p>
&lt;ul>
&lt;li>Cantor集的Lebesgue测度是0&lt;/li>
&lt;li>Cantor集是非空有界闭集&lt;/li>
&lt;li>Cantor集是完备集&lt;/li>
&lt;li>Cantor集是无处稠密集（疏朗集）&lt;/li>
&lt;li>Cantor集是不可数集，基数是&lt;span class="math">\(\aleph_1\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h2 id="集列的极限">集列的极限&lt;/h2>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(A_1,A_2,\dots,A_n,\dots\)&lt;/span>是一列集合，由&lt;strong>属于上述集列中无限多个集&lt;/strong>的那种元素的全体组成的集称为这一集列的&lt;strong>上限集&lt;/strong>。记作&lt;span class="math">\(\overline{\lim\limits_{n→\infty}}A_n\)&lt;/span> &lt;span class="math">\[\overline{\lim\limits_{n→\infty}}A_n=\{x|存在无穷多个A_n，使x ∈ A_n\}.\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;hr />
&lt;blockquote>
&lt;p>对于集列&lt;span class="math">\(A_1,A_2,\dots,A_n,\dots\)&lt;/span>，那种&lt;strong>除了有限个集合外&lt;/strong>，属于集列中&lt;strong>其余每个集&lt;/strong>的元素全体组成的集称为&lt;strong>下限集&lt;/strong>。可以理解为不含有此元素的集合有有限个。记作&lt;span class="math">\(\mathop{\underline{\lim}}\limits_{n→\infty}A_n\)&lt;/span> &lt;span class="math">\[\mathop{\underline{\lim}}\limits_{n→\infty}A_n=\{x|存在N(x),当n&amp;gt;N(x)，x ∈ A_n\}.\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>显然： &lt;span class="math">\[\bigcap\limits_{n=1}^∞ A_n\subset\mathop{\underline{\lim}}\limits_{n→\infty}A_n\subset\overline{\lim\limits_{n→\infty}}A_n\subset\bigcup\limits_{n=1}^∞ A_n\]&lt;/span>&lt;/p>
&lt;p>上极限，包含它的有无限个（不包含它的也可以有无限个）即可能同时存在无限个集合包含它并且无限个集合不包含它&lt;span class="math">\(∞=∞+∞\)&lt;/span>；下极限，不包含它的有有限个，必然是包含它的集合有无限个&lt;span class="math">\(∞=∞-N\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>定理：上极限&lt;span class="math">\(\overline{\lim\limits_{n→∞}}A_n=\bigcap\limits_{n=1}^∞\bigcup\limits_{m=n}^∞ A_m\)&lt;/span>&lt;/p>
&lt;p>下极限：&lt;span class="math">\(\mathop{\underline{\lim}}\limits_{n→∞} A_n = \bigcup\limits_{n=1}^∞\bigcap\limits_{m=n}^∞ A_m\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>如果集列&lt;span class="math">\(\{A_n\}\)&lt;/span>的上下极限相等，则称集列&lt;span class="math">\(\{A_n\}\)&lt;/span>收敛，并称&lt;span class="math">\(A=\overline{\lim\limits_{n→∞}}A_n=\mathop{\underline{\lim}}\limits_{n→∞} A_n\)&lt;/span>是集列&lt;span class="math">\(\{A_n\}\)&lt;/span>的极限，记为&lt;span class="math">\(A=\lim\limits_{n→∞}A_n\)&lt;/span>。&lt;/p>
&lt;img src="../images/集列的上下极限.jpg" alt="集列的上下极限" />
&lt;center>
知乎上的一种理解
&lt;/center>
&lt;p>&lt;a href="https://www.zhihu.com/question/56812618">原图来源&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>定理：单调集列必收敛。如果是单调增加序列则： &lt;span class="math">\[\lim\limits_{n→∞}A_n=\bigcup\limits_{n=1}^∞ A_n\]&lt;/span> 如果是单调减少序列则： &lt;span class="math">\[\lim\limits_{n→∞}A_n=\bigcap\limits_{n=1}^∞ A_n\]&lt;/span>&lt;/p>
&lt;/blockquote></description></item><item><title>测度论1之集合的集合</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA1%E4%B9%8B%E9%9B%86%E5%90%88%E7%9A%84%E9%9B%86%E5%90%88/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA1%E4%B9%8B%E9%9B%86%E5%90%88%E7%9A%84%E9%9B%86%E5%90%88/</guid><description>
&lt;h2 id="测度论之集合的集合">测度论之集合的集合&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#集合的集合">集合的集合&lt;/a>&lt;/li>
&lt;li>&lt;a href="#来自拓扑的定义">来自拓扑的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从sigma代数开始">从&lt;span class="math">\(\\sigma\)&lt;/span>代数开始&lt;/a>&lt;/li>
&lt;li>&lt;a href="#最小sigma代数">最小&lt;span class="math">\(\\sigma\)&lt;/span>代数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#borel集与borel代数域">Borel集与Borel代数(域)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单调类定理">单调类定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#集合序列">集合序列&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单调系">单调系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一般角度的单调系定理">一般角度的单调系定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#λ系与π系">λ系与π系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#π系">π系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#λ系">λ系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#λ系π系与sigma代数">λ系π系与&lt;span class="math">\(\\sigma\)&lt;/span>代数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录1-集合运算的独立性">附录1： 集合运算的独立性&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="集合的集合">集合的集合&lt;/h2>
&lt;blockquote>
&lt;p>集类(集合族,集合系)：集合的集合，X的集类表示由X的子集组成的集合。e.g. &lt;span class="math">\(\{\emptyset,\{1,2\},\{1,3,4\},\{R\}\},P(X)(X的幂集)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h3 id="来自拓扑的定义">来自拓扑的定义&lt;/h3>
&lt;blockquote>
&lt;p>环--集环&lt;br />设&lt;span class="math">\(X\)&lt;/span>为一集合，&lt;span class="math">\(S\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>上的一个非空集类。&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(S\)&lt;/span>对集合的并和差运算封闭，即&lt;span class="math">\(\forall E,F\in S\implies E\cup F\in S,E-F\in S\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(S\)&lt;/span>对集合的交和对称差运算封闭，即：&lt;span class="math">\(\forall E,F\in S\implies E\cap F\in S,E\triangle F\in S\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(S\)&lt;/span>对集合的交，差以及无交并运算封闭.对称差（&lt;span class="math">\(\triangle\)&lt;/span>）&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>当且仅当&lt;span class="math">\(S\)&lt;/span>满足以上几个条件中&lt;strong>任何一个&lt;/strong>时，&lt;span class="math">\(S\)&lt;/span>构成一个环，此时&lt;span class="math">\(S\)&lt;/span>被称为一个&lt;strong>集环&lt;/strong>。三个定义是可以相互推导的，比如利用德摩根律推导。其中对称差是加法，空集是零元，逆元是其自身，交是乘法，全集是幺元。&lt;/p>
&lt;p>若集环S还满足以下条件：&lt;/p>
&lt;blockquote>
&lt;p>若&lt;span class="math">\(X \in A\)&lt;/span>，则称&lt;span class="math">\(A\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>上的一个代数。即代数需要把&lt;span class="math">\(X\)&lt;/span>自身包含进去。&lt;/p>
&lt;/blockquote>
&lt;p>若&lt;span class="math">\(A\)&lt;/span>对&lt;strong>有限&lt;/strong>并运算，&lt;strong>有限&lt;/strong>交运算，差运算，余（补）运算封闭，则称&lt;span class="math">\(A\)&lt;/span>为代数，&lt;strong>称为&lt;span class="math">\(X\)&lt;/span>上的集代数&lt;/strong> 。&lt;/p>
&lt;h2 id="从sigma代数开始">从&lt;span class="math">\(\sigma\)&lt;/span>代数开始&lt;/h2>
&lt;p>在&lt;strong>集环&lt;/strong>的基础上（通常使用&lt;span class="math">\(S\)&lt;/span>对集合的并和差运算封闭的定义，即&lt;span class="math">\(\forall E,F\in S\implies E\cup F\in S,E-F\in S\)&lt;/span>）加上无限可数并操作的封闭：&lt;/p>
&lt;blockquote>
&lt;p>对任意一列集合&lt;span class="math">\(E_i \in A(i=1,2,\dotsb)\)&lt;/span>，都有&lt;span class="math">\(\bigcup\limits_{i=1}^{\infty}E_i\in A\)&lt;/span>，则称&lt;span class="math">\(A\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>上的&lt;strong>一个&lt;span class="math">\(\sigma\)&lt;/span>环&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>从&lt;strong>有限到无限可数（可列，阿列夫零&lt;span class="math">\(\aleph_0\)&lt;/span>）的转变&lt;/strong>。&lt;/p>
&lt;p>同样的，在&lt;strong>集代数&lt;/strong>的基础上，加上&lt;strong>无限可数并&lt;/strong>操作的封闭，即是&lt;span class="math">\(\sigma\)&lt;/span>代数或&lt;span class="math">\(\sigma\)&lt;/span>域。&lt;/p>
&lt;p>完整的定义如下：&lt;/p>
&lt;blockquote>
&lt;p>&lt;span class="math">\(X\)&lt;/span>为一集合，假设有集合系&lt;span class="math">\({\mathcal {F}}\subseteq {\mathcal {P}}(X)\)&lt;/span>，其中 &lt;span class="math">\({\mathcal {P}}(X)\)&lt;/span>代表&lt;span class="math">\(X\)&lt;/span>的幂集，若&lt;span class="math">\(\mathcal{F}\)&lt;/span>满足下列条件&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(X\in {\mathcal {F}}\)&lt;/span>;&lt;/li>
&lt;li>&lt;span class="math">\(A\in {\mathcal {F}}\;\Rightarrow \;A^{c}\in {\mathcal {F}}\)&lt;/span>;&lt;/li>
&lt;li>&lt;span class="math">\(A_{n}\in {\mathcal {F}},\;\forall n\in \mathbb {N} \;\Rightarrow \;\bigcup_{n=1}^{\infty }A_{n}\in {\mathcal {F}}.\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>则称集合系&lt;span class="math">\(\mathcal{F}\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的&lt;span class="math">\(σ-代数\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>在测度论里&lt;span class="math">\(\left(X,{\mathcal {F}}\right)\)&lt;/span>称为一个&lt;strong>可测空间&lt;/strong>。 集合族&lt;span class="math">\(\mathcal{F}\)&lt;/span>中的元素，也就是&lt;span class="math">\(X\)&lt;/span>的某子集，称为&lt;strong>可测集合&lt;/strong>。而在概率论中，这些集合被称为&lt;strong>随机事件&lt;/strong>。&lt;/p>
&lt;p>例如：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>X为任意集， X的至多可数集全体所成的集类 A为一个&lt;span class="math">\(\sigma\)&lt;/span>环。当且仅当X为至多可数集时(&lt;span class="math">\(X \in A\)&lt;/span>)，A为&lt;span class="math">\(\sigma\)&lt;/span>代数。&lt;/li>
&lt;li>设&lt;span class="math">\(A=\{\emptyset,X\}\)&lt;/span>，则A是X上的&lt;span class="math">\(\sigma\)&lt;/span>代数，且为X上的最小&lt;span class="math">\(\sigma\)&lt;/span>代数。&lt;/li>
&lt;li>设&lt;span class="math">\(P(X)\)&lt;/span>为X全体子集所成的集类，则&lt;span class="math">\(P(X)\)&lt;/span>为X上最大的&lt;span class="math">\(\sigma\)&lt;/span>代数。&lt;/li>
&lt;/ol>
&lt;h3 id="最小sigma代数">最小&lt;span class="math">\(\sigma\)&lt;/span>代数&lt;/h3>
&lt;blockquote>
&lt;p>Lemma：若&lt;span class="math">\(A_\alpha\)&lt;/span>是一个&lt;span class="math">\(\sigma代数\)&lt;/span>，&lt;span class="math">\(\forall \alpha \in I\)&lt;/span>，&lt;span class="math">\(I\)&lt;/span>为非空索引集(index set)，那么&lt;span class="math">\(\bigcap_{\alpha \in I}A_\alpha\)&lt;/span>也是一个&lt;span class="math">\(\sigma代数\)&lt;/span>。&lt;/p>
&lt;p>Proof：以两个为例，多个可以递推。&lt;span class="math">\(A_1,A_2\)&lt;/span>是两个&lt;span class="math">\(\sigma代数\)&lt;/span>。&lt;/p>
&lt;ul>
&lt;li>由于&lt;span class="math">\(\emptyset,X\in A_1;\emptyset,X\in A_2\)&lt;/span>，所以&lt;span class="math">\(\emptyset,X\in A_1\cap A_2\)&lt;/span>；&lt;/li>
&lt;li>由于集合&lt;span class="math">\(E,E^c \in A_1,A_2\)&lt;/span>，所以&lt;span class="math">\(E,E^c \in A_1\cap A_2\)&lt;/span>；&lt;/li>
&lt;li>对于可列集合&lt;span class="math">\(\bigcup _{n=1}^{\infty }E_{n}\in A_1,A_2\)&lt;/span>，所以&lt;span class="math">\(\bigcup _{n=1}^{\infty }E_{n}\in A_1\cap A_2\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>下面我们根据&lt;span class="math">\(\sigma\)&lt;/span>代数交的引理定义最小&lt;span class="math">\(\sigma\)&lt;/span>代数。&lt;/p>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(\mathcal C\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>上的任意子集类，则存在唯一的一个&lt;span class="math">\(σ代数（σ(\mathcal C)）\)&lt;/span>，它是包含&lt;span class="math">\(\mathcal C\)&lt;/span>的最小&lt;span class="math">\(σ代数\)&lt;/span>，称&lt;span class="math">\(σ(\mathcal C)\)&lt;/span>是&lt;span class="math">\(\mathcal C\)&lt;/span>生成的&lt;span class="math">\(σ\)&lt;/span>代数（&lt;span class="math">\(\mathcal C\)&lt;/span>为生成元） &lt;span class="math">\[\sigma(\mathcal C) = {\bigcap} _{\mathcal F&amp;#39;为\sigma代数且\mathcal C \subset \mathcal F&amp;#39;}\mathcal F&amp;#39;\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>注意满足条件的&lt;span class="math">\(σ\)&lt;/span>代数是存在的，因为&lt;span class="math">\(\mathcal{C}⊂P(X)\)&lt;/span>。容易看出，若&lt;span class="math">\(\mathcal{C_1}\subset \mathcal{C_2}\)&lt;/span>，那么&lt;span class="math">\(\sigma(\mathcal{C_1})\subset \sigma(\mathcal{C_2})\)&lt;/span>。 &lt;span class="math">\(\sigma(\sigma(\mathcal{C}))=\sigma(\mathcal{C})\)&lt;/span>，因为有上面&lt;span class="math">\(\sigma(\mathcal{C})\)&lt;/span>本身是一个&lt;span class="math">\(\sigma\)&lt;/span>代数, 再根据定义便可得。&lt;/p>
&lt;h2 id="borel集与borel代数域">Borel集与Borel代数(域)&lt;/h2>
&lt;p>一个拓扑空间的开集全体所生成的&lt;span class="math">\(\sigma代数\)&lt;/span>就是borel代数。&lt;/p>
&lt;p>如果我们幸运的能在度量空间或者更广义的拓扑空间&lt;span class="math">\(X\)&lt;/span>上定义一些开集，这时我们令&lt;span class="math">\(G\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>上&lt;strong>所有开子集&lt;/strong>的集合，那么&lt;span class="math">\(\sigma(G)\)&lt;/span>就是&lt;span class="math">\(X\)&lt;/span>上的 &lt;span class="math">\(Borel(-\sigma-)代数\)&lt;/span> ，记为&lt;span class="math">\(B_X\)&lt;/span>, 里面的元素就叫 Borel 集。&lt;/p>
&lt;p>令&lt;span class="math">\(X=\mathbb{R}\)&lt;/span>，那么&lt;span class="math">\(\mathcal{B}\)&lt;/span>可以从以下的集合系的&lt;span class="math">\(\sigma\)&lt;/span>代数中生成：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\mathcal C_1=\{(a,b):a,b ∈ \mathbb{R}\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathcal C_2=\{[a,b]:a,b ∈ \mathbb{R}\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathcal C_3=\{(a,b]:a,b ∈ \mathbb{R}\}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathcal C_4=\{(a,∞):a ∈ \mathbb{R}\}\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>博雷尔测度（Borel measure）是σ代数上对区间[a, b]给出值b-a的测度。&lt;/p>
&lt;p>博雷尔测度并不完备，因此习惯使用勒贝格测度：每个博雷尔可测集都是勒贝格可测的，并且它们的测度值吻合。&lt;/p>
&lt;p>Borel系的其他细节可以参考学习笔记中的《Borel 集的作用？意义？它为什么重要？》&lt;/p>
&lt;h2 id="单调类定理">单调类定理&lt;/h2>
&lt;h3 id="集合序列">集合序列&lt;/h3>
&lt;blockquote>
&lt;p>不升序列：若对一切&lt;span class="math">\(n \geq 1\)&lt;/span>，&lt;span class="math">\(A_n \subseteq A_{n+1}\)&lt;/span>，且令&lt;span class="math">\(A=\bigcup\limits_{n=1}^{\infty}A_n\)&lt;/span>，则记为&lt;span class="math">\(A_n\nearrow A\)&lt;/span>也有写成&lt;span class="math">\(A_n \uparrow A\)&lt;/span>.&lt;/p>
&lt;p>不降序列：若对一切&lt;span class="math">\(n \geq 1\)&lt;/span>，&lt;span class="math">\(A_n \supseteq A_{n+1}\)&lt;/span>，且令&lt;span class="math">\(A=\bigcap\limits_{n=1}^{\infty}A_n\)&lt;/span>，则记为&lt;span class="math">\(A_n\searrow A\)&lt;/span>也有写成&lt;span class="math">\(A_n \downarrow A\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;span class="math">\(A_n\nearrow A\)&lt;/span>当且仅当&lt;span class="math">\(A_n^c\searrow A^c\)&lt;/span>。&lt;/p>
&lt;h3 id="单调系">单调系&lt;/h3>
&lt;blockquote>
&lt;p>一个单调系(monotone class)是由集合&lt;span class="math">\(X\)&lt;/span>的子集构成的&lt;strong>集合系&lt;/strong>&lt;span class="math">\(\mathcal{M}\)&lt;/span>同时满足以下两个条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>如果集合&lt;span class="math">\(A_i\nearrow A\)&lt;/span>，且&lt;span class="math">\(\forall i,A_i\in \mathcal{M}\)&lt;/span>，那么&lt;span class="math">\(A \in \mathcal{M}\)&lt;/span>，即对不降序列的可数并封闭。&lt;/li>
&lt;li>如果集合&lt;span class="math">\(A_i\searrow A\)&lt;/span>，且&lt;span class="math">\(\forall i,A_i\in \mathcal{M}\)&lt;/span>，那么&lt;span class="math">\(A \in \mathcal{M}\)&lt;/span>，即对不升序列的可数交封闭。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>单调系的&lt;strong>交集&lt;/strong>仍然是单调系。所有包含某个集合系&lt;span class="math">\(\mathcal{C}\)&lt;/span>的单调系的交就是包含该集合系的&lt;strong>最小单调系&lt;/strong>。 &lt;span class="math">\[\mathcal{M}(\mathcal{C})= {\bigcap} _{\mathcal M&amp;#39;为单调系且\mathcal C \subset \mathcal M&amp;#39;}\mathcal M&amp;#39;\]&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(\sigma代数\)&lt;/span>与单调系的关系可以通过下面说明。 &amp;gt;引理：&lt;span class="math">\(\sigma代数\Leftrightarrow 代数+单调系\)&lt;/span>&lt;/p>
&lt;p>证明：&lt;/p>
&lt;p>&lt;span class="math">\(\sigma代数\Rightarrow 集代数\)&lt;/span>，显然&lt;span class="math">\(\sigma代数\)&lt;/span>是在集代数的定义上添加无穷可列并封闭的条件得来的。&lt;/p>
&lt;p>&lt;span class="math">\(\sigma代数\Rightarrow 单调系\)&lt;/span>，&lt;span class="math">\(\sigma代数\)&lt;/span>要求可列并和可列交封闭，满足单调系&lt;span class="math">\(\bigcap\limits_{n=1}^{\infty}A_n \in \mathcal{M},\bigcup\limits_{n=1}^{\infty}A_n\in \mathcal{M}\)&lt;/span>的定义。&lt;/p>
&lt;p>&lt;span class="math">\(代数+单调系\Rightarrow\sigma代数\)&lt;/span>，集代数提供了&lt;span class="math">\(\sigma代数\)&lt;/span>的&lt;span class="math">\(\emptyset,X\in \mathcal{M}；A\in \mathcal{M}\Rightarrow A^c\in \mathcal{M}\)&lt;/span>与有限交、并的封闭性。需要从单调系中推导出无限可数并的封闭性。于是我们对于任意集合系&lt;span class="math">\(A_n\)&lt;/span>构造如下集合序列： &lt;span class="math">\[\begin{cases}
C_1=A_1\\
C_2=A_1\cup A_2\\
\vdots\\
C_n=A_1\cup A_2\dotsb\cup A_n\\
A_n,C_n\subset X
\end{cases}\\
A_1\dotsb A_n\dotsb\in \mathcal{M}\\
\bigcup\limits_{n=1}^{\infty}A_n=\bigcup\limits_{n=1}^{\infty}C_n\nearrow C \in \mathcal{M}
\]&lt;/span> 所以无限可数并&lt;span class="math">\(\bigcup\limits_{n=1}^{\infty}A_n \in \mathcal{M}。代数+单调系\Rightarrow\sigma代数\)&lt;/span>&lt;/p>
&lt;p>得证。&lt;/p>
&lt;h3 id="一般角度的单调系定理">一般角度的单调系定理&lt;/h3>
&lt;blockquote>
&lt;p>单调系定理：令&lt;span class="math">\(\mathcal A_0\)&lt;/span>是定义在&lt;span class="math">\(X\)&lt;/span>上的一个代数，&lt;span class="math">\(\mathcal A\)&lt;/span> 是包含&lt;span class="math">\(\mathcal A_0\)&lt;/span>的最小 &lt;span class="math">\(\sigma\)&lt;/span>-代数，&lt;span class="math">\(\mathcal M\)&lt;/span>是包含&lt;span class="math">\(\mathcal A_0\)&lt;/span>的最小单调系，那么&lt;span class="math">\(\mathcal M=\mathcal{A}\)&lt;/span>。该定理联系了单调系和&lt;span class="math">\(\sigma\)&lt;/span>代数.&lt;/p>
&lt;p>Proof：由以上引理可知，&lt;span class="math">\(\sigma代数\)&lt;/span>是一个单调系且包含&lt;span class="math">\(X的子集A_0\)&lt;/span>，而&lt;span class="math">\(M\)&lt;/span>是包含&lt;span class="math">\(A_0\)&lt;/span>的最小单调系。即&lt;span class="math">\(\mathcal{M}\subset\mathcal{A}\)&lt;/span>。&lt;/p>
&lt;p>如果我们能接着证明&lt;span class="math">\(\mathcal{A}\subset\mathcal{M}\)&lt;/span>，则说明&lt;span class="math">\(\mathcal M=\mathcal{A}\)&lt;/span>。&lt;/p>
&lt;p>由以上引理中的证明可知，单调系的定义可以使得集系&lt;span class="math">\(\mathcal{M}\)&lt;/span>内对无穷可数并封闭。如果我们能证明最小单调系也是一个集代数(证明略)，那么就用引理：&lt;span class="math">\(\sigma代数\Leftrightarrow 代数+单调系\)&lt;/span>推得&lt;span class="math">\(\mathcal{M}\)&lt;/span>是一个&lt;span class="math">\(\sigma\)&lt;/span>代数。&lt;/p>
&lt;p>&lt;span class="math">\(\mathcal{M}\)&lt;/span>是一个包含&lt;span class="math">\(A_0的\sigma\)&lt;/span>代数，而&lt;span class="math">\(\mathcal A\)&lt;/span> 是包含&lt;span class="math">\(\mathcal A_0\)&lt;/span>的最小 &lt;span class="math">\(\sigma\)&lt;/span>-代数，所以&lt;span class="math">\(\mathcal{A}\subset\mathcal{M}\)&lt;/span>。&lt;/p>
&lt;p>综上所述，&lt;span class="math">\(\mathcal{A}\subset\mathcal{M}\)&lt;/span>，&lt;span class="math">\(\mathcal{A}\supset\mathcal{M}\)&lt;/span>，所以&lt;span class="math">\(\mathcal M=\mathcal{A}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="../images/sigma代数与单调系.jpg">英文版单调性与单调系定理&lt;/a>&lt;/p>
&lt;h2 id="λ系与π系">λ系与π系&lt;/h2>
&lt;h3 id="π系">π系&lt;/h3>
&lt;blockquote>
&lt;p>定义：若在&lt;span class="math">\(X\)&lt;/span>上的非空子集类&lt;span class="math">\(\Pi\)&lt;/span>满足：&lt;/p>
&lt;p>&lt;span class="math">\(A,B∈\Pi⇒A\cap B∈\Pi\)&lt;/span>,则称子集类&lt;span class="math">\(Π为π系\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>即它对交运算封闭，&lt;span class="math">\(\sigma\)&lt;/span>代数也是&lt;span class="math">\(\pi\)&lt;/span>系。&lt;span class="math">\(\pi(A)\)&lt;/span>指由&lt;span class="math">\(A\)&lt;/span>生成的&lt;span class="math">\(\pi\)&lt;/span>-系，是包含 &lt;span class="math">\(A\)&lt;/span>的最小的&lt;span class="math">\(\pi\)&lt;/span>-系。&lt;/p>
&lt;h3 id="λ系">λ系&lt;/h3>
&lt;blockquote>
&lt;p>定义：若在&lt;span class="math">\(X\)&lt;/span>上的子集类&lt;span class="math">\(\Lambda\)&lt;/span>满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X\in \Lambda\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(A\in\Lambda\)&lt;/span>，那么&lt;span class="math">\(A^c\in\Lambda\)&lt;/span>(注：这条也可等价换成&lt;span class="math">\(A,B\in\Lambda,A\subset B,B-A\subset \Lambda\)&lt;/span>)&lt;/li>
&lt;li>如果集合序&lt;span class="math">\(A_1,A_2,A_3,\dotsb,\in \Lambda\)&lt;/span>且互不相交，那么&lt;span class="math">\(\bigcup\limits_{n=1}^{\infty}A_n\in \Lambda\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>&lt;span class="math">\(\lambda\)&lt;/span>系和&lt;span class="math">\(\sigma\)&lt;/span>代数的区别是对于无穷可数并的要求&lt;span class="math">\(\lambda\)&lt;/span>系更严格，所以&lt;span class="math">\(\sigma\)&lt;/span>代数也是&lt;span class="math">\(\lambda\)&lt;/span>系。&lt;span class="math">\(\lambda(A)\)&lt;/span>指由&lt;span class="math">\(A\)&lt;/span>生成的&lt;span class="math">\(\lambda\)&lt;/span>-系，是包含 &lt;span class="math">\(A\)&lt;/span>的最小的&lt;span class="math">\(\lambda\)&lt;/span>-系。&lt;/p>
&lt;h3 id="λ系π系与sigma代数">λ系π系与&lt;span class="math">\(\sigma\)&lt;/span>代数&lt;/h3>
&lt;blockquote>
&lt;p>若一个集合系&lt;span class="math">\(\mathcal{A}\)&lt;/span>既是&lt;span class="math">\(\lambda\)&lt;/span>-系又是&lt;span class="math">\(\pi\)&lt;/span>-系，那么它也是&lt;span class="math">\(\sigma\)&lt;/span>-代数。且&lt;span class="math">\(\lambda(\mathcal{A})=\sigma(\mathcal{A})\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;h2 id="附录1-集合运算的独立性">附录1： 集合运算的独立性&lt;/h2>
&lt;p>集合的四种常见运算：交&lt;span class="math">\(\cap\)&lt;/span>、并&lt;span class="math">\(\cup\)&lt;/span>、差&lt;span class="math">\(-\)&lt;/span>、对称差&lt;span class="math">\(\oplus\)&lt;/span>并不是相互独立的，可由其中某两种运算推出其他两种，总结如下：&lt;/p>
&lt;blockquote>
&lt;p>集合运算定理1：&lt;br />(a) 集合的交、对称差运算可由集合的并、差两种运算定义&lt;br />(b) 集合的交、并运算可由集合的差、对称差两种运算定义&lt;br />(c) 集合的交、差运算可由集合的并、对称差两种运算定义&lt;br />(d) 集合的并、差运算可由集合的交、对称差两种运算定义&lt;/p>
&lt;/blockquote>
&lt;p>但是并不是四种运算的任意两种都可以推出其他运算。&lt;/p>
&lt;blockquote>
&lt;p>集合运算定理2：&lt;br />(a) 集合的差运算无法由集合的交、并两种运算定义&lt;br />(b) 集合的并运算无法由集合的交、差两种运算定义&lt;/p>
&lt;/blockquote>
&lt;p>即集合的四种基本运算：并、交、差、对称差不是相互独立的可以把并、差两种运算（或 对称差、差两种运算，或对称差、并两种运算，或对称差、交两种运算）作为两个相互独立的基本运算，其余两种运算可由这两种运算定义；但不可以只把并与交两种运算（或差与交两种运算）作为独立的基本运算，否则其余的两种运算至少有一种不能由它们定义。&lt;/p></description></item><item><title>测度论2之定义与简单性质</title><link>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA2%E4%B9%8B%E5%AE%9A%E4%B9%89%E4%B8%8E%E7%AE%80%E5%8D%95%E6%80%A7%E8%B4%A8/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B5%8B%E5%BA%A6%E8%AE%BA2%E4%B9%8B%E5%AE%9A%E4%B9%89%E4%B8%8E%E7%AE%80%E5%8D%95%E6%80%A7%E8%B4%A8/</guid><description>
&lt;h2 id="测度论2之定义与简单性质">测度论2之定义与简单性质&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#总述">总述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#测度的定义">测度的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#性质">性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#测度的有限性">测度的有限性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#测度空间的完备性">测度空间的完备性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#概率与测度">概率与测度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#补充两个测度的定理">补充两个测度的定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#测度构造的主线-粗线条">测度构造的主线-粗线条&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="总述">总述&lt;/h2>
&lt;p>这一章介绍测度的定义跟测度的简单性质，最后给出一个俯瞰的视角，勾勒出测度构造的主线。但是具体证明会在后面章节展开。&lt;/p>
&lt;p>先在直觉层面介绍一下，测度论试图建立一般测量所需要的广义且底层的逻辑。一个测度可以是长度、面积、体积、概率，等等——它们的共同点就是给一个集合赋予一个数值。比如一条线段就是一个由点构成的集合，而长度就是给这个点集赋值一个实数 （这条线段长度是5）。这个实数不是随便赋值的，它要满足一些最基本的公理：比如两条线段不相交，那么他们的长度是可以相加的；再比如一个集合为空，那么长度应该为零，等等。&lt;/p>
&lt;p>人们自然的就会问：给定一个集合，我们能不能找到这么一个“测度”，能够合理的给集合中所有的子集都赋上一个值，同时也满足我们所需的测量的最基本的公理呢？&lt;/p>
&lt;p>对于有限集或者可数集，我们确实能找到这样的测度；但是对于&lt;strong>不可数集，我们却找不到这样的测度&lt;/strong>（如果我们接受选择公理，我们总能构造出一些集合，无论我们给这些集合赋什么值都会不满足测度公理要求的条件）。&lt;/p>
&lt;p>既然大多数情况下（不可数集），我们没法找到一个测度能够合理的给集合中的每一个子集赋值，我们就退而求其次，问：”我们可以对哪些（不是全部）子集合理的赋值呢“。我们当然希望可以被测度赋值的子集越多越好————于是我们从一些子集开始，在测度依然可以合理赋值的条件下，逐步拓展、加入更多的子集；通常这些子集的集合体就会最终拓展成一个&lt;span class="math">\(\sigma\)&lt;/span>-代数。&lt;/p>
&lt;h2 id="测度的定义">测度的定义&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>测度&lt;/strong>：令&lt;span class="math">\(X\)&lt;/span>为一集合，&lt;span class="math">\(\mathcal{A}\)&lt;/span>是由&lt;span class="math">\(X\)&lt;/span>子集构成的&lt;span class="math">\(\sigma\)&lt;/span>-代数，在&lt;span class="math">\((X,\mathcal{A})\)&lt;/span>上的测度(measure)是一个映射&lt;span class="math">\(\mu:\mathcal{A}\rightarrow [0,+\infty]\)&lt;/span>，满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(\mu(\emptyset)=0\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(A_i\in\mathcal{A},i=1,2,\dotsb\)&lt;/span>两两不相交，那么：&lt;span class="math">\(\mu(\bigcup\limits_{n=1}^{\infty}A_i)=\sum\limits_{n=1}^{\infty}\mu(A_i)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注：性质(2)也被称为可列可加性(countable additivity)；若(2)中的序列为有限的&lt;span class="math">\(n\)&lt;/span>个，那么就称为有限可加 (finitely additive)。&lt;strong>有限可加未必可列可加&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>从而三元组&lt;span class="math">\((X,\mathcal{A},\mu)\)&lt;/span>也被称为测度空间 (measure space)。&lt;/p>
&lt;/blockquote>
&lt;h3 id="性质">性质&lt;/h3>
&lt;ul>
&lt;li>若&lt;span class="math">\(A,B\in \mathcal{A},A\subset B\)&lt;/span>，那么&lt;span class="math">\(\mu(A)&amp;lt;\mu(B)\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(A_i\in\mathcal{A},A=\bigcup\limits_{i=1}^{\infty}A_i\)&lt;/span>，那么&lt;span class="math">\(\mu(A)\leq \mu(\bigcup\limits_{i=1}^{\infty}A_i)\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(A_i\in\mathcal{A},A_i\nearrow A\)&lt;/span>，那么&lt;span class="math">\(\mu(A)=\lim_{n\rightarrow\infty}\mu(A_n)\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(A_i\in\mathcal{A},A_i\searrow A\)&lt;/span>，且&lt;span class="math">\(\mu(A_1)&amp;lt;\infty\)&lt;/span>,那么&lt;span class="math">\(\mu(A)=\lim_{n\rightarrow\infty}\mu(A_n)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="测度的有限性">测度的有限性&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>称测度&lt;span class="math">\(\mu\)&lt;/span>为有限测度 (finite measure), 若&lt;span class="math">\(\mu(x)&amp;lt;\infty\)&lt;/span>。若&lt;span class="math">\(\mu\)&lt;/span>是有限测度，那么&lt;span class="math">\((X,\mathcal{A},\mu)\)&lt;/span>被称为有限测度空间&lt;/li>
&lt;li>测度&lt;span class="math">\(\mu\)&lt;/span>为&lt;span class="math">\(\sigma\)&lt;/span>-有限，若存在集合&lt;span class="math">\(E_i\in \mathcal{A},i=1,2,\dotsb\)&lt;/span>, 满足&lt;span class="math">\(\forall i,\mu(E_i)&amp;lt;\infty,X=\bigcup\limits_{i=1}^\infty E_i\)&lt;/span>。若&lt;span class="math">\(\mu\)&lt;/span>是&lt;span class="math">\(\sigma\)&lt;/span>-有限，则称为&lt;span class="math">\(\sigma\)&lt;/span>-有限测度空间。相较于有限测度宽松，只要求每个事件有限。&lt;/li>
&lt;li>令&lt;span class="math">\((X,\mathcal{A},\mu)\)&lt;/span>为&lt;span class="math">\(\sigma\)&lt;/span>-有限测度空间，若&lt;span class="math">\(F_n=\bigcup\limits_{i=1}^n E_i\)&lt;/span>，则&lt;span class="math">\(\forall n，F_n&amp;lt;\infty\)&lt;/span>，且&lt;span class="math">\(F_n\nearrow X\)&lt;/span>。所以不失一般性，我们可以令上文定义中的&lt;span class="math">\(E_i\)&lt;/span>为递增集合 (&lt;span class="math">\(E_i \nearrow\)&lt;/span>)&lt;/li>
&lt;/ol>
&lt;h3 id="测度空间的完备性">测度空间的完备性&lt;/h3>
&lt;blockquote>
&lt;p>零测集：令&lt;span class="math">\((X,\mathcal{A},\mu)\)&lt;/span>为一测度空间，子集&lt;span class="math">\(A\subset X\)&lt;/span>被称为零测集 (null set) 如果存在集合&lt;span class="math">\(B\in\mathcal{A},A\subset B,且\mu(B)=0\)&lt;/span>。 &amp;gt;注：在前面的定义中，我们并不要求&lt;span class="math">\(A\in\mathcal{A}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>完备性： &amp;gt;完备测度空间：若&lt;span class="math">\(\mathcal{A}\)&lt;/span>包含&lt;strong>所有的零测集&lt;/strong>，那么&lt;span class="math">\((X,\mathcal{A},\mu)\)&lt;/span>为完备测度空间 (complete measure space).&lt;/p>
&lt;p>&lt;span class="math">\(\mathcal{A}\)&lt;/span>的完备化 (completion) 是最小的&lt;span class="math">\(\sigma\)&lt;/span>-代数&lt;span class="math">\(\mathcal{\overline{A}}\supset \mathcal{A}\)&lt;/span>，而&lt;span class="math">\((X,\mathcal{\overline{A}},\mu)\)&lt;/span>是完备的。有时候我们仅仅说 &lt;span class="math">\(\mu\)&lt;/span>是完备的，或者&lt;span class="math">\(\mathcal{A}\)&lt;/span>是完备的——它们都指&lt;span class="math">\((X,\mathcal{A},\mu)\)&lt;/span>是完备的。&lt;/p>
&lt;h2 id="概率与测度">概率与测度&lt;/h2>
&lt;p>概率(probability)或者有时候称为概率测度（probability measure)，被定义为一个测度，满足&lt;span class="math">\(\mu(X)=1\)&lt;/span>。在概率论的语境中，我们通常会写&lt;span class="math">\((\Omega,\mathcal{F},P)\)&lt;/span>, 其中&lt;span class="math">\(\mathcal{F}\)&lt;/span>被称为是&lt;span class="math">\(\sigma\)&lt;/span>-域 (&lt;span class="math">\(\sigma\)&lt;/span>-field), 这个跟&lt;span class="math">\(\sigma\)&lt;/span>-代数是一回事儿。&lt;/p>
&lt;h2 id="补充两个测度的定理">补充两个测度的定理&lt;/h2>
&lt;p>补充两个定理，在泛函分析跟概率论的时候会被用到。&lt;/p>
&lt;blockquote>
&lt;p>定理：令&lt;span class="math">\((X,\mathcal{A})\)&lt;/span>为一可测空间，&lt;span class="math">\(\mu\)&lt;/span>是一个非负的集合映射，&lt;strong>有限可加&lt;/strong>，且&lt;span class="math">\(\mu(\emptyset)=0\)&lt;/span>。如果&lt;span class="math">\(A_i\nearrow,A_i\in\mathcal{A}\)&lt;/span>，那么&lt;span class="math">\(\mu(\cup_iA_i)=\lim\limits_{i\rightarrow\infty}\mu(A_i)\)&lt;/span>。&lt;span class="math">\(\mu\)&lt;/span>是一个测度。&lt;/p>
&lt;/blockquote>
&lt;p>另一个角度来看， &amp;gt;定理：令&lt;span class="math">\((X,\mathcal{A})\)&lt;/span>为一可测空间，&lt;span class="math">\(\mu\)&lt;/span>是一个非负的集合映射，&lt;strong>有限可加&lt;/strong>，且&lt;span class="math">\(\mu(\emptyset)=0,\mu(X)&amp;lt;\infty\)&lt;/span>。如果&lt;span class="math">\(A_i\searrow\emptyset,A_i\in\mathcal{A}\)&lt;/span>，那么&lt;span class="math">\(\lim\limits_{i\rightarrow\infty}\mu(A_i)=0\)&lt;/span>。&lt;span class="math">\(\mu\)&lt;/span>是一个测度。&lt;/p>
&lt;h2 id="测度构造的主线-粗线条">测度构造的主线-粗线条&lt;/h2>
&lt;p>在这里我想列出（不证明）实数上Lebesgue测度的构造主线，力求列出最自然、跳跃最小的那条线。&lt;/p>
&lt;blockquote>
&lt;p>&lt;span class="math">\(\sigma\)&lt;/span>-全：&lt;span class="math">\(X\)&lt;/span>上的集合系&lt;span class="math">\(\Epsilon\)&lt;/span>被称为是&lt;span class="math">\(\sigma\)&lt;/span>-全的(&lt;span class="math">\(\sigma\)&lt;/span>-total)，如果存在某个序列&lt;span class="math">\(E_1,E_2,\dotsb\in\Epsilon,\bigcup\limits_{n=1}^\infty E_n=X\)&lt;/span>。 注：&lt;span class="math">\(\sigma\)&lt;/span>-有限就自带了&lt;span class="math">\(\sigma\)&lt;/span>-全&lt;/p>
&lt;/blockquote>
&lt;p>粗线条如下：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/测度构造粗线条.png" alt="测度构造粗线条" />&lt;p class="caption">测度构造粗线条&lt;/p>
&lt;/div>
&lt;p>笔者画了下图以示各个测度与可测空间之间的关系:&lt;/p>
&lt;div class="figure">
&lt;img src="../images/测度与可测空间之间的关系.jpg" alt="测度与可测空间之间的关系" />&lt;p class="caption">测度与可测空间之间的关系&lt;/p>
&lt;/div>
&lt;p>此外还有更细的细条内容，可见&lt;a href="https://zhuanlan.zhihu.com/p/61405805">测度构造的主线 - 细线条&lt;/a>。&lt;/p></description></item><item><title>概率统计随机过程之两两独立与相互独立</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E4%B8%A4%E4%B8%A4%E7%8B%AC%E7%AB%8B%E4%B8%8E%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B/</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E4%B8%A4%E4%B8%A4%E7%8B%AC%E7%AB%8B%E4%B8%8E%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B/</guid><description>
&lt;h2 id="概率统计随机过程之两两独立与相互独立">概率统计随机过程之两两独立与相互独立&lt;!-- omit in toc -->&lt;/h2>
&lt;p>&lt;span class="math">\(A,B,C\)&lt;/span>互相独立，说明&lt;span class="math">\(A,B,C\)&lt;/span>间无关联，是互相独立的，但两两独立指&lt;span class="math">\(A\)&lt;/span>和&lt;span class="math">\(B\)&lt;/span>间独立，&lt;span class="math">\(B\)&lt;/span>和&lt;span class="math">\(C\)&lt;/span>之间独立，&lt;span class="math">\(A\)&lt;/span>和&lt;span class="math">\(C\)&lt;/span>间独立，但三者放在一起，并不能判断他们是无关的。&lt;/p>
&lt;p>所以，&lt;strong>两两独立不一定相互独立，相互独立必然两两独立。&lt;/strong>&lt;/p>
&lt;p>例如：有三个随机变量&lt;span class="math">\(A,B,C\)&lt;/span>如果他们两两独立， 那么： &lt;span class="math">\[P(AB)=P(A)P(B)\\
P(AC)=P(A)P(C)\\
P(BC)=P(B)P(C)\]&lt;/span>&lt;/p>
&lt;p>但是&lt;span class="math">\(P(ABC)\)&lt;/span>不一定等于&lt;span class="math">\(P(A)P(B)P(C)\)&lt;/span>，&lt;/p>
&lt;p>如果相互独立的话，那么上式就是成立的。&lt;/p>
&lt;p>&lt;strong>请看下例&lt;/strong>:&lt;/p>
&lt;blockquote>
&lt;p>例1：设有四张外型一样的卡片,上分别写 有数字&lt;span class="math">\(2,3,5,30\)&lt;/span>今从中任取一张观察其上数字: &lt;span class="math">\[A=\{取到是2的倍数\};\\B=\{取到是3的倍数\};\\C=\{取到是5的倍数\};\]&lt;/span> 则&lt;span class="math">\(A,B,C\)&lt;/span>是两两独立而不是相互独立.&lt;/p>
&lt;p>因为: &lt;span class="math">\[A=\{2,30\}; B=\{3,30\}; C=\{5,30\};\\AB=AC=BC=ABC=\{30\}\]&lt;/span> 所以: &lt;span class="math">\(P(A)=P(B)=P(C)=2/4=1/2;P(AB)=P(A)P(B)=(1/2)*(1/2)=0.25,\)&lt;/span>&lt;/p>
&lt;p>类似&lt;span class="math">\(P(BC)=P(B)P(C))=0.25; P(AC)=P(A)P(C))=0.25\)&lt;/span>&lt;/p>
&lt;p>而&lt;span class="math">\(P(ABC)=(1/2)*(1/2)*(1/2)=0.125,\)&lt;/span>&lt;/p>
&lt;p>但&lt;span class="math">\(P(A)P(B)P(C)=0.5*0.5*0.5=0.125\)&lt;/span>两者不等.&lt;/p>
&lt;/blockquote>
&lt;p>下面这个例子来自维基百科： &amp;gt;例2： Suppose &lt;span class="math">\(X\)&lt;/span> and &lt;span class="math">\(Y\)&lt;/span> are two independent tosses of a fair coin, where we designate 1 for heads and 0 for tails. Let the third random variable &lt;span class="math">\(Z\)&lt;/span> be equal to 1 if exactly one of those coin tosses resulted in &amp;quot;heads&amp;quot;, and 0 otherwise(异或). Then jointly the triple &lt;span class="math">\((X, Y, Z)\)&lt;/span> has the following probability distribution: &amp;gt;&lt;span class="math">\[(X,Y,Z)=\left\{{\begin{matrix}(0,0,0)&amp;amp;{\text{with probability}}\ 1/4,\\(0,1,1)&amp;amp;{\text{with probability}}\ 1/4,\\(1,0,1)&amp;amp;{\text{with probability}}\ 1/4,\\(1,1,0)&amp;amp;{\text{with probability}}\ 1/4.\end{matrix}}\right.\]&lt;/span> &amp;gt;Here the marginal probability distributions are identical: &lt;span class="math">\(f_{X}(0)=f_{Y}(0)=f_{Z}(0)=1/2\)&lt;/span>, and &lt;span class="math">\(f_{X}(1)=f_{Y}(1)=f_{Z}(1)=1/2\)&lt;/span>. The bivariate distributions also agree: &lt;span class="math">\(f_{{X,Y}}=f_{{X,Z}}=f_{{Y,Z}}\)&lt;/span>, where &lt;span class="math">\(f_{{X,Y}}(0,0)=f_{{X,Y}}(0,1)=f_{{X,Y}}(1,0)=f_{{X,Y}}(1,1)=1/4\)&lt;/span>. &amp;gt; &amp;gt;Since each of the pairwise joint distributions equals the product of their respective marginal distributions, the variables are pairwise independent: &amp;gt; &amp;gt;- X and Y are independent, and &amp;gt;- X and Z are independent, and &amp;gt;- Y and Z are independent. &amp;gt; &amp;gt;However, &lt;span class="math">\(X, Y\)&lt;/span>, and &lt;span class="math">\(Z\)&lt;/span> are &lt;strong>not mutually independent&lt;/strong>, since &lt;span class="math">\({\displaystyle f_{X,Y,Z}(x,y,z)\neq f_{X}(x)f_{Y}(y)f_{Z}(z),}\)&lt;/span> the left side equalling for example 1/4 for &lt;span class="math">\((x, y, z) = (0, 0, 0)\)&lt;/span> while the right side equals 1/8 for &lt;span class="math">\((x, y, z) = (0, 0, 0)\)&lt;/span>. In fact, any of &lt;span class="math">\(\{X,Y,Z\}\)&lt;/span> is completely determined by the other two (any of &lt;span class="math">\(X, Y, Z\)&lt;/span> is the sum (modulo(模) 2) of the others). That is as far from independence as random variables can get.&lt;/p>
&lt;p>总的来说，有些随机变量可以由其他随机变量决定，因此，两两独立的随机变量之间可形成组合和其他随机变量形成联系。&lt;/p></description></item><item><title>概率统计随机过程之分析化</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%88%86%E6%9E%90%E5%8C%96%E4%B8%8E%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E5%88%86%E6%9E%90%E5%8C%96%E4%B8%8E%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</guid><description>
&lt;h2 id="概率统计随机过程之概率母函数矩母函数和特征函数">概率统计随机过程之概率母函数、矩母函数和特征函数&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#概率母函数">概率母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#性质">性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#例子">例子&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二项分布概率母函数">二项分布概率母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泊松分布概率母函数">泊松分布概率母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几何分布概率母函数">几何分布概率母函数&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#求概率">求概率&lt;/a>&lt;/li>
&lt;li>&lt;a href="#推广二维概率母函数">推广——二维概率母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#局限">局限&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩母函数">矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩母函数性质">矩母函数性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩母函数例子">矩母函数例子&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#离散型矩母函数">离散型矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数分布矩母函数">指数分布矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态分布矩母函数">正态分布矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#复合随机变量">复合随机变量&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#推广随机向量的矩母函数">推广——随机向量的矩母函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#局限性">局限性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数">特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数性质">特征函数性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征函数例子">特征函数例子&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二项分布特征函数">二项分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泊松分布特征函数">泊松分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几何分布特征函数">几何分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态分布特征函数">正态分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#均匀分布特征函数">均匀分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#柯西分布特征函数">柯西分布特征函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉普拉斯分布特征函数">拉普拉斯分布特征函数&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#更多的性质">更多的性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有趣的例题">有趣的例题&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概率母函数">概率母函数&lt;/h2>
&lt;blockquote>
&lt;p>定义：设&lt;span class="math">\(X\)&lt;/span>是非负整数的随机变量，定义其概率母函数 (probability-generating function)为 &lt;span class="math">\[g(s)=\mathbb{E}[s^X]=\sum_{j=0}^{\infty} s^j\mathbb{P}[X=j], s\in[-1,1]\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>其中约定 &lt;span class="math">\(0^0=1\)&lt;/span>。显然 &lt;span class="math">\(g(s)\)&lt;/span> 在 &lt;span class="math">\([-1,1]\)&lt;/span> 绝对收敛&lt;/p>
&lt;h3 id="性质">性质&lt;/h3>
&lt;ul>
&lt;li>&lt;span class="math">\(\mathbb{P}[X=k]=\frac{g^{(k)}(0)}{k!},\ k=0,1,\ldots\)&lt;/span>，这说明概率母函数和概率分布列一一对应&lt;/li>
&lt;li>&lt;span class="math">\(\mathbb{E}[X]=g^{(1)}(1)\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(\mathbb{E}[X]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(\mathrm{Var}[X]=g^{(2)}(1)+g^{(1)}(1)-[g^{(1)}(1)]^2\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(X_1,\ldots,X_n\)&lt;/span> 相互独立，&lt;span class="math">\(Y=X_1+\cdots+X_n\)&lt;/span>，则 &lt;span class="math">\(g_Y(s)=g_{X_1}(s)\cdots g_{X_n}(s),s\in[-1,1]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(X_1,X_2,\ldots\)&lt;/span> 是&lt;strong>独立同分布&lt;/strong>的非负整数随机变量，概率母函数为 &lt;span class="math">\(\psi(x)\)&lt;/span>; &lt;span class="math">\(N\)&lt;/span> 为取正整数值的随机变量且独立于 &lt;span class="math">\(X_i\)&lt;/span>，概率母函数为 &lt;span class="math">\(G(s)\)&lt;/span>。则 &lt;span class="math">\(Y=X_1+\cdots+X_N\)&lt;/span> 的概率母函数为 &lt;span class="math">\(H(s)=G[\psi(s)]\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>只给出性质五的证明 &lt;span class="math">\[\begin{aligned}
H(s)&amp;amp;=\mathbb{E}[\mathbb{E}[S^W\mid Y]]\\
&amp;amp;=\sum_{n=1}^{\infty}\mathbb{E}[s^{X_1+\cdots+X_n}]\mathbb{P}[Y=n]\\
&amp;amp;=\sum_{n=1}^{\infty}[\psi(s)]^n\mathbb{P}[Y=n]=G[\psi(s)]
\end{aligned}\]&lt;/span>&lt;/p>
&lt;h3 id="例子">例子&lt;/h3>
&lt;h4 id="二项分布概率母函数">二项分布概率母函数&lt;/h4>
&lt;p>二项分布 &lt;span class="math">\(B(n,p)\)&lt;/span> 的概率母函数为 &lt;span class="math">\(g(s)=(sp+q)^n\)&lt;/span>&lt;/p>
&lt;p>由此立得若 &lt;span class="math">\(X_i,\ldots,X_m\)&lt;/span> 独立，且&lt;span class="math">\(X_i\sim B(n_i,p)\)&lt;/span>，则&lt;span class="math">\(Y=X_1+\cdots+X_m\sim B(n_1+\cdots+n_m,p)\)&lt;/span>&lt;/p>
&lt;h4 id="泊松分布概率母函数">泊松分布概率母函数&lt;/h4>
&lt;p>泊松分布 &lt;span class="math">\(\mathcal{P}(\lambda)\)&lt;/span> 的概率母函数为 &lt;span class="math">\(g(s)=e^{\lambda(s-1)}\)&lt;/span>&lt;/p>
&lt;p>由此立得若 &lt;span class="math">\(X_i,\ldots,X_m\)&lt;/span> 独立，且&lt;span class="math">\(X_i\sim \mathcal{P}(\lambda_i)\)&lt;/span>，则&lt;span class="math">\(Y=X_1+\cdots+X_m\sim \mathcal{P}(\lambda_1+\cdots+\lambda_m)\)&lt;/span>&lt;/p>
&lt;h4 id="几何分布概率母函数">几何分布概率母函数&lt;/h4>
&lt;p>几何分布 &lt;span class="math">\(G(p)\)&lt;/span> 的概率母函数为 &lt;span class="math">\(g(s)=\frac{sp}{1-sq}\)&lt;/span>&lt;/p>
&lt;p>由此立得若 &lt;span class="math">\(X_i,\ldots,X_m\)&lt;/span> 独立，且&lt;span class="math">\(X_i\sim G(p)\)&lt;/span>，则&lt;span class="math">\(S_m=X_1+\cdots+X_m\)&lt;/span> 有概率母函数 &lt;span class="math">\[\begin{aligned}
g_{S_m}(s)&amp;amp;=\left(\frac{sp}{1-sq}\right)^m\\
&amp;amp;=(sp)^m\sum_{j=0}^{\infty}\frac{m(m+1)\cdots(m+j-1)}{j!}(sq)^j\\
&amp;amp;=(sp)^m\sum_{j=0}^{\infty}\binom{m+j-1}{j}(sq)^j\\
&amp;amp;=\sum_{k=m}^{\infty}\binom{k-1}{m-1}p^mq^{k-m}s^k\end{aligned}\]&lt;/span>&lt;/p>
&lt;p>于是得 Pascal 分布 &lt;span class="math">\[\mathbb{E}[S_m=k]=\binom{k-1}{m-1}p^mq^{k-m}\]&lt;/span>&lt;/p>
&lt;h3 id="求概率">求概率&lt;/h3>
&lt;p>求扔三颗骰子，总点数为 9 的概率。 记 &lt;span class="math">\(X_i\)&lt;/span> 为第 &lt;span class="math">\(i\)&lt;/span> 颗骰子的点数，其概率母函数&lt;/p>
&lt;p>&lt;span class="math">\[g(s)=\mathbb{E}[s^{X_1}]=\frac{1}{6}(s+s^2+\cdots+s^6)=\frac{1}{6}\frac{s(1-s^6)}{1-s}\]&lt;/span>&lt;/p>
&lt;p>则 &lt;span class="math">\(Y=X_1+X_2+X_3\)&lt;/span> 的概率母函数为&lt;/p>
&lt;p>&lt;span class="math">\[g_Y(s)=[g_X(s)]^3=\frac{s^3(1-s^6)^3}{6^3(1-s)^s}=\frac{1}{6^3}(s^3)(1-3s^6+3s^{12}-s^{18})\sum_{k=0}^{\infty}\binom{k+2}{2}s^k\]&lt;/span>&lt;/p>
&lt;p>则 &lt;span class="math">\(s^9\)&lt;/span> 的系数为 &lt;span class="math">\[\mathbb{P}(Y=9)=\frac{1}{6^3}[\binom{6+2}{2}-3]=\frac{25}{216}\]&lt;/span>&lt;/p>
&lt;h3 id="推广二维概率母函数">推广——二维概率母函数&lt;/h3>
&lt;blockquote>
&lt;p>设 &lt;span class="math">\((X,Y)\)&lt;/span> 是二维取非负整数值的随机向量，记 &lt;span class="math">\(p_{ik}=\mathbb{P}[X=i,Y=k]\)&lt;/span>，则其二维概率母函数为 &lt;span class="math">\[g(s,t)=\mathbb{E}[s^Xt^Y]=\sum_{i=0}^{\infty}\sum_{k=0}^{\infty}p_{ik}s^it^k,\quad s,t\in[-1,1]\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>其有如下性质&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\lvert g(s,t)\rvert\leq g(1,1)=1,\lvert s\rvert\leq 1,\lvert t\rvert\leq 1\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(g_{aX+bY+c}(s)=s^cg(s^a,s^b)\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(X,Y\)&lt;/span> 独立，则 &lt;span class="math">\(g(s,t)=g_X(s)g_Y(t)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(g(s,1)=g_X(s),g(1,t)=g_Y(t)\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(\mathbb{E}[X]&amp;lt;\infty,\mathbb{E}[Y]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\[\mathbb{E}[X]=\frac{\partial g(s,t)}{\partial s}\big|_{s=t=1},\mathbb{E}[Y]=\frac{\partial g(s,t)}{\partial t}\big|_{s=t=1}\]&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(\mathbb{E}[X^2]&amp;lt;\infty,\mathbb{E}[Y^2]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\[\mathbb{E}[X^2]=\frac{\partial^2 g(s,t)}{\partial s^2}\big|_{s=t=1},\mathbb{E}[Y^2]=\frac{\partial^2 g(s,t)}{\partial t^2}\big|_{s=t=1},\mathbb{E}[XY]=\frac{\partial^2 g(s,t)}{\partial s\partial t}\big|_{s=t=1}\]&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(p_{ik}=\frac{1}{i!k!}\frac{\partial^{i+k}g(s,t)}{\partial s^i\partial t^k}\big |_{s=t=0},\ i,k=0,1,\ldots\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h3 id="局限">局限&lt;/h3>
&lt;p>只能对取&lt;strong>非负整数值&lt;/strong>的随机变量定义&lt;/p>
&lt;h2 id="矩母函数">矩母函数&lt;/h2>
&lt;blockquote>
&lt;p>定义:设 &lt;span class="math">\(X\)&lt;/span> 是随机变量，定义其矩母函数 (moment-generating function)为 &lt;span class="math">\[M_X(s)=\mathbb{E}[e^{sX}]\]&lt;/span> 仅当 &lt;span class="math">\(\mathbb{E}[e^{sX}]&amp;lt;\infty\)&lt;/span> 时，我们称 &lt;span class="math">\(M_X(s)\)&lt;/span> 存在&lt;/p>
&lt;/blockquote>
&lt;h3 id="矩母函数性质">矩母函数性质&lt;/h3>
&lt;ul>
&lt;li>&lt;span class="math">\(M_{aX+b}(s)=e^{sb}M(sa)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\mathbb{E}[X^k]=M^{(k)}(0),k=1,2,\ldots\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(M(0)=1\)&lt;/span>&lt;/li>
&lt;li>可逆性：若&lt;span class="math">\(\exists a&amp;gt;0,\forall s\in[-a,a], M(s)&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(M(s)\)&lt;/span> 唯一地决定了 &lt;span class="math">\(X\)&lt;/span> 的分布函数&lt;/li>
&lt;li>若 &lt;span class="math">\(X_1,\ldots,X_n\)&lt;/span> 独立，&lt;span class="math">\(Y=X_1+\cdots+X_n\)&lt;/span>，则&lt;span class="math">\(M_{Y}(s)=M_{X_1}(s)\cdots M_{X_n}(s)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(X_1,X_2,\ldots\)&lt;/span> 独立同分布，矩母函数为 &lt;span class="math">\(M_X(s)\)&lt;/span>; &lt;span class="math">\(N\)&lt;/span> 为取正整数值的随机变量，矩母函数为 &lt;span class="math">\(M_N(s)\)&lt;/span>。则 &lt;span class="math">\(Y=X_1+\cdots+X_Y\)&lt;/span> 的矩母函数为 &lt;span class="math">\(M_Y(s)=\mathbb{E}[\mathbb{E}[e^{sY}\mid N=n]]=\mathbb{E}[(M_X(s))^n]=\sum_{n=1}^{\infty}(M_X(s))^n\mathbb{P}[N=n]\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>而 &lt;span class="math">\(M_N(s)=\sum_{n=1}^{\infty}[e^s]^n\mathbb{P}[N=n]\)&lt;/span> 二者有紧密的联系：将 &lt;span class="math">\(M_N(s)\)&lt;/span> 中出现的 &lt;span class="math">\(e^s\)&lt;/span> 替换为 &lt;span class="math">\(M_X(s)\)&lt;/span> 即可&lt;/p>
&lt;h3 id="矩母函数例子">矩母函数例子&lt;/h3>
&lt;h4 id="离散型矩母函数">离散型矩母函数&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">X&lt;/th>
&lt;th align="left">2&lt;/th>
&lt;th align="left">3&lt;/th>
&lt;th align="left">5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">&lt;span class="math">\(\mathbb{P}\)&lt;/span>&lt;/td>
&lt;td align="left">1/2&lt;/td>
&lt;td align="left">1/6&lt;/td>
&lt;td align="left">1/3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;span class="math">\[M(s)=\frac{1}{2}e^{2s}+\frac{1}{6}e^{3s}+\frac{1}{3}e^{5s}\]&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\[\mathbb{E}[X]=M^{(1)}(0)=(\frac{1}{2}2e^{2s}+\frac{1}{6}3e^{3s}+\frac{1}{3}5e^{5s})|_{s=0}=\frac{19}{6}\]&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(\mathbb{E}[X^2]=M^{(2)}(0)=(\frac{1}{2}4e^{2s}+\frac{1}{6}9e^{3s}+\frac{1}{3}25e^{5s})|_{s=0}=\frac{71}{6}\)&lt;/span>&lt;/p>
&lt;h4 id="指数分布矩母函数">指数分布矩母函数&lt;/h4>
&lt;p>设 &lt;span class="math">\(X\sim \mathcal{E}(\lambda)\)&lt;/span>，则当 &lt;span class="math">\(s&amp;lt;\lambda\)&lt;/span> 时，有 &lt;span class="math">\[M(s)=\lambda\int_0^\infty e^{sx}e^{-\lambda x}\,\mathrm{d}x=\frac{\lambda}{s-\lambda}e^{(s-\lambda)x}|{x=0}^{\infty}=\frac{\lambda}{\lambda-s}\]&lt;/span>&lt;/p>
&lt;p>而当 &lt;span class="math">\(s\ge\lambda\)&lt;/span> 时 &lt;span class="math">\(M(s)\)&lt;/span> 不存在&lt;/p>
&lt;p>&lt;span class="math">\[\mathbb{E}[X]=M^{(1)}(0)=\frac{\lambda}{(\lambda-s)^2}|_{s=0}=\frac{1}{\lambda}\]&lt;/span> &lt;span class="math">\[\mathbb{E}[X]=M^{(2)}(0)=\frac{2\lambda}{(\lambda-s)^3}|_{s=0}=\frac{2}{\lambda^2}\]&lt;/span>&lt;/p>
&lt;h4 id="正态分布矩母函数">正态分布矩母函数&lt;/h4>
&lt;p>&lt;span class="math">\(X\sim\mathcal{N}({\mu_1,\sigma_1^2}),Y\sim\mathcal{N}(\mu_2,\sigma_2^2), X,Y\)&lt;/span> 相互独立，求 &lt;span class="math">\(Z=X+Y\)&lt;/span> 的分布&lt;/p>
&lt;p>先计算标准正态分布的矩母函数，由定义求得 &lt;span class="math">\(M(s)=e^{s^2/2}\)&lt;/span>&lt;/p>
&lt;p>根据矩母函数的性质，有 &lt;span class="math">\[M_X(s)=e^{\mu_1 s}e^{\sigma_1^2s^2/2},\\
M_Y(s)=e^{\mu_2 s}e^{\sigma_2^2s^2/2},\\
M_Z(s)=e^{(\mu_1+\mu_2) s}e^{(\sigma_1^2+\sigma_2^2)s^2/2}\]&lt;/span>&lt;/p>
&lt;p>于是 &lt;span class="math">\(Z\sim\mathcal{N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)\)&lt;/span>&lt;/p>
&lt;h4 id="复合随机变量">复合随机变量&lt;/h4>
&lt;p>不断进行成功概率为 &lt;span class="math">\(p\)&lt;/span> 的伯努利实验直至成功，每次实验的耗时服从参数 &lt;span class="math">\(\lambda\)&lt;/span> 的指数分布，且完全独立。 求总耗时的分布&lt;/p>
&lt;p>首先 &lt;span class="math">\(X_i\sim\mathcal{E}(\lambda), N\sim G(p), Y=X_1+\cdots+X_N\)&lt;/span> , 当 &lt;span class="math">\(s&amp;lt;\lambda\)&lt;/span> 时有 &lt;span class="math">\[M_{X_i}(s)=\frac{\lambda}{\lambda-s}\]&lt;/span> 而 &lt;span class="math">\[M_N(s)=\frac{pe^s}{1-qe^s}\]&lt;/span> 故 &lt;span class="math">\[M_Y(s)=\frac{p M_X(s)}{1-qM_X(s)}=\frac{p\lambda}{\lambda-s-q\lambda}=\frac{p\lambda}{p\lambda-s}\]&lt;/span>&lt;/p>
&lt;h3 id="推广随机向量的矩母函数">推广——随机向量的矩母函数&lt;/h3>
&lt;p>设&lt;span class="math">\(\vec{X}=(X_1,X_2,\ldots,X_n)^\intercal\)&lt;/span>，则其矩母函数定义为&lt;/p>
&lt;p>&lt;span class="math">\[M_{\vec{X}}(\vec{s})=\mathbb{E}[e^{\vec{s}^\intercal\vec{X}}]=\mathbb{E}[e^{s_1X_1+\cdots+s_nX_n}] \]&lt;/span>&lt;/p>
&lt;h3 id="局限性">局限性&lt;/h3>
&lt;p>有些分布的&lt;strong>矩母函数不存在&lt;/strong>，因为&lt;strong>其积分发散&lt;/strong>，如 Cauchy 分布。为此我们引入特征函数来保证可积性。&lt;/p>
&lt;h2 id="特征函数">特征函数&lt;/h2>
&lt;blockquote>
&lt;p>定义：对随机变量 &lt;span class="math">\(X\)&lt;/span> ，定义其特征函数 (characteristic function) 为 &lt;span class="math">\[\phi(t)=\mathbb{E}[e^{itX}]=\mathbb{E}[\cos(tX)]+i\mathbb{E}[\sin(tX)], t\in \mathbb{R}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h3 id="特征函数性质">特征函数性质&lt;/h3>
&lt;ul>
&lt;li>&lt;span class="math">\(\lvert \phi(t)\rvert\leq \phi(0)=1,\quad\phi(-t)=\overline{\phi(t)}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\phi(t)\)&lt;/span> 在 &lt;span class="math">\((-\infty,\infty)\)&lt;/span> 一致连续 v若 &lt;span class="math">\(\mathbb{E}[\lvert X\rvert^k]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(\phi^{(k)}(t)=i^k\mathbb{E}[X^k e^{itX}],\phi^{(k)}(0)=i^k\mathbb{E}[X^k]\)&lt;/span>&lt;/li>
&lt;li>非负定性：&lt;span class="math">\(\forall t_1,\ldots,t_n\in\mathbb{R},\forall z_1,\ldots,z_n\in\mathbb{C},\sum_{k=1}^{n}\sum_{j=1}^{n}\phi(t_k-t_j)z_k\bar{z}_j\ge 0\)&lt;/span>&lt;/li>
&lt;li>若 &lt;span class="math">\(X_1,\ldots,X_n\)&lt;/span> 相互独立， &lt;span class="math">\(X_k\)&lt;/span> 特征函数为 &lt;span class="math">\(\phi_k(t)\)&lt;/span>，则 &lt;span class="math">\(Y=X_1+\cdots+X_n\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi_Y(t)=\phi_1(t)\cdots\phi_{k}(t)\)&lt;/span>。注意，逆命题不成立，后面给出了例子。&lt;/li>
&lt;/ul>
&lt;p>特征函数与概率分布函数是有一一对应关系的，二者可以互相确定，在概率论中叫做&lt;strong>反演定理&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>在累积概率分布函数与特征函数之间存在&lt;strong>双射&lt;/strong>。也就是说，&lt;strong>两个不同的概率分布不能有相同的特征函数&lt;/strong>。&lt;/p>
&lt;p>给定一个特征函数&lt;span class="math">\(\phi\)&lt;/span>，可以用以下公式求得对应的累积概率分布函数 &lt;span class="math">\[F_{X}(y)-F_{X}(x)=\lim_{{\tau \to +\infty }}{\frac{1}{2\pi }}\int_{{-\tau }}^{{+\tau }}{\frac{e^{{-itx}}-e^{{-ity}}}{it}}\,\varphi_{X}(t)\,dt\]&lt;/span> 一般地，这是一个广义积分；被积分的函数可能只是条件可积而不是勒贝格可积的，也就是说，它的绝对值的积分可能是无穷大。&lt;/p>
&lt;/blockquote>
&lt;h3 id="特征函数例子">特征函数例子&lt;/h3>
&lt;h4 id="二项分布特征函数">二项分布特征函数&lt;/h4>
&lt;p>二项分布 &lt;span class="math">\(B(n,p)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=(e^{it}p+q)^n\)&lt;/span>&lt;/p>
&lt;p>(对比其概率母函数 &lt;span class="math">\(g(s)=(sp+q)^n\)&lt;/span>)&lt;/p>
&lt;h4 id="泊松分布特征函数">泊松分布特征函数&lt;/h4>
&lt;p>泊松分布 &lt;span class="math">\(\mathcal{P}(\lambda)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=e^{\lambda(e^{it}-1)}\)&lt;/span>&lt;/p>
&lt;p>(对比其概率母函数 &lt;span class="math">\(g(s)=e^{\lambda(s-1)}\)&lt;/span>)&lt;/p>
&lt;h4 id="几何分布特征函数">几何分布特征函数&lt;/h4>
&lt;p>几何分布 &lt;span class="math">\(G(p)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=\frac{pe^{it}}{1-qe^{it}}\)&lt;/span>&lt;/p>
&lt;p>(对比其概率母函数 &lt;span class="math">\(g(s)=\frac{sp}{1-sq}\)&lt;/span>)&lt;/p>
&lt;h4 id="正态分布特征函数">正态分布特征函数&lt;/h4>
&lt;p>正态分布 &lt;span class="math">\(\mathcal{N}(\mu,\sigma^2)\)&lt;/span> 有特征函数 &lt;span class="math">\(\phi(t)=e^{i\mu t}e^{-\frac{1}{2}\sigma^2t^2}\)&lt;/span>&lt;/p>
&lt;p>(对比其矩母函数 &lt;span class="math">\(M(s)=e^{\mu s}e^{\sigma^2s^2/2}\)&lt;/span>)&lt;/p>
&lt;p>先考察标准正态分布。正态分布的特征函数推导不太容易，一种不太严谨的做法是做形式化运算，将 &lt;span class="math">\(i\)&lt;/span> 视为常数，则&lt;/p>
&lt;p>&lt;span class="math">\(\phi(t)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{itx}e^{-x^2/2}\,\mathrm{d}x=e^{-t^2/2}\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-(x-it)^2/2}\,\mathrm{d}x=e^{-t^2/2}\)&lt;/span>&lt;/p>
&lt;p>严格的数学推导需要一定复变函数的背景知识。&lt;/p>
&lt;p>首先 &lt;span class="math">\[\phi(t)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{itx}e^{-x^2/2}\,\mathrm{d}x=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \cos(tx)e^{-x^2/2}\,\mathrm{d}x\]&lt;/span>&lt;/p>
&lt;p>对 &lt;span class="math">\(t\)&lt;/span> 求导得 &lt;span class="math">\[\begin{aligned}\phi&amp;#39;(t)&amp;amp;=-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty x\sin(tx)e^{-x^2/2}\,\mathrm{d}x\\&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}\sin(tx)\,\mathrm{d}e^{-x^2/2}\\&amp;amp;=-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty t\cos(tx)e^{-x^2/2}\,\mathrm{d}x\\&amp;amp;=-t\phi(t)\end{aligned}\]&lt;/span>&lt;/p>
&lt;p>即 &lt;span class="math">\(\frac{\mathrm{d}}{\mathrm{d}t}[\phi(t)e^{t^2/2}]=0\)&lt;/span>，则 &lt;span class="math">\(\phi(t)e^{t^2/2}=C=\phi(0)=1\)&lt;/span>，得 &lt;span class="math">\(\phi(t)=e^{-t^2/2}\)&lt;/span>&lt;/p>
&lt;p>由此再求一般正态的特征函数&lt;/p>
&lt;p>&lt;span class="math">\(\mathbb{E}[e^{it(\mu+\sigma X)}]=e^{it\mu}\mathbb{E}[e^{it\sigma X}]=e^{it\mu}e^{-\sigma^2t^2/2}\)&lt;/span>&lt;/p>
&lt;p>同时，若 &lt;span class="math">\(X_1,\ldots,X_m\)&lt;/span> 相互独立，&lt;span class="math">\(X_j\sim\mathcal{\mu_j,\sigma_j^2}\)&lt;/span>，则&lt;/p>
&lt;p>&lt;span class="math">\(Y=X_1+\cdots+X_m\sim\mathcal{N}(\sum_{j=1}^{m}\mu_j,\sum_{j=1}^{m}\sigma_j^2)\)&lt;/span>&lt;/p>
&lt;h4 id="均匀分布特征函数">均匀分布特征函数&lt;/h4>
&lt;p>均匀分布 &lt;span class="math">\(\mathcal{U}(a,b)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=\frac{e^{itb}-e^{ita}}{it(b-a)}\)&lt;/span>&lt;/p>
&lt;p>指数分布 指数分布 &lt;span class="math">\(\mathcal{E}(\lambda)\)&lt;/span> 的特征函数为 &lt;span class="math">\(\phi(t)=(1-\frac{it}{\lambda})^{-1}\)&lt;/span>&lt;/p>
&lt;p>(与矩母函数 &lt;span class="math">\(M(s)=\frac{\lambda}{\lambda-s}=\frac{1}{1-s/\lambda}=(1-s/\lambda)^{-1}\)&lt;/span> 对比)&lt;/p>
&lt;h4 id="柯西分布特征函数">柯西分布特征函数&lt;/h4>
&lt;p>&lt;span class="math">\(f(x)=\frac{1}{\pi(1+x^2)}\)&lt;/span>，其特征函数为 &lt;span class="math">\(\phi(t)=e^{-\lvert t\rvert}\)&lt;/span>&lt;/p>
&lt;p>取 &lt;span class="math">\(Y=aX,(a&amp;gt;0)\)&lt;/span>，则 &lt;span class="math">\(\phi_Y(t)=\mathbb{E}[e^{i(at)X}]=e^{-a\lvert t\rvert}\)&lt;/span>，此时&lt;/p>
&lt;p>&lt;span class="math">\(\phi_{X+Y}(t)=\mathbb{E}[e^{it(1+a)X}]=e^{-(1+a)\lvert t\rvert}=\phi_X(t)\phi_Y(t)\)&lt;/span>，但显然 &lt;span class="math">\(X,Y\)&lt;/span> 不独立&lt;/p>
&lt;h4 id="拉普拉斯分布特征函数">拉普拉斯分布特征函数&lt;/h4>
&lt;p>&lt;span class="math">\(f(x)=\frac{1}{2}e^{-\lvert x\rvert}\)&lt;/span> ，其特征函数为 &lt;span class="math">\(\phi(t)=\frac{1}{1+t^2}\)&lt;/span>&lt;/p>
&lt;p>注意它和柯西分布的“对称性”&lt;/p>
&lt;h3 id="更多的性质">更多的性质&lt;/h3>
&lt;ul>
&lt;li>若 &lt;span class="math">\(\mathbb{E}[\lvert X\rvert^n]&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(\phi(t)=\sum_{m=0}^{n}\frac{\mathbb{E}[(itX)^m]}{m!}+o(t^n)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>特别的，若二阶矩存在，则 &lt;span class="math">\(\phi(t)=1+it\mathbb{E}[X]-\frac{1}{2}t^2\mathbb{E}[X^2]+o(t^2)\)&lt;/span>&lt;/p>
&lt;ul>
&lt;li>&lt;p>逆转公式：若累积分布函数 &lt;span class="math">\(F(x)\)&lt;/span> 在 &lt;span class="math">\((a,b)\)&lt;/span> 连续，则 &lt;span class="math">\[\frac{1}{2\pi}\lim\limits_{T\rightarrow\infty}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\phi(t)\,\mathrm{d}t=F(b)-F(a)\]&lt;/span> 若 &lt;span class="math">\(\int_{-\infty}^{\infty}\lvert\phi(t)\rvert\,\mathrm{d}t&amp;lt;\infty\)&lt;/span>，则 &lt;span class="math">\(X\)&lt;/span> 有有界连续密度函数 &lt;span class="math">\(f(x)=\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{-itx}\phi(t)\,\mathrm{d}t\)&lt;/span>&lt;/p>
这些说明了随机变量和分布函数相互唯一决定&lt;/li>
&lt;li>&lt;p>先介绍收敛性：设 &lt;span class="math">\(X\)&lt;/span> 有分布函数 &lt;span class="math">\(F(x)\)&lt;/span>， &lt;span class="math">\(X_n\)&lt;/span> 有分布函数 &lt;span class="math">\(F_n(x)\)&lt;/span>。若在 &lt;span class="math">\(F(x)\)&lt;/span> 的连续点 &lt;span class="math">\(x\)&lt;/span> 处，有 &lt;span class="math">\(\lim\limits_{n\rightarrow\infty}F_n(x)=F(x)\)&lt;/span>，则称 &lt;span class="math">\(X_n\)&lt;/span> 依分布收敛 (convergence in distribution) 到 &lt;span class="math">\(X\)&lt;/span>，记为 &lt;span class="math">\(X_n\overset{d}{\rightarrow} X\)&lt;/span>;或称 &lt;span class="math">\(F_n\)&lt;/span> 弱收敛 (weak convergence) 到 &lt;span class="math">\(F\)&lt;/span>，记为 &lt;span class="math">\(F_n\overset{w}{\rightarrow}F\)&lt;/span>&lt;/p>
连续性定理指出，&lt;span class="math">\(X_n\)&lt;/span> 依分布收敛到 &lt;span class="math">\(X\)&lt;/span> 的充分必要条件是，其对应的特征函数满足 &lt;span class="math">\[\lim\limits_{n\rightarrow\infty}\phi_n(t)=\phi(t),\quad\forall t\in\mathbb{R}\]&lt;/span>&lt;/li>
&lt;li>判定一个函数是否为特征函数有如下定理&lt;/li>
&lt;li>&lt;p>S. Bochner-Khintchine 定理&lt;/p>
&lt;p>设 &lt;span class="math">\(\phi(t),t\in\mathbb{R}\)&lt;/span> 是连续函数且 &lt;span class="math">\(\phi(0)=1\)&lt;/span>，其是特征函数的充要条件是他是非负定的，即 &lt;span class="math">\(\forall t_1,\ldots,t_n\in\mathbb{R},\forall z_1,\ldots,z_n\in\mathbb{C},\sum_{k=1}^{n}\sum_{j=1}^{n}\phi(t_k-t_j)z_k\bar{z}_j\ge 0\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>Polya 定理&lt;/p>
&lt;p>设 &lt;span class="math">\(\phi(t)\)&lt;/span> 是连续函数、偶函数，在 &lt;span class="math">\((0,\infty)\)&lt;/span> 是凸函数，且满足&lt;span class="math">\(\phi(t)\ge 0\)&lt;/span>,&lt;span class="math">\(\phi(0)=1\)&lt;/span>,&lt;span class="math">\(\phi(t)\rightarrow 0\,(t\rightarrow\infty)\)&lt;/span> 则其是特征函数。&lt;/p>&lt;/li>
&lt;li>&lt;p>J. Marcinkiewiez 定理&lt;/p>
&lt;p>若 &lt;span class="math">\(\phi(t)\)&lt;/span> 具有形式 &lt;span class="math">\(e^{P(t)}\)&lt;/span>，其中 &lt;span class="math">\(P(t)\)&lt;/span> 是多项式，则其系数不能大于 2&lt;/p>&lt;/li>
&lt;/ul>
&lt;h3 id="有趣的例题">有趣的例题&lt;/h3>
&lt;p>（1）&lt;span class="math">\(\phi(t)\)&lt;/span> 是特征函数，证明 &lt;span class="math">\(\bar{\phi},\phi^2,\lvert\phi\rvert^2,Re[\phi]\)&lt;/span> 都是特征函数&lt;/p>
&lt;p>设 &lt;span class="math">\(X,Y\)&lt;/span>独立同分布且特征函数为 &lt;span class="math">\(\phi\)&lt;/span>，则前三个分别为 &lt;span class="math">\(-X,X+Y,X-Y\)&lt;/span>&lt;/p>
&lt;p>第四个构造独立于 &lt;span class="math">\(X\)&lt;/span> 的随机变量 &lt;span class="math">\(Z\)&lt;/span> 且 &lt;span class="math">\(\mathbb{P}[Z=\pm 1]=0.5\)&lt;/span>，则&lt;span class="math">\(XZ\)&lt;/span> 的特征函数为 &lt;span class="math">\(Re[\phi]\)&lt;/span>&lt;/p>
&lt;p>（2）&lt;span class="math">\(\phi(t)\)&lt;/span> 是特征函数，则 &lt;span class="math">\(\lvert\phi(t)\rvert\)&lt;/span> 未必是特征函数&lt;/p>
&lt;p>设 &lt;span class="math">\(X\sim B(1,1/3)\)&lt;/span>，&lt;span class="math">\(\phi(t)=\frac{2}{3}+\frac{1}{3}e^{it}\)&lt;/span> 。设 &lt;span class="math">\(Y\)&lt;/span> 的特征函数为 &lt;span class="math">\(\psi(t)=\lvert\phi(t)\rvert\)&lt;/span>,则 &lt;span class="math">\(\psi^2(t)=\phi(t)\phi(-t)\)&lt;/span>，即 &lt;span class="math">\(Y_1+Y_2\)&lt;/span> 和 &lt;span class="math">\(X_1-X_2\)&lt;/span> 同分布。其中 &lt;span class="math">\(Y_1,Y_2\)&lt;/span> 与 &lt;span class="math">\(Y\)&lt;/span> 独立同分布，&lt;span class="math">\(X_1,X_2\)&lt;/span> 与 &lt;span class="math">\(X\)&lt;/span> 独立同分布。由于 &lt;span class="math">\(X_1-X_2\in\{-1,0,1\}\)&lt;/span>，则 &lt;span class="math">\(Y_i\in\{-0.5,0.5\}\)&lt;/span>，记 &lt;span class="math">\(\mathbb{P}[Y_1=0.5]=\alpha\)&lt;/span>。则&lt;/p>
&lt;p>&lt;span class="math">\(\mathbb{P}[Y_1+Y_2=1]=\alpha^2=\mathbb{P}[X_1-X_2=1]=2/9\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(\mathbb{P}[Y_1+Y_2=-11]=(1-\alpha)^2=\mathbb{P}[X_2-X_1=1]=2/9\)&lt;/span>&lt;/p>
&lt;p>此时 &lt;span class="math">\(\alpha\)&lt;/span> 无解&lt;/p>
&lt;p>（3）&lt;span class="math">\(X_1,\ldots,X_4\)&lt;/span> 独立同标准正态分布，则&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X_1X_2\)&lt;/span>的特征函数为 &lt;span class="math">\(\frac{1}{\sqrt{1+t^2}}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(X_1X_2+X_3X_4\)&lt;/span> 的特征函数为 &lt;span class="math">\(\frac{1}{1+t^2}\)&lt;/span>，即服从拉普拉斯分布&lt;/li>
&lt;/ol>
&lt;p>（4）&lt;span class="math">\(X,Y\)&lt;/span>独立同标准正态分布，&lt;span class="math">\(U,V\)&lt;/span>独立于 &lt;span class="math">\(X,Y\)&lt;/span>，则&lt;span class="math">\(Z=\frac{UX+VY}{\sqrt{U^2+V^2}}\sim\mathcal{N}(0,1)\)&lt;/span>&lt;/p>
&lt;p>（5）利用已知结果若 &lt;span class="math">\(a&amp;gt;0,b&amp;gt;0\)&lt;/span>，则&lt;span class="math">\(I(a,b)=\int_0^\infty\exp\{-a^2u^2-b^2u^{-2}\}\,\mathrm{d}u=\frac{e^{-2ab}\sqrt{\pi}}{2a}\)&lt;/span>&lt;/p>
&lt;p>证明若 &lt;span class="math">\(f(x)=\frac{1}{2\pi x^3}\exp(-\frac{1}{2x}),x&amp;gt;0\)&lt;/span>， 则 &lt;span class="math">\(\mathbb{E}[e^{-tX}]=\exp(-\sqrt{2t})\)&lt;/span>&lt;/p>
&lt;p>（6）&lt;span class="math">\(X,Y,Z\)&lt;/span>独立同标准正态分布，则&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X/Y\)&lt;/span>服从柯西分布&lt;/li>
&lt;li>&lt;span class="math">\(1/X^2\)&lt;/span>的概率密度函数为 5. 中结果&lt;/li>
&lt;li>&lt;span class="math">\((XYZ)/\sqrt{X^2Y^2+Y^2Z^2+Z^2X^2}\sim\mathcal{N}(0,1/9)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>（7）&lt;span class="math">\(X_n\)&lt;/span> 有分布函数 &lt;span class="math">\(F_n(x)=x-\frac{\sin(2n\pi x)}{2n\pi},0\leq x\leq 1\)&lt;/span>&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(X_n\)&lt;/span> 有密度函数 &lt;span class="math">\(f_n(x)=1-\cos(2n\pi x),0\leq x\leq 1\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(F_n(x)\)&lt;/span> 弱收敛于 &lt;span class="math">\(\mathcal{U}[0,1]\)&lt;/span>，但 &lt;span class="math">\(f_n(x)\)&lt;/span> 不收敛&lt;/li>
&lt;/ol></description></item><item><title>概率统计随机过程之经验函数分布</title><link>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%BB%8F%E9%AA%8C%E5%87%BD%E6%95%B0%E5%88%86%E5%B8%83/</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E4%B9%8B%E7%BB%8F%E9%AA%8C%E5%87%BD%E6%95%B0%E5%88%86%E5%B8%83/</guid><description>
&lt;h2 id="概率统计随机过程之经验函数分布">概率统计随机过程之经验函数分布&lt;!-- omit in toc -->&lt;/h2>
&lt;p>设&lt;span class="math">\(x_1, x_2, \cdots, x_n\)&lt;/span>是取自总体&lt;span class="math">\(X\)&lt;/span>的样本, 其分布函数为&lt;span class="math">\(F(x)\)&lt;/span>,&lt;span class="math">\(F(x)\)&lt;/span>是未知的. 为了估计分布函数&lt;span class="math">\(F(x)=P(X\le x)\)&lt;/span>, 使用如下统计量 &lt;span class="math">\[
F_n(x)=\frac{\#\{i: x_i\leq x\}}{n},
\]&lt;/span> 其中&lt;span class="math">\(\#A\)&lt;/span>表示集合&lt;span class="math">\(A\)&lt;/span>中元素的个数, &lt;span class="math">\(F_n(x)\)&lt;/span>称为&lt;strong>经验分布函数 (empirical distribution function)&lt;/strong>. 上式中经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>的定义体现了用&lt;strong>频率近似概率&lt;/strong>的想法.&lt;/p>
&lt;p>如果用&lt;span class="math">\(I_A(x)\)&lt;/span>表示集合&lt;span class="math">\(A\)&lt;/span>的特征函数（示性函数）, 即 &lt;span class="math">\[
I_A(x):=\begin{cases}
1,x \in A,\\
0,x \notin A,
\end{cases}
\]&lt;/span> 则经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>可以改写成 &lt;span class="math">\[
F_n(x)=\frac{1}{n}\sum_{i=0}^nI_{[x_i,\infty]}(x).\\
I_{[x_i,\infty]}(x)=\begin{cases}
1,x_i \le x,\\
0,x_i &amp;gt; x,
\end{cases}
\]&lt;/span> 将样本&lt;span class="math">\(x_1,x_2,⋯,x_n\)&lt;/span>理解成样本值时, &lt;span class="math">\(F_n(x)\)&lt;/span>是一个分布函数. 设随机变量&lt;span class="math">\(W∼F_n(x)\)&lt;/span>, 则&lt;span class="math">\(W\)&lt;/span>服从离散分布, 在&lt;span class="math">\(\{x_1,x_2,⋯,x_n\}\)&lt;/span>内取值, 如果各&lt;span class="math">\(x_i\)&lt;/span>互不相同则&lt;span class="math">\(W\)&lt;/span>服从&lt;span class="math">\(\{x_1,x_2,⋯,x_n\}\)&lt;/span>上的离散均匀分布&lt;span class="math">\(P(W=x_i)=1/n, i=1,2,⋯,n.\)&lt;/span>如果&lt;span class="math">\(\{x_1,x_2,⋯,x_n\}\)&lt;/span>中有相同的观测值则其相应的取值概率是&lt;span class="math">\(1/n\)&lt;/span>乘以重复次数.&lt;/p>
&lt;p>对样本&lt;span class="math">\(x_1,x_2,⋯,x_n\)&lt;/span>从小到大排序得到&lt;span class="math">\(x_{(1)}≤x_{(2)}≤⋯≤x_{(n)}\)&lt;/span>, 称为样本的次序统计量. 如果&lt;span class="math">\(x_{(1)}≤x_{(2)}≤⋯≤x_{(n)}\)&lt;/span>, 易见 &lt;span class="math">\[
F_n(x)=\begin{cases}
0, &amp;amp; \textrm{当}\, x&amp;lt; x_{(1)},\\
\dfrac{i}{n}, &amp;amp; \textrm{当}\,x_{(i)}\leq x&amp;lt; x_{(i+1)},\quad i=1,2,\cdots, n-1,\\
1, &amp;amp; \textrm{当}\, x\geq x_{(n)}.
\end{cases}
\]&lt;/span> 将样本&lt;span class="math">\(x_1,x_2,⋯,x_n\)&lt;/span>看成随机变量时, &lt;span class="math">\(F_n(x)\)&lt;/span>是样本统计量.&lt;/p>
&lt;p>&lt;span class="math">\(I_{[x_i,∞)}(x)\)&lt;/span>是独立同分布的随机变量, 其共同分布为两点分布&lt;span class="math">\(b(1,F(x))\)&lt;/span>. 由Glivenko-Cantelli定理可知, 当&lt;span class="math">\(n→∞\)&lt;/span>时, &lt;span class="math">\[
\sup_{x\in\mathbb R}|F_n(x)-F(x)| \xrightarrow[]{\;\;{\rm a.s.}\;\;} 0.
\]&lt;/span> 此结果表明&lt;span class="math">\(F_n(x)\)&lt;/span>是&lt;span class="math">\(F(x)\)&lt;/span>的一致强相合估计(uniformly and strongly consistent estimator). 于是当样本容量&lt;span class="math">\(n\)&lt;/span>充分大时, &lt;span class="math">\(F_n(x)\)&lt;/span>能良好地逼近总体分布函数&lt;span class="math">\(F(x)\)&lt;/span>. 这是在统计学中以样本推断总体的依据.&lt;/p>
&lt;h2 id="经验分布函数与样本均值的关系">经验分布函数与样本均值的关系&lt;/h2>
&lt;p>如果随机变量&lt;span class="math">\(W∼F_n(x)\)&lt;/span>, 显然&lt;span class="math">\(W\)&lt;/span>的期望 &lt;span class="math">\[
E(W)=\frac{1}{n}\sum_{i=1}^nx_i=\bar x,
\]&lt;/span> 即样本均值. 所以样本均值可以理解成服从经验分布的随机变量的数学期望. &lt;strong>样本均值&lt;span class="math">\(\bar x\)&lt;/span>用于估计总体均值&lt;span class="math">\(E(X)\)&lt;/span>, 其本质上是用经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>近似总体分布函数&lt;span class="math">\(F(x)\)&lt;/span>&lt;/strong>. 用经验分布函数&lt;span class="math">\(F_n(x)\)&lt;/span>近似总体分布函数&lt;span class="math">\(F(x)\)&lt;/span>的一个应用是bootstrap方法.&lt;/p>
&lt;h2 id="经验分布函数与直方图的关系">经验分布函数与直方图的关系&lt;/h2>
&lt;p>直方图 (histogram) 是估计分布密度非常直观简单的方法.&lt;/p>
&lt;h3 id="直方图作法">直方图作法&lt;/h3>
&lt;p>&lt;a href="https://www.zybuluo.com/lyc102/note/1311776">参考文献&lt;/a>&lt;/p></description></item><item><title>深度学习-吴恩达深度学习重点</title><link>https://surprisedcat.github.io/studynotes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%87%8D%E7%82%B9/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%87%8D%E7%82%B9/</guid><description>
&lt;h2 id="吴恩达深度学习重点">吴恩达深度学习重点&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#改善深层神经网络">改善深层神经网络&lt;/a>&lt;/li>
&lt;li>&lt;a href="#训练-开发-测试集划分">训练-开发-测试集划分&lt;/a>&lt;/li>
&lt;li>&lt;a href="#偏差与方差-bias-vs-variance">偏差与方差-Bias vs Variance&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正则化">正则化&lt;/a>&lt;/li>
&lt;li>&lt;a href="#归一化输入">归一化输入&lt;/a>&lt;/li>
&lt;li>&lt;a href="#梯度消失和梯度爆炸">梯度消失和梯度爆炸&lt;/a>&lt;/li>
&lt;li>&lt;a href="#梯度检查">梯度检查&lt;/a>&lt;/li>
&lt;li>&lt;a href="#优化算法">优化算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mini-batch">mini-batch&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数加权平均">指数加权平均&lt;/a>&lt;/li>
&lt;li>&lt;a href="#momentum-动量法梯度下降">Momentum 动量法梯度下降&lt;/a>&lt;/li>
&lt;li>&lt;a href="#rmsprop均方根法">RMSprop均方根法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#adam">Adam&lt;/a>&lt;/li>
&lt;li>&lt;a href="#学习率">学习率&lt;/a>&lt;/li>
&lt;li>&lt;a href="#局部最优点和鞍点">局部最优点和鞍点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#超参数调试batch正则化和softmax回归">超参数调试、Batch正则化和Softmax回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#超参数调参经验">超参数调参经验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#batch-norm">Batch Norm&lt;/a>&lt;/li>
&lt;li>&lt;a href="#softmax">Softmax&lt;/a>&lt;/li>
&lt;li>&lt;a href="#机器学习策略">机器学习策略&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单一数字评估指标">单一数字评估指标&lt;/a>&lt;/li>
&lt;li>&lt;a href="#优化指标和满足指标">优化指标和满足指标&lt;/a>&lt;/li>
&lt;li>&lt;a href="#人的表现">人的表现&lt;/a>&lt;/li>
&lt;li>&lt;a href="#人工误差分析适用于机器学习还没到人的水平">人工误差分析（适用于机器学习还没到人的水平）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#迁移学习串行">迁移学习：串行&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多任务学习并行">多任务学习：并行&lt;/a>&lt;/li>
&lt;li>&lt;a href="#端到端学习">端到端学习&lt;/a>&lt;/li>
&lt;li>&lt;a href="#卷积神经网络cnn">卷积神经网络CNN&lt;/a>&lt;/li>
&lt;li>&lt;a href="#经典神经网络">经典神经网络&lt;/a>&lt;/li>
&lt;li>&lt;a href="#循环神经网络rnn">循环神经网络RNN&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基本结构">基本结构&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gru与lstm">GRU与LSTM&lt;/a>&lt;/li>
&lt;li>&lt;a href="#生成对抗网络gan">生成对抗网络GAN&lt;/a>&lt;/li>
&lt;li>&lt;a href="#强化学习">强化学习&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="改善深层神经网络">改善深层神经网络&lt;/h2>
&lt;h3 id="训练-开发-测试集划分">训练-开发-测试集划分&lt;/h3>
&lt;p>传统：100-1000-10000 70%/30% 训练/测试 60%/20%/20% 训练/开发（交叉验证）/测试 大数据：1000000 98%/1%/1% 训练/开发/测试 或者 99.5%/0.25%/0.25% 训练/开发/测试&lt;/p>
&lt;p>如果训练集和开发/测试集的分布不同，也不太致命，最好保证开发集和测试集的分布是一致的，有时候只有开发集没有测试集也是可以的。&lt;/p>
&lt;h3 id="偏差与方差-bias-vs-variance">偏差与方差-Bias vs Variance&lt;/h3>
&lt;p>Optimal/Base Error理论最优值是机器学习不可能超过的上限，常用的近似方式就是以人类的能力为Optimal/Base Error.&lt;/p>
&lt;p>如果Optimal/Base Error很小，并且训练集和开发集分布相同：&lt;/p>
&lt;ul>
&lt;li>高偏差高方差（偏差，方差近似）：欠拟合--&amp;gt;更大的网络，更新网络结构，训练时间加长，&lt;/li>
&lt;li>高偏差高方差（偏差大，方差更大）：欠拟合同时部分数据过拟合。这种情形在高维数据中会出现，即某些维度过拟合。&lt;/li>
&lt;li>低偏差高方差：过拟合--&amp;gt;更多数据，正则化，更新网络结构&lt;/li>
&lt;li>低偏差低方差：正好&lt;/li>
&lt;/ul>
&lt;h3 id="正则化">正则化&lt;/h3>
&lt;p>L2:&lt;span class="math">\(+\frac{\lambda}{2m}\sum_l||W^{[l]}||^2_F\)&lt;/span>&lt;/p>
&lt;p>L1:&lt;span class="math">\(+\frac{\lambda}{m}\sum_l|W^{[l]}|\)&lt;/span>&lt;/p>
&lt;p>Dropout:每一轮训练中随机使一些点失效。不可与梯度验证合用。CV中几乎是默认选项。&lt;/p>
&lt;p>数据扩增（Data augemntation）:数据做些修改作为新数据（图片旋转、扭曲、图片剪裁、加噪声）&lt;/p>
&lt;p>Early Stopping： 当损失函数不在随着迭代次数下降时停止训练，本质时减少训练次数。但是不利于正交化，降低损失函数和early stopping正则化是相悖的。&lt;/p>
&lt;h3 id="归一化输入">归一化输入&lt;/h3>
&lt;p>归一化输入可以使&lt;strong>训练的速度更快&lt;/strong>。注意不要分别计算测试集和开发集的均值和方差，而是先统一归一化整体数据集，再划分训练集、开发集和测试集。&lt;/p>
&lt;h3 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸&lt;/h3>
&lt;p>在很多层的神经网络中，参数可能非常大或者非常接近0，因为每一层乘参数都是指数增长/降低的一部分。很长一段时间以来，梯度消失和爆炸都是深度学习难以解决的问题。&lt;/p>
&lt;p>一个部分解决的方案是&lt;strong>合理的初始化参数&lt;/strong>&lt;/p>
&lt;p>激活函数Relu--&amp;gt;&lt;code>np.random.randn(shape)*np.sqrt(2/n[l-1])&lt;/code>&lt;/p>
&lt;p>激活函数tanh--&amp;gt;&lt;code>np.random.randn(shape)*np.sqrt(1/n[l-1])&lt;/code>(Xavier)&lt;/p>
&lt;h3 id="梯度检查">梯度检查&lt;/h3>
&lt;p>双边差计算近似梯度&lt;span class="math">\(f&amp;#39;(\theta)\approx \frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2*\epsilon}\)&lt;/span>，其中&lt;span class="math">\(\epsilon\)&lt;/span>是一个很小的值，例如0.001.&lt;/p>
&lt;p>注记：&lt;/p>
&lt;ul>
&lt;li>梯度检查只用于debug&lt;/li>
&lt;li>查看误差大的项尝试debug&lt;/li>
&lt;li>记住正则化也影响梯度检查&lt;/li>
&lt;li>不能和dropout一起用&lt;/li>
&lt;li>如果实在怕错，那么使用机器学习框架吧(tensorflow, keras, pytorch)&lt;/li>
&lt;/ul>
&lt;h2 id="优化算法">优化算法&lt;/h2>
&lt;h3 id="mini-batch">mini-batch&lt;/h3>
&lt;p>&lt;span class="math">\(X^{[l]\{t\}(s)}\)&lt;/span>:第&lt;span class="math">\(l\)&lt;/span>层，mini-batch中第&lt;span class="math">\(t\)&lt;/span>组中第&lt;span class="math">\(s\)&lt;/span>个样本。一般的 mini-batch 大小为 64 到 1024，考虑到电脑内存设置和使用的方式，如果mini-batch 大小是2的n次方,代码会运行地快一些。&lt;/p>
&lt;h3 id="指数加权平均">指数加权平均&lt;/h3>
&lt;p>&lt;span class="math">\(V_t=\beta×V_{t-1} + (1-\beta)×\theta_t, \beta=0.9,0.99\dotsb\)&lt;/span>，大概计算了&lt;span class="math">\(1/(1-\beta)\)&lt;/span>轮的平均数据。&lt;/p>
&lt;p>但是指数加权平均在初始化的时候很不准确，需要&lt;strong>偏差修正&lt;/strong>。 &lt;span class="math">\[
V_t=\beta×V_{t-1} + (1-\beta)×\theta_t \\
V_t = V_t /(1-\beta^t)
\]&lt;/span> 当&lt;span class="math">\(t\rightarrow \infty时, 1-\beta^t\rightarrow 1\)&lt;/span>&lt;/p>
&lt;h3 id="momentum-动量法梯度下降">Momentum 动量法梯度下降&lt;/h3>
实际上就是指数加权平均/移动平均的梯度下降： &lt;span class="math">\[
dV_t = \beta×dV_{t-1} + (1-\beta) dW_t \\
(偏差修正也可以不要)\\
dV_t = dV_t/(1-\beta^t) \\
W_t = W_t - \alpha* dV_t\\
通常\beta = 0.9
\]&lt;/span> 其原理在于使用移动平均，抵消了振荡。 &lt;img src="../images/momentum_vs_vanilla.png" alt="momentum_vs_vanilla.png" />
&lt;center>
动量法效果--红色为动量法
&lt;/center>
&lt;h3 id="rmsprop均方根法">RMSprop均方根法&lt;/h3>
&lt;p>对于上面的这个椭圆形的抛物面（图中的椭圆代表等高线），沿着横轴收敛速度是最快的，所以我们希望在横轴（假设记为w1）方向步长大一些，在纵轴（假设记为w2）方向步长小一些。这时候可以通过RMSprop实现，分维度来看，迭代更新公式如下： &lt;span class="math">\[
\begin{cases} s_1=\beta_1 s_1+(1-\beta_1)dw_1^2 \\ s_2=\beta_2 s_2+(1-\beta_2)dw_2^2 \end{cases}\\
\begin{cases} w_1=w_1-\alpha \frac{dw_1}{\sqrt{s_1+\epsilon}} \\ w_2=w_2-\alpha \frac{dw_2}{\sqrt{s_2+\epsilon}} \end{cases}
\]&lt;/span> 观察上面的公式可以看到，&lt;span class="math">\(s_i\)&lt;/span>是对梯度的平方做了一次平滑。在更新&lt;span class="math">\(w_i\)&lt;/span>时，先用梯度除以&lt;span class="math">\(\sqrt{s_i+\epsilon}\)&lt;/span>，相当于对梯度做了一次归一化。如果某个方向上梯度震荡很大，应该减小其步长；而震荡大，则这个方向的&lt;span class="math">\(s_i\)&lt;/span>也较大，除完之后，归一化的梯度就小了；如果某个方向上梯度震荡很小，应该增大其步长；而震荡小，则这个方向的&lt;span class="math">\(s_i\)&lt;/span>也较小，归一化的梯度就大了。因此，通过RMSprop，我们可以调整不同维度上的步长，加快收敛速度。把上式合并后，RMSprop迭代更新公式如下： &lt;span class="math">\[
\begin{cases} s=\beta s+(1-\beta)dw^2 \\ w=w-\alpha\frac{dw}{\sqrt{s+\epsilon}} \end{cases}
\]&lt;/span> 注意，这里的&lt;span class="math">\(s,w\)&lt;/span>都是多维向量。&lt;span class="math">\(β\)&lt;/span>的典型值是0.999。公式中还有一个&lt;span class="math">\(ϵ\)&lt;/span>，这是一个很小的数，是为了防止分母为0，典型值是&lt;span class="math">\(10^{−8}\)&lt;/span>。&lt;/p>
&lt;h3 id="adam">Adam&lt;/h3>
&lt;p>Adam = Momentum+RMSprop &lt;span class="math">\[
\begin{cases} v=\beta_1 v+(1-\beta_1)dw \\ s=\beta_2 s+(1-\beta_2)dw^2 \\ w=w-\alpha\frac{v}{\sqrt{s+\epsilon}} \end{cases}
\]&lt;/span> 典型值：&lt;span class="math">\(β_1=0.9,β_2=0.999,ϵ=10^{−8}\)&lt;/span>。Adam算法相当于先把原始梯度做一个指数加权平均，再做一次归一化处理，然后再更新梯度值。&lt;/p>
&lt;p>其他新算法还有AdaMax，Nadam=Nag+Adam。&lt;/p>
&lt;h3 id="学习率">学习率&lt;/h3>
&lt;p>PS:1 epoch 代表每遍历所有训练数据一次,mini-batch中，&lt;span class="math">\(1 epoch = M/mini-batch-size\)&lt;/span>&lt;/p>
&lt;p>几种常见学习率衰减法： &lt;span class="math">\[
\begin{aligned}
&amp;amp;\alpha =\alpha_0*\frac{1}{1+decay\_rate*epoch\_num}\\
&amp;amp;\alpha =\alpha_0*decay\_rate^{epoch\_num}\\
&amp;amp;\alpha =\alpha_0* \frac{k}{\sqrt{epoch\_num}}
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>有时也会用一个离散下降的学习率，也就是某个步骤有某个学习率，一会之后，学习率减少了一半，一会儿减少一半，一会儿又一半，这就是离散下降（discrete stair cease）的意思。&lt;/p>
&lt;h3 id="局部最优点和鞍点">局部最优点和鞍点&lt;/h3>
&lt;p>一个具有高维度空间的函数，如果梯度为 0，那么在每个方向，它可能是凸函数，也可能是凹函数。如果你在 2 万维空间中，那么想要得到局部最优，所有的 2 万个方向都需要是这样，但发生的机率也许很小，也许是&lt;span class="math">\(2^{−20000}\)&lt;/span>，你更有可能遇到有些方向的曲线会这样向上弯曲，另一些方向曲线向下弯，而不是所有的都向上弯曲，因此在高维度空间，你更可能碰到鞍点，而不会碰到局部最优。&lt;/p>
&lt;p>所以我们从深度学习历史中学到的一课就是，我们对低维度空间的大部分直觉，比如你可以画出上面的图，并不能应用到高维度空间中。&lt;/p>
&lt;h2 id="超参数调试batch正则化和softmax回归">超参数调试、Batch正则化和Softmax回归&lt;/h2>
&lt;h3 id="超参数调参经验">超参数调参经验&lt;/h3>
&lt;p>随机一点，从粗到细，有重点&lt;/p>
&lt;p>合适的范围：线性，log，1-线性+log等等&lt;/p>
&lt;h3 id="batch-norm">Batch Norm&lt;/h3>
&lt;p>我们知道对输入进行归一化可以提升模型训练的速度，假设我们对每一个隐藏层进行归一化，是否能够更快的提升速度呢？答案是可以的，这就是Batch Normalization。在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。Internal Covariate Shift会导致上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低，同时网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度。&lt;/p>
&lt;p>因此，我们在非线性函数之前把第&lt;span class="math">\(l\)&lt;/span>层的数据先归一化，即 &lt;span class="math">\[
\begin{aligned}
&amp;amp;Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\
&amp;amp;\mu=\frac{1}{m}\sum_{i=1}^m Z^{[l](i)}\\
&amp;amp;\sigma=\frac{1}{m}\sum_{i=1}^m (Z^{[l](i)}-\mu)^2\\
&amp;amp;\hat Z^{[l]}=\frac{Z^{[l]}-\mu}{\sqrt{\sigma^2+\epsilon}}
\end{aligned}
\]&lt;/span> 需要指出的是BN算法一般针对mini-batch梯度下降，因此&lt;span class="math">\(\mu,\sigma^2\)&lt;/span>的计算量不会很大。&lt;span class="math">\(\epsilon=10^{-8}\)&lt;/span>为了不让分母为0。&lt;/p>
&lt;p>Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。&lt;/p>
&lt;p>因此，BN又引入了两个可学习（learnable）的参数&lt;span class="math">\(\gamma\)&lt;/span>与&lt;span class="math">\(\beta\)&lt;/span>。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即 &lt;span class="math">\[
\tilde Z^{[l]}=\gamma \hat Z^{[l]}+\beta\\
A^{[l]}=g(\tilde Z^{[l]})
\]&lt;/span> 通过上面的步骤，我们就在一定程度上保证了输入数据的表达能力。补充： 在进行normalization的过程中，由于我们的规范化操作会对减去均值，因此，偏置项 [公式] 可以被忽略掉或可以被置为0。&lt;/p>
&lt;p>在测试数据中，使用mini-batch的均值和方差的移动平均作为测试数据的均值和方差。&lt;/p>
&lt;h3 id="softmax">Softmax&lt;/h3>
&lt;p>Softmax函数用于多分类问题，定义方式如下： &lt;span class="math">\[
A_i=\frac{e^{Z_i}}{\sum_j e^{Z_j}}
\]&lt;/span> 使用这种定义一是因为其满足概率条件和为1，且特征对概率的影响是乘性的；同时方便交叉熵损失函数计算。二是反向传播求导方便。&lt;/p>
&lt;blockquote>
&lt;p>softmax VS k个二元分类器&lt;/p>
&lt;p>如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？&lt;/p>
&lt;p>这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）&lt;/p>
&lt;p>如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。&lt;/p>
&lt;p>现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？&lt;/p>
&lt;p>在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。&lt;/p>
&lt;/blockquote>
&lt;h2 id="机器学习策略">机器学习策略&lt;/h2>
&lt;h3 id="单一数字评估指标">单一数字评估指标&lt;/h3>
&lt;ul>
&lt;li>TP：实际为正，预测为正的样本数量&lt;/li>
&lt;li>FP：实际为负，预测为正的样本数量&lt;/li>
&lt;li>FN：实际为正，预测为负的样本数量&lt;/li>
&lt;li>TN：实际为负，预测为负的样本数量&lt;/li>
&lt;li>TP+FP：表示所有预测为正的样本数量&lt;/li>
&lt;li>TN+FN：表示所有预测为负的样本数量&lt;/li>
&lt;li>TP+FN：表示实际为正的样本数量&lt;/li>
&lt;li>TN+FP：表示实际为负的样本数量&lt;/li>
&lt;/ul>
&lt;img src="../images/混淆矩阵.png" alt="混淆矩阵" />
&lt;center>
混淆矩阵
&lt;/center>
&lt;p>&lt;strong>准确率（accuracy）&lt;/strong>：准确率是分类正确的样本占总样本个数，即 &lt;span class="math">\[
accuracy=\frac{TP+TN}{TP+FP+FN+TN}
\]&lt;/span> 准确率是分类问题中最简单最直观的评价指标，但存在明显的缺陷。比如正负样本的比例不均衡。假设样本中正样本占 95%，负样本占5%，那分类器只需要一直预测为正，就可以得到95% 的准确率，但其实际性能是非常低下的。&lt;/p>
&lt;p>&lt;strong>精确率（precision）&lt;/strong>：精确率指模型预测为正的样本中实际也为正的样本占被预测为正的样本的比例。计算公式为 &lt;span class="math">\[
precision=\frac{TP}{TP+FP}
\]&lt;/span> 细分：Macro、Micro、weighted&lt;/p>
&lt;p>&lt;strong>召回率（recall）&lt;/strong>：召回率指实际为正的样本中，预测也为正的样本占实际为正的样本的比例。计算公式为 &lt;span class="math">\[
recall = \frac{TP}{TP+FN}
\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>F1-score&lt;/strong>：精确率和召回率的加权平均值，计算公式为 &lt;span class="math">\[
F1 = \frac{2*precision*recall}{precision+recall}
\]&lt;/span> Precision 体现了模型对负样本的区分能力，Precision 越高，模型对负样本的区分能力越强。&lt;/p>
&lt;p>Recall 体现了模型对正样本的识别能力，Recall 越高，模型对正样本的识别能力越强。&lt;/p>
&lt;p>F1-score 是两者的综合，F1-score 越高，说明模型越稳健。&lt;/p>
&lt;h3 id="优化指标和满足指标">优化指标和满足指标&lt;/h3>
&lt;p>优化目标：最小化或最大化&lt;/p>
&lt;p>限制目标：大于，小于，等于&lt;/p>
&lt;h3 id="人的表现">人的表现&lt;/h3>
&lt;p>理论最优：贝叶斯最优错误率（Bayes optimal error）&lt;/p>
&lt;p>很多场景下人的性能接近贝叶斯最优错误率，同时很多数据是人标注的。而机器学习接近或超过人的能力后，很难在迅速提高。&lt;/p>
&lt;p>可避免的偏差：训练集上的错误率和理论（或人）的错误率之间的差距。&lt;/p>
&lt;p>对人类水平有大概的估计可以让你做出对贝叶斯错误率的估计，这样可以让你更快地作出决定是否应该专注于减少算法的偏差，或者减少算法的方差。这个决策技巧通常很有效，直到你的系统性能开始超越人类，那么你对贝叶斯错误率的估计就不再准确了，但这些技巧还是可以帮你做出明确的决定。&lt;/p>
&lt;h3 id="人工误差分析适用于机器学习还没到人的水平">人工误差分析（适用于机器学习还没到人的水平）&lt;/h3>
&lt;p>训练集标记错误：随机错误问题大不；系统性错误会有影响。&lt;/p>
&lt;p>开发集标记错误：如果这些标记错误严重影响了你在开发集上评估算法的能力，那么就应该去花时间修正错误的标签。但是，如果它们没有严重影响到你用开发集评估成本偏差的能力，那么可能就不应该花宝贵的时间去处理。但是需要指出，开发集和测试集需要一起标记修正错误。&lt;/p>
&lt;p>训练/开发/测试集划分：有大量相关数据（辅助训练）和核心数据的情形下，&lt;strong>把核心数据放在开发/测试集&lt;/strong>，而不是训练集，即使这样会导致训练集和开发集分布不太一致也无所谓。&lt;/p>
&lt;p>数据分布不匹配时的偏差与方差的分析：如果训练数据和开发数据分布不同，那么训练错误率和开发集错误率之间就不仅仅是方差问题，有可能本来开发集的数据就更难处理。为了区分时方差还是数据差异，&lt;strong>就需要构造一个和训练集同分布的训练开发集&lt;/strong>（例如random shuffle后再划分），来查看方差问题。整体过程就如下：&lt;/p>
&lt;blockquote>
&lt;p>optimal&amp;lt;-可避免偏差-&amp;gt;train set&amp;lt;-方差-&amp;gt;train-dev set&amp;lt;-数据不匹配（差距正负都有可能）&amp;gt;dev set&amp;lt;-开发集过拟合-&amp;gt;test set&lt;/p>
&lt;/blockquote>
&lt;p>数据不匹配处理方式：人工审阅训练集和开发集查找二者差别==&amp;gt;寻找更接近开发集的数据/加工训练集数据使其更与开发集更相似&lt;/p>
&lt;h3 id="迁移学习串行">迁移学习：串行&lt;/h3>
&lt;p>应用场景：类似问题之间，从数据量很多的问题迁移到数据量相对小的问题，低层次特征有相似性。&lt;/p>
&lt;p>足够多新数据，预训练（pre-training）==&amp;gt; 微调（fine tuning）。&lt;/p>
&lt;p>小数据集，只需要重新训练最后几层网络。因为，原理上来说很多低层特征是类似的。&lt;/p>
&lt;h3 id="多任务学习并行">多任务学习：并行&lt;/h3>
&lt;p>多任务学习能让你训练一个神经网络来执行许多任务。&lt;/p>
&lt;p>应用场景：任务间共享相似的低层特征；每个任务需要的数据量相似，且任务间有一定对等性（这个准则不绝对）；可以训练一个足够大的神经网络，足以做好所有的工作。&lt;/p>
&lt;p>Softmax（判断互斥） vs 多个神经网络（判断不互斥） vs 一个神经网络多个判断（不互斥）&lt;/p>
&lt;h3 id="端到端学习">端到端学习&lt;/h3>
&lt;p>端到端：忽略中间处理步骤，把深度神经网络作为一个黑盒子。&lt;/p>
&lt;p>端到端深度学习的挑战之一是，你可能需要大量数据才能让系统表现良好。&lt;/p>
&lt;h2 id="卷积神经网络cnn">卷积神经网络CNN&lt;/h2>
&lt;p>步骤：图像--&amp;gt;卷积--&amp;gt;(非线性函数)--&amp;gt;池化--&amp;gt;全连接--&amp;gt;结果&lt;/p>
&lt;h3 id="经典神经网络">经典神经网络&lt;/h3>
&lt;p>PS：非线性函数在卷积conv之后&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Lenet5(conv的参数，padding等可以根据实际输入调整，关键是结构)&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>conv 5*5*1*6 valid s=1&lt;/li>
&lt;li>pooling 2*2 same s=2&lt;/li>
&lt;li>conv 5*5*6*16 valid s=1&lt;/li>
&lt;li>pooling: 2*2 same s=2&lt;/li>
&lt;li>flatten and FC&lt;/li>
&lt;/ul>
&lt;ol start="2" style="list-style-type: decimal">
&lt;li>AlexNet&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>input :&lt;span class="math">\(227*227*3\)&lt;/span>()&lt;/li>
&lt;li>conv: &lt;span class="math">\(11*11*3*96, s=4--&amp;gt; 55*55*96\)&lt;/span>&lt;/li>
&lt;li>maxpool: &lt;span class="math">\(3*3, s=2 ,--&amp;gt;27*27*96\)&lt;/span>&lt;/li>
&lt;li>conv: &lt;span class="math">\(5*5*96*256, s=1, padding=same,--&amp;gt; 27*27*256\)&lt;/span>&lt;/li>
&lt;li>maxpool: &lt;span class="math">\(3*3,s=2, --&amp;gt;13*13*256\)&lt;/span>&lt;/li>
&lt;li>conv: &lt;span class="math">\(3*3*256*384,s=1, padding=same\)&lt;/span> 两次 &lt;span class="math">\(--&amp;gt;13*13*385\)&lt;/span>&lt;/li>
&lt;li>conv: &lt;span class="math">\(3*3*384*256, s=1, padding=same--&amp;gt;13*13*256\)&lt;/span>&lt;/li>
&lt;li>maxpool: &lt;span class="math">\(3*3,s=2--&amp;gt;6*6*256=9216\)&lt;/span>&lt;/li>
&lt;li>flatten: &lt;span class="math">\(9216 --&amp;gt; FC(4096) --&amp;gt; FC(4096) --&amp;gt;softmax(1000)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;ol start="3" style="list-style-type: decimal">
&lt;li>VGG16(conv: 3*3, s=1 ,same; maxpool: 2*2, s=2)&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>conv, c=64 两次&lt;/li>
&lt;li>maxpool&lt;/li>
&lt;li>conv, c=128 两次&lt;/li>
&lt;li>maxpool&lt;/li>
&lt;li>conv, c=256 三次&lt;/li>
&lt;li>maxpool&lt;/li>
&lt;li>conv, c=512 三次&lt;/li>
&lt;li>maxpool&lt;/li>
&lt;li>conv, c=512 三次&lt;/li>
&lt;li>maxpool&lt;/li>
&lt;li>FC input--&amp;gt;4096--&amp;gt;4096--&amp;gt;1000--&amp;gt;softmax&lt;/li>
&lt;li>VGG16：16个卷积和全连接层，约1.38亿参数&lt;/li>
&lt;/ul>
&lt;h2 id="循环神经网络rnn">循环神经网络RNN&lt;/h2>
&lt;p>RNN很适合处理序列数据/模型。&lt;span class="math">\(x^{\lt t\gt}--&amp;gt;y^{\lt t\gt}\)&lt;/span> &lt;span class="math">\[
x^{\lt t\gt (i)}:第i个样本序列第t个序列项\\
T_x^{i}:样本i的长度
\]&lt;/span>&lt;/p>
&lt;h3 id="基本结构">基本结构&lt;/h3>
&lt;div class="figure">
&lt;img src="../images/RNN_desc.jpg" alt="RNN_desc" />&lt;p class="caption">RNN_desc&lt;/p>
&lt;/div>
&lt;p>如果把上面左上角有W的那个带箭头的圈（或者大图中左边部分）去掉，它就变成了最普通的全连接神经网络。&lt;strong>x是一个向量&lt;/strong>，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；&lt;strong>s是一个向量&lt;/strong>，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。那么，现在我们来看看W是什么。循环神经网络的隐藏层的值s&lt;strong>不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s&lt;/strong>。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。&lt;/p>
&lt;p>RNN只能使用之前输入的信息，而忽略之后的输入，因此有人提出&lt;strong>BRNN&lt;/strong>，双向RNN来利用当前点以后的输入信息。&lt;/p>
&lt;p>&lt;span class="math">\[
\begin{aligned}
\mathrm{o}_t&amp;amp;=g(V\mathrm{s}_t+b_2)\qquad\qquad\quad(式1)\\
\mathrm{s}_t&amp;amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1}+b_1)\qquad(式2)\\
\end{aligned}
\]&lt;/span> 式1是输出层的计算公式，输出层是一个全连接层。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值&lt;span class="math">\(s_{t-1}\)&lt;/span>作为这一次的输入的权重矩阵，f是激活函数。一般激活函数使用tanh或relu。&lt;/p>
&lt;h3 id="gru与lstm">GRU与LSTM&lt;/h3>
&lt;p>记住一些信息，形成长依赖。&lt;/p>
&lt;h2 id="生成对抗网络gan">生成对抗网络GAN&lt;/h2>
&lt;h2 id="强化学习">强化学习&lt;/h2></description></item><item><title>深度学习-必须要知道的25个概念</title><link>https://surprisedcat.github.io/studynotes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%BF%85%E9%A1%BB%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%8425%E4%B8%AA%E6%A6%82%E5%BF%B5/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%BF%85%E9%A1%BB%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%8425%E4%B8%AA%E6%A6%82%E5%BF%B5/</guid><description>
&lt;h2 id="深度学习入门者必看25个你一定要知道的概念">深度学习入门者必看：25个你一定要知道的概念&lt;!-- omit in toc -->&lt;/h2>
&lt;blockquote>
&lt;p>人工智能，深度学习，机器学习……不管你在从事什么工作，都需要了解这些概念。否则的话，三年之内你就会变成一只恐龙。 —— 马克·库班&lt;/p>
&lt;/blockquote>
&lt;p>库班的这句话，乍听起来有些偏激，但是“话糙理不糙”，我们现在正处于一场由大数据和超算引发的改革洪流之中。&lt;/p>
&lt;p>首先，我们设想一下，如果一个人生活在20世纪早期却不知电为何物，是怎样一种体验。在过去的岁月里，他已经习惯于用特定的方法来解决相应的问题，霎时间周围所有的事物都发生了剧变。以前需要耗费大量人力物力的工作，现在只需要一个人和电就能完成了。&lt;/p>
&lt;p>而在现在的背景下，机器学习、深度学习就是新的“电力”。&lt;/p>
&lt;p>所以呢，如果你还不了解深度学习有多么强大，不妨就从这篇文章开始。在这篇文章中，作者Dishashree Gupta为想了解深度学习的人，罗列并解释了25个这一领域最常用的术语。&lt;/p>
&lt;p>这25个术语被分成三组：&lt;/p>
&lt;ul>
&lt;li>神经网络中的基础概念(包含常用的一些激活函数)&lt;/li>
&lt;li>卷积神经网络&lt;/li>
&lt;li>递归神经网络&lt;/li>
&lt;/ul>
&lt;h2 id="基础概念">基础概念&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>神经元(Neuron)&lt;/li>
&lt;/ol>
&lt;p>正如我们大脑中的基本组成单元，神经元是组成神经网络的基础结构。设想一下当接触到新的信息时，我们的身体会对其进行处理，最后产生一些特定的反应。&lt;/p>
&lt;p>相似地，在神经网络中，在收到输入的信号之后，神经元通过处理，然后把结果输出给其它的神经元或者直接作为最终的输出。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/neuron.jpg" alt="神经图" />&lt;p class="caption">神经图&lt;/p>
&lt;/div>
&lt;ol start="2" style="list-style-type: decimal">
&lt;li>加权/权重(Weights)&lt;/li>
&lt;/ol>
&lt;p>当输入信号进入到神经元后，会被乘以相应的权重因子。举例来说，假设一个神经元有两个输入信号，那么每个输入将会存在着一个与之相应的权重因子。在初始化网络的时候，这些权重会被随机设置，然后在训练模型的过程中再不断地发生更改。&lt;/p>
&lt;p>在经过训练后的神经网络中，一个输入具有的权重因子越高，往往意味着它的重要性更高，对输出的影响越大。另一方面，当权重因子为0时意味着这个输入是无价值的。&lt;/p>
&lt;p>如下图所示，假设输入为a，相应的权重为W1。那么通过赋权节点后相应的输入应变为a*W1。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/neuron_weight.png" alt="加权/权重" />&lt;p class="caption">加权/权重&lt;/p>
&lt;/div>
&lt;p>(3)偏差（Bias）&lt;/p>
&lt;p>除了权重之外，另一个被应用于输入的线性分量被称为偏差。它被加到权重与输入相乘的结果中。基本上添加偏差的目的是来改变权重与输入相乘所得结果的范围的。添加偏差后，结果将看起来像a* W1 +偏差。这是输入变换的最终线性分量。&lt;/p>
&lt;p>(4)激活函数（Activation Function）&lt;/p>
&lt;p>一旦将线性分量应用于输入，将会需要应用一个非线性函数。这通过将激活函数应用于线性组合来完成。激活函数将输入信号转换为输出信号。应用激活函数后的输出看起来像&lt;span class="math">\(f(a*W_1 + b）\)&lt;/span>，其中&lt;span class="math">\(f()\)&lt;/span>就是激活函数。&lt;/p>
&lt;p>在下图中，我们将“n”个输入给定为&lt;span class="math">\(X_1\)&lt;/span>到&lt;span class="math">\(X_n\)&lt;/span>而与其相应的权重为&lt;span class="math">\(W_{k1}\)&lt;/span>到&lt;span class="math">\(W_{kn}\)&lt;/span>。我们有一个给定值为&lt;span class="math">\(b_k\)&lt;/span>的偏差。权重首先乘以与其对应的输入，然后与偏差加在一起。而这个值叫做&lt;span class="math">\(U\)&lt;/span>。 &lt;span class="math">\[U=\sum W*X+b\]&lt;/span> 激活函数被应用于&lt;span class="math">\(U\)&lt;/span>，即&lt;span class="math">\(f(U)\)&lt;/span>，并且我们会从神经元接收最终输出，如&lt;span class="math">\(y_k=f(U)\)&lt;/span>。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/神经元激活函数.jpg" alt="神经元激活函数" />&lt;p class="caption">神经元激活函数&lt;/p>
&lt;/div>
&lt;p>接下来我们讲一讲常用的一些激活函数：Sigmoid函数， 线性整流函数(ReLU) 和 softmax函数&lt;/p>
&lt;ol style="list-style-type: lower-alpha">
&lt;li>sigmoid函数&lt;/li>
&lt;/ol>
&lt;p>&lt;span class="math">\[sigmoid(x)=\frac{1}{1+e^{-x}}\]&lt;/span> sigmoid函数为值域在0到1之间的光滑函数，当需要观察输入信号数值上微小的变化时，与阶梯函数相比，平滑函数(比如Sigmoid函数)的表现更好。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/sigmoid.png" alt="Sigmoid函数" />&lt;p class="caption">Sigmoid函数&lt;/p>
&lt;/div>
&lt;ol start="2" style="list-style-type: lower-alpha">
&lt;li>线性整流函数(ReLU-Rectified Linear Units)&lt;/li>
&lt;/ol>
&lt;p>与Sigmoid函数不同的是，最近的网络更喜欢使用ReLu激活函数来处理隐藏层。该函数定义为： &lt;span class="math">\[f(x)=max(x,0)\]&lt;/span> 当x大于0时，函数输出x，其余的情况输出为0。函数的图像是： &lt;img src="../images/线性整流函数.png" alt="线性整流函数" />&lt;/p>
&lt;p>使用ReLU函数的最主要的好处是对于大于0的所有输入来说，它都有一个不变的导数值。常数导数值有助于网络训练进行得更快。&lt;/p>
&lt;ol start="3" style="list-style-type: lower-alpha">
&lt;li>Softmax&lt;/li>
&lt;/ol>
&lt;p>Softmax激活函数通常用于输出层，用于分类问题。它与sigmoid函数是很类似的，唯一的区别就是输出被归一化为总和为1。Sigmoid函数将发挥作用以防我们有一个二进制输出，但是如果我们有一个多类分类问题，softmax函数使为每个类分配值这种操作变得相当简单，而这可以将其解释为概率。&lt;/p>
&lt;p>以这种方式来操作的话，我们很容易看到——假设你正在尝试识别一个可能看起来像8的6。该函数将为每个数字分配值如下。我们可以很容易地看出，最高概率被分配给6，而下一个最高概率分配给8，依此类推……&lt;/p>
&lt;ol start="5" style="list-style-type: decimal">
&lt;li>神经网络&lt;/li>
&lt;/ol>
&lt;p>神经网络是构成深度学习系统的框架。神经网络的任务是找到一个未知函数的近似表达方式，它是由彼此相连的神经元所组成，这些神经元会在训练网络的过程中根据误差来更改它们的权重和偏置。激活函数将非线性变化用线性变化的组合来表示，最终产生输出。&lt;/p>
&lt;p>关于神经网络最好的定义是由Matthew Mayo给出的：&lt;/p>
&lt;p>神经网络是由大量彼此相连、概念化的人造神经元组成的，这些神经元彼此之间传递着数据，相应的权重也会随着神经网络的经历而进行调整。神经元们有着激活的阈值，当它们遇到相应的数据以及权重时会被激活，这些被激活的神经元组合起来导致了“学习”行为的产生。&lt;/p>
&lt;ol start="6" style="list-style-type: decimal">
&lt;li>输入层/输出层/隐藏层&lt;/li>
&lt;/ol>
&lt;p>从名字中就能看出，输入层代表接受输入数据的一层，基本上是网络的第一层；输出层是产生输出的一层，或者是网络的最后一层，而网络中间的处理层叫做隐藏层。&lt;/p>
&lt;p>这些隐藏层对输入的数据进行特定的处理，再将其输入到下一层。输入层和输出层是可见的，而中间层通常是被隐藏起来的。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/输入_输出_隐藏层.jpg" alt="输入/输出/隐藏层" />&lt;p class="caption">输入/输出/隐藏层&lt;/p>
&lt;/div>
&lt;ol start="7" style="list-style-type: decimal">
&lt;li>MLP（多层感知器）&lt;/li>
&lt;/ol>
&lt;p>一个单一的神经元不能够完成复杂的任务，因此需要将它们堆叠起来工作进而产生有用的输出。&lt;/p>
&lt;p>最简单的神经网络包括一个输入层、一个隐藏层和一个输出层。每一层都由多个神经元组成，每一层的每个神经元都与下一层中的所有神经元相连。这样的网络可以被称为是全连接网络。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/多层感知器.jpg" alt="多层感知器" />&lt;p class="caption">多层感知器&lt;/p>
&lt;/div>
&lt;ol start="8" style="list-style-type: decimal">
&lt;li>正向传播（Forward Propagation）&lt;/li>
&lt;/ol>
&lt;p>正向传播是指输入通过隐藏层到输出层的运动。在正向传播中，信息沿着一个单一方向前进。输入层将输入提供给隐藏层，然后生成输出。这过程中是没有反向运动的。&lt;/p>
&lt;ol start="9" style="list-style-type: decimal">
&lt;li>成本函数（Cost Function）&lt;/li>
&lt;/ol>
&lt;p>当我们建立一个网络时，网络试图将输出预测得尽可能靠近实际值。我们使用成本/损失函数来衡量网络的准确性。而成本或损失函数会在发生错误时尝试惩罚网络。&lt;/p>
&lt;p>我们在运行网络时的目标是提高我们的预测精度并减少误差，从而最大限度地降低成本。最优化的输出是那些成本或损失函数值最小的输出。&lt;/p>
&lt;p>如果我将成本函数定义为均方误差，则可以写为： &lt;span class="math">\[C= 1/m\sum(y–a)^2\]&lt;/span> 其中&lt;span class="math">\(m\)&lt;/span>是训练输入的数量，&lt;span class="math">\(a\)&lt;/span>是预测值，&lt;span class="math">\(y\)&lt;/span>是该特定示例的实际值。学习过程围绕最小化成本来进行。&lt;/p>
&lt;ol start="10" style="list-style-type: decimal">
&lt;li>梯度下降（Gradient Descent）&lt;/li>
&lt;/ol>
&lt;p>梯度下降是一种最小化成本的优化算法。要直观地想一想，在爬山的时候，你应该会采取小步骤，一步一步走下来，而不是一下子跳下来。因此，我们所做的就是，如果我们从一个点 x 开始，我们向下移动一点，即Δh，并将我们的位置更新为 x-Δh，并且我们继续保持一致，直到达到底部。考虑最低成本点。在数学上，为了找到函数的局部最小值，我们通常采取与函数梯度的负数成比例的步长。&lt;/p>
&lt;ol start="11" style="list-style-type: decimal">
&lt;li>学习率（Learning Rate）&lt;/li>
&lt;/ol>
&lt;p>学习率被定义为每次迭代中成本函数中最小化的量。简单来说，我们下降到成本函数的最小值的速率是学习率。我们应该非常仔细地选择学习率，因为它不应该是非常大的，以至于最佳解决方案被错过，也不应该非常低，以至于网络需要融合。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/学习率影响.jpg" alt="学习率影响" />&lt;p class="caption">学习率影响&lt;/p>
&lt;/div>
&lt;ol start="12" style="list-style-type: decimal">
&lt;li>反向传播（Backpropagation）&lt;/li>
&lt;/ol>
&lt;p>当我们定义神经网络时，我们为我们的节点分配随机权重和偏差值。一旦我们收到单次迭代的输出，我们就可以计算出网络的错误。然后将该错误与成本函数的梯度一起反馈给网络以更新网络的权重。 最后更新这些权重，以便减少后续迭代中的错误。使用成本函数的梯度的权重的更新被称为反向传播。&lt;/p>
&lt;p>在反向传播中，网络的运动是向后的，错误随着梯度从外层通过隐藏层流回，权重被更新。&lt;/p>
&lt;p>&lt;a href="https://blog.csdn.net/weixin_38347387/article/details/82936585">反向传播具体解析&lt;/a>&lt;/p>
&lt;ol start="13" style="list-style-type: decimal">
&lt;li>批次（Batches）&lt;/li>
&lt;/ol>
&lt;p>在训练神经网络的同时，不用一次发送整个输入，我们将输入分成几个随机大小相等的块。与整个数据集一次性馈送到网络时建立的模型相比，批量训练数据使得模型更加广义化。&lt;/p>
&lt;ol start="14" style="list-style-type: decimal">
&lt;li>周期（Epochs）&lt;/li>
&lt;/ol>
&lt;p>周期被定义为向前和向后传播中所有批次的单次训练迭代。这意味着1个周期是整个输入数据的单次向前和向后传递。&lt;/p>
&lt;p>你可以选择你用来训练网络的周期数量，更多的周期将显示出更高的网络准确性，然而，网络融合也需要更长的时间。另外，你必须注意，如果周期数太高，&lt;strong>网络可能会过度拟合&lt;/strong>。&lt;/p>
&lt;ol start="15" style="list-style-type: decimal">
&lt;li>丢弃（Dropout）&lt;/li>
&lt;/ol>
&lt;p>Dropout 是一种正则化技术，可防止网络过度拟合套。顾名思义，在训练期间，隐藏层中的一定数量的神经元被随机地丢弃。这意味着训练发生在神经网络的不同组合的神经网络的几个架构上。你可以将 Dropout 视为一种综合技术，然后将多个网络的输出用于产生最终输出。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/dropout.jpg" alt="droput" />&lt;p class="caption">droput&lt;/p>
&lt;/div>
&lt;ol start="16" style="list-style-type: decimal">
&lt;li>批量归一化？？（Batch Normalization）&lt;/li>
&lt;/ol>
&lt;p>作为一个概念，批量归一化可以被认为是我们在河流中设定为特定检查点的水坝。这样做是为了确保数据的分发与希望获得的下一层相同。当我们训练神经网络时，权重在梯度下降的每个步骤之后都会改变，这会改变数据的形状如何发送到下一层。&lt;/p>
&lt;p>&lt;a href="https://blog.csdn.net/u013289254/article/details/99690730">深度学习中的五种归一化&lt;/a>&lt;/p>
&lt;h2 id="卷积神经网络cnn">卷积神经网络CNN&lt;/h2>
&lt;ol start="17" style="list-style-type: decimal">
&lt;li>滤波器（Filters）&lt;/li>
&lt;/ol>
&lt;p>CNN中的滤波器与加权矩阵一样，它与输入图像的一部分相乘以产生一个回旋输出。我们假设有一个大小为28*28的图像，我们随机分配一个大小为3*3的滤波器，然后与图像不同的3*3部分相乘，形成所谓的卷积输出。滤波器尺寸通常小于原始图像尺寸。在成本最小化的反向传播期间，滤波器值被更新为重量值。&lt;/p>
&lt;p>参考一下，这里filter是一个3*3矩阵： &lt;span class="math">\[\begin{bmatrix}
1&amp;amp;0&amp;amp;1\\
0&amp;amp;1&amp;amp;0\\
1&amp;amp;0&amp;amp;1\\
\end{bmatrix}\]&lt;/span> 与图像的每个3*3部分相乘以形成卷积特征。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/cnn_example.jpg" alt="cnn_example" />&lt;p class="caption">cnn_example&lt;/p>
&lt;/div>
&lt;ol start="18" style="list-style-type: decimal">
&lt;li>卷积神经网络（CNN）&lt;/li>
&lt;/ol>
&lt;p>卷积神经网络基本上应用于图像数据。假设我们有一个输入的大小（28*28*3），如果我们使用正常的神经网络，将有2352（28*28*3）参数。并且随着图像的大小增加参数的数量变得非常大。我们“卷积”图像以减少参数数量（如上面滤波器定义所示）。当我们将滤波器滑动到输入体积的宽度和高度时，将产生一个二维激活图，给出该滤波器在每个位置的输出。我们将沿深度尺寸堆叠这些激活图，并产生输出量。&lt;/p>
&lt;p>你可以看到下面的图，以获得更清晰的印象。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/CNN_Structure.jpg" alt="CNN_Structure" />&lt;p class="caption">CNN_Structure&lt;/p>
&lt;/div>
&lt;p>(19)池化（Pooling）&lt;/p>
&lt;p>通常在卷积层之间定期引入池层。这基本上是为了减少一些参数，并防止过度拟合。最常见的池化类型是使用MAX操作的滤波器尺寸(2,2)的池层。它会做的是，它将占用原始图像的每个4*4矩阵的最大值。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/cnn_pooling.jpg" alt="cnn_pooling" />&lt;p class="caption">cnn_pooling&lt;/p>
&lt;/div>
&lt;p>你还可以使用其他操作（如平均池）进行池化，但是最大池数量在实践中表现更好。&lt;/p>
&lt;p>(20)填充（Padding）&lt;/p>
&lt;p>填充是指在图像之间添加额外的零层，以使输出图像的大小与输入相同。这被称为相同的填充。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/cnn_padding.jpg" alt="cnn_padding" />&lt;p class="caption">cnn_padding&lt;/p>
&lt;/div>
&lt;p>在应用滤波器之后，在相同填充的情况下，卷积层具有等于实际图像的大小。&lt;/p>
&lt;p>有效填充是指将图像保持为具有实际或“有效”的图像的所有像素。在这种情况下，在应用滤波器之后，输出的长度和宽度的大小在每个卷积层处不断减小。&lt;/p>
&lt;p>(21)数据增强（Data Augmentation）&lt;/p>
&lt;p>数据增强是指从给定数据导出的新数据的添加，这可能被证明对预测有益。例如，如果你使光线变亮，可能更容易在较暗的图像中看到猫，或者例如，数字识别中的9可能会稍微倾斜或旋转。在这种情况下，旋转将解决问题并提高我们的模型的准确性。通过旋转或增亮，我们正在提高数据的质量。这被称为数据增强。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/cnn_data_augmentation.jpg" alt="Data_Augmentation" />&lt;p class="caption">Data_Augmentation&lt;/p>
&lt;/div>
&lt;h2 id="循环神经网络">循环神经网络&lt;/h2>
&lt;div class="figure">
&lt;img src="../images/rnn_structure.jpg" alt="rnn_structure" />&lt;p class="caption">rnn_structure&lt;/p>
&lt;/div>
&lt;ol start="22" style="list-style-type: decimal">
&lt;li>循环神经元（Recurrent Neuron）&lt;/li>
&lt;/ol>
&lt;p>循环神经元是在 T 时间内将神经元的输出发送回给它。如上图，输出将返回输入 t 次。展开的神经元看起来像连接在一起的 t 个不同的神经元。这个神经元的基本优点是它给出了更广义的输出。&lt;/p>
&lt;ol start="23" style="list-style-type: decimal">
&lt;li>循环神经网络（RNN）&lt;/li>
&lt;/ol>
&lt;p>循环神经网络特别用于顺序数据，其中先前的输出用于预测下一个输出。在这种情况下，网络中有循环。隐藏神经元内的循环使他们能够存储有关前一个单词的信息一段时间，以便能够预测输出。隐藏层的输出在 t 时间戳内再次发送到隐藏层。展开的神经元看起来像上图。只有在完成所有的时间戳后，循环神经元的输出才能进入下一层。发送的输出更广泛，以前的信息保留的时间也较长。&lt;/p>
&lt;p>然后根据展开的网络将错误反向传播以更新权重。这被称为通过时间的反向传播（BPTT）。&lt;/p>
&lt;p>(24)消失梯度问题（Vanishing Gradient Problem）&lt;/p>
&lt;p>激活函数的梯度非常小的情况下会出现消失梯度问题。在权重乘以这些低梯度时的反向传播过程中，它们往往变得非常小，并且随着网络进一步深入而“消失”。这使得神经网络忘记了长距离依赖。这对循环神经网络来说是一个问题，长期依赖对于网络来说是非常重要的。&lt;/p>
&lt;p>这可以通过使用不具有小梯度的激活函数ReLu来解决。&lt;/p>
&lt;ol start="25" style="list-style-type: decimal">
&lt;li>激增梯度问题（Exploding Gradient Problem）&lt;/li>
&lt;/ol>
&lt;p>这与消失的梯度问题完全相反，激活函数的梯度过大。在反向传播期间，它使特定节点的权重相对于其他节点的权重非常高，这使得它们不重要。这可以通过剪切梯度来轻松解决，使其不超过一定值。&lt;/p></description></item><item><title>线性代数与矩阵之正规矩阵</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E6%AD%A3%E8%A7%84%E7%9F%A9%E9%98%B5/</link><pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E6%AD%A3%E8%A7%84%E7%9F%A9%E9%98%B5/</guid><description>
&lt;h2 id="正规矩阵">正规矩阵&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#例子">例子&lt;/a>&lt;/li>
&lt;li>&lt;a href="#性质">性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相似对角化">相似对角化&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>在数学中，正规矩阵（英语：normal matrix）&lt;span class="math">\(\mathbf {A}\)&lt;/span>是与自己的共轭转置满足交换律的复系数方块矩阵，也就是说，&lt;span class="math">\(\mathbf {A}\)&lt;/span>满足 &lt;span class="math">\[\mathbf{A}^\ast\mathbf{A} = \mathbf{A} \mathbf{A}^\ast\]&lt;/span> 其中&lt;span class="math">\(\mathbf{A}^\ast\)&lt;/span>是&lt;span class="math">\(\mathbf{A}\)&lt;/span>的共轭转置。&lt;/p>
&lt;p>如果&lt;span class="math">\(\mathbf{A}\)&lt;/span>是实系数矩阵，则&lt;span class="math">\(\mathbf{A}^\ast = \mathbf{A}^T\)&lt;/span>，从而条件简化为&lt;span class="math">\(\mathbf {A} ^{T}\mathbf {A} =\mathbf {A} \mathbf {A} ^{T}\)&lt;/span>其中&lt;span class="math">\(\mathbf{A}^T\)&lt;/span>是&lt;span class="math">\(\mathbf{A}\)&lt;/span>的转置矩阵。&lt;/p>
&lt;p>任何一个正规矩阵，都是某个正规算子在一组标准正交基下的矩阵；反之，任一正规算子在一组标准正交基下的矩阵都为正规矩阵。&lt;/p>
&lt;p>矩阵的正规性充要条件：&lt;/p>
&lt;blockquote>
&lt;p>任意正规矩阵都可在经过一个酉变换后变为对角矩阵，反过来所有可在经过一个酉变换后变为对角矩阵的矩阵都是正规矩阵。&lt;/p>
&lt;/blockquote>
&lt;h2 id="例子">例子&lt;/h2>
&lt;p>在复系数矩阵中，所有的酉矩阵、埃尔米特矩阵和斜埃尔米特矩阵都是正规的。同理，在实系数矩阵中，所有的正交矩阵、对称矩阵和斜对称矩阵都是正规的。&lt;/p>
&lt;h2 id="性质">性质&lt;/h2>
&lt;h3 id="相似对角化">相似对角化&lt;/h3>
&lt;p>正规矩阵的概念十分重要，因为它们正是能使谱定理成立的对象。&lt;strong>矩阵&lt;span class="math">\(\mathbf{A}\)&lt;/span>正规当且仅当它可以被写成&lt;span class="math">\(\mathbf{A} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^\ast\)&lt;/span>的形式&lt;/strong>。其中的&lt;span class="math">\(\mathbf{\Lambda} = \operatorname{diag}(\lambda_1, \lambda_2, \dots)\)&lt;/span>为对角矩阵，&lt;span class="math">\(\mathbf{U}\)&lt;/span>为酉矩阵： &lt;span class="math">\[\mathbf{U}^\ast\mathbf{U} = \mathbf{U} \mathbf{U}^\ast = \mathbf{I}\]&lt;/span> 矩阵&lt;span class="math">\(Λ\)&lt;/span>对角线上的元素是&lt;span class="math">\(A\)&lt;/span>的特征值，而组成&lt;span class="math">\(U\)&lt;/span>的列向量则是&lt;span class="math">\(A\)&lt;/span>相应的特征向量。&lt;/p></description></item><item><title>线性代数与矩阵之雅可比矩阵与海森矩阵</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5%E4%B8%8E%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5/</link><pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5%E4%B8%8E%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5/</guid><description>
&lt;h2 id="jacobian矩阵和hessian矩阵">Jacobian矩阵和Hessian矩阵&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#jacobian">Jacobian&lt;/a>&lt;/li>
&lt;li>&lt;a href="#jacobian矩阵">Jacobian矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#jacobian行列式">Jacobian行列式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#hessian矩阵">Hessian矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#hessian矩阵对驻点影响">Hessian矩阵对驻点影响&lt;/a>&lt;/li>
&lt;li>&lt;a href="#海森矩阵在牛顿法中的应用">海森矩阵在牛顿法中的应用&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="jacobian">Jacobian&lt;/h2>
&lt;p>在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式. 还有, 在代数几何中, 代数曲线的雅可比量表示雅可比簇：伴随该曲线的一个代数群, 曲线可以嵌入其中。&lt;strong>Jacobian矩阵（行列式）和一阶导数（梯度、偏导数）相关&lt;/strong>。&lt;/p>
&lt;h3 id="jacobian矩阵">Jacobian矩阵&lt;/h3>
&lt;p>雅可比矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近. 因此, 雅可比矩阵类似于多元函数的导数.&lt;/p>
&lt;p>假设&lt;span class="math">\(F: R_n→R_m\)&lt;/span>是一个从欧式n维空间转换到欧式m维空间的函数. 这个函数由m个实函数组成: y1(x1,…,xn), …, ym(x1,…,xn). 这些函数的偏导数(如果存在)可以组成一个m行n列的矩阵(因变量为行，自变量为列), 这就是所谓的雅可比矩阵： &lt;span class="math">\[\begin{bmatrix} \frac{\partial y_1}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial y_1}{\partial x_n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{\partial y_m}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial y_m}{\partial x_n} \end{bmatrix}\]&lt;/span>&lt;/p>
&lt;p>此矩阵表示为:&lt;span class="math">\({J_F}({x_1}, \ldots ,{x_n})\)&lt;/span>,或者&lt;span class="math">\(\frac{{\partial ({y_1}, \ldots ,{y_m})}}{{\partial ({x_1}, \ldots ,{x_n})}}\)&lt;/span>&lt;/p>
&lt;p>这个矩阵的第i行是由&lt;strong>梯度函数&lt;/strong>的转置yi(i=1,…,m)表示的.&lt;/p>
&lt;p>如果&lt;span class="math">\(P\)&lt;/span>是&lt;span class="math">\(R_n\)&lt;/span>中的一点, &lt;span class="math">\(F\)&lt;/span>在&lt;span class="math">\(P\)&lt;/span>点可微分, 那么在这一点的导数由&lt;span class="math">\(J_F(p)\)&lt;/span>给出(这是求该点导数最简便的方法). 在此情况下, 由&lt;span class="math">\(F(p)\)&lt;/span>描述的线性算子即接近点&lt;span class="math">\(P\)&lt;/span>的&lt;span class="math">\(F\)&lt;/span>的最优线性逼近, &lt;span class="math">\(\mathbf{x}\)&lt;/span>逼近于&lt;span class="math">\(P\)&lt;/span>,则有: &lt;span class="math">\[F({\bf{x}}) \approx F({\bf{p}}) + {J_F}({\bf{p}}) \cdot ({\bf{x}} – {\bf{p}})\]&lt;/span>&lt;/p>
&lt;h3 id="jacobian行列式">Jacobian行列式&lt;/h3>
&lt;p>如果m = n, 那么&lt;span class="math">\(F\)&lt;/span>是从n维空间到n维空间的函数, 且它的雅可比矩阵是一个方块矩阵. 于是我们可以&lt;strong>取它的行列式&lt;/strong>, 称为Jacobian行列式.&lt;/p>
&lt;p>在某个给定点的雅可比行列式提供了 在接近该点时的表现的重要信息. 例如, 如果连续可微函数&lt;span class="math">\(F\)&lt;/span>在&lt;span class="math">\(P\)&lt;/span>点的雅可比行列式不是零, 那么它在该点附近具有反函数. 这称为反函数定理. 更进一步, 如果&lt;span class="math">\(P\)&lt;/span>点的雅可比行列式是正数, 则&lt;span class="math">\(F\)&lt;/span>在&lt;span class="math">\(P\)&lt;/span>点的取向不变；如果是负数, 则&lt;span class="math">\(F\)&lt;/span>的取向相反. 而从雅可比行列式的绝对值, 就可以知道函数&lt;span class="math">\(F\)&lt;/span>在&lt;span class="math">\(P\)&lt;/span>点的缩放因子；这就是为什么它出现在换元积分法中.&lt;/p>
&lt;p>对于取向问题可以这么理解, 例如一个物体在平面上匀速运动, 如果施加一个正方向的力F, 即取向相同, 则加速运动, 类比于速度的导数加速度为正；如果施加一个反方向的力F, 即取向相反, 则减速运动, 类比于速度的导数加速度为负.&lt;/p>
&lt;h2 id="hessian矩阵">Hessian矩阵&lt;/h2>
&lt;p>在数学中, 海森矩阵(Hessian matrix或Hessian)是一个自变量为向量的&lt;strong>实值&lt;/strong>函数（区别Jacobian矩阵）的二阶偏导数组成的方块矩阵, 此函数如下： &lt;span class="math">\[f(x_1,x_2,\ldots,x_n)\]&lt;/span> 如果&lt;span class="math">\(f\)&lt;/span>的所有二阶导数都存在, 那么&lt;span class="math">\(f\)&lt;/span>的海森矩阵即： &lt;span class="math">\[H{(f)_{ij}}(x) = {D_i}{D_j}f(x)\]&lt;/span> 其中&lt;span class="math">\(x=(x_1,x_2,\ldots,x_n)\)&lt;/span>，即&lt;span class="math">\(H(f)\)&lt;/span>为： &lt;span class="math">\[\begin{bmatrix}\frac{\partial^2 f}{\partial x_1^2} &amp;amp; \frac{\partial^2 f}{\partial x_1\,\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial x_1\,\partial x_n} \\ \\
\frac{\partial^2 f}{\partial x_2\,\partial x_1} &amp;amp; \frac{\partial^2 f}{\partial x_2^2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial x_2\,\partial x_n} \\ \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \\
\frac{\partial^2 f}{\partial x_n\,\partial x_1} &amp;amp; \frac{\partial^2 f}{\partial x_n\,\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial x_n^2}\end{bmatrix}\]&lt;/span> (也有人把海森定义为以上矩阵的行列式)海森矩阵被应用于牛顿法解决的大规模优化问题.&lt;/p>
&lt;h3 id="hessian矩阵对驻点影响">Hessian矩阵对驻点影响&lt;/h3>
&lt;p>Hessian矩阵我们已经知道是二阶导数矩阵，有时候二阶导数仍然带有未知数，所以求给定点的Hessian矩阵才有意义，给定坐标后，Hessain矩阵变成常数矩阵，然后就可以求其特征值&lt;/p>
&lt;p>1.如果Hessian矩阵所有特征值均为正：开口向上凹的点&lt;br />&lt;img src="../images/hessian_matrix_1.png" alt="hessian_matrix_1" />&lt;/p>
&lt;p>2.如果均为负：开口向下凹的点&lt;br />&lt;img src="../images/hessian_matrix_2.png" alt="hessian_matrix_2" />&lt;/p>
&lt;p>3.如果有正有负：存在鞍点&lt;br />&lt;img src="../images/hessian_matrix_3.png" alt="hessian_matrix_3" />&lt;/p>
&lt;p>4.如果有一项为0：不确定情况。&lt;/p>
&lt;h3 id="海森矩阵在牛顿法中的应用">海森矩阵在牛顿法中的应用&lt;/h3>
&lt;p>一般来说, &lt;strong>牛顿法&lt;/strong>主要应用在两个方面, 1, 求方程的根; 2, 最优化.&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>求方程的根&lt;/p>
&lt;p>牛顿法的最初提出是用来求解方程的根问题。并不是所有的方程都有求根公式, 或者求根公式很复杂, 导致求解困难. 利用牛顿法, 可以迭代求解. 原理是利用泰勒公式, 在&lt;span class="math">\(x_0\)&lt;/span>处展开, 且展开到一阶, 即&lt;span class="math">\(f(x)=f(x_0)+(x–x_0)f&amp;#39;(x_0)\)&lt;/span> 求解方程&lt;span class="math">\(f(x)=0\)&lt;/span>, 即&lt;span class="math">\(f(x_0)+(x–x_0)f&amp;#39;(x_0)=0\)&lt;/span>, 解&lt;span class="math">\(x=x_1=x_0–f(x_0)/f&amp;#39;(x_0)\)&lt;/span>, 因为这是利用泰勒公式的一阶展开.但是&lt;span class="math">\(f(x)与f(x_0)+(x–x_0)f&amp;#39;(x_0)\)&lt;/span>处并不是完全相等, 而是近似相等, 这里求得的&lt;span class="math">\(x_1\)&lt;/span>并不能让&lt;span class="math">\(f(x)=0\)&lt;/span>, 只能说&lt;span class="math">\(f(x_1)\)&lt;/span>的值比&lt;span class="math">\(f(x_0)\)&lt;/span>更接近&lt;span class="math">\(f(x)=0\)&lt;/span>, 于是乎, 迭代求解的想法就很自然了, 可以进而推出&lt;span class="math">\(x_{n+1}=x_n–f(x_n)/f&amp;#39;(x_n)\)&lt;/span>, 通过迭代, 这个式子必然在&lt;span class="math">\(f(x∗)=0\)&lt;/span>的时候收敛. 整个过程如下图：&lt;/p>
&lt;img src="../images/newton_root.gif" alt="newton_root.gif" />
&lt;center>
图1牛顿法求根
&lt;/center>&lt;/li>
&lt;li>&lt;p>最优化&lt;/p>
&lt;p>在最优化的问题中, 线性最优化至少可以使用单纯形法(或称不动点算法)求解, 但对于非线性优化问题, 牛顿法提供了一种求解的办法. 假设任务是优化一个目标函数&lt;span class="math">\(f\)&lt;/span>, 求函数&lt;span class="math">\(f\)&lt;/span>的极大极小问题, 可以转化为求解函数&lt;span class="math">\(f\)&lt;/span>的导数&lt;span class="math">\(f&amp;#39;=0\)&lt;/span>的问题, 这样求可以把优化问题看成方程求解问题(&lt;span class="math">\(f&amp;#39;=0\)&lt;/span>). 剩下的问题就和第一部分提到的牛顿法函数的根很相似了.&lt;/p>
&lt;p>这次为了求解&lt;span class="math">\(f&amp;#39;=0\)&lt;/span>的根, 首先把&lt;span class="math">\(f(x)\)&lt;/span>在探索点&lt;span class="math">\(x_n\)&lt;/span>处泰勒展开, 展开到2阶形式进行近似： &lt;span class="math">\[f(x) = f({x_n}) + f&amp;#39;({x_n})(x – {x_n}) + \frac{{f&amp;#39;&amp;#39;({x_n})}}{2}{(x – {x_n})^2}\]&lt;/span> 相对于&lt;span class="math">\(f(x)\)&lt;/span>的二阶展开，我们现在求的是&lt;span class="math">\(f&amp;#39;=0\)&lt;/span>的根，是&lt;span class="math">\(f&amp;#39;(x)\)&lt;/span>的一阶展开，所以用以下展开式： &lt;span class="math">\[f&amp;#39;(x)=f&amp;#39;(x_0)+f&amp;#39;&amp;#39;(x_0)(x-x_0)\]&lt;/span> 根据牛顿法求解函数的根，我们同理可得到： &lt;span class="math">\[{x_{n + 1}} = {x_n}{\rm{ – }}\frac{{f&amp;#39;({x_n})}}{{f”({x_n})}},n = 0,1,…\]&lt;/span> 一般认为牛顿法可以利用到曲线本身的信息, 比梯度下降法更容易收敛（迭代更少次数）, 如下图是一个最小化一个目标方程的例子, 红色曲线是利用牛顿法迭代求解, 绿色曲线是利用梯度下降法求解。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/newtonvsgradient.jpg" alt="牛顿法vs梯度法" />&lt;p class="caption">牛顿法vs梯度法&lt;/p>
&lt;/div>
&lt;p>图2牛顿法vs梯度法&lt;/p>
&lt;p>在上面讨论的是2维情况, 高维情况的牛顿迭代公式是： &lt;span class="math">\[x_{n+1}=x_n-[Hf(x_n)]^(-1)\nabla f(x_n),n\geqslant 0\]&lt;/span> 其中&lt;span class="math">\(H\)&lt;/span>是hessian矩阵, 定义见上.&lt;/p>
&lt;p>高维情况依然可以用牛顿迭代求解, 但是问题是Hessian矩阵引入的复杂性, 使得牛顿迭代求解的难度大大增加, 但是已经有了解决这个问题的办法就是Quasi-Newton method, 不再直接计算hessian矩阵, 而是每一步的时候使用梯度向量更新hessian矩阵的近似.&lt;/p>&lt;/li>
&lt;/ol></description></item><item><title>线性代数与矩阵之特征值估计</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%89%B9%E5%BE%81%E5%80%BC%E4%BC%B0%E8%AE%A1/</link><pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%89%B9%E5%BE%81%E5%80%BC%E4%BC%B0%E8%AE%A1/</guid><description>
&lt;h2 id="特征值估计">特征值估计&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#盖尔圆方法">盖尔圆方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#谱半径估计">谱半径估计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#hermite矩阵的rayleigh商方法">Hermite矩阵的Rayleigh商方法&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="盖尔圆方法">盖尔圆方法&lt;/h2>
&lt;p>设&lt;span class="math">\(A=(a_{ij})_{n*n}\)&lt;/span>，称&lt;span class="math">\(A\)&lt;/span>的特征值集合为&lt;span class="math">\(A\)&lt;/span>的谱，特征值中模最大的为&lt;span class="math">\(A\)&lt;/span>的谱半径，记为&lt;span class="math">\(\rho(A)\)&lt;/span> &lt;span class="math">\[\rho(A)=\max_i(\lambda_i)\]&lt;/span> 记 &lt;span class="math">\[R_i=|a_{i1}|+\dotsb+|a_{ii-1}|+|a_{ii+1}|+\dotsb+|a_{in}|（行和）\\
C_i=\{z||z-a_{ii}|≤R_i\}\]&lt;/span> 称为&lt;span class="math">\(A\)&lt;/span>的第&lt;span class="math">\(i\)&lt;/span>个盖尔圆。可以看出，第&lt;span class="math">\(i\)&lt;/span>个盖尔圆以&lt;span class="math">\(a_{ii}\)&lt;/span>为圆心，&lt;span class="math">\(R_i\)&lt;/span>为半径。所有盖尔圆组成&lt;span class="math">\(A\)&lt;/span>的盖尔圆系 &lt;span class="math">\[G=\bigcup_{i=1}^n C_i\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理：矩阵&lt;span class="math">\(A\)&lt;/span>的特征值必定在&lt;span class="math">\(A\)&lt;/span>的盖尔圆系中。&lt;/p>
&lt;/blockquote>
&lt;p>需要注意的是并不是每一个盖尔圆中都有特征值，但是在盖尔圆外必无特征值，例如 &lt;span class="math">\[A=\begin{bmatrix}
-4&amp;amp;-10\\1&amp;amp;6
\end{bmatrix}\]&lt;/span> 第一个盖尔圆&lt;span class="math">\(C_1\)&lt;/span>为-4为圆心，10为半径的盖尔圆，第二个盖尔圆&lt;span class="math">\(C_2\)&lt;/span>为6为圆心，1为半径的盖尔圆。特征值为&lt;span class="math">\(\lambda=-1\plusmn\sqrt{15}\)&lt;/span>。&lt;span class="math">\(C_1\)&lt;/span>中有两个特征值，而&lt;span class="math">\(C_2\)&lt;/span>中没有特征值。&lt;/p>
&lt;p>这样来看，盖尔圆和特征值之间的关系很弱，还是没法估计特征值，因此我们需要进一步探讨盖尔圆和特征值的关系。&lt;/p>
&lt;blockquote>
&lt;p>定义：设&lt;span class="math">\(A\in C^{n*n}\)&lt;/span>，在&lt;span class="math">\(A\)&lt;/span>的&lt;span class="math">\(n\)&lt;/span>个盖尔圆中，有&lt;span class="math">\(k\)&lt;/span>个圆构成一个连通区（相切也算连通）但与其余&lt;span class="math">\(n-k\)&lt;/span>个盖尔圆都不相交，则称这个连通区域为&lt;span class="math">\(k-区\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>我们为什么要考虑连通区域呢？这是因为特征值个数和连通区包含的盖尔圆个数是对应的：&lt;/p>
&lt;blockquote>
&lt;p>定理：&lt;span class="math">\(A\)&lt;/span>的盖尔圆的k-区中&lt;strong>有且仅有&lt;/strong>&lt;span class="math">\(A\)&lt;/span>的k个特征值。&lt;/p>
&lt;/blockquote>
&lt;p>例如： &lt;span class="math">\[A=\begin{bmatrix}
2&amp;amp;1&amp;amp;0\\1&amp;amp;4&amp;amp;0\\0&amp;amp;1&amp;amp;2
\end{bmatrix}，
B=\begin{bmatrix}
0&amp;amp;1.2&amp;amp;0\\0&amp;amp;4&amp;amp;0.5\\0&amp;amp;0.5&amp;amp;1\end{bmatrix}\]&lt;/span>&lt;/p>
&lt;img src="../images/盖尔圆1.png" alt="盖尔圆1" />
&lt;center>
3-区：特征值：2.0000，4.4142，1.5858
&lt;/center>
&lt;img src="../images/盖尔圆2.png" alt="盖尔圆2" />
&lt;center>
2区：特征值0.00000，0.91886；1区：特征值4.08114
&lt;/center>
&lt;blockquote>
&lt;p>推论：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>如果&lt;span class="math">\(A\)&lt;/span>的n个盖尔圆互不相交，则&lt;span class="math">\(A\)&lt;/span>有n个互不相等的特征值。&lt;/li>
&lt;li>如果&lt;span class="math">\(A\)&lt;/span>的n个盖尔圆互不相交，则&lt;span class="math">\(A\)&lt;/span>一定对角相似。&lt;/li>
&lt;li>如果&lt;span class="math">\(A\)&lt;/span>的n个盖尔圆互不相交，则&lt;span class="math">\(A\)&lt;/span>的特征值都是实数。&lt;/li>
&lt;li>由于转置不改变特征值，又综合&lt;span class="math">\(A^T\)&lt;/span>的盖尔圆综合判断。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h3 id="谱半径估计">谱半径估计&lt;/h3>
&lt;blockquote>
&lt;p>定理：谱半径（谱范数）小于等于任一范数。简单的可以用行和范数和列和范数估计其上界。 设&lt;span class="math">\(A=(a_{ij})_{n*n}\)&lt;/span>， &lt;span class="math">\[\rho_1=\max_{1≤i≤n}\{\sum_{j=1}^n|a_{ij}|\}，\rho_2=\max_{1≤j≤n}\{\sum_{i=1}^n|a_{ij}|\}\]&lt;/span> 则&lt;span class="math">\(\rho(A)≤\min(\rho_1,\rho_2)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="hermite矩阵的rayleigh商方法">Hermite矩阵的Rayleigh商方法&lt;/h2>
&lt;blockquote>
&lt;p>定义：设&lt;span class="math">\(A\)&lt;/span>是n阶Hermite矩阵，则&lt;span class="math">\(\forall x ∈C^n，x^HAx ∈ R\)&lt;/span>，可以定义一复变量的实值函数： &lt;span class="math">\[R(x)=\frac{x^HAx}{x^Hx},\forall x \neq 0,x ∈ c^n\]&lt;/span> 称此函数为&lt;span class="math">\(A\)&lt;/span>的Rayleigh商。&lt;/p>
&lt;/blockquote>
&lt;p>需要指出的是Rayleigh商的定义、定理只适用于Hermite矩阵。因为Hermite矩阵&lt;span class="math">\(A\in C^{n*n}\)&lt;/span>的特征值均为实数，所以可以把他们记作（按照大小进行排序）：&lt;span class="math">\(\lambda_{min}=\lambda_n≤\lambda_{n-1}\dotsb≤\lambda_2≤\lambda_1=\lambda_{max}\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>定理：&lt;span class="math">\(\lambda_{min}\)&lt;/span>是Rayleigh商的最小值，&lt;span class="math">\(\lambda_{max}\)&lt;/span>是Rayleigh商的最大值。 &lt;span class="math">\[\lambda_{min}=\min_{x\in C^n,x\neq 0}R(x),\lambda_{max}=\max_{x\in C^n,x\neq 0}R(x)\]&lt;/span>&lt;/p>
&lt;/blockquote></description></item><item><title>线性代数与矩阵之Jordan标准型与相似性</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%9B%B8%E4%BC%BC%E6%80%A7%E4%B8%8Ejordan%E6%A0%87%E5%87%86%E5%9E%8B/</link><pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%9B%B8%E4%BC%BC%E6%80%A7%E4%B8%8Ejordan%E6%A0%87%E5%87%86%E5%9E%8B/</guid><description>
&lt;h2 id="线性代数与矩阵之jordan标准型与相似性">线性代数与矩阵之Jordan标准型与相似性&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#相似矩阵">相似矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相似矩阵的性质">相似矩阵的性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相似矩阵的特征值与特征向量">相似矩阵的特征值与特征向量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相似矩阵与可对角化条件">相似矩阵与可对角化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#jordan标准型">Jordan标准型&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>矩阵之间可以通过相似变换进行转换，这种转换保留了很多不变的性质。如果一个矩阵能够和一个对角矩阵相似，那么研究该矩阵就会简单许多，然而并不是所有的矩阵都可以相似对角化。但是，所有矩阵都可以与Jordan标准型相似。&lt;/p>
&lt;h2 id="相似矩阵">相似矩阵&lt;/h2>
&lt;blockquote>
&lt;p>相似矩阵（英语：similar matrix）是指存在相似关系的矩阵。相似关系是两个矩阵之间的一种等价关系。两个&lt;span class="math">\(n × n\)&lt;/span>矩阵&lt;span class="math">\(A\)&lt;/span>与&lt;span class="math">\[为相似矩阵当且仅当存在一个$n × n$的可逆矩阵$P$，使得：
\]&lt;/span>P^{{-1}}AP=B&lt;span class="math">\($ \)&lt;/span>P&lt;span class="math">\(被称为矩阵\)&lt;/span>A&lt;span class="math">\(与\)&lt;/span>B$之间的相似变换矩阵。&lt;/p>
&lt;/blockquote>
&lt;p>相似变换是矩阵之间的一种等价关系，也就是说满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>反身性：任意矩阵都与其自身相似。&lt;/li>
&lt;li>对称性：如果&lt;span class="math">\(A\)&lt;/span>和&lt;span class="math">\(B\)&lt;/span>相似，那么&lt;span class="math">\(B\)&lt;/span>也和&lt;span class="math">\(A\)&lt;/span>相似。&lt;/li>
&lt;li>传递性：如果&lt;span class="math">\(A\)&lt;/span>和&lt;span class="math">\(B\)&lt;/span>相似，&lt;span class="math">\(B\)&lt;/span>和&lt;span class="math">\(C\)&lt;/span>相似，那么&lt;span class="math">\(A\)&lt;/span>也和&lt;span class="math">\(C\)&lt;/span>相似。&lt;/li>
&lt;/ol>
&lt;h3 id="相似矩阵的性质">相似矩阵的性质&lt;/h3>
&lt;p>相似矩阵保留了矩阵的许多性质，因此许多对矩阵性质的研究可以通过研究更简单的相似矩阵而得到解决。&lt;/p>
&lt;p>两个相似的矩阵相同的性质主要有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>两者拥有同样的特征多项式。&lt;/li>
&lt;li>两者的秩相等。&lt;/li>
&lt;li>两者的行列式值相等。&lt;/li>
&lt;li>两者的迹数相等。&lt;/li>
&lt;li>两者拥有同样的特征值，尽管相应的特征向量一般不同。&lt;/li>
&lt;/ol>
&lt;p>其实，后面四个性质都可以算是第一个性质的推论，我们在下一小节证明。&lt;/p>
&lt;p>两个相似的矩阵可以看做是同一个线性变换的「两面」，即在两个不同的基下的表现。因此，在给定了矩阵&lt;span class="math">\(A\)&lt;/span>后，只要能找到一个与之相似而又足够「简单」的「规范形式」&lt;span class="math">\(B\)&lt;/span>，那么对A的研究就可以转化为对更简单的矩阵&lt;span class="math">\(B\)&lt;/span>的研究。&lt;/p>
&lt;h3 id="相似矩阵的特征值与特征向量">相似矩阵的特征值与特征向量&lt;/h3>
&lt;p>我们在相似矩阵性质中最常提到相似矩阵拥有同样的特征值，这其实是第1个性质：两者拥有同样的特征多项式的必然结果。我们从矩阵行列式的角度来证明。&lt;/p>
&lt;p>证明：&lt;/p>
&lt;blockquote>
&lt;p>假设&lt;span class="math">\(n\)&lt;/span>阶矩阵&lt;span class="math">\(A,B\)&lt;/span>相似，&lt;span class="math">\(A\sim B\)&lt;/span>，其特征多项式分别为： &lt;span class="math">\[f(\lambda)=|A-\lambda I|\\g(\lambda)=|B-\lambda I|\]&lt;/span> 由二者相似可知，存在可逆矩阵&lt;span class="math">\(P\)&lt;/span>，使得&lt;span class="math">\(B=P^{-1}AP\)&lt;/span>，即 &lt;span class="math">\[|B-\lambda I|=|(P^{-1}AP)-\lambda I|=|(P^{-1}AP)-\lambda P^{-1}IP|\\
=|P^{-1}(A-\lambda I)P|\]&lt;/span> 根据行列式性质，我们可以将其中的&lt;span class="math">\(P,P^{-1}\)&lt;/span>提出来： &lt;span class="math">\[|P^{-1}(A-\lambda I)P|=|P^{-1}| |A-\lambda I| |P|=|A-\lambda I|\]&lt;/span> 这是因为行列式的计算结果都是数字，且&lt;span class="math">\(|P^{-1}||P|=1\)&lt;/span>，综上可得 &lt;span class="math">\[f(\lambda)=|A-\lambda I|=|B-\lambda I|=g(\lambda)\]&lt;/span> 即矩阵&lt;span class="math">\(A,B\)&lt;/span>拥有相同的特征多项式，因此他们的的特征值也相同。&lt;/p>
&lt;/blockquote>
&lt;p>由于相似矩阵的特征多项式一样，他们的特征值也一样。而特征值是否为0决定了矩阵的秩，因此二者秩相等；特征值的和等于矩阵的迹，因此二者迹相等；特征值的积等于行列式值，因此两者的行列式值相等。&lt;/p>
&lt;p>相似矩阵在不改变特征值的时候，特征向量有什么变化呢？&lt;/p>
&lt;p>由于&lt;span class="math">\(B=P^{-1}AP\)&lt;/span>中，那么对于&lt;span class="math">\(B\)&lt;/span>的特征值&lt;span class="math">\(\lambda\)&lt;/span>与相应的特征向量&lt;span class="math">\(x_B\)&lt;/span>有： &lt;span class="math">\[
Bx_B=\lambda x_B
\]&lt;/span> 将&lt;span class="math">\(B=P^{-1}AP\)&lt;/span>代入得： &lt;span class="math">\[
\lambda x_B=Bx_B=P^{-1}APx_B
\]&lt;/span> 两边同时左乘&lt;span class="math">\(P\)&lt;/span>： &lt;span class="math">\[
\lambda(Px_B)=PP^{-1}APx_B=A(Px_B)
\]&lt;/span> 由于&lt;span class="math">\(\lambda\)&lt;/span>也是&lt;span class="math">\(A\)&lt;/span>的特征值，那么&lt;span class="math">\(x_A=Px_B\)&lt;/span>就是&lt;span class="math">\(A\)&lt;/span>的特征向量。&lt;/p>
&lt;h3 id="相似矩阵与可对角化条件">相似矩阵与可对角化条件&lt;/h3>
&lt;p>首先我们引入一个定义：（详细有关正规矩阵的内容见笔记&lt;a href="线性代数与矩阵之正规矩阵.md">线性代数与矩阵之正规矩阵&lt;/a>）&lt;/p>
&lt;blockquote>
&lt;p>正规矩阵：在数学中，正规矩阵（英语：normal matrix）&lt;span class="math">\(A\)&lt;/span>是与自己的共轭转置满足交换律的复系数方块矩阵，也就是说，&lt;span class="math">\(A\)&lt;/span>满足 &lt;span class="math">\[A^\ast A=AA^\ast\]&lt;/span> 其中&lt;span class="math">\(A\ast\)&lt;/span>是&lt;span class="math">\(A\)&lt;/span>的共轭转置。如果&lt;span class="math">\(A\)&lt;/span>是实系数矩阵，则&lt;span class="math">\(A^\ast=A^T\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>可对角化条件：&lt;/p>
&lt;p>可对角化的矩阵一定是正规矩阵吗？&lt;/p>
&lt;p>不一定。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>矩阵可对角化，意思就是矩阵与对角阵相似。&lt;/li>
&lt;li>矩阵与对角阵相似，分为正交（酉）相似、和非正交（酉）相似。&lt;/li>
&lt;li>正交（酉）相似于对角阵，当且仅当是正规矩阵。&lt;/li>
&lt;li>非正规矩阵，可以与对角阵相似，但不是正交（酉）相似。&lt;/li>
&lt;/ol>
&lt;p>关于正交相似：在线性代数中，实对称矩阵一定可以对角化，而且是正交对角化，也叫正交相似于对角阵。&lt;/p>
&lt;h2 id="jordan标准型">Jordan标准型&lt;/h2>
&lt;p>如果把矩阵转换成其相似的对角阵，那么矩阵的研究会简化许多。然而，可惜的是并不是所有矩阵都可以相似对角化。只有正规矩阵可以相似对角化，那么有没有另一种简单而标准的形态，使得矩阵都可与之相似呢？&lt;/p>
&lt;p>答案是Jordan标准型。&lt;/p>
&lt;blockquote>
&lt;p>Jordan标准型：是一种分块对角矩阵。 &lt;span class="math">\[
J={\begin{bmatrix}J_{1}&amp;amp;\;&amp;amp;\;\\\;&amp;amp;\ddots &amp;amp;\;\\\;&amp;amp;\;&amp;amp;J_{p}\end{bmatrix}}
\]&lt;/span> 其中，每一个分块矩阵&lt;span class="math">\(J_i\)&lt;/span>都具备一种很简单的形状 &lt;span class="math">\[
J_{i}=\begin{bmatrix}
\lambda_{i}&amp;amp;1&amp;amp;\;&amp;amp;\;\\
\;&amp;amp;\lambda_{i}&amp;amp;\ddots &amp;amp;\;\\
\;&amp;amp;\;&amp;amp;\ddots &amp;amp;1\\
\;&amp;amp;\;&amp;amp;\;&amp;amp;\lambda_{i}
\end{bmatrix}
\]&lt;/span> 其中主对角线上都是同一个系数，而对角线上方一排全是1。形同以上&lt;span class="math">\(J_{i}\)&lt;/span>的矩阵称为Jordan矩阵。而矩阵&lt;span class="math">\(J\)&lt;/span>中每一个这样的小块被称为Jordan块。&lt;/p>
&lt;/blockquote>
&lt;p>线性代数中有如下的结果：&lt;/p>
&lt;p>对任意系数域为&lt;span class="math">\(\mathbb {K}\)&lt;/span>，例如实数域，复数域的矩阵&lt;span class="math">\(M\)&lt;/span>，只要其特征值都在&lt;span class="math">\(\mathbb {K}\)&lt;/span>中，就存在一个与之相似的Jordan标准型&lt;span class="math">\(J：M=PJP^{{-1}}\)&lt;/span>，其中&lt;span class="math">\(P\)&lt;/span>是一个可逆矩阵。并且满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>矩阵&lt;span class="math">\(J\)&lt;/span>的特征值（计入重数）就是主对角线上的系数。&lt;/li>
&lt;li>对于&lt;span class="math">\(J\)&lt;/span>的一个特征值&lt;span class="math">\(\lambda_i\)&lt;/span>，它的几何重数就是属于特征值&lt;span class="math">\(\lambda_i\)&lt;/span>的Jordan块的个数。&lt;/li>
&lt;li>所有属于特征值&lt;span class="math">\(\lambda_i\)&lt;/span>的Jordan块的维数之和是特征值&lt;span class="math">\(\lambda_i\)&lt;/span>的代数重数。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Jordan定理：任意&lt;span class="math">\(n\)&lt;/span>阶矩阵&lt;span class="math">\(A\)&lt;/span>都与一个Jordan矩阵&lt;span class="math">\(J\)&lt;/span>相似。Jordan矩阵中的每一个Jordan块对应一个特征向量。若矩阵具有&lt;span class="math">\(n\)&lt;/span>个不同的特征向量，则可以对角化，此时其Jordan标准型&lt;span class="math">\(J\)&lt;/span>就是对角矩阵&lt;span class="math">\(Λ\)&lt;/span>。若出现重特征值，则特征向量个数可能变少，减少的数量取决于Jordan块大小。&lt;/p>
&lt;/blockquote>
&lt;p>Jordan标准型虽然很好，但是将矩阵转换为Jordan标准型并不是一个容易的过程。因此，Jordan标准型的应用也受到了限制。&lt;/p></description></item><item><title>线性代数与矩阵之理解向量、线性变换与矩阵乘法</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%90%86%E8%A7%A3%E5%90%91%E9%87%8F%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/</link><pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%90%86%E8%A7%A3%E5%90%91%E9%87%8F%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/</guid><description>
&lt;h2 id="线性代数与矩阵之理解向量线性变换与矩阵乘法">线性代数与矩阵之理解向量、线性变换与矩阵乘法&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#三个需要认可的前提">三个需要认可的前提&lt;/a>&lt;/li>
&lt;li>&lt;a href="#向量的表示及其分解">向量的表示及其分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性组合与基">线性组合与基&lt;/a>&lt;/li>
&lt;li>&lt;a href="#空间的基">空间的基&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性变换与矩阵">线性变换与矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性变换">线性变换&lt;/a>&lt;/li>
&lt;li>&lt;a href="#用矩阵描述线性变换">用矩阵描述线性变换&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从基变换的角度再看矩阵">从基变换的角度再看矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩阵乘以矩阵">矩阵乘以矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从左边乘一个矩阵">从左边乘一个矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从右边乘一个矩阵">从右边乘一个矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bp-1ap">&lt;span class="math">\(B=P^{-1}AP\)&lt;/span>&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tips">Tips&lt;/a>&lt;/li>
&lt;li>&lt;a href="#n维空间有多少个向量">N维空间有多少个向量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#逆矩阵">逆矩阵&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="三个需要认可的前提">三个需要认可的前提&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>对于空间，只考虑拓扑维数，不考虑分形维数。 （像这种“不说人话”的描述其实无助于理解，其实就是想说，我们考虑的空间都是自然数维度的，零维、一维、二维、三维、四维……） &lt;img src="../images/空间维数.svg" alt="空间维数" />&lt;/li>
&lt;li>事物是客观存在的，对事物的观测是主观的。 对于同一个事物，如果观测的角度、方式等不同，会得出不同的结果。 &lt;img src="../images/多角度观测.jpg" alt="多角度观测" />&lt;/li>
&lt;li>运动是相对的。比如右下图，如果要以地球为坐标系，太阳系天体实际上运动是复杂且诡异的。但是也不能说右下图的描述不对或无用，只是参考坐标系不一样。比如天球坐标系是位置天文学上很实用的工具。 &lt;embed src="../images/日心说地心说.webp" />&lt;/li>
&lt;/ol>
&lt;h2 id="向量的表示及其分解">向量的表示及其分解&lt;/h2>
&lt;p>到了这里，别忘了这篇文章不是讲哲学，也不敢尝试探究过深的问题，只是谈谈自己对向量、线性变化、矩阵的理解。&lt;/p>
&lt;p>首先说的是最基础的向量(不做特殊说明时一般都以列向量的形式表示)。对于向量的理解，各有各的说法。向量可以是任何东西，只需要保证：两个&lt;strong>向量相加及数字与向量相乘&lt;/strong>是有意义的即可。向量的加法和数乘是两个基础运算贯穿始终。而所有这些向量组成的集合，称为&lt;strong>空间&lt;/strong>。&lt;a href="#N维空间有多少个向量">Tips：N维空间有多少个向量？&lt;/a>&lt;/p>
&lt;p>&lt;img src="../images/向量是什么.png" alt="向量是什么" /> 截图自&lt;a href="https://www.bilibili.com/video/av6731067?p=2">线性代数的本质-系列合集&lt;/a>&lt;/p>
&lt;p>我们主要关注的是，我们是如何描述一个向量的。还记得，我们的第一个前提吗？“我们考虑的空间都是自然数维度的”，&lt;strong>维度&lt;/strong>是指空间中独立参数的数目，也就是我们用来描述空间中&lt;strong>所有&lt;/strong>向量&lt;strong>最少&lt;/strong>需要的参数个数。比如，在二维平面中，我们至少需要两个量&lt;span class="math">\([a_1,a_2]^T\)&lt;/span>来描述空间中&lt;strong>任一个点&lt;/strong>的位置，类似的，在三维空间中至少需要3个量&lt;span class="math">\([a_1,a_2,a_3]^T\)&lt;/span>。&lt;/p>
&lt;p>线性代数描述的三维空间中，我们使用3个数字表示一个向量，例如&lt;span class="math">\([a_1,a_2,a_3]^T\)&lt;/span>，我们在使用这个3个数字的时候，经常只关注到数值，并没有关心3个数的顺序，也是说，不仅仅数值中含有空间向量的信息，他们的顺序也是含有信息的，顺序+数值=空间向量。但是，回过头来说，这3个数字+顺序到底代表这什么呢？&lt;/p>
&lt;p>现实中，我们如何描述某地的位置呢？假设我们需要把一个快递送到指定位置，如下图的中航广场4楼。如果超人，可以无视地理环境沿着绿色的箭头直接飞过去就行了。但实际上，送快递的活计都是凡人在做，所以我们得先向前走400m，然后左转再走200m，最后坐电梯向上10m。有意思的地方来了，虽然我们无法直接飞达目的地，但是我们能够通过麻烦一点的方法，向着三个方向走三次，到达同样的目的地。这两种方式异曲同工。从另一个角度来讲，&lt;strong>把绿色向量这个向量可以拆分成三个红色向量的和（向量的加法）&lt;/strong>。而且，我们在描述这3个方向的步骤时，可以换一种更细碎的描述：&lt;em>先向前走1m，然后沿着这个方向走，直到走到1m的400倍；然后向左走1米，沿着向左的方向走直到走到1m的200倍；最后坐电梯向上1m，沿着向上的方向坐电梯直到1m的10倍&lt;/em>。也就是说，我们沿某一个方向行进时，可以行进某一特定长度的倍数（&lt;strong>向量的数乘&lt;/strong>），如果每个方向都取一样的特定长度，那我们就把这个方向与特定长度的组合称为&lt;strong>单位向量&lt;/strong>（例子中特定长度不一定非要是1，其他值也行）。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/三维坐标.png" alt="三维坐标.png" />&lt;p class="caption">三维坐标.png&lt;/p>
&lt;/div>
&lt;p>现在，我们知道可以把在三维空间中的1个向量，分解成3个单位向量&lt;strong>分别&lt;/strong>乘以其倍数的和（向量加法与数乘的线性组合）。那么能不能分成2个向量之和呢？当然可以！如果我们把向左200m的向量和向上10m的（红色）向量，变成1个等效的向量比如向左上方&lt;span class="math">\(\sqrt{200^2+10^2}m\)&lt;/span>（黄色的向量）就可以了啊。但是现在问题来了，收快递的人说我现在不在4楼，被发配到地下1层了，需要坐电梯向下3m到负1层。那么按照&lt;span class="math">\([向前,向左上]\)&lt;/span>的2个向量行进方式，是无论如何也到不了中航广场地下1层的。然而，原来用3个向量的分解方式，只要说在最后一个方向上，沿着向上的方向坐电梯直到1m的-3倍，那么还是能够完成送达的任务。总结来说，如果是分成2个方向，换了一个地点，那么方向大概率也得换（这个概率→1，因为三维空间中的二维平面测度是0，所以目标点恰好落在两个特定向量组成的平面的概率也是0）；而分成3个方向，依旧保证只沿着3个方向目标可达。因此，在我们生活的三维空间中，&lt;strong>所有&lt;/strong>位置都可以用&lt;strong>至少&lt;/strong>3个单位向量&lt;strong>分别&lt;/strong>乘以其倍数的和到达。在三维空间中用至少用3个向量是为了保证组成向量的&lt;strong>任意性&lt;/strong>。&lt;/p>
&lt;p>回顾一下，之前说三维空间中至少需要3个量&lt;span class="math">\([a_1,a_2,a_3]^T\)&lt;/span>来描述空间中&lt;strong>任一个点&lt;/strong>的位置，我们把这个向量换一种表述方式，即&lt;span class="math">\(a_1\vec{x}+a_2\vec{y}+a_3\vec{z}\)&lt;/span>。是不是就是上例中的3个单位向量&lt;strong>分别&lt;/strong>乘以其倍数的和？3个数字，分别对应了在3个方向上的行进倍数，因此它们的顺序是不能调换的。这3个单位向量的选择不是唯一的，只要他们&lt;strong>线性无关&lt;/strong>即可。这个概念下节会说的，先按下不表。我们同样可以想象：N维空间所有点至少需要N个向量来表示，可数无穷维空间所有点至少需要可数无穷个向量来表示。&lt;/p>
&lt;p>现在总结一下这一节的内容，如何表示向量呢？在N维空间中，可以用N个有序的数字表示&lt;span class="math">\([a_1,a_2,\dots,a_N]^T\)&lt;/span>，这是一种方便的描述方式。而这个向量，一定可以分解成N个单位向量的特定倍数的和。&lt;/p>
&lt;h2 id="线性组合与基">线性组合与基&lt;/h2>
&lt;p>上一节中，提到一个向量可以分解成&lt;strong>多个单位向量特定倍数的和&lt;/strong>，即&lt;span class="math">\(\vec{v}=a_1\vec{v_1}+a_2\vec{v_2}+a_3\vec{v_3}+\dotsb\)&lt;/span>。总是这么描述很费劲，因此，我们给他起个名字，叫它&lt;strong>线性组合&lt;/strong>。&lt;/p>
&lt;p>所谓线性，&lt;em>我觉得&lt;/em>是一种&lt;strong>平直均匀&lt;/strong>的特性（记住以后还会提到）。具体来看，其实主要是两条，&lt;strong>一个是数乘，一个是相加&lt;/strong>。假设一组向量只包含这两种运算，那我们就可以把他叫做线性组合。写成符号语言就是：&lt;span class="math">\(a\vec{u}+b\vec{v}+c\vec{w}\)&lt;/span>。N维空间所有点至少需要N个向量来表示。也就是说，一个N维空间中的任意向量，一定能表示成至少N个单位向量的线性组合。&lt;/p>
&lt;p>那么换个角度来看，N个单位向量的线性组合可以表示多少个向量呢？&lt;/p>
&lt;p>首先，我们先说明向量&lt;strong>张成&lt;/strong>的空间。我们把&lt;strong>所有&lt;/strong>可以表示为给定向量线性组合的&lt;strong>向量的集合&lt;/strong>，被称为给定向量&lt;strong>张成的空间&lt;/strong>。&lt;strong>张成&lt;/strong>英文原文为“span”，有包括；遍及的意思（If something spans a range of things, all those things are included in it. 柯林斯高阶英汉双解词典）。张成二字可以理解为由N个单位向量&lt;strong>所有的线性组合&lt;/strong>。这样就好理解了，N个单位向量的线性组合能够表示的向量都在它们所张成的空间中。如果这N个向量可以线性组和成N维空间中的任意一个向量，那么这N个向量张成的空间就等于这个N维空间。&lt;/p>
&lt;p>有人心里又有疑惑：&lt;em>来表示任一N维向量的N个单位向量是否是随便选的？或者说，满足什么条件的N个单位向量才能表示出N维空间的所有向量？&lt;/em>&lt;/p>
&lt;p>之前说过，&lt;em>N维空间所有点至少需要N个向量来表示&lt;/em>。那么选定N个单位向量张成的空间就是N维吗？有时候所选的N个单位向量能被降维成N-1个单位向量，并且张成的向量空间维数（≥N-1维）与降维前一致 ，那么这N个单位向量就无法表示出N维空间的所有向量。也就是说，N个单位向量中存在没用（冗余）的向量，那些用所选的N个单位向量能表示出来的向量，都可以用其中N-1个单位向量表示出来。假如N个单位向量&lt;span class="math">\({\vec{v_1},\dotsb,\vec{v}_N}\)&lt;/span>中，多余的向量是&lt;span class="math">\(\vec{v_r}(1≤r≤N)\)&lt;/span>，那么 &lt;span class="math">\[\begin{aligned}
&amp;amp;\forall \vec{w}=a_1\vec{v}_1+a_2\vec{v}_2+\dotsb+a_N\vec{v}_N\\
&amp;amp;可以改写成：\\
&amp;amp;\vec{w}=b_1\vec{v}_1+\dotsb+b_{r-1}\vec{v}_{r-1}+b_{r+1}\vec{v}_{r+1}+\dotsb+b_N\vec{v}_N
\end{aligned}\]&lt;/span> 由于这两个式子是相等的，即 &lt;span class="math">\[\begin{aligned}
&amp;amp;\quad a_1\vec{v}_1+a_2\vec{v}_2+\dotsb+a_N\vec{v}_N\\
&amp;amp;=b_1\vec{v}_1+\dotsb+b_{r-1}\vec{v}_{r-1}+b_{r+1}\vec{v}_{r+1}+\dotsb+b_N\vec{v}_N \\
&amp;amp;\Rightarrow(a_1-b_1)\vec{v}_1+\dotsb+(a_{r-1}-b_{r-1})\vec{v}_{r-1}\\
&amp;amp;+a_r\vec{v}_r+(a_{r+1}-b_{r+1})\vec{v}_{r+1}+\dotsb+(b_N-a_N)\vec{v}_N=0\\
&amp;amp;(单独列出a_r\vec{v}_r)\\
&amp;amp;\Rightarrow a_r\vec{v}_r=(b_1-a_1)\vec{v}_1+\dotsb+(b_{r-1}-a_{r-1})\vec{v}_{r-1}\\
&amp;amp;+(b_{r+1}-a_{r+1})\vec{v}_{r+1}+\dotsb+(b_N-a_N)\vec{v}_N
\end{aligned}\]&lt;/span> 这个式子最后发现，如果存在多余的单位向量&lt;span class="math">\(\vec{v_r}(1≤r≤N)\)&lt;/span>，那么这个多余的向量必然可以被其他N-1个单位向量线性组合出来。换句话说，他们之间是有关系的。基于这种线性组合关系（加法与数乘），我们称这种关系为&lt;strong>线性相关&lt;/strong>。总结一下，线性相关的两个角度描述：&lt;/p>
&lt;ul>
&lt;li>【表达一】你有多个向量，并且可以移除其中一个而不减小张成的空间，我们称它们（这些向量）线性相关&lt;/li>
&lt;li>【表达二】其中一个向量，可以表示为其他向量的线性组合，因为这个向量已经落在其他向量张成的空间之中&lt;/li>
&lt;/ul>
&lt;p>在几何中，线性相关表现在2D是共线，3D中是共面或共线（比如&lt;span class="math">\([0,0,1]^T,[0,0,2]^T,[0,0,3]^T\)&lt;/span> 3个三维线性相关向量共线。）&lt;/p>
&lt;p>相反的，如果N个单位向量中，任一个向量&lt;span class="math">\(\vec{v_i}\)&lt;/span>都不能表示成其他N-1个向量&lt;span class="math">\(\vec v_1,\vec v_2,\dots,\vec v_{i-1},\vec v_{i+i},\dots,\vec v_N\)&lt;/span>的线性组合，那么这N个向量就是&lt;strong>线性无关&lt;/strong>的，也就是他们之前没有线性组合的关系。&lt;/p>
&lt;h3 id="空间的基">空间的基&lt;/h3>
&lt;p>现在回看问题：满足什么条件的N个单位向量才能表示出N维空间的所有向量？答案就是&lt;strong>线性无关的N个单位向量&lt;/strong>。讲了这么多，我们终于知道满足线性无关要求的N个单位向量能够表示出N维空间所有的向量，这些满足条件的N个单位向量，我们称之为&lt;strong>N维空间的基（Basis）&lt;/strong>。所谓基，以基为砖，万物皆可构筑:dog:。&lt;strong>用基可以构建出空间中任一向量，任一向量也可分解成空间基的线性组合&lt;/strong>。&lt;/p>
&lt;p>相应的，每一组基都是一个极大的线性无关集合，也就是说在N维空间中，找不出N+1个线性无关的向量。因为第N+1个向量必然属于N维空间，而N维空间的所有向量都可以被表示为N个线性无关的单位向量的线性组合，那么第N+1个向量必然也可以是这N个单位向量的线性组合，即是线性相关的。&lt;/p>
&lt;p>最后还要强调一点，大家先再读一遍&lt;a href="#三个需要认可的前提">三个需要认可的前提&lt;/a>的第2条。之前在说基或单位向量的时候，一直避免说基的具体值，比为二维空间中&lt;span class="math">\([1,0]^T,[0,1]^T\)&lt;/span>这样常见的基，是因为不想给大家一个固定印象，认为基的取法是固定的。下图中，左右两幅图中同一个向量在不同基下，表示方式也不同。&lt;/p>
&lt;img src="../images/不同基底对空间中同一向量的描述.png" alt="不同基底对空间中同一向量的描述" />
&lt;center>
不同基底对空间中同一向量的描述
&lt;/center>
&lt;p>在基&lt;span class="math">\([1,0]^T,[0,1]^T\)&lt;/span>下，向量为&lt;span class="math">\([4,5]^T\)&lt;/span>；在基&lt;span class="math">\([\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}]^T,[-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}]^T\)&lt;/span>下，向量为&lt;span class="math">\([\frac{9}{\sqrt{2}},\frac{1}{\sqrt{2}}]^T\)&lt;/span>。向量的值，只是表示对应单位向量在线性组合中的倍数，因此在不同基下&lt;span class="math">\([4,5]^T\)&lt;/span>与&lt;span class="math">\([\frac{9}{\sqrt{2}},\frac{1}{\sqrt{2}}]^T\)&lt;/span>是同一个向量也就不奇怪了。大多数人一开始以为基就是&lt;span class="math">\([1,0]^T,[0,1]^T\)&lt;/span>这样的向量组，是因为它们实在是太常用了，以至于很多情况下默认的基就是这个样子。类似于&lt;span class="math">\({e_1=(1,0,0), e_2=(0,1,0), e_3=(0,0,1)}\)&lt;/span>组成的基&lt;strong>全称是标准正交基&lt;/strong>。&lt;/p>
&lt;p>关于上面那幅不同基底对空间中同一向量的描述的图，留一个思考题： &lt;span class="math">\[\begin{bmatrix}1&amp;amp;0\\0&amp;amp;1\end{bmatrix}\begin{bmatrix}4\\5\end{bmatrix}\overset{?}{=}\begin{bmatrix}\frac{1}{\sqrt{2}}&amp;amp;-\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}&amp;amp;\frac{1}{\sqrt{2}}\end{bmatrix}\begin{bmatrix}\frac{9}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\end{bmatrix}\]&lt;/span> 直观的解释，为什么会相等呢？&lt;/p>
&lt;h2 id="线性变换与矩阵">线性变换与矩阵&lt;/h2>
&lt;p>矩阵，最直观的理解当然是一个写成方阵的数字&lt;span class="math">\(\begin{bmatrix}1&amp;amp;2\\3&amp;amp;4\end{bmatrix}\)&lt;/span>。这一节的核心是为了说明：矩阵从变换的角度来看就是一种线性变换。&lt;/p>
&lt;h3 id="线性变换">线性变换&lt;/h3>
&lt;p>【变换】本质上是【函数】（左）的一种花哨的说法，它接受输入内容，并输出对应结果。那矩阵也是变换吗？是的。以矩阵为变换（右），其过程表示为接收一个向量，然后输出另一个向量如下图。也可以说矩阵是【向量的函数】。&lt;/p>
&lt;p>&lt;img src="../images/变换与函数.gif" width=200px>&lt;/img> &lt;img src="../images/变换与函数2.gif" width=200px>&lt;/img>&lt;/p>
&lt;p>【变换】，直观的解释就是向量从一个地方变到了另一个地方，这暗示了我们可以用运动的方法来理解【向量的函数】这一概念。可以用可视化的方法来展现这组【变换】即输入-输出关系：&lt;/p>
&lt;p>&lt;img src="../images/InputOutput.gif" alt="InputOutput.gif" width=400px>&lt;/img>&lt;/p>
&lt;p>但是通常所说的运动是一个连续的过程，而变换是将向量直接放到另一个地方。有点像瞬移：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/teleport.gif" alt="瞬移" />&lt;p class="caption">瞬移&lt;/p>
&lt;/div>
&lt;p>在变换中，有一类变换特别重要，就是线性变换。几何角度来说，具有以下两个性质的就是线性变换（直观可视化如下图）：&lt;/p>
&lt;img src="../images/LinearTransform.gif" alt="LinearTransform.gif" />
&lt;center>
线性变换的几何演示
&lt;/center>
&lt;ul>
&lt;li>直线在变换后&lt;strong>仍然保持为直线&lt;/strong>，不能有所弯曲（&lt;strong>平直性，这也是线性的直观几何反映&lt;/strong>）。从图上来看，线性变换是“保持网格线平行且等距分布（均匀性）”的变换。&lt;/li>
&lt;li>&lt;strong>原点&lt;/strong>必须保持&lt;strong>固定&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>如果保持保持直线但&lt;strong>原点改变&lt;/strong>就称为：仿射变换（Affine Transformation）。仿射变换就是在线性变换的基础上加了一个偏移量。比如&lt;span class="math">\(y=2x\)&lt;/span>是线性变换，&lt;span class="math">\(y=2x+3\)&lt;/span>就是仿射变换。实际上，N维空间的仿射变换等价于N+1维空间的线性变换。详细理解参见&lt;a href="https://www.matongxue.com/madocs/244/">如何通俗的解释仿射变换？&lt;/a>&lt;/p>
&lt;p>线性变换有个非常重要的特性。如果&lt;span class="math">\(\vec{w}=a_1\vec{v}_1+a_2\vec{v}_2+\dotsb+a_N\vec{v}_N\)&lt;/span>。那么 &lt;span class="math">\[L(\vec{w})=L(a_1\vec{v}_1)+L(a_2\vec{v}_2)+\dotsb+L(a_N\vec{v}_N)\\
=a_1L(\vec{v}_1)+a_2L(\vec{v}_2)+\dotsb+a_NL(\vec{v}_N)\]&lt;/span> 其中&lt;span class="math">\(L(\cdot)\)&lt;/span>代表线性变换。我们在“线性组合和基”那一节说过，线性是一种&lt;strong>平直均匀&lt;/strong>的特性。上面的公式就是说的&lt;strong>均匀&lt;/strong>这一点（平直反映在直线线性变换后仍然时直线）。所谓均匀的特性，就是线性变换对&lt;strong>整体的每一部分变换都是一致的&lt;/strong>，这样的话，我们就可以把整体拆成一个个部分，对每一个部分先做线性变换，然后再合并。在上图“线性变换的几何演示”中，我们可以发现整体的变换和局部是一致的：一个大正方形（四个小正方形组成）变成平四边形等效于其中每个小正方形变成平行四边形再组合。上面的等式是用数学的方式表示，对向量整体的线性变换，等于对组成向量整体的每一部分分别做一样的线性变换再组合。&lt;strong>这就是线性的可加性&lt;/strong>。数乘可以算是可加性的一种特例，就是&lt;span class="math">\(a∈R\)&lt;/span>个向量&lt;span class="math">\(\vec{v}\)&lt;/span>相加，根据上面对线性变换均匀特性的解释，也就容易解释为何&lt;span class="math">\(aL(\vec{v})=L(a\vec{v})\)&lt;/span>了。&lt;strong>这称为线性的齐次性&lt;/strong>。&lt;/p>
&lt;h3 id="用矩阵描述线性变换">用矩阵描述线性变换&lt;/h3>
&lt;p>在开始的时候，我们谈到空间中用一组有序数列的方式描述一个向量。那么在空间中，描述线性变换是什么呢？就是矩阵。直观上来看，矩阵一点也体现不出线性。但是，我们回想一下刚刚所说的，对向量整体的线性变换，等于对组成向量整体的每一部分分别做一样的线性变换再组合。&lt;/p>
&lt;p>这里需要使用上一节提到的工具，空间的基，也就是单位向量。“线性组合与基”一节中，我们已经知道，空间中任一向量可以表示成空间基的线性组合。为了方便描述，我们选取&lt;strong>标准正交基&lt;/strong>来分解向量。例如，二维空间中，有向量&lt;span class="math">\(\vec w\)&lt;/span>分解： &lt;span class="math">\[\vec{w}=\begin{bmatrix}-1\\2\end{bmatrix}=-1*\begin{bmatrix}1\\0\end{bmatrix}+2*\begin{bmatrix}0\\1\end{bmatrix}\]&lt;/span> 令&lt;span class="math">\(e_1=[1,0]^T,e_2=[0,1]^T\)&lt;/span>，所以简写成&lt;span class="math">\(\vec{w}=-1\vec{e}_1+2\vec{e}_2\)&lt;/span>。现在有一个线性变换&lt;span class="math">\(L(\cdot)\)&lt;/span>，则 &lt;span class="math">\[\begin{aligned}
&amp;amp;L(\vec{w})=L(-1\vec{e}_1+2\vec{e}_2)\\
&amp;amp;根据上小节线性变换的特性有：\\
&amp;amp;=-1L(\vec{e}_1)+2L(\vec{e}_2)
\end{aligned}\]&lt;/span> 假设这个线性变换将2个基向量分别变成： &lt;span class="math">\[e_1&amp;#39;=L(\vec{e}_1)=\begin{bmatrix}3\\1\end{bmatrix},e_2&amp;#39;=L(\vec{e}_2)=\begin{bmatrix}1\\2\end{bmatrix}\]&lt;/span> 则&lt;span class="math">\(L(\vec{w})\)&lt;/span>有： &lt;span class="math">\[L(\vec{w})=-1*\begin{bmatrix}3\\1\end{bmatrix}+2*\begin{bmatrix}1\\2\end{bmatrix}\]&lt;/span> 如果我们再把变换后的基写到一起，那么就有 &lt;span class="math">\[[e_1&amp;#39;,e_2&amp;#39;]=\begin{bmatrix}3&amp;amp;1\\1&amp;amp;2\end{bmatrix}\]&lt;/span> 这就构成了一个矩阵。说白了，矩阵是从&lt;strong>局部变换&lt;/strong>的角度描述线性变换，局部就是向量分解成的基向量。也可以说矩阵是从&lt;strong>基变换&lt;/strong>的角度描述线性变换，只需要关注基向量变换后的位置即可。&lt;/p>
&lt;p>上个例子的几何描述如下图所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/线性变换和矩阵.gif" alt="线性变换和矩阵.gif" />&lt;p class="caption">线性变换和矩阵.gif&lt;/p>
&lt;/div>
&lt;p>更加一般的情况，我们用变量来代替其中的具体值：绿色代表&lt;span class="math">\(\vec i\)&lt;/span> 变换后的向量，红色代表&lt;span class="math">\(\vec j\)&lt;/span>变换后的向量 &lt;span class="math">\[\begin{bmatrix} \color{green}{a}&amp;amp;\color{red}b \\ \color{green}c&amp;amp;\color{red}d \end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix}=\underbrace{x \begin{bmatrix}\color{green}a\\\color{green}c \end{bmatrix} + y \begin{bmatrix} \color{red}b\\\color{red}d\end{bmatrix}}_{\text{直观的部分这里}} =\begin{bmatrix} \color{green}{a}\color{black}{x}+\color{red}{b}\color{black}{y}\\\color{green}{c}\color{black}{x}+\color{red}{d}\color{black}{y}\end{bmatrix}\]&lt;/span> 上面的公式就是我们常说的矩阵乘法公式，就是线性变换后基的线性组合。&lt;/p>
&lt;h3 id="从基变换的角度再看矩阵">从基变换的角度再看矩阵&lt;/h3>
&lt;p>还记得我们在上节中如何导出矩阵的吗？是把几个变换后基向量写到了一起。现在到了关键的一步。看上去矩阵就是由一组向量组成的，而且如果矩阵非奇异的话（现在只考虑这种情况，如果是奇异矩阵就是个降维了的坐标系），那么组成这个矩阵的那一组向量也就是线性无关的了，也就可以成为度量线性空间的一个基。结论：&lt;strong>矩阵描述了一个空间的基&lt;/strong>。刚刚不是还说矩阵是变换描述向量的运动吗？怎么变成了基呢？现在回去读读&lt;a href="#三个需要认可的前提">三个需要认可的前提&lt;/a>第3点，实际上物体的运动可以等效成参考系的运动。&lt;/p>
&lt;p>在“线性组合和基”小节的最后，我们留了个思考题： &amp;gt;&lt;span class="math">\[\begin{bmatrix}1&amp;amp;0\\0&amp;amp;1\end{bmatrix}\begin{bmatrix}4\\5\end{bmatrix}\overset{?}{=}\begin{bmatrix}\frac{1}{\sqrt{2}}&amp;amp;-\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}&amp;amp;\frac{1}{\sqrt{2}}\end{bmatrix}\begin{bmatrix}\frac{9}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\end{bmatrix}\]&lt;/span> &amp;gt;直观的解释，为什么会相等呢？&lt;/p>
&lt;p>因为它们只是用不同的基描述同一个向量啊。从这个角度看，矩阵&lt;span class="math">\(M\vec a = \vec b\)&lt;/span>的意思是：有一个向量，它在空间基&lt;span class="math">\(M\)&lt;/span>的度量下得到的度量结果向量为&lt;span class="math">\(\vec a\)&lt;/span>，那么它在标准正交基&lt;span class="math">\(E\)&lt;/span>的度量下，这个向量的度量结果是&lt;span class="math">\(\vec b\)&lt;/span>，而本质上它俩说的是一个向量，所以是相等的。更完整的写法应该是这样：&lt;span class="math">\(M\vec a = E\vec b\)&lt;/span>。&lt;/p>
&lt;p>如果这个角度还不好懂，我再提供一个大神的理解方式&lt;a href="https://www.bilibili.com/video/av6731067/">3B1B的关于线性代数系列视频&lt;/a>，大体内容如下：&lt;/p>
&lt;p>如果假设有一个二维向量，使用标准正交基&lt;span class="math">\(\vec{i}=e_1\)&lt;/span> 和 &lt;span class="math">\(\vec{j}=e_2\)&lt;/span> 来描述是 &lt;span class="math">\(\begin{bmatrix} 3 \\ 2 \end{bmatrix}\)&lt;/span> ，我们把这种描述称为：我们的语言。如果有另一组基向量&lt;span class="math">\(\vec{i}&amp;#39;=\begin{bmatrix}2 \\1\end{bmatrix}\)&lt;/span>和 &lt;span class="math">\(\vec{j}&amp;#39; = \begin{bmatrix} -1 \\ 1 \end{bmatrix}\)&lt;/span> （写成列向量的形式是为了形式上的统一）来描述同样一个向量变成 &lt;span class="math">\(\begin{bmatrix} \frac{5}{3} \\ \frac{1}{3} \end{bmatrix}\)&lt;/span> ，我们把这种语言记为：詹妮弗的语言。显然两种语言描述同一样东西，所以： &lt;span class="math">\[\begin{bmatrix}1&amp;amp;0\\0&amp;amp;1\end{bmatrix}\begin{bmatrix}3\\2\end{bmatrix}=\begin{bmatrix}2&amp;amp;-1\\1&amp;amp;1\end{bmatrix}\begin{bmatrix}5/3\\1/3\end{bmatrix}\]&lt;/span> 在不同的【语言】之间的转化使用矩阵向量乘法，在上面的例子中，转移矩阵是 &lt;span class="math">\(\mathbf T = \begin{bmatrix} 2 &amp;amp; -1 \\ 1 &amp;amp; 1 \end{bmatrix}\)&lt;/span> ，矩阵的列表示用我们的语言表达詹妮弗的基向量，称为基变换。反过来，就是求转移矩阵的逆 &lt;span class="math">\(\mathbf T^{-1}\)&lt;/span> ，称为&lt;strong>基变换矩阵的逆&lt;/strong>，作用是可以表示从詹妮弗的基向量转换回我们的语言需要做的变换。&lt;/p>
&lt;h2 id="矩阵乘以矩阵">矩阵乘以矩阵&lt;/h2>
&lt;p>我们已经知道矩阵是线性变换的一种描述。那么&lt;strong>多次线性变换&lt;/strong>就可以用&lt;strong>多个矩阵&lt;/strong>来描述。举个例子：如果对一个向量先进行一次&lt;em>旋转变换&lt;/em>，再进行一次&lt;em>剪切变换&lt;/em>（ &lt;span class="math">\(\vec{i}\)&lt;/span> 保持不变第一列为&lt;span class="math">\([1,0]^T\)&lt;/span>， &lt;span class="math">\(\vec{j}\)&lt;/span> 移动到坐标&lt;span class="math">\([1,1]^T\)&lt;/span>） ，如下图左半边所示：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/复合变换与矩阵乘法.png" alt="复合变换与矩阵乘法" />&lt;p class="caption">复合变换与矩阵乘法&lt;/p>
&lt;/div>
&lt;p>那么通过旋转矩阵和剪切矩阵两次线性变换的过程是否可以用一个复合线性变换（复合矩阵）来表示呢（上图右边部分）？为了解决这个问题，我们定义这个复合的过程叫做矩阵的乘法（矩阵乘矩阵）。&lt;/p>
&lt;p>在这里我们需要指出，矩阵乘法的变换顺序是&lt;strong>从右往左&lt;/strong>读的（这一个常识很重要），进一步联系和思考发现，和复合函数的形式，如 &lt;span class="math">\(f(g(x))\)&lt;/span> ，是一致的。&lt;/p>
&lt;h3 id="从左边乘一个矩阵">从左边乘一个矩阵&lt;/h3>
&lt;p>之前我们说过，矩阵乘法的变换顺序是&lt;strong>从右往左&lt;/strong>读。从左边乘一个矩阵等效于施加一个线性变换。每从左边乘一个矩阵就是施加一次线性变换。假设有两个矩阵： &lt;span class="math">\[M_1=\begin{bmatrix}a&amp;amp;b\\c&amp;amp;d\end{bmatrix},M_2=\begin{bmatrix}e&amp;amp;f\\g&amp;amp;h\end{bmatrix}\]&lt;/span> 我们先施加线性变换&lt;span class="math">\(M_1\)&lt;/span>，再施加线性变换&lt;span class="math">\(M_2\)&lt;/span>，表达式为&lt;span class="math">\(M_2*M_1(从右向左原则)\)&lt;/span>。通过“线性变换与矩阵”那一节的描述，我们知道&lt;span class="math">\(M_1\)&lt;/span>每一列表示一个线性变换后的新基向量，我们按照列向量的形式重写矩阵&lt;span class="math">\(M_1=[w_1,w_2]\)&lt;/span>，其中&lt;span class="math">\(w_1=[a,c]^T,w_2=[b,d]^T\)&lt;/span>。那么&lt;span class="math">\(M_2*[w_1,w_2]\)&lt;/span>可以看成&lt;span class="math">\(M_2\)&lt;/span>分别和两个向量的线性变换，按照向量分解为基再线性变换的理解方式，分别对&lt;span class="math">\(w_1,w_2\)&lt;/span>进行操作，就能够类推出矩阵乘以矩阵运算规则：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/矩阵乘法.gif" alt="矩阵乘法.gif" />&lt;p class="caption">矩阵乘法.gif&lt;/p>
&lt;/div>
&lt;p>总结，矩阵从左边乘，是线性变换的复合，而我们观察两次线性变换的角度都没有变，都是最开始的标准正交基，每次变换都是在标准正交基的度量下。&lt;/p>
&lt;h3 id="从右边乘一个矩阵">从右边乘一个矩阵&lt;/h3>
&lt;p>先补充一点，矩阵从右边乘以&lt;strong>行向量&lt;/strong>等效于矩阵的&lt;strong>行&lt;/strong>进行线性组合。那么矩阵也可以从右边乘一个矩阵。整体的效果上来看，大多数情况下&lt;span class="math">\(M_1*M\neq M*M_1\)&lt;/span>，如果矩阵从左边乘理解成施加一个线性变换，那么矩阵右乘是什么意思？是观察点的变化，或者说坐标系的变化。如果说左乘矩阵是发现物体&lt;span class="math">\(P\)&lt;/span>有了向东的加速度&lt;span class="math">\(\vec a_1\)&lt;/span>，又有了向北的加速度&lt;span class="math">\(\vec a_2\)&lt;/span>，那么看看其整体的加速度是什么。那么右乘矩阵就是，我看物体&lt;span class="math">\(P\)&lt;/span>有加速度&lt;span class="math">\(\vec b_1\)&lt;/span>，然后别人观察我，认为我有加速度&lt;span class="math">\(\vec b_2\)&lt;/span>，那么那个人看物体&lt;span class="math">\(P\)&lt;/span>的加速度是多少，这就是观察点的变化。&lt;/p>
&lt;h2 id="bp-1ap">&lt;span class="math">\(B=P^{-1}AP\)&lt;/span>&lt;/h2>
&lt;h2 id="tips">Tips&lt;/h2>
&lt;h3 id="n维空间有多少个向量">N维空间有多少个向量&lt;/h3>
&lt;p>N维空间当然有无穷个向量。这是显然的。更精确的说，N维空间有&lt;span class="math">\(\aleph_1\)&lt;/span>个向量。那&lt;span class="math">\(\aleph_1\)&lt;/span>具体是多少呢？我们先做这样一个标记，记自然数的个数有&lt;span class="math">\(\aleph_0\)&lt;/span>个，那么&lt;span class="math">\(\aleph_1=2^{\aleph_0}\)&lt;/span>。更有意思的是1维（实数个数），2维，3维，……，N维空间的向量个数是一样多的（应该叫等势的）。具体了解可看集合论、测度论和实变函数相关内容（万恶的康托尔和勒贝格啊，:dog:）。&lt;/p>
&lt;h3 id="逆矩阵">逆矩阵&lt;/h3>
&lt;p>所谓逆，就是反过来的意思。根据基向量代表整个空间，已经变换过的 &lt;span class="math">\(\vec{i}’\)&lt;/span> 和 &lt;span class="math">\(\vec{j}’\)&lt;/span> 如何通过一个矩阵变换，变回 &lt;span class="math">\(\vec{i}\)&lt;/span>和 &lt;span class="math">\(\vec{j}\)&lt;/span> ，这个矩阵就是逆矩阵 ，写作 &lt;span class="math">\(\mathbf A^{-1}\)&lt;/span>，直观理解如下图&lt;/p>
&lt;div class="figure">
&lt;img src="../images/ReverseMatrix.gif" alt="逆变换与逆矩阵" />&lt;p class="caption">逆变换与逆矩阵&lt;/p>
&lt;/div>
&lt;p>逆矩阵乘原矩阵等于恒等变换，写作 &lt;span class="math">\(\mathbf A \mathbf A^{-1} = \mathbf I\)&lt;/span> 。&lt;span class="math">\(\mathbf I\)&lt;/span> 矩阵表示基向量，对角线元素为1，其余为0（矩阵说对角线，默认为左上方到右下方）&lt;/p></description></item><item><title>线性代数与矩阵之资料网址</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E8%B5%84%E6%96%99%E7%BD%91%E5%9D%80/</link><pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E8%B5%84%E6%96%99%E7%BD%91%E5%9D%80/</guid><description>
&lt;h2 id="线性代数与矩阵之资料网址">线性代数与矩阵之资料网址&lt;!-- omit in toc -->&lt;/h2>
&lt;p>线性代数的几何解释&lt;a href="https://space.bilibili.com/88461692/channel/detail?cid=9450">https://space.bilibili.com/88461692/channel/detail?cid=9450&lt;/a>&lt;/p>
&lt;h2 id="学渣的笔记流mit各种数学">学渣的笔记流—MIT各种数学&lt;/h2>
&lt;p>知乎：学渣的笔记流—MIT各种数学&lt;a href="https://www.zhihu.com/column/c_1029672383375949824">https://www.zhihu.com/column/c_1029672383375949824&lt;/a>&lt;/p></description></item><item><title>线性代数与矩阵之矩阵分解</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/</link><pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/</guid><description>
&lt;h2 id="线性代数与矩阵论之矩阵分解">线性代数与矩阵论之矩阵分解&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#三角分解">三角分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lu分解">LU分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#crout分解">Crout分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#doolittle分解">Doolittle分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#舒尔分解">舒尔分解&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#拓展实矩阵的舒尔分解">拓展：实矩阵的舒尔分解&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#极分解">极分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#qr分解">QR分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#满秩分解">满秩分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征值分解谱分解">特征值分解（谱分解）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#奇异值分解svd">奇异值分解(SVD)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#svd几何解释">SVD几何解释&lt;/a>&lt;/li>
&lt;li>&lt;a href="#奇异值分解与特征值分解的联系">奇异值分解与特征值分解的联系&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="三角分解">三角分解&lt;/h2>
&lt;p>三角分解是矩阵分解的基本形式，最初的三角分解是LU分解，它来自矩阵的高斯消元法。其他三角分解还有：当&lt;span class="math">\(L\)&lt;/span>是单位下三角（主对角元都是1的下三角矩阵）时，称为Doolittle分解，当&lt;span class="math">\(U\)&lt;/span>是单位下三角时，称为Crout分解，以及任何矩阵都可以进行的舒尔分解。&lt;/p>
&lt;h3 id="lu分解">LU分解&lt;/h3>
&lt;p>在线性代数与数值分析中，LU分解是矩阵分解的一种，将一个矩阵分解为一个&lt;strong>下三角矩阵和一个上三角矩阵的乘积&lt;/strong>，&lt;strong>有时需要再乘上一个置换矩阵&lt;/strong>。LU分解可以被视为高斯消元法的矩阵形式。在数值计算上，LU分解经常被用来解线性方程组、且在求逆矩阵和计算行列式中都是一个关键的步骤。&lt;/p>
&lt;p>对于方阵&lt;span class="math">\(A\)&lt;/span>，&lt;span class="math">\(A\)&lt;/span>的LU分解是将它分解成一个下三角矩阵&lt;span class="math">\(L\)&lt;/span>与上三角矩阵&lt;span class="math">\(U\)&lt;/span> 的乘积，也就是 &lt;span class="math">\[A=LU\]&lt;/span> 通常，我们需要让&lt;span class="math">\(L\)&lt;/span>矩阵的对角线元素为1，如果矩阵&lt;span class="math">\(A\)&lt;/span>的对角线上出现0元素，我们应适当的改变&lt;span class="math">\(A\)&lt;/span>的行的顺序，在此尝试将&lt;span class="math">\(A\)&lt;/span>做LU分解。&lt;/p>
&lt;p>举例来说一个&lt;span class="math">\(3\times 3\)&lt;/span>的矩阵&lt;span class="math">\(A\)&lt;/span>，其 LU 分解会写成下面的形式： &lt;span class="math">\[A={\begin{bmatrix}
a_{11}&amp;amp;a_{12}&amp;amp;a_{13}\\
a_{21}&amp;amp;a_{22}&amp;amp;a_{23}\\
a_{31}&amp;amp;a_{32}&amp;amp;a_{33}\\
\end{bmatrix}}=
{\begin{bmatrix}
1&amp;amp;0&amp;amp;0\\
l_{21}&amp;amp;1&amp;amp;0\\
l_{31}&amp;amp;l_{32}&amp;amp;1\\
\end{bmatrix}}
{\begin{bmatrix}
u_{11}&amp;amp;u_{12}&amp;amp;u_{13}\\
0&amp;amp;u_{22}&amp;amp;u_{23}\\
0&amp;amp;0&amp;amp;u_{33}\\
\end{bmatrix}}\]&lt;/span> 事实上，并不是每个矩阵都有 LU 分解。例如，从上式可知&lt;span class="math">\(a_{11}=u_{11}\)&lt;/span>，若&lt;span class="math">\(a_{11}=0\)&lt;/span>，则&lt;span class="math">\(u_{11}\)&lt;/span>等于 0，故&lt;span class="math">\(L\)&lt;/span>或&lt;span class="math">\(U\)&lt;/span>是不可逆矩阵，&lt;span class="math">\(A\)&lt;/span>必然也是不可逆矩阵。&lt;/p>
&lt;p>然而，存在着可逆矩阵&lt;span class="math">\(A\)&lt;/span>不可LU分解的情况。例如，主对角线元素&lt;span class="math">\(a_{11}=0\)&lt;/span>，如果按照LU分解的一般步骤，&lt;span class="math">\(A\)&lt;/span>就是没有&lt;span class="math">\(LU\)&lt;/span>分解的例子。该问题可借由置换&lt;span class="math">\(A\)&lt;/span>的各行顺序来解决，最终会得到一个&lt;span class="math">\(A\)&lt;/span>的&lt;span class="math">\(PLU\)&lt;/span>分解，其中&lt;span class="math">\(P\)&lt;/span>是置换矩阵。&lt;/p>
&lt;p>LU分解的具体步骤和&lt;strong>高斯消元法是相同&lt;/strong>的。我们以一个例子来进行说明：&lt;/p>
&lt;blockquote>
&lt;p>将一个简单的3×3矩阵A进行LU分解： &lt;span class="math">\[A={\begin{bmatrix}1&amp;amp;2&amp;amp;3\\2&amp;amp;5&amp;amp;7\\3&amp;amp;5&amp;amp;3\\\end{bmatrix}}\]&lt;/span> 先将矩阵第一列元素中&lt;span class="math">\(a_{11}\)&lt;/span>以下的所有元素变为0，即 &lt;span class="math">\[L_{{1}}A={\begin{bmatrix}1&amp;amp;0&amp;amp;0\\-2&amp;amp;1&amp;amp;0\\-3&amp;amp;0&amp;amp;1\\\end{bmatrix}}\times {\begin{bmatrix}1&amp;amp;2&amp;amp;3\\2&amp;amp;5&amp;amp;7\\3&amp;amp;5&amp;amp;3\\\end{bmatrix}}={\begin{bmatrix}1&amp;amp;2&amp;amp;3\\0&amp;amp;1&amp;amp;1\\0&amp;amp;-1&amp;amp;-6\\\end{bmatrix}}\]&lt;/span> 再将矩阵第二列元素中a22以下的所有元素变为0，即 &lt;span class="math">\[L_{{2}}(L_{{1}}A)={\begin{bmatrix}1&amp;amp;0&amp;amp;0\\0&amp;amp;1&amp;amp;0\\0&amp;amp;1&amp;amp;1\\\end{bmatrix}}\times {\begin{bmatrix}1&amp;amp;2&amp;amp;3\\0&amp;amp;1&amp;amp;1\\0&amp;amp;-1&amp;amp;-6\\\end{bmatrix}}={\begin{bmatrix}1&amp;amp;2&amp;amp;3\\0&amp;amp;1&amp;amp;1\\0&amp;amp;0&amp;amp;-5\\\end{bmatrix}}=U\]&lt;/span> 显然，&lt;span class="math">\(L=(L_1^{-1}L_2^{-1})\)&lt;/span>，根据&lt;a href="线性代数与矩阵之逆矩阵.md">矩阵求逆笔记&lt;/a>可知，消元矩阵的逆矩阵是主对角线元素不变，其他元素取反，则有： &lt;span class="math">\[L_{1}^{-1}L_{2}^{-1}={\begin{bmatrix}1&amp;amp;0&amp;amp;0\\2&amp;amp;1&amp;amp;0\\3&amp;amp;0&amp;amp;1\\\end{bmatrix}} {\begin{bmatrix}1&amp;amp;0&amp;amp;0\\0&amp;amp;1&amp;amp;0\\0&amp;amp;-1&amp;amp;1\\\end{bmatrix}}\Rightarrow\\
L={\begin{bmatrix}1&amp;amp;0&amp;amp;0\\2&amp;amp;1&amp;amp;0\\3&amp;amp;-1&amp;amp;1\\\end{bmatrix}}\]&lt;/span> 因此有 &lt;span class="math">\[A=LU={\begin{bmatrix}1&amp;amp;0&amp;amp;0\\2&amp;amp;1&amp;amp;0\\3&amp;amp;-1&amp;amp;1\\\end{bmatrix}}{\begin{bmatrix}1&amp;amp;2&amp;amp;3\\0&amp;amp;1&amp;amp;1\\0&amp;amp;0&amp;amp;-5\\\end{bmatrix}}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>如果存在主对角线元素为0时，则使用一个置换矩阵，把下面的一个非0元素换上来，如果该列元素都为0，则说明此矩阵不可逆。（但是也能进行LU分解，不管这一行，继续从下行开始消元分解。）&lt;/p>
&lt;h3 id="crout分解">Crout分解&lt;/h3>
&lt;p>TODO&lt;/p>
&lt;h3 id="doolittle分解">Doolittle分解&lt;/h3>
&lt;p>TODO&lt;/p>
&lt;h3 id="舒尔分解">舒尔分解&lt;/h3>
&lt;p>舒尔分解(Schur分解)是最基本的矩阵分解之一，在矩阵分析中作为重要的理论工具，能够将&lt;strong>任何一般方阵&lt;/strong>转化成上三角矩阵来研究。舒尔分解可以用来求解非对称矩阵的特征值，求不可对角化方阵的幂等。此外，舒尔分解也是推导特征值分解和SVD分解的一个有效途径。&lt;/p>
&lt;blockquote>
&lt;p>舒尔分解定理：如果&lt;span class="math">\(A∈\mathbb{C}^n\)&lt;/span>是&lt;span class="math">\(n\)&lt;/span>阶的&lt;strong>复方阵&lt;/strong>，则存在&lt;span class="math">\(n\)&lt;/span>阶酉矩阵&lt;span class="math">\(U\)&lt;/span>，&lt;span class="math">\(n\)&lt;/span>阶上三角矩阵&lt;span class="math">\(T\)&lt;/span>，使得 &lt;span class="math">\[A=UTU^{-1}=UTU^{H}\]&lt;/span> 即任何一个&lt;span class="math">\(n\)&lt;/span>阶复方阵&lt;span class="math">\(A\)&lt;/span>酉相似于一个&lt;span class="math">\(n\)&lt;/span>阶上三角矩阵&lt;span class="math">\(T\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>由于&lt;span class="math">\(U\)&lt;/span>为酉矩阵，所以有&lt;span class="math">\(U^{-1}=U^H\)&lt;/span>。因为&lt;span class="math">\(A，T\)&lt;/span>相似，所以两者有相同的特征值，且相同特征值的代数重数也相同。又因&lt;span class="math">\(T\)&lt;/span>是上三角矩阵，所以&lt;span class="math">\(T\)&lt;/span>的对角元素实际上是&lt;span class="math">\(A\)&lt;/span>的所有特征值。&lt;/p>
&lt;p>可通过数学归纳法证明。&lt;/p>
&lt;blockquote>
&lt;p>证明：显然，对于任意一阶矩阵，这个舒尔分解是平凡的，任意&lt;span class="math">\(A_{1×1}=[1]A[1]^H\)&lt;/span>，而&lt;span class="math">\([1]_{1×1}\)&lt;/span>显然是个最简单的酉矩阵，&lt;span class="math">\(A\)&lt;/span>只有一个元素，符合三角矩阵的定义。&lt;/p>
&lt;p>现在我们假设对于任意&lt;span class="math">\(n-1\)&lt;/span>维矩阵，舒尔分解定理是成立的。现在我们需要证明对于&lt;span class="math">\(n\)&lt;/span>阶矩阵，舒尔分解定理也是成立的。最重要的是构建连接&lt;span class="math">\(n\)&lt;/span>维矩阵和&lt;span class="math">\(n-1\)&lt;/span>维矩阵的桥梁。构造方法如下：&lt;/p>
&lt;p>我们首先找到矩阵&lt;span class="math">\(A\)&lt;/span>任一非0特征值&lt;span class="math">\(\lambda_1\)&lt;/span>（如果特征值都是0，那么此矩阵只能是零矩阵，显然也满足舒尔分解）以及对应的标准化的（模为1）特征向量&lt;span class="math">\(x_1\)&lt;/span>，接下来我们在与&lt;span class="math">\(x_1\)&lt;/span>所在一维子空间互补的&lt;span class="math">\(n-1\)&lt;/span>维子空间中，选出另外&lt;span class="math">\(n-1\)&lt;/span>个相互正交的单位向量&lt;span class="math">\(x_2,x_3,\dotsb,x_n\)&lt;/span>，关于互补空间的概念请看笔记&lt;a href="线性代数与矩阵之四类空间.md">线性代数与矩阵之四类空间&lt;/a>。&lt;/p>
&lt;p>由于&lt;span class="math">\(n-1\)&lt;/span>维子空间必然存在&lt;span class="math">\(n-1\)&lt;/span>个线性不相关的向量，因此我们可以选择任意&lt;span class="math">\(n-1\)&lt;/span>个线性不相关向量进行施密特正交化，就可以得到&lt;span class="math">\(n-1\)&lt;/span>个相互正交的单位向量。因为互补空间的向量是相互正交的，因此&lt;span class="math">\(x_1\)&lt;/span>也垂直与&lt;span class="math">\(x_2,\dotsb,x_n\)&lt;/span>。我们将&lt;span class="math">\(x_1,x_2,\dotsb,x_n\)&lt;/span>组合到一起，就得到一个&lt;strong>酉矩阵&lt;/strong>&lt;span class="math">\(X\)&lt;/span>，其中第1个向量是&lt;span class="math">\(A\)&lt;/span>的标准化的特征向量，后面&lt;span class="math">\(n-2\)&lt;/span>个是一般的向量。那么 &lt;span class="math">\[AX=A\begin{bmatrix}x_1&amp;amp;x_2&amp;amp;\dotsb&amp;amp;x_n\end{bmatrix}=\begin{bmatrix}\lambda_1x_1&amp;amp;Ax_2&amp;amp;\dotsb&amp;amp;Ax_n\end{bmatrix}\\
X^HAX=\begin{bmatrix}x_1^H\\x_2^H\\\vdots\\x_n^H\end{bmatrix}
\begin{bmatrix}\lambda_1x_1&amp;amp;Ax_2&amp;amp;\dotsb&amp;amp;Ax_n\end{bmatrix}=\begin{bmatrix}
\lambda_1x_1^Hx_1&amp;amp;x_1^HAx_2&amp;amp;\dotsb&amp;amp;x_1^HAx_n\\
\lambda_1x_2^Hx_1&amp;amp;x_2^HAx_2&amp;amp;\dotsb&amp;amp;x_2^HAx_n\\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\
\lambda_1x_n^Hx_1&amp;amp;x_1^HAx_n&amp;amp;\dotsb&amp;amp;x_n^HAx_n
\end{bmatrix}\\
(x_i \perp x_j,i\neq j)=\left[\begin{array}{c:c}
\lambda_1 &amp;amp; &amp;amp;A_{12}&amp;amp; \\
\hdashline0&amp;amp; &amp;amp; &amp;amp;\\
\vdots&amp;amp; &amp;amp;A_{22} &amp;amp;\\
0&amp;amp; &amp;amp; &amp;amp;\\
\end{array}\right]=\hat{T}\]&lt;/span> 其中，&lt;span class="math">\(A_{12}=\begin{bmatrix}x_1^HAx_2&amp;amp;\dotsb&amp;amp;x_1^HAx_n\end{bmatrix}\)&lt;/span>是一个&lt;span class="math">\(1×n\)&lt;/span>维矩阵。&lt;span class="math">\(A_{22}\)&lt;/span>是由剩下的元素组成的&lt;span class="math">\((n-1)×(n-1)\)&lt;/span>维矩阵。&lt;/p>
&lt;p>现在我们已经有了一个雏形，&lt;span class="math">\(X^HAX=\hat{T}\Rightarrow A=XAX^H\)&lt;/span>，如果从分块矩阵的角度来看，这已经是一个分块上三角矩阵了，而且我们知道里面&lt;span class="math">\((n-1)×(n-1)\)&lt;/span>维矩阵&lt;span class="math">\(A_{22}\)&lt;/span>是能够进行了舒尔分解成&lt;span class="math">\(A_{22}=P_1^H\tilde{T}P_1\)&lt;/span>，现在我们要做的就是将&lt;span class="math">\(\hat{T}\)&lt;/span>与&lt;span class="math">\(A_{22}=P_1^H\tilde{T}P_1\)&lt;/span>联系起来。其实，&lt;span class="math">\(\hat{T}\)&lt;/span>就是多了一维，因此我们可以将&lt;span class="math">\(n-1\)&lt;/span>维酉矩阵&lt;span class="math">\(P_1\)&lt;/span>进行升维，如下： &lt;span class="math">\[
P=\left[\begin{array}{c:c}
1 &amp;amp; 0 &amp;amp; \dotsb &amp;amp; 0\\
\hdashline0&amp;amp;\\
\vdots&amp;amp; &amp;amp;P_1\\
0&amp;amp;\\
\end{array}\right]
\]&lt;/span> 显然，&lt;span class="math">\(P^HP=I\)&lt;/span>是一个&lt;span class="math">\(n\)&lt;/span>维酉矩阵。而&lt;span class="math">\(P^H\hat{T}P\)&lt;/span>的结果为： &lt;span class="math">\[
P^H\hat{T}P=\left[\begin{array}{c:c}
1 &amp;amp; 0 &amp;amp; \dotsb &amp;amp; 0\\
\hdashline0&amp;amp;\\
\vdots&amp;amp; &amp;amp;P^H_1\\
0&amp;amp;\\
\end{array}\right]
\left[\begin{array}{c:c}
\lambda_1 &amp;amp; &amp;amp;A_{12}&amp;amp; \\
\hdashline0&amp;amp; &amp;amp; &amp;amp;\\
\vdots&amp;amp; &amp;amp;A_{22} &amp;amp;\\
0&amp;amp; &amp;amp; &amp;amp;\\
\end{array}\right]
\left[\begin{array}{c:c}
1 &amp;amp; 0 &amp;amp; \dotsb &amp;amp; 0\\
\hdashline0&amp;amp;\\
\vdots&amp;amp; &amp;amp;P_1\\
0&amp;amp;\\
\end{array}\right]\\
=\left[\begin{array}{c:c}
\lambda_1 &amp;amp; &amp;amp;A_{12}P_1&amp;amp; \\
\hdashline0&amp;amp; &amp;amp; &amp;amp;\\
\vdots&amp;amp; &amp;amp;P_1^HA_{22}P_1 &amp;amp;\\
0&amp;amp; &amp;amp; &amp;amp;\\
\end{array}\right]
=\left[\begin{array}{c:c}
\lambda_1 &amp;amp; &amp;amp;A_{12}P_1&amp;amp; \\
\hdashline0&amp;amp; &amp;amp; &amp;amp;\\
\vdots&amp;amp; &amp;amp;\tilde{T} &amp;amp;\\
0&amp;amp; &amp;amp; &amp;amp;\\
\end{array}\right]=T
\]&lt;/span> 其中，&lt;span class="math">\(\tilde{T}\)&lt;/span>为上三角矩阵，因此&lt;span class="math">\(P^H\hat{T}P\)&lt;/span>的结果为一个上三角矩阵&lt;span class="math">\(T\)&lt;/span>。我们再将&lt;span class="math">\(\hat{T}=X^HAX\)&lt;/span>代入&lt;span class="math">\(T=P^H\hat{T}P\)&lt;/span>可得： &lt;span class="math">\[T=X^HP^HAXP=(XP)^HAXP\]&lt;/span> 因为，&lt;span class="math">\(X,P\)&lt;/span>都是&lt;span class="math">\(n\)&lt;/span>维酉矩阵，而酉矩阵的乘积依然是酉矩阵，因此&lt;span class="math">\(XP\)&lt;/span>也是酉矩阵，我们有： &lt;span class="math">\[A=((XP)^H)^{-1}T(XP)^{-1}=(XP)T(XP)^H\\
\overset{令U=XP}{=}UTU^H\]&lt;/span> 其中，&lt;span class="math">\(U\)&lt;/span>是一个酉矩阵，&lt;span class="math">\(T\)&lt;/span>是一个上三角矩阵。至此，我们证明了对于任意&lt;span class="math">\(n\)&lt;/span>阶矩阵，舒尔分解定理也是成立的。得证。&lt;/p>
&lt;/blockquote>
&lt;p>舒尔分解的证明过程证明了舒尔分解的存在性的同时，也提供了一种舒尔分解构造的方法，即从一阶矩阵开始，逐步使用&lt;span class="math">\(1\sim n-1\)&lt;/span>维酉矩阵经过升维相乘，最终得到&lt;span class="math">\(n\)&lt;/span>维酉矩阵&lt;span class="math">\(U\)&lt;/span>。然后，左乘、右乘原矩阵，得到上三角矩阵&lt;span class="math">\(T\)&lt;/span>（虽然这种方法相当麻烦）。此外，由于步骤中正交基的选择并不唯一，由此舒尔分解也是不唯一的。&lt;/p>
&lt;h4 id="拓展实矩阵的舒尔分解">拓展：实矩阵的舒尔分解&lt;/h4>
&lt;p>舒尔分解是针对复矩阵而言的，如果将数域缩小到实数域，舒尔分解就不完全适用了。问题就在于一个矩阵可能存在非实数特征值，即我们在递推过程中存在某个子矩阵找不到实数域内的特征值，导致后面就无法继续。&lt;/p>
&lt;blockquote>
&lt;p>实数域的舒尔分解：&lt;del>设&lt;span class="math">\(A\in R^{n\times n}\)&lt;/span>，则存在实正交矩阵&lt;span class="math">\(U\)&lt;/span>和实上三角矩阵&lt;span class="math">\(T\)&lt;/span>使得&lt;span class="math">\(A=UTU^H\)&lt;/span>。&lt;/del>&lt;/p>
&lt;/blockquote>
&lt;p>上面的这结论是错的。然而在实数域，我们可以对上述舒尔分解定理做适当修正，使得在实数域也可以进行舒尔分解。&lt;/p>
&lt;blockquote>
&lt;p>实数域的舒尔分解定理：如果&lt;span class="math">\(A∈\mathbb{R}^n\)&lt;/span>是&lt;span class="math">\(n\)&lt;/span>阶的&lt;strong>实方阵&lt;/strong>，则存在&lt;span class="math">\(n\)&lt;/span>阶正交矩阵&lt;span class="math">\(Q\)&lt;/span>，&lt;span class="math">\(n\)&lt;/span>阶&lt;strong>拟上三角&lt;/strong>矩阵&lt;span class="math">\(T\)&lt;/span>，使得 &lt;span class="math">\[A=QTQ^{-1}=QTQ^T\]&lt;/span> 即任何一个&lt;span class="math">\(n\)&lt;/span>阶实方阵&lt;span class="math">\(A\)&lt;/span>正交相似于一个&lt;span class="math">\(n\)&lt;/span>阶&lt;strong>拟上三角&lt;/strong>矩阵&lt;span class="math">\(T\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>然而，虽然不能将任意实矩阵正交相似上三角化，但我们可以放宽要求，将任意实矩阵正交相似拟上三角化。区别就在于&lt;strong>拟三角&lt;/strong>！ &lt;span class="math">\[
T=\begin{bmatrix}
R_{11}&amp;amp;R_{12}&amp;amp;\dotsb&amp;amp;R_{1m}\\
&amp;amp;R_{22}&amp;amp;\dotsb&amp;amp;R_{2m}\\
&amp;amp; &amp;amp; \ddots &amp;amp; \vdots\\
&amp;amp; &amp;amp; &amp;amp; R_{mm}
\end{bmatrix}
\]&lt;/span> 其中对角子块&lt;span class="math">\(R_{ii}\)&lt;/span>是&lt;span class="math">\(1 × 1\)&lt;/span>矩阵或有一对共轭的虚特征值的&lt;span class="math">\(2\times 2\)&lt;/span>矩阵，也因为对角子块的存在，最终的行列也不是&lt;span class="math">\(n\)&lt;/span>，而是&lt;span class="math">\(m\leq n\)&lt;/span>。证明方法也是数学归纳法，这里我们就省略不证了。&lt;/p>
&lt;h2 id="极分解">极分解&lt;/h2>
&lt;p>一个复系数矩阵&lt;span class="math">\(A\)&lt;/span>的极分解将其分解成两个矩阵的乘积，可以表示为： &lt;span class="math">\[A=UP\]&lt;/span> 其中&lt;span class="math">\(U\)&lt;/span>是一个&lt;strong>酉矩阵&lt;/strong>，&lt;span class="math">\(P\)&lt;/span>是一个&lt;strong>半正定的埃尔米特矩阵&lt;/strong>。这样的分解对&lt;strong>任意&lt;/strong>的矩阵&lt;span class="math">\(A\)&lt;/span>都存在。当&lt;span class="math">\(A\)&lt;/span>是可逆矩阵时，分解是唯一的，并且&lt;span class="math">\(P\)&lt;/span>必然为正定矩阵。&lt;/p>
&lt;h2 id="qr分解">QR分解&lt;/h2>
&lt;p>见&lt;a href="线性代数与矩阵之正交（酉）矩阵与正交化.md">《线性代数与矩阵之正交（酉）矩阵与正交化》&lt;/a>&lt;/p>
&lt;h2 id="满秩分解">满秩分解&lt;/h2>
&lt;blockquote>
&lt;p>定义：满秩分解：对于&lt;span class="math">\(m×n\)&lt;/span>的矩阵&lt;span class="math">\(A\)&lt;/span>，假设其秩为&lt;span class="math">\(r\)&lt;/span>，若存在秩同样为&lt;span class="math">\(r\)&lt;/span>两个矩阵：&lt;span class="math">\(F_{m×r}\)&lt;/span>（列满秩）和&lt;span class="math">\(G_{r×n}\)&lt;/span>（行满秩），使得&lt;span class="math">\(A=FG\)&lt;/span>，则称其为矩阵&lt;span class="math">\(A\)&lt;/span>的满秩分解。&lt;/p>
&lt;/blockquote>
&lt;p>根据可逆矩阵和高斯消元法，容易得到以下定理。&lt;/p>
&lt;blockquote>
&lt;p>定理：满秩分解有两个性质：不唯一性和存在性&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>满秩分解不唯一：假设存在&lt;span class="math">\(r\)&lt;/span>阶可逆方阵&lt;span class="math">\(D\)&lt;/span>，则&lt;span class="math">\(A=FG=F(DD^{−1})G=(FD)(D^{−1}G)=F&amp;#39;G&amp;#39;\)&lt;/span>；&lt;/li>
&lt;li>任何非零矩阵一定存在满秩分解。&lt;/li>
&lt;/ol>
&lt;p>证明：假设存在初等变换矩阵&lt;span class="math">\(B_{m×m}\)&lt;/span>，使得 &lt;span class="math">\[BA=\begin{bmatrix}G\\O\end{bmatrix}\]&lt;/span> 显然，&lt;span class="math">\(B\)&lt;/span>可以看成是带置换的高斯消元法。其中&lt;span class="math">\(G\)&lt;/span>是个&lt;span class="math">\(m×r\)&lt;/span>的行满秩矩阵。由上面的公式，可以推出， &lt;span class="math">\[\begin{aligned}
A &amp;amp;= B^{-1}\left(
\begin{array}{c}
G\\
O
\end{array}
\right)\\
&amp;amp;= (F|S) \left(
\begin{array}{c}
G\\
O
\end{array}
\right)\\
&amp;amp;= FG
\end{aligned}\]&lt;/span> 公式第二行中，我们将&lt;span class="math">\(B^{−1}\)&lt;/span>分块为&lt;span class="math">\((F|S)\)&lt;/span>，其中&lt;span class="math">\(F\)&lt;/span>为&lt;span class="math">\(m×r\)&lt;/span>矩阵（秩为&lt;span class="math">\(r\)&lt;/span>），&lt;span class="math">\(G\)&lt;/span>为&lt;span class="math">\(r×n\)&lt;/span>矩阵（秩为&lt;span class="math">\(r\)&lt;/span>）。&lt;/p>
&lt;/blockquote>
&lt;p>从满秩分解的存在性证明，也可以看出满秩分解的求法：先导出高斯消元的初等矩阵和主元矩阵，然后求高斯消元的初等矩阵的逆矩阵，在分割成两部分。此外，还可以用&lt;strong>Hermite标准型&lt;/strong>来求满秩分解。&lt;/p>
&lt;blockquote>
&lt;p>定义：Hermite标准型：对于&lt;span class="math">\(m×n\)&lt;/span>的矩阵&lt;span class="math">\(H\)&lt;/span>，假设其秩为&lt;span class="math">\(r\)&lt;/span>，若&lt;span class="math">\(H\)&lt;/span>满足 &lt;span class="math">\(H\)&lt;/span>的&lt;span class="math">\(j_1,j_2,…,j_r\)&lt;/span>列是单位矩阵&lt;span class="math">\(E_m\)&lt;/span>的前&lt;span class="math">\(r\)&lt;/span>行，则称&lt;span class="math">\(H\)&lt;/span>为Hermite标准型。简单来说，&lt;span class="math">\(H\)&lt;/span>具有以下形式： &lt;span class="math">\[H=\begin{bmatrix}I_{r×r}&amp;amp;X_{r×(n-r)}\\O_{(m-r)×r}&amp;amp;O_{(m-r)×(n-r)}\end{bmatrix}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>Hermite标准型就是将秩为&lt;span class="math">\(r\)&lt;/span>的&lt;span class="math">\(m×n\)&lt;/span>矩阵&lt;span class="math">\(A\)&lt;/span>经初等变换（高斯消元）而成的&lt;strong>阶梯型矩阵&lt;/strong>。所以也叫做Hermite最简型。&lt;/p>
&lt;p>算出Hermite标准型后，对于矩阵的满秩分解&lt;span class="math">\(A=FG\)&lt;/span>来说，矩阵&lt;span class="math">\(F\)&lt;/span>就是矩阵&lt;span class="math">\(A\)&lt;/span>中&lt;span class="math">\(j_1,j_2,…,j_r\)&lt;/span>列构成的&lt;span class="math">\(m×r\)&lt;/span>矩阵，而&lt;span class="math">\(G\)&lt;/span>则是Hermite标准型的前&lt;span class="math">\(r\)&lt;/span>行构成的矩阵。&lt;/p>
&lt;h2 id="特征值分解谱分解">特征值分解（谱分解）&lt;/h2>
&lt;p>见&lt;a href="线性代数与矩阵之特征值与特征向量.md">线性代数与矩阵之特征值与特征向量&lt;/a>&lt;/p>
&lt;h2 id="奇异值分解svd">奇异值分解(SVD)&lt;/h2>
&lt;p>特征值分解可以将矩阵分解成对角矩阵的形式，大大方便了矩阵的研究与计算，然后我们也知道只有正规矩阵才可以进行特征分解，那么是否能有一种能够分解成类似模式的分解，但是并没有矩阵类似的要求呢？&lt;/p>
&lt;p>这就是这一小节要引入的奇异值分解（Singular value decomposition），简称SVD。&lt;strong>SVD的一大好处就是任何矩阵都可以进行奇异值分解&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>假设&lt;span class="math">\(A\)&lt;/span>是一个&lt;span class="math">\(m×n\)&lt;/span>阶矩阵，其中的元素全部属于域&lt;span class="math">\(K\)&lt;/span>，也就是实数域或复数域。如此则存在一个分解使得 &lt;span class="math">\[A= U \Sigma V^*\]&lt;/span> 其中&lt;span class="math">\(U\)&lt;/span>是&lt;span class="math">\(m×m\)&lt;/span>阶酉矩阵；&lt;span class="math">\(Σ\)&lt;/span>是&lt;span class="math">\(m×n\)&lt;/span>阶非负实数对角矩阵；而&lt;span class="math">\(V*\)&lt;/span>，即&lt;span class="math">\(V\)&lt;/span>的共轭转置，是&lt;span class="math">\(n×n\)&lt;/span>阶酉矩阵。这样的分解就称作&lt;span class="math">\(A\)&lt;/span>的&lt;strong>奇异值分解&lt;/strong>。&lt;span class="math">\(Σ\)&lt;/span>对角线上的元素&lt;span class="math">\(Σ_{ii}\)&lt;/span>即为&lt;span class="math">\(A\)&lt;/span>的奇异值。&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../images/Singular_value_decomposition_visualisation.svg" alt="奇异值分解" />&lt;p class="caption">奇异值分解&lt;/p>
&lt;/div>
&lt;p>常见的做法是将奇异值由大而小排列。如此&lt;span class="math">\(Σ\)&lt;/span>便能由&lt;span class="math">\(A\)&lt;/span>唯一确定了。（虽然&lt;span class="math">\(U\)&lt;/span>和&lt;span class="math">\(V\)&lt;/span>仍然不能确定。）&lt;/p>
&lt;p>SVD的存在性证明可以用谱定理或者变分法证明，这里不详述，维基百科的英文版上有。&lt;/p>
&lt;p>那么我们如果找到这个奇异值分解呢？&lt;/p>
&lt;p>这个分解中有相互正交的向量和很像特征值的奇异值，觉得和特征分解类似，那我们可不可以往特征分解上靠一靠呢。如果能有对称矩阵，就可以套用特征分解的方法获得特征值和正交（酉）矩阵了。&lt;/p>
&lt;p>显然，&lt;span class="math">\(A^TA,AA^T\)&lt;/span>都是由&lt;span class="math">\(A\)&lt;/span>构成的对称矩阵，而且在&lt;span class="math">\(A=U\Sigma V^T\)&lt;/span>的前提下有： &lt;span class="math">\[
A^TA=(U\Sigma V^T)^TU\Sigma V^T=V^T\Sigma^2V\\
AA^T=U\Sigma V^T(U\Sigma V^T)^T=U^T\Sigma^2U
\]&lt;/span> 这正好是两个对称矩阵&lt;span class="math">\(A^TA,AA^T\)&lt;/span>的对角化分解！那么我们只需要求出&lt;span class="math">\(A^TA,AA^T\)&lt;/span>的特征分解，就可以求出&lt;span class="math">\(U,V,\Sigma\)&lt;/span>的矩阵！！！&lt;/p>
&lt;p>需要注意的是，我们&lt;span class="math">\(A^TA,AA^T\)&lt;/span>求出来的是奇异值的平方，实际使用记得要开根号。下面我们举两个例子：&lt;/p>
&lt;p>例1：对矩阵&lt;span class="math">\(A=\begin{bmatrix}0&amp;amp;1\\1&amp;amp;1\\1&amp;amp;0\end{bmatrix}\)&lt;/span>进行奇异值分解。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/svd分解例1.png" alt="svd分解例1" />&lt;p class="caption">svd分解例1&lt;/p>
&lt;/div>
&lt;p>接下来这个例子，给我们说明，完全依赖上面的方法有可能是错的！&lt;/p>
&lt;p>例2：对矩阵&lt;span class="math">\(A=\begin{bmatrix}4&amp;amp;4\\-3&amp;amp;3\end{bmatrix}\)&lt;/span>进行奇异值分解。&lt;/p>
&lt;p>我们首先求出&lt;span class="math">\(A^TA,AA^T\)&lt;/span>： &lt;span class="math">\[
A^TA=\begin{bmatrix}4&amp;amp;-3\\4&amp;amp;3\end{bmatrix}\begin{bmatrix}4&amp;amp;4\\-3&amp;amp;3\end{bmatrix}=\begin{bmatrix}25&amp;amp;7\\7&amp;amp;25\end{bmatrix}\\
AA^T=\begin{bmatrix}4&amp;amp;4\\-3&amp;amp;3\end{bmatrix}\begin{bmatrix}4&amp;amp;-3\\4&amp;amp;3\end{bmatrix}=\begin{bmatrix}32&amp;amp;0\\0&amp;amp;18\end{bmatrix}
\]&lt;/span> 出现了一个对角矩阵，显然奇异值为：&lt;span class="math">\(\lambda_1=\sqrt{32},\lambda_2=\sqrt{18}\)&lt;/span>，剩余奇异值都是0。 进而求出&lt;span class="math">\(V\)&lt;/span>中的特征向量： &lt;span class="math">\[A^TAv_1=\lambda_1 v_1=\sqrt{32}v_1\Rightarrow v_1=\begin{bmatrix}1\over\sqrt{2}\\1\over\sqrt{2}\end{bmatrix}\\
A^TAv_2=\lambda_2 v_2=\sqrt{18}v_2\Rightarrow v_2=\begin{bmatrix}1\over\sqrt{2}\\-1\over\sqrt{2}\end{bmatrix}\\
V=\begin{bmatrix}1\over\sqrt{2}&amp;amp;1\over\sqrt{2}\\1\over\sqrt{2}&amp;amp;-1\over\sqrt{2}\end{bmatrix}\]&lt;/span> 同样求出&lt;span class="math">\(U\)&lt;/span>的特征向量： &lt;span class="math">\[AA^Tu_1=\lambda_1 u_1=\sqrt{32}u_1\Rightarrow u_1=\begin{bmatrix}1\\0\end{bmatrix}\\
AA^Tu_2=\lambda_2 u_2=\sqrt{18}u_2\Rightarrow u_2=\begin{bmatrix}0\\1\end{bmatrix}\\
U=\begin{bmatrix}1&amp;amp;0\\0&amp;amp;1\end{bmatrix}\]&lt;/span> 到这里看似没有问题，但是我们计算一下： &lt;span class="math">\[
U\Sigma V^\ast=\begin{bmatrix}1&amp;amp;0\\0&amp;amp;1\end{bmatrix}\begin{bmatrix}\sqrt{32}&amp;amp;0\\0&amp;amp;\sqrt{18}\end{bmatrix}\begin{bmatrix}1\over\sqrt{2}&amp;amp;1\over\sqrt{2}\\1\over\sqrt{2}&amp;amp;-1\over\sqrt{2}\end{bmatrix}\\
=\begin{bmatrix}4&amp;amp;4\\3&amp;amp;-3\end{bmatrix}
\]&lt;/span> 这和矩阵&lt;span class="math">\(A\)&lt;/span>并不一样！！！底下两个正负号反了，难道奇异值分解有BUG吗？&lt;/p>
&lt;p>这是因为确定特征向量的过程中，特征向量反向仍然符合要求，通过&lt;span class="math">\(A^TA,AA^T\)&lt;/span>求解特征向量的方法无法确认向量的正负符号，但是一旦我们确认&lt;span class="math">\(V\)&lt;/span>中向量的方向之后，&lt;span class="math">\(U\)&lt;/span>中向量的方向也就随之确定。关键是，&lt;strong>我们在开&lt;span class="math">\(\Sigma^2\)&lt;/span>的时候只考虑使用正值，人为地忽略了&lt;span class="math">\(U,V\)&lt;/span>之间的正负号联系&lt;/strong>，为此可以使用&lt;span class="math">\(AV=U\Sigma\)&lt;/span>替代计算可以避免这种问题。 &lt;span class="math">\[
U=AV\Sigma^{-1}=2\begin{bmatrix}1&amp;amp;0\\0&amp;amp;-1\end{bmatrix}
\]&lt;/span> 归一化可得&lt;span class="math">\(U=\begin{bmatrix}1&amp;amp;0\\0&amp;amp;-1\end{bmatrix}\)&lt;/span>。由于本例中&lt;span class="math">\(\Sigma\)&lt;/span>可逆，才可以直接用&lt;span class="math">\(\Sigma^{-1}\)&lt;/span>，否则就老老实实求解。&lt;/p>
&lt;p>因此&lt;span class="math">\(A\)&lt;/span>正确的SVD为： &lt;span class="math">\[
A=U\Sigma V^\ast=\begin{bmatrix}1&amp;amp;0\\0&amp;amp;-1\end{bmatrix}\begin{bmatrix}\sqrt{32}&amp;amp;0\\0&amp;amp;\sqrt{18}\end{bmatrix}\begin{bmatrix}1\over\sqrt{2}&amp;amp;1\over\sqrt{2}\\1\over\sqrt{2}&amp;amp;-1\over\sqrt{2}\end{bmatrix}\\
\]&lt;/span>&lt;/p>
&lt;p>SVD应用的资料还可以看：&lt;a href="../网页资料/线性代数与矩阵-这次终于彻底理解了奇异值分解(SVD)原理及应用.html">这次终于彻底理解了奇异值分解(SVD)原理及应用&lt;/a>&lt;/p>
&lt;h3 id="svd几何解释">SVD几何解释&lt;/h3>
&lt;p>在几何中，任意的线性变换都可以拆成三步：旋转-拉伸-再旋转。至于为什么能这样，我还不知道。但是，这三布正好对应了SVD的三个矩阵分量。&lt;/p>
&lt;p>我们知道，&lt;span class="math">\(U,V\)&lt;/span>是酉矩阵（单位正交矩阵），模长为1，这叫矩阵的几何意义正是向量的旋转。所以&lt;span class="math">\(U\Sigma V^\ast\)&lt;/span>就相当于先使用一个旋转矩阵&lt;span class="math">\(V^\ast\)&lt;/span>转动向量，然后再用对角矩阵&lt;span class="math">\(\Sigma\)&lt;/span>拉伸各个向量分量，最后再用矩阵&lt;span class="math">\(U\)&lt;/span>再旋转一次，得到线性变换的结果。&lt;/p>
&lt;p>下图是矩阵&lt;span class="math">\(M=U\Sigma V^\ast\)&lt;/span>各个矩阵作用与一个二维空间基的演示：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/SVD几何意义.gif" alt="SVD几何意义" />&lt;p class="caption">SVD几何意义&lt;/p>
&lt;/div>
&lt;h3 id="奇异值分解与特征值分解的联系">奇异值分解与特征值分解的联系&lt;/h3>
&lt;p>奇异值分解能够用于任意&lt;span class="math">\(m\times n\)&lt;/span>矩阵，而特征分解只能适用于特定类型的方阵，故奇异值分解的适用范围更广。不过，这两个分解之间是有关联的。给定一个&lt;span class="math">\(M\)&lt;/span>的奇异值分解，根据上面的论述，两者的关系式如下：&lt;/p>
&lt;p>&lt;span class="math">\[
M^{*} M = V \Sigma^{*} U^{*}\, U \Sigma V^{*} =
V (\Sigma^{*} \Sigma) V^{*}\,\]&lt;/span> &lt;span class="math">\[M M^{*} = U \Sigma V^{*} \, V \Sigma^{*} U^{*} =
U (\Sigma \Sigma^{*}) U^{*}\,\]&lt;/span> 关系式的右边描述了关系式左边的特征值分解。于是：&lt;/p>
&lt;p>&lt;span class="math">\(V\)&lt;/span>的列向量（右奇异向量）是&lt;span class="math">\(M^{*}M\)&lt;/span>的特征向量。&lt;/p>
&lt;p>&lt;span class="math">\(U\)&lt;/span>的列向量（左奇异向量）是&lt;span class="math">\(MM^{*}\)&lt;/span>的特征向量。&lt;/p>
&lt;p>&lt;span class="math">\(\Sigma\)&lt;/span>的非零对角元（非零奇异值）是&lt;span class="math">\(M^{*}M\)&lt;/span>或者&lt;span class="math">\(MM^{*}\)&lt;/span>的非零特征值的平方根。&lt;/p>
&lt;p>特殊情况下，当&lt;span class="math">\(M\)&lt;/span>是一个&lt;strong>正规矩阵&lt;/strong>（因而必须是方阵）根据谱定理，M可以被一组特征向量酉对角化，所以它可以表为： &lt;span class="math">\[M = U D U^\ast\]&lt;/span> 其中&lt;span class="math">\(U\)&lt;/span>为一个酉矩阵，&lt;span class="math">\(D\)&lt;/span>为一个对角阵。如果&lt;span class="math">\(M\)&lt;/span>是半正定的，&lt;span class="math">\(M = U D U^\ast\)&lt;/span>的分解也是一个奇异值分解。&lt;/p>
&lt;p>然而，一般矩阵的特征分解跟奇异值分解不同。特征分解如下： &lt;span class="math">\[M=UDU^{-1}\]&lt;/span> 其中&lt;span class="math">\(U\)&lt;/span>是不需要是酉的，&lt;span class="math">\(D\)&lt;/span>也不需要是半正定的。而奇异值分解如下： &lt;span class="math">\[M=U\Sigma V^\ast\]&lt;/span> 其中&lt;span class="math">\(\Sigma\)&lt;/span>是对角半正定矩阵，&lt;span class="math">\(U\)&lt;/span>和&lt;span class="math">\(V\)&lt;/span>是酉矩阵，两者除了通过矩阵&lt;span class="math">\(M\)&lt;/span>没有必然的联系。&lt;/p></description></item><item><title>线性代数与矩阵之行列式</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E8%A1%8C%E5%88%97%E5%BC%8F/</link><pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E8%A1%8C%E5%88%97%E5%BC%8F/</guid><description>
&lt;h2 id="行列式">行列式&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#行列式的几何意义">行列式的几何意义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#行列式计算">行列式计算&lt;/a>&lt;/li>
&lt;li>&lt;a href="#k阶子式">K阶子式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#k阶主子式">K阶主子式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#顺序主子式">顺序主子式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#余子式和代数余子式">余子式和代数余子式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#余因子矩阵与伴随矩阵">余因子矩阵与伴随矩阵&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="行列式的几何意义">行列式的几何意义&lt;/h2>
&lt;p>行列式看起来计算非常复杂，但是有十分明确的几何意义。行列式可以看做是有向面积或体积的概念在一般的欧几里得空间中的推广。或者说，在欧几里得空间中，行列式描述的是一个线性变换对“体积”所造成的影响。&lt;/p>
&lt;p>当作为一个N维空间的标准基&lt;span class="math">\(E=(\vec{e_1},\vec{e_2},\dotsb,\vec{e_N})\)&lt;/span>，其中&lt;span class="math">\(\vec{e_i}\)&lt;/span>是N维单位列向量，经过一个线性变换矩阵&lt;span class="math">\(M\)&lt;/span>后，其单位“体积”有原来的1变成了&lt;span class="math">\(\det(M)\)&lt;/span>。即行列式反映了矩阵对线性空间的拉伸效果。此外，行列式也等同于矩阵&lt;span class="math">\(M\)&lt;/span>的各个分向量&lt;span class="math">\(M=(\vec{m_1},\vec{m_2},\dotsb,\vec{m_N})\)&lt;/span>所围成的N维方体的“体积”，其中&lt;span class="math">\(\vec{m_i}\)&lt;/span>是N维列向量。&lt;/p>
&lt;p>在二维空间中，行列式的值是2个2维向量组成的平行四边形的面积。&lt;span class="math">\(\det (M_{2\times 2})=\vec{m_1}\times \vec{m_2}=||\vec{m_1}||_2||\vec{m_2}||_2\sin\theta\)&lt;/span>，其中&lt;span class="math">\(\times\)&lt;/span>表示叉乘或外积，&lt;span class="math">\(\theta\)&lt;/span>是向量&lt;span class="math">\(\vec{m_1}\)&lt;/span>逆时针转到&lt;span class="math">\(\vec{m_2}\)&lt;/span>的角度。显然，有&lt;span class="math">\(\vec{m_1}\times \vec{m_2}=-\vec{m_2}\times \vec{m_1}\)&lt;/span>。这其实有些不准确，因为叉乘的结果是一个垂直于此二维平面的向量，面积大小等于&lt;span class="math">\(\det (M_{2\times 2})\)&lt;/span>。疑问：这还是二维空间向量吗？&lt;/p>
&lt;p>在三维空间中，行列式的值是3个3维向量组成的平行六面体的体积。&lt;span class="math">\(\det (M_{3\times 3})=(\vec{m_1}\times \vec{m_2})\cdot \vec{m_3}\)&lt;/span>，其中&lt;span class="math">\(\times\)&lt;/span>表示叉乘或外积，&lt;span class="math">\(\cdot\)&lt;/span>表示点积或内积，在一起称为&lt;strong>混合积&lt;/strong>。从几何角度很好理解混合积，就是求体积。前面的叉乘的计算出大小是组成的平行六面体底面积大小，其向量方向就是平行六面体高的方向，后面的点积是第三个向量在高上的投影，计算结果最终就是：底面积×高=平行六面体的体积。&lt;/p>
&lt;h2 id="行列式计算">行列式计算&lt;/h2>
&lt;p>版权声明：以下文章为CSDN博主「PoemK」的原创文章，遵循 cc 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。 原文链接：&lt;a href="https://blog.csdn.net/yskyskyer123/java/article/details/87891051">https://blog.csdn.net/yskyskyer123/java/article/details/87891051&lt;/a>&lt;/p>
&lt;h3 id="k阶子式">K阶子式&lt;/h3>
&lt;p>以3阶行列式为例：&lt;/p>
&lt;p>&lt;span class="math">\[\begin{vmatrix} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3\\ c_1 &amp;amp; c_2 &amp;amp; c_3 \end{vmatrix}\]&lt;/span> 则它的3阶子式是它本身。&lt;/p>
&lt;p>它的2阶子式有 第1、2行和第1、2列相交处元素组成的行列式： &lt;span class="math">\[\begin{vmatrix} a_1 &amp;amp; a_2 \\ b_1 &amp;amp; b_2 \end{vmatrix}\]&lt;/span> 第1、2行和第1、3列相交处元素组成的行列式： &lt;span class="math">\[\begin{vmatrix} a_1 &amp;amp; a_3 \\ b_1 &amp;amp; b_3 \end{vmatrix}\]&lt;/span> 等等。行列式的每一项都是一个一阶子式。&lt;/p>
&lt;p>方法就是选取&lt;span class="math">\(k\)&lt;/span>行再选取&lt;span class="math">\(k\)&lt;/span>列 可以试着划出&lt;span class="math">\(2k\)&lt;/span>条线 然后相交处的元素组成的新的行列式就是&lt;span class="math">\(k\)&lt;/span>阶子式。&lt;/p>
&lt;h3 id="k阶主子式">K阶主子式&lt;/h3>
&lt;p>在子式的基础上，要求子式包含的&lt;strong>行序数和包含的列序数相同&lt;/strong>。详细说来：在n 阶行列式中，选取行号（例如 1、3、7行），再选取相同行号的列号（1、3、7 列），则有行和列都为i个的行列式即为n阶行列式的r阶主子式，也可以说由上述选取的行列交汇处的元素所组成的新的行列式就称为“n 阶行列式的一个r阶主子式”。&lt;/p>
&lt;p>举个例子： &lt;span class="math">\[A=\begin{vmatrix} -3&amp;amp;1&amp;amp;-3\\20&amp;amp;3&amp;amp;10\\2&amp;amp;-2&amp;amp;4\end{vmatrix}\]&lt;/span> 其一阶主子式就是&lt;span class="math">\(-3，3，4\)&lt;/span>。二阶主子式是 &lt;span class="math">\[\begin{vmatrix} -3&amp;amp;1\\20 &amp;amp;3\end{vmatrix}=-29,
\begin{vmatrix} 3&amp;amp;10\\-2&amp;amp;4\end{vmatrix}=32,
\begin{vmatrix} -3&amp;amp;-3\\2&amp;amp;4\end{vmatrix}=-6.\]&lt;/span> 三阶主子式是行列式本身&lt;span class="math">\(det(A)=-18\)&lt;/span>&lt;/p>
&lt;p>主子式是从n个元素中挑出r个，因此r阶主子式共有&lt;span class="math">\(c_n^{r}=\frac{n!}{r!(n-r)!}\)&lt;/span>个。&lt;/p>
&lt;h3 id="顺序主子式">顺序主子式&lt;/h3>
&lt;p>由第1→r行和第1→r列所确定的子式即为“n 阶行列式的r阶&lt;strong>顺序主子式&lt;/strong>”。&lt;/p>
&lt;p>例如：&lt;/p>
&lt;ul>
&lt;li>1阶时：取第1行，第1列&lt;/li>
&lt;li>2阶时：取第1、2行，第1、2列&lt;/li>
&lt;li>3阶时：取第1、2、3行，第1、2、3列&lt;/li>
&lt;li>4阶时：取第1、2、3、4行，第1、2、3、4列&lt;/li>
&lt;/ul>
&lt;p>实际上，主子式的主对角线元素是原 n 阶行列式的主对角线元素的一部分，且顺序相同。值得注意的是，根据定义，r阶主子式是&lt;strong>不唯一&lt;/strong>的，而r阶&lt;strong>顺序主子式是唯一&lt;/strong>的。&lt;/p>
&lt;h3 id="余子式和代数余子式">余子式和代数余子式&lt;/h3>
&lt;p>一个矩阵&lt;span class="math">\(A\)&lt;/span>的余子式（又称余因式，英语：minor）是指将&lt;span class="math">\(A\)&lt;/span>的某些行与列去掉之后所余下的方阵的行列式。相应的方阵有时被称为余子阵。&lt;/p>
&lt;p>在&lt;span class="math">\(n\)&lt;/span>阶行列式中，划去元&lt;span class="math">\(a_{ij}\)&lt;/span>所在的第i行与第j列的元，剩下的元不改变原来的顺序所构成的n-1阶行列式称为&lt;strong>元&lt;span class="math">\(a_{ij}\)&lt;/span>的余子式&lt;/strong>。数学表示上计作&lt;span class="math">\(M_{ij}\)&lt;/span>。&lt;/p>
&lt;p>&lt;span class="math">\(a_{ij}\)&lt;/span>的&lt;strong>代数余子式&lt;/strong>：&lt;span class="math">\(c_{ij}= (-1)^{i+j} M_{ij}\)&lt;/span>&lt;/p>
&lt;p>对矩阵 &lt;span class="math">\[\begin{pmatrix}\,\,\,1&amp;amp;4&amp;amp;7\\\,\,\,3&amp;amp;0&amp;amp;5\\-1&amp;amp;9&amp;amp;\!11\\\end{pmatrix}\]&lt;/span> 要计算代数余子式&lt;span class="math">\(c_{23}\)&lt;/span>。首先计算余子式&lt;span class="math">\(M_{23}\)&lt;/span>，也就是原矩阵去掉第2行和第3列后的子矩阵的行列式： &lt;span class="math">\[\begin{vmatrix}\,\,1&amp;amp;4&amp;amp;\Box \,\\\,\Box &amp;amp;\Box &amp;amp;\Box \,\\-1&amp;amp;9&amp;amp;\Box \,\\\end{vmatrix},即\begin{vmatrix}\,\,\,1&amp;amp;4\,\\-1&amp;amp;9\,\\\end{vmatrix}=9-(-4)=13\]&lt;/span> 因此，&lt;span class="math">\(c_{23}\)&lt;/span>等于&lt;span class="math">\((-1)^{2+3}M_{23}=-13\)&lt;/span>&lt;/p>
&lt;p>其某一行列式&lt;span class="math">\(\det A\)&lt;/span>可以用余因子表示： &lt;span class="math">\[\det(A)=a_{{1j}}c_{{1j}}+a_{{2j}}c_{{2j}}+a_{{3j}}c_{{3j}}+...+a_{{nj}}c_{{nj}}\]&lt;/span> （对第 j 纵行的余因子分解） &lt;span class="math">\[\det(A)=a_{{i1}}c_{{i1}}+a_{{i2}}c_{{i2}}+a_{{i3}}c_{{i3}}+...+a_{{in}}c_{{in}}\]&lt;/span> （对第 i 横列的余因子分解）&lt;/p>
&lt;h3 id="余因子矩阵与伴随矩阵">余因子矩阵与伴随矩阵&lt;/h3>
&lt;p>&lt;span class="math">\(A\)&lt;/span>的余子矩阵是指将&lt;span class="math">\(A\)&lt;/span>的&lt;span class="math">\((i, j)\)&lt;/span>项代数余子式&lt;span class="math">\(c_{ij}\)&lt;/span>摆在第i行第j列所得到的矩阵，记为&lt;span class="math">\(C\)&lt;/span>。&lt;/p>
&lt;p>余子矩阵&lt;span class="math">\(C\)&lt;/span>的&lt;strong>转置&lt;/strong>矩阵称为&lt;span class="math">\(A\)&lt;/span>的伴随矩阵&lt;span class="math">\(A^\ast\)&lt;/span>，伴随矩阵类似于逆矩阵，并且当A可逆时可以用来计算它的逆矩阵。 &lt;span class="math">\[A^{-1}=\frac{1}{\det(A)}A^\ast=\frac{1}{\det(A)}C^T\]&lt;/span>&lt;/p></description></item><item><title>线性代数与矩阵之-海森矩阵校正</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B-%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5%E6%A0%A1%E6%AD%A3/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B-%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5%E6%A0%A1%E6%AD%A3/</guid><description>
&lt;h2 id="海森矩阵校正">海森矩阵校正&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#海森矩阵校正的两种思路">海森矩阵校正的两种思路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#modified_cholesky分解">Modified_Cholesky分解&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="海森矩阵校正的两种思路">海森矩阵校正的两种思路&lt;/h2>
&lt;p>解决海森矩阵不正定问题的方法称为海森校正（Hessian Modification）。思路是让海森矩阵加上一个微小的量，使其正定，相当于用&lt;span class="math">\(B_k=\nabla^2f(x_k)+E_k\)&lt;/span>替代&lt;span class="math">\(\nabla^2f(x_k)\)&lt;/span>。其中，&lt;span class="math">\(E_k\)&lt;/span>为修正矩阵，需要尽可能小，以避免修改后的方向&lt;span class="math">\(d_k=-B_k^{-1}g_k\)&lt;/span>与牛顿方向&lt;span class="math">\(-H_k^{-1}g_k\)&lt;/span>相差太远。&lt;/p>
&lt;p>我们知道令&lt;span class="math">\(B_k=\nabla^2f(x_k)+v_kI\)&lt;/span>，当 &lt;span class="math">\(v_k&amp;gt;|\lambda_1|\)&lt;/span>时，&lt;span class="math">\(B_k\)&lt;/span>正定。其中&lt;span class="math">\(\lambda_1\)&lt;/span>是&lt;span class="math">\(\nabla^2f(x_k)\)&lt;/span>最小的特征值。因此可以得到&lt;span class="math">\(E_k=\tau I\)&lt;/span>，其中&lt;span class="math">\(\tau=\max\{0,\delta-\lambda_1\}\)&lt;/span>，&lt;strong>&lt;span class="math">\(\delta\)&lt;/span>是一个略大于零的常数&lt;/strong>。这样原矩阵加上了一个标量乘以单位矩阵，所有特征值都大于零，&lt;span class="math">\(B_k\)&lt;/span>正定。&lt;/p>
&lt;p>但是这种方法不是改动最小的方案，如果我们做的更精细一些：对于任意对称矩阵&lt;span class="math">\(A\)&lt;/span>，可以将其分解为&lt;span class="math">\(A=Q\Lambda Q^T\)&lt;/span>，其中&lt;span class="math">\(\Lambda\)&lt;/span>是由&lt;span class="math">\(A\)&lt;/span>的特征值组成的对角矩阵。如果我们令&lt;span class="math">\(A&amp;#39;=Q(\Lambda+\mathop{diag(\tau_i)})Q^T\)&lt;/span>，其中&lt;span class="math">\(\tau_i\)&lt;/span>由&lt;span class="math">\(\lambda_i\)&lt;/span>确定，并满足 &lt;span class="math">\[\tau_i=\begin{cases}
0,&amp;amp;\lambda_i&amp;gt;\delta\\
\delta-\lambda_i,&amp;amp;\lambda_i&amp;lt;\delta
\end{cases}\]&lt;/span> 这意味着直接修改&lt;span class="math">\(A\)&lt;/span>的特征值，将所有小于&lt;span class="math">\(\delta\)&lt;/span>的特征值调整到&lt;span class="math">\(\delta\)&lt;/span>，从而保证&lt;span class="math">\(A&amp;#39;\)&lt;/span>的正定性。该方法相当于令&lt;span class="math">\(E_k=Q\mathop{diag(\tau_i)}Q^T\)&lt;/span>。&lt;/p>
&lt;p>这两种方法都是根据特征值确定&lt;span class="math">\(E_k\)&lt;/span>，哪种更好呢？&lt;/p>
&lt;p>其实说不上哪个更好，本质上，他们是&lt;strong>在不同范数约束下的最优结果&lt;/strong>。前面提到，我们希望&lt;span class="math">\(E_k\)&lt;/span>越小越好，因此可以用矩阵范数来衡量&lt;span class="math">\(E_k\)&lt;/span>的大小。第一种方案其实是在欧氏范数下的最小改动，因为&lt;span class="math">\(||E_k||=||\tau I||=\tau\)&lt;/span>，不存在使&lt;span class="math">\(||E_k||\)&lt;/span>更小的其它&lt;span class="math">\(\tau\)&lt;/span>。第二种方案则是在Frobenius范数下的最小改动。Frobenius范数定义为 &lt;span class="math">\[||A||_F=\bigg(\sum_{i=1}^m\sum_{j=1}^na_{ij}^2\bigg)^{1/2}\]&lt;/span> 即所有矩阵元素的平方和再开根号，该范数刻画了矩阵所有元素值的大小。在该定义下， &lt;span class="math">\[||E_k||_F=||Q\mathop{diag(\tau_i)}Q^T||_F\\
=||\mathop{diag(\tau_i)}||_F=\sqrt{\sum_i^N \tau_i^2}\]&lt;/span> 第二行去掉了矩阵&lt;span class="math">\(Q\)&lt;/span>和&lt;span class="math">\(Q^T\)&lt;/span>，因为实对称矩阵的特征向量构成的矩阵&lt;span class="math">\(Q\)&lt;/span>是正交矩阵，而正交矩阵相当于旋转，不改变Frobenius范数。容易验证，第一种方案的Frobenius范数为&lt;span class="math">\(||\tau I||_F=\sqrt{N}\tau\)&lt;/span>，做个简单的放缩就能发现&lt;span class="math">\(\sqrt{\sum_i^N \tau_i^2}≤\sqrt{N}\tau\)&lt;/span>，所以第二种方案是Frobenius范数下的最小改动。&lt;/p>
&lt;p>了解上面两种方法，其实并不能在实际中帮上忙。&lt;strong>因为海森矩阵的特征值我们根本无从得知，那是一个需要花大代价才能知道的数字&lt;/strong>。在实际中，我们一般采取&lt;span class="math">\(E_k=\tau I\)&lt;/span>这种方案，但不通过特征向量确定&lt;span class="math">\(\tau\)&lt;/span>，而是以迭代的方式&lt;strong>把可行的&lt;span class="math">\(\tau\)&lt;/span>试出来&lt;/strong>。&lt;/p>
&lt;p>具体怎么做呢？我们可以先从一个稍小的数开始尝试，比如令&lt;span class="math">\(\tau_0=||A||_F/2\)&lt;/span>，在每次迭代中对校正后的海森矩阵进行Cholesky分解， &lt;span class="math">\[LL^T=A+\tau_k I=B_k\]&lt;/span> 如果分解成功，那么分解的结果&lt;span class="math">\(L\)&lt;/span>就可以用来快速求解式&lt;span class="math">\(B_kd_k=-g_k\)&lt;/span>。如果分解失败，那么将&lt;span class="math">\(\tau_k\)&lt;/span>扩大一倍，进入下一次尝试，直到分解成功。&lt;/p>
&lt;h2 id="modified_cholesky分解">Modified_Cholesky分解&lt;/h2>
&lt;p>对海森矩阵进行校正，即叠加一个小矩阵，使其正定。但是海森校正苦于无法得知特征值的大小，只能多试几次，试的次数越多，浪费的时间就越多。&lt;/p>
&lt;p>但问题难不倒聪明的研究者，有人提出了一种新的海森校正方法，称为&lt;strong>modified Cholesky分解，可以在做Cholesky分解的过程中调整原矩阵的值，不需要迭代，不需要试错，分解完毕后直接得到新的正定海森矩阵&lt;/strong>。让我们看看这种方法是如何实现的。&lt;/p>
&lt;p>对于任意一个正定对称矩阵，一定可以分解为如下形式 &lt;span class="math">\[A=LDL^T\]&lt;/span> 其中，&lt;span class="math">\(L\)&lt;/span>是对角线元素全为1的下三角矩阵，&lt;span class="math">\(D\)&lt;/span>是对角矩阵。为了直观地解释分解的过程，我们以3×3对称矩阵为例 &lt;span class="math">\[\begin{bmatrix}
a_{11}&amp;amp;a_{21}&amp;amp;a_{31}\\
a_{21}&amp;amp;a_{22}&amp;amp;a_{32}\\
a_{31}&amp;amp;a_{32}&amp;amp;a_{33}
\end{bmatrix}=
\begin{bmatrix}1&amp;amp;0&amp;amp;0\\l_{21}&amp;amp;1&amp;amp;0\\l_{31}&amp;amp;l_{32}&amp;amp;1\end{bmatrix}
\begin{bmatrix}d_1&amp;amp;0&amp;amp;0\\0&amp;amp;d_2&amp;amp;0\\0&amp;amp;0&amp;amp;d_3\end{bmatrix}
\begin{bmatrix}1&amp;amp;l_{21}&amp;amp;l_{31}\\0&amp;amp;1&amp;amp;l_{32}\\0&amp;amp;0&amp;amp;1\end{bmatrix}\\
=\begin{bmatrix}d_1&amp;amp;l_{21}d_1&amp;amp;l_{31}d_1\\l_{21}d_1&amp;amp;l_{21}^2d_1+d_2&amp;amp;l_{31}l_{21}d_1+l_{32}d_2\\l_{31}d_1&amp;amp;l_{31}l_{21}d_1+l_{32}d_2&amp;amp;l_{31}^2d_1+l_{32}^2d_2+d_3\end{bmatrix}\]&lt;/span> 令等号左右第一列相等，可以得到 &lt;span class="math">\[d_1=a_{11}\\
l_{21}=a_{21}/d_1\\
l_{31}=a_{31}/d_1\]&lt;/span> 再令第二列、第三列相等，可以得到 &lt;span class="math">\[d_2=a_{22}-l_{21}^2d_1\\
l_{32}=(a_{32}-l_{31}l_{21}d_1)/d_2\\
d_3=a_{33}-l_{31}^2d_1-l_{32}^2d_2\]&lt;/span>&lt;/p>
&lt;p>如果矩阵&lt;span class="math">\(A\)&lt;/span>不正定，那么计算出的&lt;span class="math">\(d_i\)&lt;/span>很可能是负的，同时导致&lt;span class="math">\(L\)&lt;/span>矩阵的元素值不稳定，很可能过大。也就是说，分解成&lt;span class="math">\(LDL^T\)&lt;/span>形式后再修改&lt;span class="math">\(d_i\)&lt;/span>的值已经来不及了，我们需要在分解的过程中随时修改&lt;span class="math">\(d_i\)&lt;/span>。以上面的例子为例，如果&lt;span class="math">\(d_1\)&lt;/span>和&lt;span class="math">\(d_2\)&lt;/span>都是正数，而&lt;span class="math">\(d_3\)&lt;/span>是个负数，那么我们就直接把&lt;span class="math">\(d_3\)&lt;/span>替换为一个正数。由于&lt;span class="math">\(d_3\)&lt;/span>是最后一步算出来的，所以它的改变不影响其它分解结果。但假设另一种情况，&lt;span class="math">\(d_1\)&lt;/span>是正数，&lt;span class="math">\(d_2\)&lt;/span>就已经是负数了，这时我们仍然直接把&lt;span class="math">\(d_2\)&lt;/span>替换为正数，不过这会影响下一步&lt;span class="math">\(l_{32}\)&lt;/span>的计算，所以这种情况下&lt;span class="math">\(L\)&lt;/span>矩阵会随之改变。&lt;/p>
&lt;p>为什么要这样做呢？回到我们最初的目标，如果&lt;span class="math">\(A\)&lt;/span>不正定，我们想要找到一个正定矩阵&lt;span class="math">\(A&amp;#39;=A+E\)&lt;/span>来替代&lt;span class="math">\(A\)&lt;/span>，这样一来，虽然&lt;span class="math">\(A=LDL^T\)&lt;/span>可能不存在，但我们可以找到使&lt;span class="math">\(A&amp;#39;=L&amp;#39;D&amp;#39;L&amp;#39;^T\)&lt;/span>成立的&lt;span class="math">\(A&amp;#39;\)&lt;/span>。上面的分解步骤相当于找到了一个&lt;span class="math">\(A&amp;#39;=L&amp;#39;(D+diag(\tau_i))L&amp;#39;^T\)&lt;/span>，其中&lt;span class="math">\(\tau_i=\begin{cases} 0,&amp;amp;d_i&amp;gt;\delta\\ \delta-d_i,&amp;amp;d_i&amp;lt;\delta \end{cases}\)&lt;/span>。&lt;/p>
&lt;p>到这里还没完，这一方法有一个神奇的性质，&lt;span class="math">\(L&amp;#39;D&amp;#39;L&amp;#39;^T=LDL^T+diag(\tau_i)\)&lt;/span>恒成立，即&lt;span class="math">\(A&amp;#39;=A+diag(\tau_i)\)&lt;/span>。我们在&lt;span class="math">\(D\)&lt;/span>的对角线元素上的改动竟然直接相当于修改&lt;span class="math">\(A\)&lt;/span>的对角线元素，同时保持非对角线元素不变。这个性质非常完美，可以最大限度地保留&lt;span class="math">\(A\)&lt;/span>本身的值，只对某些对角线元素做微小的修改，它的改动最小，也最高效。至于为什么会存在这样的性质，观察上面的推导，可以发现&lt;span class="math">\(A\)&lt;/span>的非对角线元素都包含&lt;span class="math">\(l_{ij}d_i\)&lt;/span>项，而这一项在整个推导过程中其实是不变的。&lt;/p>
&lt;p>由于篇幅有限，modified Cholesky的实际操作上的细节我们就不展开讲了。&lt;/p></description></item><item><title>线性代数与矩阵之广义逆矩阵</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E5%B9%BF%E4%B9%89%E9%80%86%E7%9F%A9%E9%98%B5/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E5%B9%BF%E4%B9%89%E9%80%86%E7%9F%A9%E9%98%B5/</guid><description>
&lt;h2 id="广义逆矩阵">广义逆矩阵&lt;!-- omit in toc -->&lt;/h2>
&lt;p>广义逆（Generalized inverse），是线性代数中针对矩阵的一种运算。一个矩阵&lt;span class="math">\(A\)&lt;/span>的广义逆叫做&lt;span class="math">\(A\)&lt;/span>的广义逆阵，是指具有部分逆矩阵的特性，但是不一定具有逆矩阵的所有特性的另一矩阵。假设一矩阵&lt;span class="math">\(A\in \mathbb {R} ^{n\times m}\)&lt;/span>及另一矩阵&lt;span class="math">\(A^{\mathrm {g} }\in \mathbb {R} ^{m\times n}\)&lt;/span>，若&lt;span class="math">\(A^{\mathrm {g} }\)&lt;/span>满足&lt;span class="math">\(AA^{\mathrm {g} }A=A\)&lt;/span>，则&lt;span class="math">\(A^{\mathrm {g} }\)&lt;/span>即为&lt;span class="math">\(A\)&lt;/span>的广义逆阵。&lt;/p>
&lt;p>广义逆也称为伪逆（pseudoinverse），有些时候，伪逆特指&lt;strong>摩尔－彭若斯广义逆&lt;/strong>。&lt;/p>
&lt;h2 id="摩尔彭若斯广义逆">摩尔－彭若斯广义逆&lt;/h2>
&lt;p>彭若斯条件可以用来定义不同的广义逆阵：针对&lt;span class="math">\(A\in \mathbb {R} ^{n\times m}\)&lt;/span>及&lt;span class="math">\(A^{\mathrm {g} }\in \mathbb {R} ^{m\times n}\)&lt;/span>，&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\({\displaystyle AA^{\mathrm {g} }A=A}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\displaystyle A^{\mathrm {g} }AA^{\mathrm {g} }=A^{\mathrm {g}}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\({\displaystyle (AA^{\mathrm {g} })^{\mathrm {T} }=AA^{\mathrm {g}}}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\({\displaystyle (A^{\mathrm {g} }A)^{\mathrm {T} }=A^{\mathrm {g} }A}\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>若&lt;span class="math">\({\displaystyle A^{\mathrm {g} }}\)&lt;/span>满足条件(1)，即为&lt;span class="math">\(A\)&lt;/span>的广义逆阵，若满足条件(1)和(2)，则为&lt;span class="math">\(A\)&lt;/span>的广义反身逆阵（generalized reflexive inverse），若四个条件都满足，则为&lt;span class="math">\(A\)&lt;/span>的摩尔－彭若斯广义逆。&lt;/p>
&lt;p>广义逆矩阵可以通过满秩分解或者SVD分解求得，虽然方法不同，满秩分解也不是唯一的，但是计算出的广义逆矩阵是一样的。&lt;/p>
&lt;p>假设&lt;span class="math">\(A=BC\)&lt;/span>为任一满秩分解，则&lt;span class="math">\(A^g=C^H(CC^H)^{-1}(B^HB)^{-1}B^H\)&lt;/span>&lt;/p>
&lt;p>假设&lt;span class="math">\(A=U\Sigma V^T\)&lt;/span>为SVD分解，则&lt;span class="math">\(A^g=V\Sigma^gU^T\)&lt;/span>，其中&lt;span class="math">\(\Sigma^g\)&lt;/span>中元素为对应&lt;span class="math">\(\Sigma\)&lt;/span>中非零元素的倒数。&lt;/p>
&lt;h2 id="单边逆矩阵">单边逆矩阵&lt;/h2>
&lt;p>单边逆矩阵（左逆矩阵或是右逆矩阵）若矩阵&lt;span class="math">\(A\)&lt;/span>的维度是&lt;span class="math">\(n\times m\)&lt;/span>且为(行或列)满秩，若&lt;span class="math">\(n&amp;gt;m\)&lt;/span>则用左逆矩阵，若&lt;span class="math">\(n&amp;lt;m\)&lt;/span>则用右逆矩阵。&lt;/p>
&lt;p>左逆矩阵为&lt;span class="math">\(A_{\mathrm {left} }^{-1}=\left(A^{\mathrm {T} }A\right)^{-1}A^{\mathrm {T} }\)&lt;/span>，也就是&lt;span class="math">\(A_{\mathrm {left} }^{-1}A=I_{m}\)&lt;/span>，其中&lt;span class="math">\(I_{m}\)&lt;/span>为&lt;span class="math">\(m\times m\)&lt;/span>单位矩阵。&lt;/p>
&lt;p>右逆矩阵为&lt;span class="math">\(A_{\mathrm {right} }^{-1}=A^{\mathrm {T} }\left(AA^{\mathrm {T} }\right)^{-1}\)&lt;/span>，也就是&lt;span class="math">\(AA_{\mathrm {right} }^{-1}=I_{n}\)&lt;/span>，其中&lt;span class="math">\(I_n\)&lt;/span>为&lt;span class="math">\(n\times n\)&lt;/span>单位矩阵。&lt;/p>
&lt;h2 id="性质">性质&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>可逆矩阵有&lt;span class="math">\((AB)^{-1}=B^{-1}A^{-1}\)&lt;/span>，但是广义逆&lt;span class="math">\((AB)^g\)&lt;/span>与&lt;span class="math">\(B^gA^g\)&lt;/span>一般不相等。&lt;/li>
&lt;li>&lt;span class="math">\((A^g)^g=A\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\((A^H)^g=(A^g)^H\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(k\in R\)&lt;/span>，则有&lt;span class="math">\((kA^g)=k^gA^g，k^g=\begin{cases}k^{-1}，k\neq 0\\0，k=0\end{cases}\)&lt;/span>&lt;/li>
&lt;li>区别于性质1，有&lt;span class="math">\((A^H A)^g=A^g(A^H)^g\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(A^g=(A^HA)^gA^H=A^H(AA^H)^g\)&lt;/span>&lt;/li>
&lt;li>对于酉矩阵&lt;span class="math">\(U,V\)&lt;/span>，则&lt;span class="math">\((UAV)^g=V^HA^gU^H\)&lt;/span>。从这点也可以看出用SVD分解求广义逆的方法。&lt;/li>
&lt;li>&lt;span class="math">\(A^gAB=A^gAC\Leftrightarrow AB=AC\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;h2 id="广义逆矩阵的应用">广义逆矩阵的应用&lt;/h2>
&lt;p>当&lt;span class="math">\(Ax=b\)&lt;/span>无解时可以用广义逆矩阵求近似解，实际是就是求&lt;span class="math">\(||Ax-b||_2\)&lt;/span>最小。&lt;/p>
&lt;p>其通解为 &lt;span class="math">\[x=A^gb+(I-A^gA)y，y\in C^n\]&lt;/span> 其中，&lt;span class="math">\(A^gb\)&lt;/span>是唯一的极小最小二乘解。极小指长度（范数）最小。&lt;/p></description></item><item><title>线性代数与矩阵之对称矩阵</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5/</link><pubDate>Sun, 19 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5/</guid><description>
&lt;h2 id="线性代数与矩阵之对称矩阵">线性代数与矩阵之对称矩阵&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#对称矩阵">对称矩阵&lt;/a>&lt;/li>
&lt;li>&lt;a href="#简单性质">简单性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实对称矩阵的特征值与特征向量">实对称矩阵的特征值与特征向量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征值都是实数">特征值都是实数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#不同特征值的特征向量正交">不同特征值的特征向量正交&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对称矩阵与正定性">对称矩阵与正定性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对称矩阵的分解">对称矩阵的分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特征分解谱分解">特征分解（谱分解）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#cholesky分解">Cholesky分解&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#分解唯一性">分解唯一性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分解方法">分解方法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#cholesky分解变形ldl分解">Cholesky分解变形——LDL分解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拓展埃米特矩阵">拓展：埃米特矩阵&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="对称矩阵">对称矩阵&lt;/h2>
&lt;p>在线性代数中，对称矩阵（英语：symmetric matrix）是一个方形矩阵，其&lt;strong>转置矩阵和自身相等&lt;/strong>。 &lt;span class="math">\[A = A^{\textrm{T}}\]&lt;/span> 对称矩阵中的右上至左下方向元素以主对角线（左上至右下）为轴进行对称。若将其写作&lt;span class="math">\(A = (a_{ij})\)&lt;/span>，则对所有的&lt;span class="math">\(i和j\)&lt;/span>， &lt;span class="math">\[a_{ij}=a_{ji}.\]&lt;/span> 下列是3×3的对称矩阵： &lt;span class="math">\[\begin{bmatrix}
1 &amp;amp; 2 &amp;amp; 3\\
2 &amp;amp; 4 &amp;amp; -5\\
3 &amp;amp; -5 &amp;amp; 6\end{bmatrix}\]&lt;/span> 下列是&lt;strong>斜对称矩阵&lt;/strong>（英语：skew-symmetric matrix，又称&lt;strong>反对称矩阵&lt;/strong>，英语：antisymmetric matrix）： &lt;span class="math">\[\begin{bmatrix}
0 &amp;amp; -3 &amp;amp; 4\\
3 &amp;amp; 0 &amp;amp; -5\\
-4 &amp;amp; 5 &amp;amp; 0\end{bmatrix}\]&lt;/span>&lt;/p>
&lt;h2 id="简单性质">简单性质&lt;/h2>
&lt;ul>
&lt;li>对于任何方形矩阵&lt;span class="math">\(X\)&lt;/span>，&lt;span class="math">\(X+X^T\)&lt;/span>是对称矩阵。（此外，对称矩阵的和也是对称矩阵。）&lt;/li>
&lt;li>&lt;span class="math">\(A\)&lt;/span>为方形矩阵是&lt;span class="math">\(A\)&lt;/span>为对称矩阵的&lt;strong>必要条件&lt;/strong>，即对称矩阵行数必等于列数（显而易见）。&lt;/li>
&lt;li>对角矩阵都是对称矩阵（显而易见）。&lt;/li>
&lt;li>每个实方形矩阵都可写作&lt;strong>两个实对称矩阵的积&lt;/strong>，每个复方形矩阵都可写作&lt;strong>两个复对称矩阵的积&lt;/strong>。（神奇，未曾自己证明）&lt;/li>
&lt;li>若对称矩阵&lt;span class="math">\(A\)&lt;/span>的每个元素均为&lt;strong>实数&lt;/strong>，&lt;span class="math">\(A\)&lt;/span>是实对称矩阵。&lt;/li>
&lt;li>一个矩阵同时为对称矩阵及斜对称矩阵当且仅当所有元素都是零。（显而易见）&lt;/li>
&lt;li>如果X是对称矩阵，那么&lt;span class="math">\(AXA^T\)&lt;/span> 也是对称矩阵.证明：&lt;span class="math">\((AXA^T)^T=(A^T)^TX^TA^T=AX^TA^T;\because X^T=X;\therefore AX^TA^T=AXA^T\)&lt;/span>。&lt;/li>
&lt;/ul>
&lt;h2 id="实对称矩阵的特征值与特征向量">实对称矩阵的特征值与特征向量&lt;/h2>
&lt;p>先说结论：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>实对称矩阵的特征值都是&lt;strong>实数&lt;/strong>。实际上，即使是对称矩阵在复数域上的推广埃米特矩阵，其特征值也都是实数。关于埃米特矩阵的简介，见笔记末尾&lt;a href="#拓展埃米特矩阵">拓展：埃米特矩阵&lt;/a>。&lt;/li>
&lt;li>实对称矩阵的&lt;strong>属于不同特征值&lt;/strong>的特征向量都是&lt;strong>正交&lt;/strong>的。&lt;/li>
&lt;li>特征值符号和主元符号相同，即正特征值数等于正主元数。（证明略）&lt;/li>
&lt;/ol>
&lt;h3 id="特征值都是实数">特征值都是实数&lt;/h3>
&lt;p>对于实对称矩阵的特征值都是&lt;strong>实数&lt;/strong>的证明，我们直接证明埃米特矩阵&lt;span class="math">\(A\)&lt;/span>，因为实对称矩阵只是埃米特矩阵的特殊情况。&lt;/p>
&lt;blockquote>
&lt;p>我们需要用到&lt;strong>共轭转置&lt;/strong>，即&lt;span class="math">\(A^H,x^H\)&lt;/span>，具体操作为所有元素行列位置交换并取共轭，共轭转置是转置在复数域的推广。 设&lt;span class="math">\(\lambda_1, x_1\)&lt;/span>分别是埃米特矩阵&lt;span class="math">\(A\)&lt;/span>的&lt;strong>任意一对&lt;/strong>特征值和特征向量，因而有： &lt;span class="math">\[Ax_1=\lambda_1 x_1\]&lt;/span> 两边同时左乘&lt;span class="math">\(x_1^H\)&lt;/span>可得 &lt;span class="math">\[x_1^H A x_1=\lambda_1 x_1^Hx=\lambda_1 |x_1|^2\tag{1}\]&lt;/span> 同时，由于&lt;span class="math">\(A\)&lt;/span>是埃米特矩阵，有&lt;span class="math">\(A^H=A\)&lt;/span>，所以有： &lt;span class="math">\[(Ax_1)^H=(\lambda_1 x_1)^H=x_1^H\underbrace{A^H}_{=A}=\underbrace{\bar{\lambda}_1}_{取共轭}x_1^H\]&lt;/span> 两边同时右乘&lt;span class="math">\(x_1\)&lt;/span>可得： &lt;span class="math">\[x_1^H A x_1=\bar{\lambda}_1 x_1^Hx=\bar{\lambda}_1 |x_1|^2\tag{2}\]&lt;/span> 显然，式（1）等于式（2），所以： &lt;span class="math">\[\lambda_1 |x_1|^2=\bar{\lambda}_1 |x_1|^2\Rightarrow \lambda_1=\bar{\lambda}_1\]&lt;/span> 即&lt;span class="math">\(\lambda_1\)&lt;/span>是实数。因为&lt;span class="math">\(\lambda_1\)&lt;/span>是埃米特矩阵&lt;span class="math">\(A\)&lt;/span>的任意一特征值，所以&lt;span class="math">\(A\)&lt;/span>的所有特征值都是实数。&lt;/p>
&lt;/blockquote>
&lt;h3 id="不同特征值的特征向量正交">不同特征值的特征向量正交&lt;/h3>
&lt;p>这里需要指出的是，这个特性只针对不同特征值下的特征向量。而对于同一特征值下的特征向量，我们可以取该特征值特征空间中的一组正交基，来保证其正交性。&lt;strong>后面我们将通过对称矩阵特征值分解的存在性，说明特征空间维数等于特征值代数重数，所以我们能在&lt;span class="math">\(n\)&lt;/span>维对称矩阵中找出&lt;span class="math">\(n\)&lt;/span>个正交的特征向量&lt;/strong>。&lt;/p>
&lt;p>现在，我们先证明不同特征值条件下的特征向量正交性。&lt;/p>
&lt;blockquote>
&lt;p>我们假设对称矩阵&lt;span class="math">\(A\)&lt;/span>任意两个不同的特征值&lt;span class="math">\(\lambda_1,\lambda_2\)&lt;/span>相应的特征向量为&lt;span class="math">\(x_1,x_2\)&lt;/span>，显然有： &lt;span class="math">\[Ax_1=\lambda_1 x_1\quad Ax_2=\lambda_2 x_2\]&lt;/span> 我们将第二个式子做转置加上&lt;span class="math">\(A\)&lt;/span>的对称性可得：&lt;span class="math">\((Ax_2)^T=x_2^TA^T=x_2^TA=\lambda_2 x_2^T\)&lt;/span>。我们用&lt;span class="math">\(x_2^T\)&lt;/span>左乘&lt;span class="math">\(Ax_1=\lambda_1 x_1\)&lt;/span>，再用&lt;span class="math">\(x_1\)&lt;/span>右乘&lt;span class="math">\(x_2^TA=\lambda_2 x_2^T\)&lt;/span>可得： &lt;span class="math">\[x_2^TAx_1=\lambda_1 x_2^Tx_1\\x_2^TAx_1=\lambda_2 x_2^Tx_1\]&lt;/span> 因此可得： &lt;span class="math">\[\lambda_1 x_2^Tx_1=\lambda_2 x_2^Tx_1\Rightarrow (\lambda_1-\lambda_2)x_2^Tx_1=0\]&lt;/span> 由于&lt;span class="math">\(\lambda_1\neq \lambda_2\)&lt;/span>，所以&lt;span class="math">\(x_2^Tx_1=0\)&lt;/span>，即&lt;span class="math">\(x_2^T\perp x_1\)&lt;/span>二者正交。又因为特征值&lt;span class="math">\(\lambda_1,\lambda_2\)&lt;/span>，特征向量&lt;span class="math">\(x_1,x_2\)&lt;/span>的任意性，我们可证不同特征值的特征向量正交。&lt;/p>
&lt;/blockquote>
&lt;h2 id="对称矩阵与正定性">对称矩阵与正定性&lt;/h2>
&lt;p>谈到对称矩阵多少会聊到矩阵的正定性，通常&lt;strong>正定性是定义在对称矩阵（或埃米特矩阵）上&lt;/strong>的，如果一个矩阵不是对称矩阵，就不具备讨论正定性的前提条件。&lt;/p>
&lt;blockquote>
&lt;p>一个&lt;span class="math">\(n×n\)&lt;/span>的实对称矩阵&lt;span class="math">\(A\)&lt;/span>是&lt;strong>正定的&lt;/strong>，当且仅当对于所有的非零实系数向量&lt;span class="math">\(x\)&lt;/span>，都有&lt;span class="math">\(x^TAx&amp;gt;0\)&lt;/span>，其中&lt;span class="math">\(x^T\)&lt;/span>表示x的转置。&lt;/p>
&lt;p>类似的，如果&lt;span class="math">\(x^TAx\geq 0\)&lt;/span>，则&lt;span class="math">\(A\)&lt;/span>称为&lt;strong>半正定矩阵&lt;/strong>；如果&lt;span class="math">\(x^TAx&amp;lt;0\)&lt;/span>，则&lt;span class="math">\(A\)&lt;/span>称为&lt;strong>负定矩阵&lt;/strong>；如果&lt;span class="math">\(x^TAx\leq 0\)&lt;/span>，则&lt;span class="math">\(A\)&lt;/span>称为&lt;strong>半负定矩阵&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>正定性还有这几个等价命题：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的所有特征值为正&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的所有主元为正&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的顺序主子式为正&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>与单位阵&lt;span class="math">\(I\)&lt;/span>合同，即存在可逆矩阵&lt;span class="math">\(C\)&lt;/span>，使得&lt;span class="math">\(A=C^TIC\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>类似的，半正定矩阵有以下等价命题：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的所有特征值为非负&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的所有主元为非负&lt;/li>
&lt;li>矩阵&lt;span class="math">\(A\)&lt;/span>的顺序主子式为非负&lt;/li>
&lt;/ol>
&lt;p>我们可以通过例子说明，非对称矩阵可以在满足上述1，2，3的前提下不满足正定矩阵的定义&lt;span class="math">\(x^TAx&amp;gt;0\)&lt;/span>。如下例 &lt;span class="math">\[
\begin{bmatrix}
1&amp;amp;-100\\0&amp;amp;1
\end{bmatrix}
\]&lt;/span> 显然，其主元、特征值都是1，顺序主子式也都大于0，但是对于&lt;span class="math">\(x^TAx\)&lt;/span>，我们随便找一个向量&lt;span class="math">\(x=[1,1]^T\)&lt;/span>，则 &lt;span class="math">\[
[1\quad 1]\begin{bmatrix}1&amp;amp;-100\\0&amp;amp;1\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix}=-98&amp;lt;0
\]&lt;/span> 这并不满足&lt;span class="math">\(x^TAX&amp;gt;0\)&lt;/span>。这说明，&lt;strong>正定性必须建立在对称矩阵上&lt;/strong>。&lt;/p>
&lt;h2 id="对称矩阵的分解">对称矩阵的分解&lt;/h2>
&lt;p>对称矩阵分解最重要的就是特征分解，又称谱分解。本节将着重介绍对称矩阵特征分解的存在性。同时，对于对称矩阵而言，SVD分解和特征分解是一样的。最后，我们对埃米特矩阵补充了Cholesky分解的相关内容，Cholesky分解是正定埃尔米特矩阵的LU分解，且具有唯一性。&lt;/p>
&lt;h3 id="特征分解谱分解">特征分解（谱分解）&lt;/h3>
&lt;p>特征分解的具体方法可见笔记&lt;a href="线性代数与矩阵之特征值与特征向量.md">线性代数与矩阵之特征值与特征向量&lt;/a>。这节我们着重说明对称矩阵特征分解的必定存在性。&lt;/p>
&lt;p>首先，我们需要引入实矩阵的舒尔分解，详细内容见&lt;a href="线性代数与矩阵之矩阵分解.md">线性代数与矩阵之矩阵分解&lt;/a>：&lt;/p>
&lt;blockquote>
&lt;p>实数域的舒尔分解定理：如果&lt;span class="math">\(A∈\mathbb{R}^n\)&lt;/span>是&lt;span class="math">\(n\)&lt;/span>阶的&lt;strong>实方阵&lt;/strong>，则存在&lt;span class="math">\(n\)&lt;/span>阶正交矩阵&lt;span class="math">\(Q\)&lt;/span>，&lt;span class="math">\(n\)&lt;/span>阶&lt;strong>拟上三角&lt;/strong>矩阵&lt;span class="math">\(T\)&lt;/span>，使得 &lt;span class="math">\[A=QTQ^{-1}=QTQ^T\]&lt;/span> 即任何一个&lt;span class="math">\(n\)&lt;/span>阶实方阵&lt;span class="math">\(A\)&lt;/span>正交相似于一个&lt;span class="math">\(n\)&lt;/span>阶&lt;strong>拟上三角&lt;/strong>矩阵&lt;span class="math">\(T\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>对于对称矩阵而言，由于&lt;span class="math">\(A^T=A\)&lt;/span>，所以矩阵&lt;span class="math">\(A\)&lt;/span>和其转置&lt;span class="math">\(A^T\)&lt;/span>写成舒尔分解形式有： &lt;span class="math">\[\left . \begin{aligned}A=QTQ^T\\A^T=QT^TQ^T\\A=A^T\end{aligned}\right\}\Rightarrow QT^TQ^T=QTQ^T\Rightarrow T^T=T\]&lt;/span> 由于&lt;span class="math">\(T\)&lt;/span>是拟三角矩阵，当其也为对称矩阵的时候，&lt;span class="math">\(T\)&lt;/span>必然是一个对角矩阵。我们用&lt;span class="math">\(\Lambda\)&lt;/span>表示。这个证明用转置相等可以证明，在此就不详细写了。所以，对称矩阵&lt;span class="math">\(A\)&lt;/span>一定可以分解成正交矩阵和对角阵的组合，即&lt;span class="math">\(A=Q\Lambda Q^T\)&lt;/span>。&lt;/p>
&lt;p>以上说明，对称矩阵&lt;span class="math">\(A\)&lt;/span>是一定可以对角化的，而构建对角化的方法，就是笔记&lt;a href="线性代数与矩阵之特征值与特征向量.md">线性代数与矩阵之特征值与特征向量&lt;/a>中提到的特征分解方法。&lt;/p>
&lt;p>最后，加一个注：&lt;strong>对称矩阵的SVD分解等于谱分解&lt;/strong>。&lt;/p>
&lt;h3 id="cholesky分解">Cholesky分解&lt;/h3>
&lt;p>Cholesky分解是正定埃尔米特矩阵的LU分解。&lt;/p>
&lt;p>Cholesky分解是指将一个&lt;strong>正定的埃尔米特矩阵&lt;/strong>分解成一个&lt;strong>下三角矩阵与其共轭转置之乘积&lt;/strong>。 &lt;span class="math">\[\mathbf A=\mathbf L\mathbf L^\ast\]&lt;/span> 当矩阵&lt;span class="math">\(\mathbf{A}\)&lt;/span>是一个&lt;strong>半正定&lt;/strong>的埃尔米特矩阵，若允许&lt;span class="math">\(\mathbf {L}\)&lt;/span>的对角线元素为&lt;strong>零&lt;/strong>，则&lt;span class="math">\(\mathbf{A}\)&lt;/span>也存在上述形式的分解。&lt;/p>
&lt;p>当&lt;span class="math">\(\mathbf{A}\)&lt;/span>为实数矩阵，则&lt;span class="math">\(\mathbf {L}\)&lt;/span>也为实数矩阵且Cholesky分解可改写成 &lt;span class="math">\[\mathbf {A} =\mathbf {LL} ^{\mathbf {T} }\]&lt;/span>&lt;/p>
&lt;h4 id="分解唯一性">分解唯一性&lt;/h4>
&lt;p>当&lt;span class="math">\(\mathbf{A}\)&lt;/span>是&lt;strong>正定&lt;/strong>矩阵时，Cholesky分解是&lt;strong>唯一&lt;/strong>的，即只存在一个对角元素均严格大于零的下三角矩阵，使&lt;span class="math">\(\mathbf {A} =\mathbf {LL} ^{*}\)&lt;/span>成立。然而，当&lt;span class="math">\(\mathbf{A}\)&lt;/span>是&lt;strong>半正定&lt;/strong>时，分解则&lt;strong>不一定&lt;/strong>是唯一的。&lt;/p>
&lt;p>定理的逆命题自然成立：对于某些可逆矩阵&lt;span class="math">\(\mathbf {L}\)&lt;/span>（下三角矩阵或其他矩阵），如果&lt;span class="math">\(\mathbf{A}\)&lt;/span>可被写成&lt;span class="math">\(\mathbf {LL} ^{*}\)&lt;/span>，则&lt;span class="math">\(\mathbf{A}\)&lt;/span>是一个正定的埃尔米特矩阵。&lt;/p>
&lt;h4 id="分解方法">分解方法&lt;/h4>
&lt;p>Cholesky分解是LU分解的高斯消元法改进。&lt;/p>
&lt;p>这种分解方式在提高代数运算效率、蒙特卡罗方法等场合中十分有用。实数矩阵的Cholesky分解由安德烈·路易·科列斯基（英语：André-Louis Cholesky）最先发明。实际应用中，Cholesky分解在求解线性方程组中的效率约两倍于LU分解。&lt;/p>
&lt;h3 id="cholesky分解变形ldl分解">Cholesky分解变形——LDL分解&lt;/h3>
&lt;p>经典Cholesky分解的一个变形是LDL分解，即 &lt;span class="math">\[\mathbf {A} =\mathbf {LDL} ^{*}\]&lt;/span> 其中，&lt;span class="math">\(\mathbf {L}\)&lt;/span>是一个单位下三角矩阵，&lt;span class="math">\(\mathbf {D}\)&lt;/span>是一个对角矩阵。&lt;/p>
&lt;p>该分解与经典Cholesky分解犹有关系，如下：&lt;/p>
&lt;p>&lt;span class="math">\[\mathbf {A} =\mathbf {LDL} ^{*}=\mathbf {LD} ^{\frac {1}{2}}(\mathbf {D} ^{\frac {1}{2}})^{*}\mathbf {L} ^{*}=\mathbf {LD} ^{\frac {1}{2}}(\mathbf {LD} ^{\frac {1}{2}})^{*}\]&lt;/span>&lt;/p>
&lt;p>LDL变形如果得以有效运行，构造及使用时所需求的空间及计算的复杂性与经典Cholesky分解是相同的，但是可避免提取平方根。某些不存在Cholesky分解的不定矩阵，也可以运行LDL分解，&lt;span class="math">\(\mathbf {D}\)&lt;/span>中会出现负数元素。因此人们更倾向于使用LDL分解。对于实数矩阵，该种分解的形式可被改写成 &lt;span class="math">\[\mathbf {A} =\mathbf {LDL} ^{\mathbf {T} }\]&lt;/span> 此形式通常称为LDLT分解（或LDLT分解）。它与实对称矩阵的特征分解密切相关，因为对于实对称矩阵，存在特征分解&lt;span class="math">\(\mathbf {A} =\mathbf {Q\Lambda Q}^T\)&lt;/span>。&lt;/p>
&lt;h2 id="拓展埃米特矩阵">拓展：埃米特矩阵&lt;/h2>
&lt;p>埃米特矩阵是对称矩阵在复数域的推广，将实对称矩阵的转置&lt;span class="math">\(A=A^T\)&lt;/span>升级为共轭转置&lt;span class="math">\(A=A^H\)&lt;/span>，其中字母“H”来自埃米特的英文“Hermite”。&lt;/p>
&lt;p>对称矩阵的性质在复数空间中由埃米特矩阵继承。&lt;/p>
&lt;p>埃尔米特矩阵主对角线上的元素都是实数的，其特征值也是实数。埃尔米特矩阵是正规矩阵，因此埃尔米特矩阵可被酉对角化，而且得到的对角阵的元素都是实数。这意味着埃尔米特矩阵的特征值都是实的，而且不同的特征值所对应的特征向量相互正交，因此可以在这些特征向量中找出一组&lt;span class="math">\(C^n\)&lt;/span>的正交基。&lt;/p></description></item><item><title>拓扑学之同坯</title><link>https://surprisedcat.github.io/studynotes/%E6%8B%93%E6%89%91%E5%AD%A6%E4%B9%8B%E5%90%8C%E5%9D%AF/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%8B%93%E6%89%91%E5%AD%A6%E4%B9%8B%E5%90%8C%E5%9D%AF/</guid><description>
&lt;h2 id="同坯的通俗解释">同坯的通俗解释&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="同胚的通俗解释">同胚的通俗解释&lt;/h2>
&lt;p>拓扑学是研究几何图形或空间在连续改变形状后还能保持不变的一些性质的学科。它只考虑物体间的位置关系而不考虑它们的形状和大小。在拓扑学里，重要的拓扑性质包括连通性与紧致性 ——百度百科&lt;/p>
&lt;p>拓扑学也被称为&lt;strong>橡皮泥几何学&lt;/strong> ，拓扑学不是很在意空间的距离或物体的长度，而是更重视空间的形态，比如在拓扑学家眼中，骰子和台球可以归类到一种物体，但是他们两个和面包圈却不是同一种物体，我们通过拉伸，挤压等“温和操作”，可以把骰子“捏”成台球，所以拓扑学认为他俩是一种东西，但是面包圈有一个洞，只有通过撕裂，钻孔，粘合等“暴力操作”，才能把没洞的台球变成有洞的面包圈，所以拓扑学认为他俩是两种东西。&lt;/p>
&lt;p>因此只要是一个“胚子”捏出来的形状，在拓扑学中就是同一种东西，这种概念叫做&lt;strong>同胚&lt;/strong>，也可以叫做&lt;strong>拓扑等价&lt;/strong>。&lt;/p>
&lt;p>两个物体是否同胚，要看在拓扑变换后，点、线、体的数目和原来是不是一样。一般来说，对于任意形状的闭曲面，只要不把曲面撕裂或割破，它的变换就可以算作是拓扑变换，变换后的各种形态都是拓扑等价的。&lt;/p>
&lt;h2 id="同胚的数学描述">同胚的数学描述&lt;/h2>
&lt;blockquote>
&lt;p>同胚：&lt;span class="math">\(\varphi：X→Y\)&lt;/span>称为同胚的，若&lt;span class="math">\(\varphi\)&lt;/span>为双射，且&lt;span class="math">\(\varphi\)&lt;/span>连续，&lt;span class="math">\(\varphi^{-1}\)&lt;/span>连续。&lt;/p>
&lt;/blockquote>
&lt;p>先要搞清楚：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>什么是拓扑空间。&lt;/li>
&lt;li>什么是拓扑空间之间的连续映射。&lt;/li>
&lt;/ol>
&lt;h2 id="同胚涉及了拓扑学的终极目标">同胚涉及了拓扑学的终极目标&lt;/h2>
&lt;p>哪些空间是一样的？即分类互不同胚的空间。&lt;/p>
&lt;p>重要的方法：拓扑不变量（代数拓扑）来证明不同胚从而区分空间。可以是亏格，群，环 &amp;gt;拓扑不变量：设对任意空间&lt;span class="math">\(X\)&lt;/span>定义了一个量&lt;span class="math">\(\mu(X)\)&lt;/span>，若&lt;span class="math">\(\mu\)&lt;/span>满足：只要&lt;span class="math">\(X\)&lt;/span>与&lt;span class="math">\(Y\)&lt;/span>同胚，则&lt;span class="math">\(\mu(X)=\mu(Y)\)&lt;/span>，则称&lt;span class="math">\(\mu\)&lt;/span>为一个拓扑不变量。其中相等“=”可以换成同构。&lt;/p>
&lt;p>e.g. 欧拉示性数（v-e+f）&lt;/p>
&lt;p>拓扑不变量的常用方法是用其逆否命题，即拓扑不变量不等，二者不同胚。&lt;/p>
&lt;p>另一类常见的问题是证明两个空间同胚，这个一般比较困难。&lt;/p></description></item><item><title>拓扑学之紧性</title><link>https://surprisedcat.github.io/studynotes/%E6%8B%93%E6%89%91%E5%AD%A6%E4%B9%8B%E7%B4%A7%E6%80%A7/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%8B%93%E6%89%91%E5%AD%A6%E4%B9%8B%E7%B4%A7%E6%80%A7/</guid><description>
&lt;h2 id="拓扑学之紧性">拓扑学之紧性&lt;!-- omit in toc -->&lt;/h2>
&lt;p>详见：文档中拓扑课程讲义第三课。&lt;/p>
&lt;h2 id="连续函数">连续函数&lt;/h2>
&lt;p>一般定义与六种等价定义&lt;/p>
&lt;h2 id="极限">极限&lt;/h2>
&lt;h3 id="极限唯一性条件">极限唯一性条件&lt;/h3>
&lt;p>在Hausdorff空间中极限具有唯一性&lt;/p>
&lt;h2 id="hausdorff空间">Hausdorff空间&lt;/h2>
&lt;p>T2空间，应该我以后遇到的大多数空间都是Hausdorff空间&lt;/p>
&lt;h3 id="度量空间与hausdorff空间">度量空间与Hausdorff空间&lt;/h3>
&lt;p>度量空间都是Hausdorff空间&lt;/p>
&lt;h3 id="极限与映射的可交换性">极限与映射的可交换性&lt;/h3>
&lt;p>Hausdorff空间中的连续映射可以交换极限和映射。&lt;/p>
&lt;h2 id="紧性">紧性&lt;/h2>
&lt;p>有限覆盖定理&lt;/p>
&lt;blockquote>
&lt;p>定义1：&lt;/p>
&lt;/blockquote>
&lt;p>紧性的必要性&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>无限化为有限，便于操作&lt;/li>
&lt;li>可放心使用定积分&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>定义2：&lt;/p>
&lt;/blockquote>
&lt;hr />
&lt;blockquote>
&lt;p>定义3：&lt;/p>
&lt;/blockquote>
&lt;p>两种形式的紧性定义的一致性&lt;/p>
&lt;h3 id="absubset-r是紧子集">&lt;span class="math">\([a,b]\subset R\)&lt;/span>是紧子集&lt;/h3>
&lt;h3 id="紧与闭">紧与闭&lt;/h3>
&lt;p>&lt;strong>什么时候紧&lt;span class="math">\(\Rightarrow\)&lt;/span>闭&lt;/strong>？&lt;/p>
&lt;p>Hausdorff空间&lt;/p>
&lt;h3 id="rn空间中紧集与有界闭集等价性">&lt;span class="math">\(R^n\)&lt;/span>空间中紧集与有界闭集等价性&lt;/h3></description></item><item><title>线性代数与矩阵之度量矩阵与广义内积</title><link>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E5%BA%A6%E9%87%8F%E7%9F%A9%E9%98%B5%E4%B8%8E%E5%B9%BF%E4%B9%89%E5%86%85%E7%A7%AF/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%8B%E5%BA%A6%E9%87%8F%E7%9F%A9%E9%98%B5%E4%B8%8E%E5%B9%BF%E4%B9%89%E5%86%85%E7%A7%AF/</guid><description>
&lt;h2 id="度量矩阵与广义内积">度量矩阵与广义内积&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="度量矩阵">度量矩阵&lt;/h2>
&lt;h3 id="gram矩阵">Gram矩阵&lt;/h3>
&lt;p>设&lt;span class="math">\(A\)&lt;/span>为一个&lt;span class="math">\(m\times n\)&lt;/span>阶实矩阵，&lt;span class="math">\(n\)&lt;/span>阶方阵&lt;span class="math">\(G=[g_{ij}]=A^TA\)&lt;/span>称为Gramian或Gram矩阵，也有人称之为交互乘积(cross-product)矩阵。考虑&lt;span class="math">\(A\)&lt;/span>的行向量表达式&lt;span class="math">\(A=\begin{bmatrix} \mathbf{a}_1&amp;amp;\mathbf{a}_2&amp;amp;\cdots&amp;amp;\mathbf{a}_n \end{bmatrix}，\mathbf{a}_i\in\mathbb{R}^m\)&lt;/span>，则 &lt;span class="math">\[G=A^TA=\begin{bmatrix} \mathbf{a}_1^T\\ \mathbf{a}_2^T\\ \vdots\\ \mathbf{a}_n^T \end{bmatrix}\begin {bmatrix} \mathbf{a}_1&amp;amp;\mathbf{a}_2&amp;amp;\cdots&amp;amp;\mathbf{a}_n \end{bmatrix}=\begin{bmatrix} \mathbf{a}_1^T\mathbf{a}_1&amp;amp;\mathbf {a}_1^T\mathbf{a}_2&amp;amp;\cdots&amp;amp;\mathbf{a}_1^T\mathbf{a}_n\\ \mathbf{a}_2^T\mathbf{a}_1&amp;amp;\mathbf{a}_2 ^T\mathbf{a}_2&amp;amp;\cdots&amp;amp;\mathbf{a}_2^T\mathbf{a}_n\\ ~&amp;amp;~&amp;amp;~&amp;amp;~\\ \mathbf{a}_n^T\mathbf{a}_1&amp;amp;\mathbf{a}_n^T\mathbf{a}_2&amp;amp;\cdots&amp;amp;\mathbf{a}_n^T\mathbf{a}_n \end{bmatrix}\]&lt;/span> 这指出&lt;span class="math">\(g_{ij}=\mathbf{a}_i^T\mathbf{a}_j\)&lt;/span>。推广至一般情况，设向量集&lt;span class="math">\(\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_n\)&lt;/span>属于内积空间&lt;span class="math">\(\mathcal{V}，n\times n\)&lt;/span>阶Gramian矩阵&lt;span class="math">\(G\)&lt;/span>的&lt;span class="math">\((i,j)\)&lt;/span>元定义为&lt;span class="math">\(\mathbf{v}_i和\mathbf{v}_j\)&lt;/span>的内积，以&lt;span class="math">\(g_{ij}=\left\langle\mathbf{v}_i,\mathbf{v}_j\right\rangle\)&lt;/span>表示。&lt;/p>
&lt;p>本文仅讨论广泛应用于统计学与控制系统的实Gramian矩阵，以下列举几个实Gramian矩阵&lt;span class="math">\(G=A^TA\)&lt;/span>的基本性质。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(A^TA\)&lt;/span>是对称矩阵。&lt;/li>
&lt;/ol>
&lt;p>明显地，&lt;span class="math">\(g_{ij}=\mathbf{a}_i^T\mathbf{a}_j=\mathbf{a}_j^T\mathbf{a}_i=g_{ji}\)&lt;/span>。&lt;/p>
&lt;p>(2)对任一实矩阵&lt;span class="math">\(A，\mathrm{rank}(A^TA)=\mathrm{rank}A\)&lt;/span>。&lt;/p>
&lt;p>(3)若&lt;span class="math">\(A^TA=0，则A=0\)&lt;/span>。&lt;/p>
&lt;p>性质(3)是性质(2)的必然结果。若&lt;span class="math">\(A^TA=0\)&lt;/span>，则&lt;span class="math">\(\mathrm{rank}A=\mathrm{rank}A^TA=\mathrm{rank}0=0\)&lt;/span>，惟有零矩阵其矩阵秩为零，推得&lt;span class="math">\(A=0\)&lt;/span>。&lt;/p>
&lt;p>Gramian矩阵和正定矩阵有密切的关系，归结为以下三个性质。&lt;/p>
&lt;p>(4)对任一实矩阵&lt;span class="math">\(A，A^TA\)&lt;/span>是半正定矩阵。设&lt;span class="math">\(\mathbf{x}\in\mathbb{R}^n且\mathbf{x}\neq\mathbf{0}\)&lt;/span>，则&lt;span class="math">\(\mathbf{x}^TA^TA\mathbf{x}=(A\mathbf{x})^T(A\mathbf{x})=\Vert A\mathbf{x}\Vert^2\ge 0\)&lt;/span>。&lt;/p>
&lt;p>(5)任一实对称半正定矩阵&lt;span class="math">\(M\)&lt;/span>皆可表示为Grammin矩阵，亦即存在一矩阵&lt;span class="math">\(A\)&lt;/span>使得&lt;span class="math">\(M=A^TA\)&lt;/span>。&lt;/p>
&lt;p>实对称矩阵&lt;span class="math">\(M\)&lt;/span>可正交对角化为&lt;span class="math">\(M=Q\Lambda Q^{T}\)&lt;/span>，其中&lt;span class="math">\(Q^TQ=I，\Lambda=\mathrm{diag}(\lambda_1,\ldots,\lambda_n)\)&lt;/span>。又&lt;span class="math">\(M\)&lt;/span>为半正定，这意味对所有&lt;span class="math">\(i，\lambda_i\ge 0\)&lt;/span>，故可令&lt;span class="math">\(\Lambda^{1/2}=\mathrm{diag}(\sqrt{\lambda_1},\ldots,\sqrt{\lambda_n})\)&lt;/span>且&lt;span class="math">\(A=\Lambda^{1/2}Q^T\)&lt;/span>，立得&lt;span class="math">\(M=Q\Lambda Q^{T}=(Q\Lambda^{1/2})(\Lambda^{1/2}Q^T)=(\Lambda^{1/2}Q^T)^ T(\Lambda^{1/2}Q^T)=A^TA\)&lt;/span>。&lt;/p>
&lt;p>(6)唯当&lt;span class="math">\(A\)&lt;/span>的列向量&lt;span class="math">\(\mathbf{a}_1,\ldots,\mathbf{a}_n\)&lt;/span>是线性独立时，&lt;span class="math">\(A^TA\)&lt;/span>可逆，故为正定矩阵。因为&lt;span class="math">\(A\)&lt;/span>有线性独立的列向量意味&lt;span class="math">\(\mathrm{rank}A=n\)&lt;/span>，由性质(2)可知&lt;span class="math">\(\mathrm{rank}(A^TA)=n\)&lt;/span>，反向陈述显然成立。另外也可以考虑&lt;span class="math">\(A\mathbf{x}=\mathbf{0}\)&lt;/span>，当A有线性独立行向量时，零空间&lt;span class="math">\(N(A)=\{\mathbf{0}\}\)&lt;/span>，性质(4)的等号仅发生于&lt;span class="math">\(\mathbf{x}=\mathbf{0}\)&lt;/span>，故&lt;span class="math">\(A\)&lt;/span>为正定矩阵。&lt;/p>
&lt;h2 id="广义内积">广义内积&lt;/h2>
&lt;p>有复空间&lt;span class="math">\(C^n\)&lt;/span>，有一组基&lt;span class="math">\(bases_1=\{\varepsilon_1,\varepsilon_2,\dotsb,\varepsilon_n\}\)&lt;/span>，&lt;span class="math">\(\alpha,\beta\)&lt;/span>用&lt;span class="math">\(base_1\)&lt;/span>表示的向量，那么&lt;span class="math">\(\alpha,\beta\)&lt;/span>在&lt;span class="math">\(base_1\)&lt;/span>下的内积可以定义为 &lt;span class="math">\[\langle\alpha,\beta\rangle=\sum_{i=1}^n\sum_{j=1}^n x_i\bar y_i\langle\varepsilon_i,\varepsilon_j\rangle\\=x^TA\bar y\]&lt;/span> 其中，&lt;span class="math">\(A=(\langle\varepsilon_i,\varepsilon_j\rangle)_{n*n}\)&lt;/span>，称&lt;span class="math">\(A\)&lt;/span>是空间&lt;span class="math">\(C^n\)&lt;/span>在基&lt;span class="math">\(\{\varepsilon_1,\varepsilon_2,\dotsb,\varepsilon_n\}\)&lt;/span>下的度量矩阵，这种带有度量矩阵的内积称为广义内积。&lt;/p>
&lt;p>对于欧几里得空间&lt;span class="math">\(R^n\)&lt;/span>，度量矩阵是对称矩阵&lt;span class="math">\(A^T=A\)&lt;/span>；对于酉空间&lt;span class="math">\(C^n\)&lt;/span>，度量矩阵是Hermite矩阵&lt;span class="math">\(A^H=A\)&lt;/span>。对于一般内积，可以看成度量矩阵是n阶单位矩阵&lt;span class="math">\(I_{n*n}\)&lt;/span>。&lt;/p>
&lt;p>可以看出定义内积的方式是多种多样的，因此我们能够指出只要满足哪些问题就可以算是内积。&lt;/p>
&lt;h2 id="内积空间">内积空间&lt;/h2>
&lt;p>标量域&lt;span class="math">\({\displaystyle F}\)&lt;/span>是指实数域&lt;span class="math">\(\mathbb {R}\)&lt;/span>或复数域&lt;span class="math">\(\mathbb {C}\)&lt;/span>。&lt;/p>
&lt;p>正式地，一个内积空间是域&lt;span class="math">\(F\)&lt;/span>上的向量空间&lt;span class="math">\(V\)&lt;/span>与一个内积(即一个映射)构成的。&lt;span class="math">\(V\)&lt;/span>上的一个内积定义为正定、非退化的共轭双线性形式（&lt;span class="math">\(F = \mathbb{R}\)&lt;/span>时，内积是一个正定、对称、非退化的双线性形式），记为&lt;span class="math">\(\langle \cdot, \cdot \rangle : V \times V \rightarrow F\)&lt;/span>。&lt;/p>
&lt;p>它满足以下设定：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>共轭对称；&lt;span class="math">\(\forall x,y\in V, \; \; \langle x,y\rangle =\overline{\langle y,x\rangle}\)&lt;/span>，这个设定蕴含了：&lt;span class="math">\(\forall x \in V, \; \; \langle x,x\rangle \in \mathbb{R}\)&lt;/span>，因为&lt;span class="math">\(\langle x,x\rangle = \overline{\langle x,x\rangle}\)&lt;/span>.&lt;/li>
&lt;li>对第一个元素线性； &lt;span class="math">\[\forall a\in F,\ \forall x,y\in V,\ \langle ax,y\rangle= a \langle x,y\rangle, \\ \forall x,y,z\in V,\ \langle x+y,z\rangle= \langle x,z\rangle+ \langle y,z\rangle.\]&lt;/span> 前两条可以推断出： &lt;span class="math">\[\forall b\in F,\ \forall x,y\in V,\ \langle x,by \rangle= \overline{b} \langle x,y\rangle,\\ \forall x,y,z\in V,\ \langle x,y+z\rangle= \langle x,y\rangle+ \langle x,z\rangle.\]&lt;/span> 因此&lt;span class="math">\(\langle \cdot , \cdot \rangle\)&lt;/span>实际上是一个半双线性形式。&lt;/li>
&lt;li>非负性：&lt;span class="math">\(\forall x \in V,\ \langle x,x\rangle \ge 0.\)&lt;/span>&lt;/li>
&lt;li>非退化：从&lt;span class="math">\(V\)&lt;/span>到对偶空间&lt;span class="math">\(V^\ast\)&lt;/span>的映射：&lt;span class="math">\(x\mapsto \langle x,\cdot\rangle\)&lt;/span>是同构映射。在有限维的向量空间中，只需要验证它是单射：&lt;span class="math">\(\langle x,y\rangle = 0 \; \forall y \in V\)&lt;/span>,当且仅当&lt;span class="math">\(x = 0\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;p>拥有以上性质的共轭双线性形式被称为&lt;strong>埃尔米特形式&lt;/strong>。&lt;strong>内积是一个埃尔米特形式&lt;/strong>。如果&lt;span class="math">\(F\)&lt;/span>是实数域&lt;span class="math">\(\mathbb {R}\)&lt;/span>那么共轭对称性质就等价于对称性：&lt;span class="math">\(\langle x,y\rangle=\langle y,x\rangle\)&lt;/span>，也就是说，共轭双线性变成了一般的双线性。&lt;/p></description></item><item><title>拓扑学之基础</title><link>https://surprisedcat.github.io/studynotes/%E6%8B%93%E6%89%91%E5%AD%A6%E4%B9%8B%E5%9F%BA%E7%A1%80/</link><pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%8B%93%E6%89%91%E5%AD%A6%E4%B9%8B%E5%9F%BA%E7%A1%80/</guid><description>
&lt;h2 id="拓扑数学">拓扑数学&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#拓扑空间">拓扑空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#子空间拓扑">子空间拓扑&lt;/a>&lt;/li>
&lt;li>&lt;a href="#度量空间">度量空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#紧度量空间">紧度量空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#完备空间">完备空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有界与完全有界空间">有界与完全有界空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#紧致空间">紧致空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#banach空间">Banach空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#柯西序列">柯西序列&lt;/a>&lt;/li>
&lt;li>&lt;a href="#子序列">子序列&lt;/a>&lt;/li>
&lt;li>&lt;a href="#空间">空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性空间关系">线性空间关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#空间关系">空间关系&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="拓扑空间">拓扑空间&lt;/h2>
&lt;p>&lt;strong>拓扑空间&lt;/strong>是一个集&lt;span class="math">\(X\)&lt;/span>和其上定义的拓扑结构&lt;span class="math">\(\tau\)&lt;/span>组成的二元组&lt;span class="math">\((X,\tau)\)&lt;/span>，简记为&lt;span class="math">\(X\)&lt;/span>。&lt;span class="math">\(X\)&lt;/span>的元素&lt;span class="math">\(x\)&lt;/span>通常称为拓扑空间&lt;span class="math">\((X,\tau )\)&lt;/span>的点。而拓扑结构&lt;span class="math">\(\tau\)&lt;/span>一词涵盖了&lt;strong>开集，闭集，邻域，开核，闭包，导集，滤子&lt;/strong>等若干概念。从这些概念出发，可以给拓扑空间&lt;span class="math">\((X,\tau)\)&lt;/span>作出若干种等价的定义。在教科书中最常见的定义是从&lt;strong>开集&lt;/strong>开始的。&lt;/p>
&lt;blockquote>
&lt;p>开集公理： &lt;span class="math">\(X\)&lt;/span>的子集的集合族&lt;span class="math">\({\mathfrak {O}}\)&lt;/span>称为&lt;strong>开集系&lt;/strong>（其中的元素称为&lt;strong>开集&lt;/strong>），当且仅当其满足如下&lt;strong>开集公理&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>O1&lt;/strong>：&lt;span class="math">\(\varnothing \in {\mathfrak {O}}，X\in {\mathfrak {O}}\)&lt;/span>。&lt;/li>
&lt;li>&lt;strong>O2&lt;/strong>:若&lt;span class="math">\(A_{\lambda }\in {\mathfrak {O}}（\lambda \in \Lambda）\)&lt;/span>，则&lt;span class="math">\(\bigcup_{{\lambda \in \Lambda }}A_{{\lambda }}\in {\mathfrak {O}}\)&lt;/span>（对&lt;strong>任意并&lt;/strong>运算封闭）。&lt;/li>
&lt;li>&lt;strong>O3&lt;/strong>:若&lt;span class="math">\(A,B\in {\mathfrak {O}}\)&lt;/span>，则&lt;span class="math">\(A\cap B\in {\mathfrak {O}}\)&lt;/span>。（对&lt;strong>有限交&lt;/strong>运算封闭）。&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>需要指出的是：以上所有信息构成了&lt;span class="math">\(X\)&lt;/span>上拓扑的定义，集合系&lt;span class="math">\(\mathfrak{O}\)&lt;/span>中的元素称为开集。集合&lt;span class="math">\(X\)&lt;/span>和其上的拓扑结构组成了拓扑空间。&lt;/p>
&lt;p>注：在拓扑空间中，开集是基础性的概念。你可以&lt;strong>从任意集合X出发，再选取&lt;span class="math">\(X\)&lt;/span>的某个特定的子集族&lt;span class="math">\(\mathfrak{F}\)&lt;/span>，使&lt;span class="math">\(\mathfrak{F}\)&lt;/span>中的集合都满足作为开集应有的每一性质&lt;/strong>。我们过去说的开集都是度量空间中的开集，实际上，&lt;strong>开集的拓扑定义推广了度量空间定义&lt;/strong>。因此自然地，任何度量空间都是拓扑空间。&lt;/p>
&lt;h3 id="子空间拓扑">子空间拓扑&lt;/h3>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(X\)&lt;/span>为一个拓扑空间，&lt;span class="math">\(Y\subset X\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>的一个子集，则&lt;span class="math">\(Y\)&lt;/span>上可以如下定义一个拓扑结构： &lt;span class="math">\[\mathfrak{F}=\{U\cap Y\big\vert U\underset{open}{\subset}X\}\]&lt;/span> 则&lt;span class="math">\(\mathfrak{F}\)&lt;/span>定义了&lt;span class="math">\(Y\)&lt;/span>上的一个拓扑空间结构，此结构称为&lt;span class="math">\(X\)&lt;/span>在&lt;span class="math">\(Y\)&lt;/span>上诱导的拓扑，或称为&lt;span class="math">\(Y\)&lt;/span>被赋予子空间拓扑。&lt;/p>
&lt;/blockquote>
&lt;h2 id="度量空间">度量空间&lt;/h2>
&lt;p>&lt;strong>度量空间&lt;/strong>是个有序对&lt;span class="math">\((M,d)\)&lt;/span>，这里的&lt;span class="math">\(M\)&lt;/span>是集合而&lt;span class="math">\(d\)&lt;/span>是在&lt;span class="math">\(M\)&lt;/span>上的度量(metric)，即为函数&lt;span class="math">\(d:M\times M\rightarrow \mathbb{R}\)&lt;/span>。&lt;/p>
&lt;p>使得对于任何在&lt;span class="math">\(M\)&lt;/span>内的&lt;span class="math">\(x、y、z\)&lt;/span>，下列条件均成立：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(d(x, y) ≥ 0\)&lt;/span> (非负性)&lt;/li>
&lt;li>&lt;span class="math">\(d(x, y) = 0\)&lt;/span> 当且仅当&lt;span class="math">\(x = y\)&lt;/span>(不可区分者的同一性)&lt;/li>
&lt;li>&lt;span class="math">\(d(x, y) = d(y, x)\)&lt;/span>(对称性)&lt;/li>
&lt;li>&lt;span class="math">\(d(x, z) ≤ d(x, y) + d(y, z)\)&lt;/span>(三角不等式)。&lt;/li>
&lt;/ul>
&lt;p>条件1可由其他三个条件中导出。条件1做为度量空间的性质更恰当一些，但是很多课本都将其包含于定义之中。（以上四条要求也可以称之为度量，metric）&lt;/p>
&lt;h3 id="紧度量空间">紧度量空间&lt;/h3>
&lt;ul>
&lt;li>度量空间&lt;span class="math">\(X\)&lt;/span>是紧的(compact)&lt;span class="math">\(\Leftrightarrow X\)&lt;/span>中的每个序列都有收敛子序列&lt;/li>
&lt;li>紧闭集而有界&lt;/li>
&lt;li>集合的每一个开覆盖都有有限的子覆盖。&lt;/li>
&lt;li>这个理论强大在于强调&lt;strong>每一个&lt;/strong>开覆盖都有有限自覆盖，如果能构造出一个开覆盖没有有限自覆盖则不能称之为紧的(compact)。&lt;/li>
&lt;/ul>
&lt;h2 id="完备空间">完备空间&lt;/h2>
&lt;p>度量空间&lt;span class="math">\(M\)&lt;/span>称之为完备的，若&lt;strong>每个柯西序列均收敛于&lt;span class="math">\(M\)&lt;/span>内&lt;/strong>，亦即：若&lt;span class="math">\(d(x_{n},x_{m})\to 0\)&lt;/span>，其中&lt;span class="math">\(n\)&lt;/span>与&lt;span class="math">\(m\)&lt;/span>各自趋近于无限大，则存在某个&lt;span class="math">\(y\in M\)&lt;/span>，使得&lt;span class="math">\(d(x_{n},y)\to 0\)&lt;/span>。&lt;/p>
&lt;p>每个&lt;strong>欧氏空间&lt;/strong>都是完备的，而且该空间的每个闭子集也都是完备空间。使用绝对值度量&lt;span class="math">\(d(x,y) = \vert x - y \vert\)&lt;/span>的有理数集合则不是完备的。&lt;/p>
&lt;p>相关定理：&lt;/p>
&lt;ul>
&lt;li>任一紧致度量空间都是完备的。实际上，一个度量空间是紧致的当且仅当该空间是完备且完全有界的。&lt;/li>
&lt;li>完备空间的任一子空间是完备的当且仅当它是一个闭子集。&lt;/li>
&lt;/ul>
&lt;h2 id="有界与完全有界空间">有界与完全有界空间&lt;/h2>
&lt;p>度量空间&lt;span class="math">\(M\)&lt;/span>被称为有界的，如果存在某个数&lt;span class="math">\(r\)&lt;/span>，使得对于所有&lt;span class="math">\(M\)&lt;/span>中的&lt;span class="math">\(x\)&lt;/span>和&lt;span class="math">\(y\)&lt;/span>有&lt;span class="math">\(d(x,y)≤r\)&lt;/span>。&lt;span class="math">\(r\)&lt;/span>最小可能的值称之为 M 的直径。&lt;/p>
&lt;p>空间&lt;span class="math">\(M\)&lt;/span>称之为预紧致的或完全有界的，如果对于所有&lt;span class="math">\(r &amp;gt; 0\)&lt;/span>存在有限多个半径为&lt;span class="math">\(r\)&lt;/span>的开球，其并集覆盖&lt;span class="math">\(M\)&lt;/span>。因为这些球为&lt;strong>有限个&lt;/strong>，所以该空间的&lt;strong>直径亦为有限值&lt;/strong>，从而得出(使用三角不等式)所有完全有界空间都是&lt;strong>有界&lt;/strong>的。但逆命题不成立。&lt;/p>
&lt;h2 id="紧致空间">紧致空间&lt;/h2>
&lt;p>度量空间&lt;span class="math">\(M\)&lt;/span>是紧致的，若每个&lt;span class="math">\(M\)&lt;/span>内的序列均有个子序列，会收敛于&lt;span class="math">\(M\)&lt;/span>内的一点。这称为序列紧致性，且在度量空间（但不是一般拓扑空间）里，这等价于可数紧致与以开覆盖定义之紧致性等拓扑性质。&lt;strong>每个紧致集合的闭子集亦是紧致的&lt;/strong>。&lt;/p>
&lt;p>一度量空间为紧致的，当且仅当该空间是&lt;strong>完备&lt;/strong>的，且为完全有界的。这即是所谓的海涅－博雷尔定理。须注意，紧致性仅决取于拓扑，而有界性则决取于度量。&lt;/p>
&lt;p>勒贝格数引理表示，对于紧致度量空间 M 内的每个开覆盖，均存在一个“勒贝格数”δ，使得每个 M 内直径 &amp;lt; δ 的子集均会被包含于某些覆盖内。&lt;/p>
&lt;h2 id="banach空间">Banach空间&lt;/h2>
&lt;p>巴拿赫空间（英语：Banach space）是一个完备赋范向量空间。更精确地说，巴拿赫空间是一个具有范数并对此范数完备的向量空间。其完备性体现在，空间内任意向量的柯西序列总是收敛到一个良定义的位于空间内部的极限。&lt;/p>
&lt;h2 id="柯西序列">柯西序列&lt;/h2>
&lt;p>在数学中，&lt;strong>柯西序列、柯西列、柯西数列或基本列&lt;/strong>是指这样一个数列，它的元素随着序数的增加而愈发靠近。更确切地说，在去掉有限个元素后，可以使得余下的元素中任何两点间的距离的最大值不超过任意给定的正数。&lt;/p>
&lt;p>柯西列的定义依赖于&lt;strong>距离的定义&lt;/strong>，所以只有在&lt;strong>度量空间&lt;/strong>中柯西列才有意义。&lt;/p>
&lt;p>在完备空间中，所有的柯西数列都有极限且极限在这空间里。&lt;/p>
&lt;h3 id="子序列">子序列&lt;/h3>
&lt;p>给定序列&lt;span class="math">\(\{x^m\}\)&lt;/span>，设有一个严格递增函数&lt;span class="math">\(m(k)\)&lt;/span>，他将每个正整数&lt;span class="math">\(k\)&lt;/span>分配给一个正整数&lt;span class="math">\(m(k)\)&lt;/span>，则序列&lt;span class="math">\(\{x^{m(k)}\}\)&lt;/span>称为&lt;span class="math">\(\{x^m\}\)&lt;/span>的&lt;strong>子序列&lt;/strong>（subsequence）。&lt;/p>
&lt;h2 id="空间">空间&lt;/h2>
&lt;p>空间是附加结构的&lt;strong>集合&lt;/strong>。现代数学&lt;/p>
&lt;h2 id="线性空间关系">线性空间关系&lt;/h2>
&lt;p>线性空间等同于向量空间。&lt;/p>
&lt;p>研究向量空间很自然涉及一些额外结构。额外结构如下：&lt;/p>
&lt;ul>
&lt;li>一个实数或复数向量空间加上长度概念（就是范数）则成为赋范向量空间。&lt;/li>
&lt;li>一个实数或复数向量空间加上长度和角度的概念则成为内积空间。&lt;/li>
&lt;li>一个向量空间加上拓扑结构并满足连续性要求（加法及标量乘法是连续映射）则成为拓扑向量空间。&lt;/li>
&lt;li>一个向量空间加上双线性算子（定义为向量乘法）则成为域代数。&lt;/li>
&lt;/ul>
&lt;h2 id="空间关系">空间关系&lt;/h2>
&lt;div class="figure">
&lt;img src="../images/空间关系维恩图.jpeg" alt="空间关系维恩图.jpeg" />&lt;p class="caption">空间关系维恩图.jpeg&lt;/p>
&lt;/div>
&lt;p>注：图中黄色的字是拓扑空间。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/空间关系说明表.jpeg" alt="空间关系维恩图.jpeg" />&lt;p class="caption">空间关系维恩图.jpeg&lt;/p>
&lt;/div>
&lt;p>现代数学的一个特点就是以集合为研究对象，这样的好处就是可以将很多不同问题的本质抽象出来，变成同一个问题，当然这样的坏处就是描述起来比较抽象，很多人就难以理解了。&lt;/p>
&lt;p>既然是研究集合，每个人感兴趣的角度不同，研究的方向也就不同。为了能有效地研究集合，必须给集合赋予一些“结构”（从一些具体问题抽象出来的结构）。 从数学的本质来看，最基本的集合有两类：线性空间（有线性结构的集合）、度量空间（有度量结构的集合）。&lt;/p>
&lt;p>对线性空间而言，主要研究集合的描述，直观地说就是如何清楚地告诉地别人这个集合是什么样子。为了描述清楚，就引入了基（相当于三维空间中的坐标系）的概念，所以对于一个线性空间来说，只要知道其基即可，集合中的元素只要知道其在给定基下的坐标即可。&lt;/p>
&lt;p>但线性空间中的元素没有“长度”（相当于三维空间中线段的长度），为了量化线性空间中的元素，所以又在线性空间引入特殊的“长度”，即范数。赋予了范数的线性空间即称为赋范线性空间。同时，两个赋范线性空间的所有线性算子构成了另一个空间，叫线性算子空间。&lt;/p>
&lt;p>但赋范线性空间中两个元素之间没有角度的概念，为了解决该问题，所以在线性空间中又引入了内积的概念。&lt;/p>
&lt;p>因为有度量，所以可以在度量空间、赋范线性空间以及内积空间中引入极限，但抽象空间中的极限与实数上的极限有一个很大的不同就是，极限点可能不在原来给定的集合中，所以又引入了完备的概念，完备的内积空间就称为Hilbert空间。&lt;/p>
&lt;p>这几个空间之间的关系是：&lt;/p>
&lt;ul>
&lt;li>线性空间与度量空间是两个不同的概念，没有必然联系。线性空间中，不定义度量结构就不是度量空间，度量也可以定义在非线性空间中。二者的交集叫度量线性空间。&lt;/li>
&lt;li>赋范线性空间就是赋予了范数的线性空间，也是度量空间（具有线性结构的度量空间），因为范数也是距离的一种，即度量和原点的距离。&lt;/li>
&lt;li>Banach空间是完备的赋范线性空间。&lt;/li>
&lt;li>内积空间必为赋范线性空间，但是在其基础上引入了角度，并产生了一些负数运算结果；但线性赋范空间不一定是内积空间。任何有限维内积空间都是完备的，因此也是banach空间。&lt;/li>
&lt;li>希尔伯特空间就是完备的内积空间，也可以说是定义了内积的Banach空间，算是banach空间和内积空间的交集。&lt;/li>
&lt;li>有限维欧式空间、有限维酉空间都是Banach空间以及希尔伯特空间&lt;/li>
&lt;/ul>
&lt;p>其他阅读材料：如何理解空间，这是最基础的。 &lt;a href="https://www.jianshu.com/p/1a8241cae23f">https://www.jianshu.com/p/1a8241cae23f&lt;/a>&lt;/p></description></item><item><title>组合数学-递归与母函数（上）</title><link>https://surprisedcat.github.io/studynotes/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6-%E9%80%92%E5%BD%92%E4%B8%8E%E6%AF%8D%E5%87%BD%E6%95%B0%E4%B8%8A/</link><pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6-%E9%80%92%E5%BD%92%E4%B8%8E%E6%AF%8D%E5%87%BD%E6%95%B0%E4%B8%8A/</guid><description>
&lt;h2 id="递归与母函数上">递归与母函数（上）&lt;!-- omit in toc -->&lt;/h2>
&lt;p>递推关系是计数的一个强有力的工具，特别是在做算法分析时是必需的。递推关系的求解主要是利用母函数。当然母函数尚有其他用处，但这主要是介绍解递推关系上的应用。&lt;/p>
&lt;p>通常序列&lt;span class="math">\(a_0,a_1,\dotsb,a_n,\dotsb\)&lt;/span>与某个问题序列&lt;span class="math">\(P_0,P_1,\dotsb,P_n,\dotsb\)&lt;/span>的计数问题相对应，若已知序列的母函数，则可确定该序列，从而可以解决相应的计数问题。&lt;/p>
&lt;p>注意：&lt;strong>母函数==生成函数&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#组合数学引子">组合数学引子&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有重复组合公式及其证明方法">有重复组合公式及其证明方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#母函数定义">母函数定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#普通母函数ordinary-generating-functionogf">普通母函数(Ordinary generating function，OGF)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数生成函数exponential-generating-functionegf">指数生成函数(Exponential generating function，EGF)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泊松生成函数">泊松生成函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#普通母函数举例">普通母函数举例&lt;/a>&lt;/li>
&lt;li>&lt;a href="#母函数的性质">母函数的性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#整数的拆分">整数的拆分&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数型母函数详解">指数型母函数详解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#递推关系">递推关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#汉诺塔">汉诺塔&lt;/a>&lt;/li>
&lt;li>&lt;a href="#求n位十进制数中出现偶数个5的数的个数">求n位十进制数中出现偶数个5的数的个数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从n个元素中取r个进行允许重复的组合">从n个元素中取r个进行允许重复的组合&lt;/a>&lt;/li>
&lt;li>&lt;a href="#fibonacci数列">Fibonacci数列&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="组合数学引子">组合数学引子&lt;/h2>
&lt;h3 id="有重复组合公式及其证明方法">有重复组合公式及其证明方法&lt;/h3>
&lt;p>从n个不同元素中取出m的元素（方法是从n个元素中每次取出一个后，放回，再取另外一个，直到取出m个元素），并成一组，叫做n个不同元素的一个m-可重组合。n个不同元素的m-可重组合数为&lt;span class="math">\(C(m,n+m-1)\)&lt;/span>，&lt;span class="math">\(m\)&lt;/span>可以是任意的正整数。&lt;/p>
&lt;p>证明：实际上大家还应该注意到一点，就是有重复组合不考虑取出的元素的顺序，通俗来说，你第一次取出一号元素第二次取出三号元素和你第一次取出三号元素第二次取出一号元素是一样的情况。可以把该过程看作是一个“放球模型”；n个不同的元素看作是n个格子，去掉头尾之后中间一共有&lt;span class="math">\((n-1)\)&lt;/span>块相同的隔板；用m个相同的小球代表取m次；则原问题可以简化为将m个不加区别的小球放进n个格子里面，问有多少种放法；&lt;/p>
&lt;p>注意到格子的头尾两块隔板无论什么情况下位置都是不变的，故去掉不用考虑；相当于&lt;span class="math">\(m\)&lt;/span>个相同的小球和&lt;span class="math">\((n-1)\)&lt;/span>块相同的隔板先进行全排列：一共有&lt;span class="math">\((m+n-1)!\)&lt;/span>种排法，再由于m个小球和&lt;span class="math">\((n-1)\)&lt;/span>块隔板是分别不加以区分的，所以除以重复的情况：&lt;span class="math">\(m!*(n-1)!\)&lt;/span>；&lt;/p>
&lt;p>于是答案就是：&lt;span class="math">\(\frac{(m+n-1)!}{m!*(n-1)!}=C(m,n+m-1)\)&lt;/span>。&lt;/p>
&lt;h2 id="母函数定义">母函数定义&lt;/h2>
&lt;p>母函数的是形式化的函数，是一种形式幂级数，其每一项的系数可以提供关于这个序列的信息。母函数可分为很多种，包括普通母函数、指数母函数、L级数、贝尔级数和狄利克雷级数。对每个序列都可以写出以上每个类型的一个母函数。构造母函数的目的一般是为了解决某个特定的问题，因此选用何种母函数视乎序列本身的特性和问题的类型。&lt;/p>
&lt;p>注意母函数本身并不是一个从某个定义域射到某个上域的函数，名字中的“函数”只是出于历史原因而保留。&lt;/p>
&lt;h3 id="普通母函数ordinary-generating-functionogf">普通母函数(Ordinary generating function，OGF)&lt;/h3>
&lt;p>对于序列&lt;span class="math">\(a_n\)&lt;/span>，其OGF定义为： &lt;span class="math">\[G(a_n;x)=\sum_{n=0}^\infty(a_n x^n) \tag{1.1}\]&lt;/span> 当没有特别之处是哪一种生成函数的时候，通常就用OGF。如果&lt;span class="math">\(a_n\)&lt;/span>是概率密度函数或者离散随机变量，那么OGF可以成为概率生成函数（PGF）。对于多维生成函数可以用多个变量表示，例如： &lt;span class="math">\[G(a_{m,n};x,y)=\sum_{m,n=0}^\infty a_{m,n}x^m y^n\tag{1.2}\]&lt;/span>&lt;/p>
&lt;h3 id="指数生成函数exponential-generating-functionegf">指数生成函数(Exponential generating function，EGF)&lt;/h3>
&lt;p>&lt;span class="math">\[\operatorname {EG} (a_{n};x)=\sum_{n=0}^{\infty}a_n{\frac{x^{n}}{n!}}\tag{1.3}.\]&lt;/span> 在组合枚举问题中EGF一般比OGF更方便使用。&lt;/p>
&lt;h3 id="泊松生成函数">泊松生成函数&lt;/h3>
&lt;p>&lt;span class="math">\[\operatorname{PG}(a_{n};x)=\sum_{n=0}^{\infty}a_{n}e^{-x}{\frac{x^{n}}{n!}}=e^{-x}\operatorname {EG} (a_{n};x)\tag{1.4}.\]&lt;/span>&lt;/p>
&lt;h2 id="普通母函数举例">普通母函数举例&lt;/h2>
&lt;p>一个关键的生成函数是对于常序列1，1，1，1，1，1，……，它的生成函数是几何级数： &lt;span class="math">\[\sum_{n=0}^\infty x^n=\frac{1}{1-x}\quad x\in(-1,1) \tag{2.1}\]&lt;/span> 证明方法可以1，左边是右边的Maclaurin级数。2，用等比级数求和公式。从这个母函数，我们可以轻易的推导出其他母函数。&lt;/p>
&lt;p>序列：&lt;span class="math">\(1,a,a^2,a^3,a^4,\dotsb\)&lt;/span> &lt;span class="math">\[\sum_{n=0}^{\infty}{(ax)}^{n}=\frac{1}{1-ax}\tag{2.2}\]&lt;/span> 序列：&lt;span class="math">\(1,-1,1,-1,1,\dotsb\)&lt;/span>(a=-1) &lt;span class="math">\[\sum_{n=0}^{\infty}(-1)^n x^n=\frac{1}{1+x}\tag{2.3}\]&lt;/span> 序列：&lt;span class="math">\(1,0,1,0,1,\dotsb\)&lt;/span>(a=-1) &lt;span class="math">\[\sum_{n=0}^{\infty}x^{2n}=\frac{1}{1-x^2}\tag{2.4}\]&lt;/span> 序列：&lt;span class="math">\(1,2,3,4,5,\dotsb\)&lt;/span>,可以看成是原始级数的导数： &lt;span class="math">\[\bigg [\sum_{n=0}^\infty x^n\bigg ]&amp;#39;=\sum_{n=0}^\infty (n+1)x^n\\
=\bigg [\frac{1}{1-x}\bigg ]&amp;#39;=\frac{1}{(1-x)^2}\tag{2.5}\]&lt;/span> 递推： &lt;span class="math">\[1+\sum_{k=1}^\infty\frac{n(n+1)\dotsb(n+k-1)}{k!}x^k=\frac{1}{(1-x)^n}\tag{2.6}\]&lt;/span> 序列：&lt;span class="math">\(C(n,0),C(n,1),\dotsb,C(n,n)\)&lt;/span>，展开式公式。 &lt;span class="math">\[\sum_{i=0}^{n}C(n,i)x^i=(1+x)^n\tag{2.7}\]&lt;/span> 对公式两边求导： &lt;span class="math">\[C(n,1)+2C(n,2)x+\dotsb+nC(n,n)x^{n-1}=n(1+x)^{n-1}\tag{2.8}\]&lt;/span> 再令x=1得： &lt;span class="math">\[C(n,1)+2C(n,2)+\dotsb+nC(n,n)=n2^{n-1}\tag{2.9}\]&lt;/span> 如果对等式&lt;span class="math">\((2.7)\)&lt;/span>两边同时乘以&lt;span class="math">\(x\)&lt;/span>， &lt;span class="math">\[C(n,1)x+2C(n,2)x^2+\dotsb+nC(n,n)x^n=nx(1+x)^{n-1}\tag{2.10}\]&lt;/span> 然后再对等式的两边求导得： &lt;span class="math">\[C(n,1)+2^2 C(n,2)x+\dotsb+n^2 C(n,n)x^{n-1}\\
=n(1+x)^{n-1}+n(n-1)x(1+x)^{n-2}\tag{2.11}\]&lt;/span> 再令x=1得： &lt;span class="math">\[C(n,1)+2^2 C(n,2)+\dotsb+n^2 C(n,n)\\=n2^{n-1}+n(n-1)x2^{n-2}=n(n+1)2^{n-2}
\tag{2.12}\]&lt;/span> 递推：牛顿二项式公式 &lt;span class="math">\[(1\plusmn x)^\alpha=\sum_{k=0}^\infty C(\alpha,k)(\plusmn x)^k\]&lt;/span> 当&lt;span class="math">\(\alpha&amp;lt;0\)&lt;/span>时，&lt;/p>
&lt;h2 id="母函数的性质">母函数的性质&lt;/h2>
&lt;blockquote>
&lt;p>位移性质：设&lt;span class="math">\({x_k},{y_k}\)&lt;/span>为两个序列，其相应的母函数为&lt;span class="math">\(A(x),B(x)\)&lt;/span>。若 &lt;span class="math">\[b_k=\begin{cases}0 &amp;amp;k&amp;lt;l\\
a_{k-l} &amp;amp; k \ge l\end{cases} \tag{3.1}\]&lt;/span> 则&lt;span class="math">\(B(x)=x^lA(x)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>简单证明： &lt;span class="math">\[\begin{aligned}B(x)&amp;amp;=b_0+b_1 x+b_2 x^2+\dotsb+b_{l-1}x^{l-1}+b_{l}x^{l}+b_{l+1}x^{l+1}+\dotsb \\
&amp;amp;=0+0+\dotsb+0+a_0 x^l+a_1 x^{l+1}+\dotsb \tag{3.2}\\
&amp;amp;=x^l(a_0+a_1 x+\dotsb)=x^l A(x)
\end{aligned}\]&lt;/span> 总结来说类似于傅里叶变换的向右移位。&lt;/p>
&lt;p>推论：若&lt;span class="math">\(b_k=a_{k+l}\)&lt;/span>，则 &lt;span class="math">\[B(x)=[A(x)-\sum_{k=0}^{l-1}a_k x^k]/x^l\tag{3.3}\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>若&lt;span class="math">\(b_k=\sum\limits_{i=0}^k a_i\)&lt;/span>，则 &lt;span class="math">\[B(x)=\frac{A(x)}{1-x}\tag{3.4}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>进一步引申：&lt;/p>
&lt;blockquote>
&lt;p>若&lt;span class="math">\(\sum\limits_{k=0}^\infty a_k\)&lt;/span>收敛，&lt;span class="math">\(b_k=\sum\limits_{i=0}^k a_i\)&lt;/span>，则 &lt;span class="math">\[B(x)=\frac{A(1)-xA(x)}{1-x}\tag{3.5}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>类似于：等比数列求和公式&lt;/p>
&lt;blockquote>
&lt;p>若&lt;span class="math">\(b_k=ka_k\)&lt;/span>，则 &lt;span class="math">\[B(x)=xA&amp;#39;(x)\tag{3.6}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>简单证明： &lt;span class="math">\[A&amp;#39;(x)=\frac{d}{dx}(\sum_{k=0}^\infty a_k x^k) \\
=\sum_{k=1}^\infty k a_k x^{k-1}\]&lt;/span> &lt;span class="math">\[xA&amp;#39;(x)=x\sum_{k=1}^\infty k a_k x^{k-1}\\
=\sum_{k=1}^\infty k a_k x^k \quad\text{(添加k=0项无影响)}\\
=\sum_{k=0}^\infty (k a_k) x^k=\sum_{k=0}^\infty b_k x^k=B(x)\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>推论：若&lt;span class="math">\(b_k=\frac{a_k}{k+1}\)&lt;/span>，则 &lt;span class="math">\[B(x)=\frac{1}{x}\int_0^x A(x)dx\tag{3.7}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="整数的拆分">整数的拆分&lt;/h2>
&lt;blockquote>
&lt;p>定理4.1：正整数n拆分成&lt;strong>不同整数&lt;/strong>之和的拆分数等于拆分成&lt;strong>奇整数&lt;/strong>之和的拆分数。&lt;/p>
&lt;/blockquote>
&lt;p>可以用Ferrers图像和描述性质。&lt;/p>
&lt;h2 id="指数型母函数详解">指数型母函数详解&lt;/h2>
&lt;p>问题：设有n个元素，其中元素&lt;span class="math">\(a_1\)&lt;/span>重复了&lt;span class="math">\(n_1\)&lt;/span>次，&lt;span class="math">\(a_2\)&lt;/span>重复了&lt;span class="math">\(n_2\)&lt;/span>次，...，&lt;span class="math">\(a_k\)&lt;/span>重复了&lt;span class="math">\(n_k\)&lt;/span>次，&lt;span class="math">\(n_1+n_2+\dotsb+n_k=n\)&lt;/span>，从中取出r个排列，一共有多少种&lt;strong>排列&lt;/strong>？&lt;/p>
&lt;blockquote>
&lt;p>简化定理5.1:设&lt;span class="math">\(S={n_1a_1,n_2a_2,\dotsb,n_ka_k}\)&lt;/span>为一多重集，其中&lt;span class="math">\(n_1+n_2+\dotsb+n_k=n\)&lt;/span>，那么从S中取n个元素的&lt;strong>排列数&lt;/strong>为： &lt;span class="math">\[\frac{n!}{n_1!n_2!\dotsb n_k!}\tag{5.1}\]&lt;/span> 证明: &lt;span class="math">\[C(n,n_1)C(n-n_1,n_2)\dotsb C(n-n_1-\dotsb-n_{k-1},n_k)\\
=\frac{n!}{n_1!n_2!\dotsb n_k!}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>定义：对于序列&lt;span class="math">\(a_0，a_1，...，a_n，...\)&lt;/span> &lt;span class="math">\[\operatorname {EG} (a_{n};x)=\sum_{n=0}^{\infty}a_n{\frac{x^{n}}{n!}}\tag{5.2}.\]&lt;/span> 为序列的指数型母函数（EGF）。&lt;/p>
&lt;p>例如：序列&lt;span class="math">\(1,1,...\)&lt;/span>的EGF为： &lt;span class="math">\[1+\frac{x}{1!}+\frac{x^2}{2!}+\dotsb=e^x\]&lt;/span>&lt;/p>
&lt;p>例如：序列&lt;span class="math">\(0!,1!,2!,...,k!,\dotsb\)&lt;/span>的EGF为： &lt;span class="math">\[0!+1!\frac{x}{1!}+2!\frac{x^2}{2!}+\dotsb+k!\frac{x^k}{k!}\\
=1+x+x^2+\dotsb=\frac{1}{1-x}\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理5.2：设&lt;span class="math">\(S=\{n_1 b_1,n_2 b_2,\dotsb,n_k b_k\}\)&lt;/span>为一多重集，并设&lt;span class="math">\(a_r(r=0,1,2,\dotsb)\)&lt;/span>为S的r排列，则&lt;span class="math">\({a_r}\)&lt;/span>的指数型母函数为： &lt;span class="math">\[G_e(x)=\\
(1+\frac{x}{1!}+\dotsb+\frac{x^{n_1}}{n_1!})(1+\frac{x}{1!}+\dotsb+\frac{x^{n_2}}{n_2!})\dotsb(1+\frac{x}{1!}+\dotsb+\frac{x^{n_k}}{n_k!})\tag{5.3}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="递推关系">递推关系&lt;/h2>
&lt;h3 id="汉诺塔">汉诺塔&lt;/h3>
&lt;div class="figure">
&lt;img src="../images/汉诺塔.png" alt="汉诺塔" />&lt;p class="caption">汉诺塔&lt;/p>
&lt;/div>
&lt;p>算法的分析：设&lt;span class="math">\(h_n\)&lt;/span>为将n个圆盘从塔座A移动到塔座B上所需要的移动次数。我们要如何计算&lt;span class="math">\(h_n\)&lt;/span>呢？&lt;/p>
&lt;p>首先我们假设我们已经掌握了将&lt;span class="math">\(h_{n-1}\)&lt;/span>个圆盘挪动的方法。那么我们先将n-1个圆盘移动到中介柱子C，需要&lt;span class="math">\(h_{n-1}\)&lt;/span>步；然后把最下面的第n层放到B柱子，需要一步；最后把C柱子的n-1个圆盘移动到B柱子，有需要&lt;span class="math">\(h_{n-1}\)&lt;/span>步。因此共需要&lt;span class="math">\(2h_{n-1}+1\)&lt;/span>步。用递推公式表达： &lt;span class="math">\[\begin{cases}h_n=2h_{n-1}+1 &amp;amp;n&amp;gt;1 \\
h_1=1 &amp;amp;n=1\end{cases}\tag{6.1}\]&lt;/span>&lt;/p>
&lt;p>解法1： &lt;span class="math">\[\begin{aligned}
h_n&amp;amp;=2h_{n-1}+1=2(2h_{n-2}+1)+1\\
&amp;amp;=2^2h_{n-2}+2+1=2^2(2h_{n-3}+1)+2+1\\
&amp;amp;=2^3h_{n-3}+2^2+2+1\\
&amp;amp;=2^{n-1}h_1+2^{n-2}+\dotsb+2+1\\
&amp;amp;=2^n-1
\end{aligned}\]&lt;/span>&lt;/p>
&lt;p>解法2：OGF母函数求解 &lt;span class="math">\[\begin{aligned}
H(x)&amp;amp;=h_1 x+h_2 x^2+\dotsb+h_n x^n+\dotsb\\
-2xH(x)&amp;amp;=-2h_1 x^2-\dotsb-2h_{n-1}x^n+\dotsb\\
\hline\\
\because h_n&amp;amp;=2h_{n-1}+1\\
(1-2x)H(x)&amp;amp;=x+x^2+\dotsb+x^n+\dotsb=\frac{x}{1-x}
\end{aligned}\]&lt;/span> &lt;span class="math">\[H(x)=\frac{x}{(1-x)(1-2x)}\\
=\frac{1}{1-2x}-\frac{1}{1-x}\\
=\sum_{n=0}^\infty(2^n-1)x^n\\
\Rightarrow h(n)=2^n-1\]&lt;/span>&lt;/p>
&lt;h3 id="求n位十进制数中出现偶数个5的数的个数">求n位十进制数中出现偶数个5的数的个数&lt;/h3>
&lt;p>设：&lt;span class="math">\(a_n=n\)&lt;/span>位十进制数中出现偶数个5的数的个数，&lt;/p>
&lt;p>&lt;span class="math">\(b_n=n\)&lt;/span>位十进制数中出现奇数个5的数的个数。&lt;/p>
&lt;p>则有： &lt;span class="math">\[\begin{cases}
a_n=9a_{n-1}+b_{n-1}, &amp;amp; n&amp;gt;1 \\
b_n=9b_{n-1}+a_{n-1}, &amp;amp; n&amp;gt;1\tag{6.2A}\\
a_1=8,b_1=1
\end{cases}\]&lt;/span> 解释：n位中，5为偶数个的个数&lt;span class="math">\(a_n\)&lt;/span>可以看成两组：1.在n-1位数中，5为偶数个的个数&lt;span class="math">\(a_{n-1}\)&lt;/span>，这样第n位可以选除了5的任意数字，有9种；2.在n-1位数中，5为奇数个的个数&lt;span class="math">\(b_{n-1}\)&lt;/span>，这样第n位只可以选5，有1种；共有&lt;span class="math">\(a_n=9a_{n-1}+b_{n-1}\)&lt;/span>种。奇数&lt;span class="math">\(b_n\)&lt;/span>同理。首位不能为0。&lt;/p>
&lt;p>解法1：设序列&lt;span class="math">\(\{a_n\}\)&lt;/span>和序列&lt;span class="math">\(\{b_n\}\)&lt;/span>的母函数分别为&lt;span class="math">\(A(x),B(x)\)&lt;/span>。 &lt;span class="math">\[A(x)=a_1 x+a_2 x^2+\dotsb+a_n x^n+\dotsb\\
B(x)=b_1 x+b_2 x^2+\dotsb+b_n x^n+\dotsb\]&lt;/span> &lt;span class="math">\[\begin{aligned}
A(x)=a_1 x+a_2 x^2&amp;amp;+\dotsb+a_n x^n+\dotsb\\
-9xA(x)=\quad-9a_1 x^2&amp;amp;-\dotsb-9a_{n-1}x^n-\dotsb\\
-xB(x)=\quad-b_1 x^2&amp;amp;-\dotsb-b_{n-1}x^n-\dotsb\\
\hline\\
(1-9x)A(x)-xB(x)&amp;amp;=a_1 x=8x
\end{aligned}
\]&lt;/span> 类似地，可得： &lt;span class="math">\[(1-9x)B(x)-xA(x)=b_1 x=x\]&lt;/span> 联立方程 &lt;span class="math">\[\begin{cases}
(1-9x)A(x)-xB(x)=8x \\
(1-9x)B(x)-xA(x)=x
\end{cases}\]&lt;/span> 用克莱姆法则求解： &lt;span class="math">\[\begin{aligned}
A(x)&amp;amp;=\frac{\begin{vmatrix}
8x&amp;amp;-x\\
x&amp;amp;(1-9x)
\end{vmatrix}}{\begin{vmatrix}
(1-9x)&amp;amp;-x\\
-x&amp;amp;(1-9x)
\end{vmatrix}}\\
&amp;amp;=\frac{-71x^2+8x}{(1-8x)(1-10x)}\\
&amp;amp;=\frac{x}{2}(\frac{7}{1-8x}+\frac{9}{1-10x})\\
&amp;amp;=\frac{1}{2}\sum_{n=0}^\infty(7*8^n+9*10^n)x^{n+1}\\
&amp;amp;\Rightarrow a_n=\frac{1}{2}(7*8^{n-1}+9*10^{n-1})
\end{aligned}\]&lt;/span>&lt;/p>
&lt;p>解法2：设序列&lt;span class="math">\(\{a_n\}\)&lt;/span>和序列&lt;span class="math">\(\{b_n\}\)&lt;/span>的母函数分别为&lt;span class="math">\(A(x),B(x)\)&lt;/span>。 &lt;span class="math">\[\begin{cases}
a_n=9a_{n-1}+b_{n-1}, &amp;amp; n&amp;gt;1 \\
b_{n-1}=9*10^{n-2}-a_{n-1}, &amp;amp; n&amp;gt;2\tag{6.2B}\\
a_1=8,b_1=1
\end{cases}\]&lt;/span> 这一步是因为在所有n位数中，5有奇数个和偶数个的和必为所有n位数。 式&lt;span class="math">\({6.2B}\)&lt;/span>可以推得&lt;span class="math">\(a_n=8a_{n-1}+9*10^{n-2}(n&amp;gt;1)\)&lt;/span> &lt;span class="math">\[\begin{aligned}
A(x)=a_1 x+a_2 x^2&amp;amp;+\dotsb+a_n x^n+\dotsb\\
-8xA(x)=\quad-8a_1 x^2&amp;amp;-\dotsb-8a_{n-1}x^n-\dotsb\\
\hline\\
(1-8x)A(x)=a_1 x+9*&amp;amp;10^0 x^2+9*10x^3+\dotsb\\
=8x+\frac{9x^2}{1-10x}&amp;amp;=\frac{8x-71x^2}{1-10x}
\end{aligned}
\]&lt;/span> 接下来依据解法1可得&lt;span class="math">\(a_n=\frac{1}{2}(7*8^{n-1}+9*10^{n-1})\)&lt;/span>。&lt;/p>
&lt;h3 id="从n个元素中取r个进行允许重复的组合">从n个元素中取r个进行允许重复的组合&lt;/h3>
&lt;p>这是对引子的另一种证明方法。&lt;/p>
&lt;p>假定允许重复的组合数用&lt;span class="math">\(C^\ast(n,r)\)&lt;/span>表示，那么有以下递推关系： &lt;span class="math">\[\begin{cases}
C^\ast(n,r)=C^\ast(n,r-1)+C^\ast(n-1,r)\tag{6.3}\\
C^\ast(n,0)=1
\end{cases}
\]&lt;/span> 凑单解法： &lt;span class="math">\[\begin{aligned}
G_n(x)&amp;amp;=C^\ast(n,0)+C^\ast(n,1)x+C^\ast(n,2)x^2+\dotsb\\
-xG_n(x)&amp;amp;=\qquad\qquad\quad C^\ast(n,0)x+C^\ast(n,1)x^2+\dotsb\\
-G_{n-1}(x)&amp;amp;=C^\ast(n-1,0)+C^\ast(n-1,1)x+C^\ast(n-1,2)x^2+\dotsb\\
\hline\\
&amp;amp;(1-x)G_n(x)-G_{n-1}(x)=0\\
\end{aligned}\]&lt;/span> 可以获得&lt;span class="math">\(G_n(x)\)&lt;/span>的递推公式： &lt;span class="math">\[\begin{aligned}
G_n(x)&amp;amp;=\frac{1}{1-x}G_{n-1}(x)\\
&amp;amp;=\dotsb\\
&amp;amp;=\frac{1}{(1-x)^{n-1}}G_1(x)\\
&amp;amp;=\frac{1}{(1-x)^{n-1}}[C^\ast(1,0)+C^\ast(1,1)x+C^\ast(1,2)x^2+\dotsb]\\
&amp;amp;=\frac{1}{(1-x)^{n-1}}[1+x+x^2+\dotsb]\\
&amp;amp;=\frac{1}{(1-x)^n}\\
\end{aligned}\]&lt;/span> 从&lt;span class="math">\((1-x)^n\)&lt;/span>的对于序列来看： &lt;span class="math">\[\begin{aligned}
(1-x)^{-n}&amp;amp;=\sum_{r=0}^\infty C(-n,r)(-x)^r\\
&amp;amp;=\sum_{r=0}^\infty\frac{(-n)(-n-1)\dotsb(-n-r+1)}{r!}(-x)^r\\
&amp;amp;=\sum_{r=0}^\infty\frac{n(n+1)\dotsb(n+r-1)}{r!}x^r\\
&amp;amp;=\sum_{r=0}^\infty C(n+r-1,r)x^r\\
&amp;amp;\Rightarrow C^\ast(n,r)=C(n+r-1,r)\\
\end{aligned}\]&lt;/span>&lt;/p>
&lt;h3 id="fibonacci数列">Fibonacci数列&lt;/h3>
&lt;p>典型的Fibonacci数列的递推公式如下所示： &lt;span class="math">\[\begin{cases}
F_n=F_{n-1}+F_{n-2}\tag{6.4}\\
F_1=1,F_2=2\\
\end{cases}\]&lt;/span> 假设&lt;span class="math">\(F_n\)&lt;/span>的递推公式为&lt;span class="math">\(G(x)\)&lt;/span>: &lt;span class="math">\[\begin{aligned}
G(x)&amp;amp;=F_1x+F_2x^2+F_3x^3+\dotsb+F_nx^n+\dotsb\\
-xG(x)&amp;amp;=\qquad -F_1x^2-F_2x^3-\dotsb-F_{n-1}x^n-\dotsb\\
-x^2G(x)&amp;amp;=\qquad\qquad\quad-F_1x^3-\dotsb-F_{n-2}x^n-\dotsb\\
\hline\\
&amp;amp;(1-x-x^2)G(x)=F_1x=x\\
\Rightarrow G(x)&amp;amp;=\frac{x}{1-x-x^2}=\frac{x}{(1-\frac{1+\sqrt{5}}{2}x)(1-\frac{1-\sqrt{5}}{2}x)}
\end{aligned}\]&lt;/span> 通过解方程的因式分解得： &lt;span class="math">\[\begin{aligned}
G(x)&amp;amp;=\frac{1}{\sqrt{5}}[\frac{1}{1-(1+\sqrt{5})x/2}-\frac{1}{1-(1-\sqrt{5})x/2}]\\
&amp;amp;=\frac{1}{\sqrt{5}}\sum_{n=0}^\infty[(\frac{1+\sqrt{5}}{2})^n-(\frac{1-\sqrt{5}}{2})^n]x^n\\
&amp;amp;\Rightarrow F_n=\frac{1}{\sqrt{5}}[(\frac{1+\sqrt{5}}{2})^n-(\frac{1-\sqrt{5}}{2})^n]
\end{aligned}\]&lt;/span> 其他一些性质：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(F_1+F_2+\dotsb+F_n=F_{n+2}-1\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(F_1+F_3+\dotsb+F_{2n-1}=F_{2n}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(F_1^2+F_2^2+\dotsb+F_n^2=F_nF_{n+1}\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>这些都可以通过累加法证明。&lt;/p></description></item><item><title>组合数学-递归与母函数（下）</title><link>https://surprisedcat.github.io/studynotes/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6-%E9%80%92%E5%BD%92%E4%B8%8E%E6%AF%8D%E5%87%BD%E6%95%B0%E4%B8%8B/</link><pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6-%E9%80%92%E5%BD%92%E4%B8%8E%E6%AF%8D%E5%87%BD%E6%95%B0%E4%B8%8B/</guid><description>
&lt;h2 id="递归与母函数下">递归与母函数（下）&lt;!-- omit in toc -->&lt;/h2>
&lt;p>递推关系是计数的一个强有力的工具，特别是在做算法分析时是必需的。递推关系的求解主要是利用母函数。当然母函数尚有其他用处，但这主要是介绍解递推关系上的应用。&lt;/p>
&lt;p>通常序列&lt;span class="math">\(a_0,a_1,\dotsb,a_n,\dotsb\)&lt;/span>与某个问题序列&lt;span class="math">\(P_0,P_1,\dotsb,P_n,\dotsb\)&lt;/span>的计数问题相对应，若已知序列的母函数，则可确定该序列，从而可以解决相应的计数问题。&lt;/p>
&lt;p>注意：&lt;strong>母函数==生成函数&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>组合数学引子&lt;/li>
&lt;li>有重复组合公式及其证明方法&lt;br />&lt;/li>
&lt;li>母函数定义&lt;/li>
&lt;li>普通母函数(Ordinary generating function，OGF)&lt;/li>
&lt;li>指数生成函数(Exponential generating function，EGF)&lt;/li>
&lt;li>泊松生成函数&lt;/li>
&lt;li>普通母函数举例&lt;/li>
&lt;li>母函数的性质&lt;/li>
&lt;li>整数的拆分&lt;/li>
&lt;li>指数型母函数详解&lt;/li>
&lt;li>递推关系&lt;/li>
&lt;li>汉诺塔&lt;/li>
&lt;li>求n位十进制数中出现偶数个5的数的个数&lt;/li>
&lt;li>从n个元素中取r个进行允许重复的组合&lt;/li>
&lt;li>Fibonacci数列&lt;/li>
&lt;li>&lt;a href="#线性常系数递推关系">线性常系数递推关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#k阶常系数线性齐次递推关系">k阶常系数线性齐次递推关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常系数线性非齐次递推关系">常系数线性非齐次递推关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非线性递推关系举例">非线性递推关系举例&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多项式系数">多项式系数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#Stirling数">Stirling数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#Catalan数">Catalan数&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="线性常系数递推关系">线性常系数递推关系&lt;/h2>
&lt;p>定义：设k是给定的正整数，如果序列&lt;span class="math">\(\{a_n\}\)&lt;/span>满足 &lt;span class="math">\[a_n+c_1 a_{n-1}+c_2 a_{n-2}+\dotsb+c_k a_{n-k}=f(n)\tag{7.1}\\
n&amp;gt;k,c_k\neq0\]&lt;/span> 其中&lt;span class="math">\(c_1,c_2,\dotsb,c_k\)&lt;/span>是常数，则该方程成为序列&lt;span class="math">\(\{a_n\}\)&lt;/span>的&lt;strong>k阶常系数线性&lt;/strong>递推关系。若&lt;span class="math">\(f(n)=0\)&lt;/span>，则线性递推关系成为&lt;strong>齐次&lt;/strong>的。&lt;/p>
&lt;p>定义：方程 &lt;span class="math">\[x^k+c_1 x^{k-1}+c_2 x^{k -2}+\dotsb+c_{k-1}x+c_k=0\tag{7.2}\]&lt;/span> 称为递推关系的&lt;strong>特征方程&lt;/strong>，其根称为递推关系的&lt;strong>特征根&lt;/strong>。&lt;/p>
&lt;h3 id="k阶常系数线性齐次递推关系">k阶常系数线性齐次递推关系&lt;/h3>
&lt;p>齐次：没有常数项，表达式： &lt;span class="math">\[a_n+c_1 a_{n-1}+c_2 a_{n-2}+\dotsb+c_k a_{n-k}=0\tag{7.3}\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>引理1：r设为非零数，则&lt;span class="math">\(a_n=r^n\)&lt;/span>是齐次递推关系的一个解&lt;strong>当且仅当&lt;/strong>r是递推关系的特征根。&lt;/p>
&lt;/blockquote>
&lt;p>证明：必要性。若&lt;span class="math">\(a_n=r^n\)&lt;/span>是递推关系的一个解，那么有 &lt;span class="math">\[\begin{aligned}
r^n+c_1*r^{n-1}+\dotsb+c_k r^{n-k}&amp;amp;=0\\
r^{n-k}(r^k+c_1 r^{k-1}+\dotsb+c_k)&amp;amp;=0\\
r^k+c_1 r^{k-1}+\dotsb+c_k&amp;amp;=0\\
\Rightarrow r是特征根
\end{aligned}\]&lt;/span>&lt;/p>
&lt;p>充分性：若r是递推关系的特征根。那么有 &lt;span class="math">\[r^k+c_1 r^{k-1}+\dotsb+c_k=0\]&lt;/span> 在方程的两端同乘以&lt;span class="math">\(r^{n-k}\)&lt;/span>，便得 &lt;span class="math">\[r^n+c_1*r^{n-1}+\dotsb+c_k r^{n-k}=0\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>引理2：线性。&lt;span class="math">\(A_1,A_2\)&lt;/span>是任意常数，若&lt;span class="math">\(\{a_n\},\{b_n\}\)&lt;/span>都是齐次递推关系的解，那么&lt;span class="math">\(\{A_1 a_n+A_2 b_n\}\)&lt;/span>也是齐次递推关系的解。&lt;/p>
&lt;/blockquote>
&lt;p>推论： &amp;gt;设&lt;span class="math">\(\{a_n^1\},\{a_n^2\},\dotsb,\{a_n^t\}\)&lt;/span>是齐次递推关系的t个不同的解，若对齐次递推关系的每一个解，都存在常数&lt;span class="math">\(A_1&amp;#39;,A_2&amp;#39;,\dotsb,A_t&amp;#39;\)&lt;/span>，使得 &amp;gt;&lt;span class="math">\[a_n=A_1&amp;#39;a_n^1+A_2&amp;#39;a_n^2+\dotsb+A_t&amp;#39;a_n^t\]&lt;/span> &amp;gt;成立，则称&lt;span class="math">\(A_1&amp;#39;a_n^1+A_2&amp;#39;a_n^2+\dotsb+A_t&amp;#39;a_n^t\)&lt;/span>为齐次递推关系的通解，其中&lt;span class="math">\(A_1&amp;#39;,A_2&amp;#39;,\dotsb,A_t&amp;#39;\)&lt;/span>为任意常数。&lt;/p>
&lt;p>非常类似于微分方程的解。&lt;/p>
&lt;blockquote>
&lt;p>定理7.1：设&lt;span class="math">\(r_1,r_2,\dotsb,r_k\)&lt;/span>是齐次递推关系的&lt;span class="math">\(k\)&lt;/span>个不同的特征根，那么 &lt;span class="math">\[a_n=A_1 r_1^n+A_2 r_2^n+\dotsb+A_k r_k^n\tag{7.4}\]&lt;/span> 是齐次递推关系的通解。&lt;strong>此定理只有在k阶特征方程解出k个不同根才有效&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：设&lt;span class="math">\(\{h_n\}\)&lt;/span>是齐次递推关系的任意一个解，那么&lt;span class="math">\(\{h_n\}\)&lt;/span>由k个初值&lt;span class="math">\(h_0=d_0,h_1=d_1,\dotsb,h_{k-1}=d_{k-1},\)&lt;/span>唯一确定。于是有 &lt;span class="math">\[\begin{cases}
h_0=A_1+A_2+\dotsb+A_k=d_0\\
h_1=A_1 r_1+A_2 r_2+\dotsb+A_k r_k=d_1\\
\dotsb\\
h_{k-1}=A_1 r_1^{k-1}+A_2 r_2^{k-1}+\dotsb+A_k r_k^{k-1}=d_{k-1}\\
\end{cases}\]&lt;/span> 此一次方程组的系数行列式为范德蒙矩阵行列 &lt;span class="math">\[\begin{vmatrix}
1&amp;amp;1&amp;amp;\dotsb&amp;amp;1\\
r_1&amp;amp;r_2&amp;amp;\dotsb&amp;amp;r_k\\
\dotsb&amp;amp;\dotsb&amp;amp;\dotsb&amp;amp;\dotsb\\
r_1^{k-1}&amp;amp;r_2^{k-1}&amp;amp;\dotsb&amp;amp;r_k^{k-1}\\
\end{vmatrix}=\prod_{1\le j&amp;lt;i \le k}(r_i-r_j) \neq 0\]&lt;/span> 因此k个自由度需要k个一次线性不相关方程来唯一确定。&lt;/p>
&lt;p>&lt;strong>例题&lt;/strong>：求解递归关系 &lt;span class="math">\[\begin{cases}
a_n-9a_{n-1}+26a_{n-2}-24a_{n-3}=0, &amp;amp;n \ge 3\\ \tag{7.5}
a_0=6,a_1=17,a_2=53
\end{cases}\]&lt;/span>&lt;/p>
&lt;p>解：特征方程为 &lt;span class="math">\[x^3-9x^2+26x-24=0\]&lt;/span> 解方程得： &lt;span class="math">\[x_1=2,x_2=3,x_3=4\]&lt;/span> 故通解为： &lt;span class="math">\[a_n=A_1 2^n+A_2 3^n+A_3 4^n\]&lt;/span> 根据初始条件联立方程组： &lt;span class="math">\[\begin{cases}
A_1+A_2+A_3=6\\
A_1*2+A_2*3+A_3*4=17\\
A_1*4+A_2*9+A_3*16=53\\
\end{cases}\]&lt;/span> 得&lt;span class="math">\(A_1=3,A_2=1,A_3=2\)&lt;/span>，因此 &lt;span class="math">\[a_n=3*2^n+3^n+2*4^n\]&lt;/span>&lt;/p>
&lt;p>如果用原来的凑单法会相当麻烦：&lt;/p>
&lt;p>&lt;img src="../images/凑单法解题1.png" alt="凑单法解题1" /> &lt;img src="../images/凑单法解题2.png" alt="凑单法解题2" /> &lt;span class="math">\[A(x)=\frac{3}{1-2x}+\frac{1}{1-3x}+\frac{2}{1-4x}\\
a_n=3*2^n+3^n+2*4^n,n \ge 0
\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>例题&lt;/strong>：求解递归关系 &lt;span class="math">\[\begin{cases}
a_n-4a_{n-1}+4a_{n-2}=0, &amp;amp;n \ge 2\\ \tag{7.6}
a_0=1,a_1=3\\
\end{cases}\]&lt;/span> 其特征方程为&lt;span class="math">\(x^2-4x+4=0 \Rightarrow x_1=x_2=2\)&lt;/span>，此时有 &lt;span class="math">\[a_n=A_1 2^n+A_2 2^n=(A_1+A_2)2^n\]&lt;/span> 为了满足初始条件&lt;span class="math">\(a_0=1,a_1=3\)&lt;/span>，有 &lt;span class="math">\[\begin{cases}
A_1+A_2=1\\
2A_1+2A_2=3\\
\end{cases}\]&lt;/span> 但这是不可能的。&lt;strong>引出有重根的情形&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>定理7.2：设&lt;span class="math">\(r_1,r_2,\dotsb,r_s\)&lt;/span>是k阶齐次递推关系的&lt;span class="math">\(s(s&amp;lt;k)\)&lt;/span>个不同的特征根,其重数分别为&lt;span class="math">\(h_1,h_2,\dotsb,h_s\)&lt;/span>，且&lt;span class="math">\(h_1+h_2+\dotsb+h_s=k\)&lt;/span>那么齐次递推关系的通解是 &lt;span class="math">\[a_n=\alpha_n^1+\alpha_n^2+\dotsb+\alpha_n^s.\tag{7.7}\]&lt;/span> 其中，&lt;span class="math">\(\alpha_n^i=(b_0^i+b_1^in+b_2^i n^2+\dotsb+b_{h_i}^i n^{h_i-1})r_i^n,i=1,2,\dotsb,s\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明略。&lt;/p>
&lt;p>很显然，没有重根的是这个定理7.2公式的特殊情况。&lt;/p>
&lt;p>重回上个例题： &lt;span class="math">\[a_n=(A+Bn)2^n\\
\begin{cases}
A=1\\
2(A+B)=3\\
\end{cases}\\
\Rightarrow A=1,B=0.5\\
\Rightarrow a_n=(1+0.5n)*2^n\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>例题&lt;/strong>：求解递推关系 &lt;span class="math">\[\begin{cases}
a_n=-a_{n-1}+3a_{n-2}+5a_{n-3}+2a_{n-4},n \ge 4\\ \tag{7.8}
a_0=1,a_1=0,a_2=1,a_3=2\\
\end{cases}\]&lt;/span> 解：其特征方程为 &lt;span class="math">\[x^4+x^3-3x^2-5x-2=0\]&lt;/span> 其特征根为：&lt;span class="math">\(x_1=x_2=x_3=-1,x_4=2\)&lt;/span> 其通解为： &lt;span class="math">\[a_n=(t_1+t_2 n +t_3 n^2)(-1)^n+t_4 2^n\]&lt;/span> 联立方程组： &lt;span class="math">\[\begin{cases}
t_1+t_4=1\\
(t_1+t_2+t_3)*(-1)+t_4*2=0\\
(t_1+t_2*2+t_3*4)+t_4*4=1\\
(t_1+t_2*3+t_3*9)*(-1)+t_4*8=2\\
\end{cases}\\
\Rightarrow t_1=7/9,t_2=-1/3,t_3=0,t_4=2/9\]&lt;/span> 由此可得： &lt;span class="math">\[a_n=(7/9-1/3*n)(-1)^n+2/9*2^n,n \ge 0\]&lt;/span>&lt;/p>
&lt;h3 id="常系数线性非齐次递推关系">常系数线性非齐次递推关系&lt;/h3>
&lt;p>非齐次：必有常数项，表达式： &lt;span class="math">\[a_n+c_1 a_{n-1}+c_2 a_{n-2}+\dotsb+c_k a_{n-k}=f(n) \neq 0 \tag{7.9}\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理7.3：k阶常系数线性非齐次递推关系&lt;span class="math">\((7.9)\)&lt;/span>的通解是该递推关系的&lt;strong>一个特解加上其相应的齐次递推关系&lt;/strong>的通解。&lt;/p>
&lt;/blockquote>
&lt;p>这个关系非常类似非齐次微分方程中特解和通解的关系的情况。&lt;/p>
&lt;p>证明：设&lt;span class="math">\(a_n&amp;#39;,a_n^\ast\)&lt;/span>分别为式&lt;span class="math">\((7.9)\)&lt;/span>的特解和式&lt;span class="math">\((7.3)\)&lt;/span>的通解。则 &lt;span class="math">\[a_n&amp;#39;+c_1 a_{n-1}&amp;#39;+c_2 a_{n-2}&amp;#39;+\dotsb+c_k a_{n-k}&amp;#39;=f(n)\\
a_n^\ast+c_1 a_{n-1}^\ast+c_2 a_{n-2}^\ast+\dotsb+c_k a_{n-k}^\ast=0\]&lt;/span> 因此有 &lt;span class="math">\[(a_n&amp;#39;+a_n^\ast)+c_1(a_{n-1}&amp;#39;+a_{n-1}^\ast)+c_2(a_{n-2}&amp;#39;+a_{n-2}^\ast)+\dotsb\\
+c_k(a_{n-k}&amp;#39;+a_{n-k}^\ast)=f(n)\]&lt;/span> 若&lt;span class="math">\(a_n\)&lt;/span>为式&lt;span class="math">\(7.9\)&lt;/span>的任意一个解，易知&lt;span class="math">\(a_n-a_n&amp;#39;=a_n^\ast\)&lt;/span>是式(7.3)的通解，所以 &lt;span class="math">\[a_n=a_n&amp;#39;+a_n^\ast\tag{7.10}\]&lt;/span>&lt;/p>
&lt;p>不难看出，一般递推的关系的特解比较难求，而通解可以通过定理7.2求得。而且非齐次递推关系的&lt;strong>特解通常是没有普遍的解法的&lt;/strong>。只有在某些简单的情况下可用待定系数法求出&lt;span class="math">\(a_n&amp;#39;\)&lt;/span>。 &amp;gt;定理7.4：&lt;strong>特解形式&lt;/strong>。若线性非齐次递推关系 &amp;gt;&lt;span class="math">\[a_n+c_1 a_{n-1}+c_2 a_{n-2}+\dotsb+c_k a_{n-k}=f(n),c_k \neq 0\\
f(n)=(b_t n^t+b_{t-1} n^{t-1}+\dotsb+b_1 n+b_0)s^n\]&lt;/span> &amp;gt;的非齐次项&lt;span class="math">\(f(n)\)&lt;/span>满足特定条件，则递推关系的特解可以有以下特定形式： &amp;gt; &amp;gt;1. 当&lt;span class="math">\(f(n)\)&lt;/span>中无&lt;span class="math">\(s^n\)&lt;/span>指数项时，对应的特解形式为&lt;span class="math">\(a_n&amp;#39;=P_t n^t+P_{t-1} n^{t-1}+\dotsb+P_1 n+P_0\)&lt;/span>，其中&lt;span class="math">\(P_0,P_1,\dotsb,P_t\)&lt;/span>为待定系数。 &amp;gt;2. 当&lt;span class="math">\(f(n)中，s\)&lt;/span>不是对应的齐次递推关系的特征根，则对应的特解是&lt;span class="math">\((P_t n^t+P_{t-1} n^{t-1}+\dotsb+P_1 n+P_0)s^n\)&lt;/span>，其中&lt;span class="math">\(P_0,P_1,\dotsb,P_t\)&lt;/span>为待定系数。 &amp;gt;3. 当&lt;span class="math">\(f(n)中，s\)&lt;/span>是特征方程的&lt;span class="math">\(m\)&lt;/span>重特征根，则对应的特解是&lt;span class="math">\(n^m(P_t n^t+P_{t-1} n^{t-1}+\dotsb+P_1 n+P_0)s^n\)&lt;/span>，其中&lt;span class="math">\(P_0,P_1,\dotsb,P_t\)&lt;/span>为待定系数。&lt;/p>
&lt;p>&lt;strong>例题&lt;/strong>求解递推关系 &lt;span class="math">\[\begin{cases}
a_n-a_{n-1}-6a_{n-2}=5\cdot4^n\\ \tag{7.11}
a_0=5,a_1=3
\end{cases}\]&lt;/span> 解：其特征方程为 &lt;span class="math">\[x^2-x-6=(x-3)(x+2)=0\]&lt;/span> 其特征根为&lt;span class="math">\(x=3,-2\)&lt;/span>，通解形式为：&lt;span class="math">\(A3^n+B(-2)^n\)&lt;/span>。&lt;/p>
&lt;p>&lt;span class="math">\(f(n)=5\cdot 4^n\)&lt;/span>，其中4不是特征方程的根。特解形式为&lt;span class="math">\(P_0\cdot 4^n\)&lt;/span>，带入递推关系得： &lt;span class="math">\[P_0 4^n-P_0 4^{n-1}-6P_0 4^{n-2}=5\cdot 4^n\\
\Rightarrow P_0=\frac{40}{3}\]&lt;/span> 该递推关系的通解为： &lt;span class="math">\[a_n=A3^n+B(-2)^n+\frac{40}{3}4^n\]&lt;/span> 带入初始条件得 &lt;span class="math">\[a_n=\frac{-67}{5}3^n+\frac{76}{15}(-2)^n+\frac{40}{3}4^n\]&lt;/span>&lt;/p>
&lt;h2 id="非线性递推关系举例">非线性递推关系举例&lt;/h2>
&lt;h3 id="多项式系数">多项式系数&lt;/h3>
&lt;p>&lt;strong>引子&lt;/strong>：在下列展开式 &lt;span class="math">\[(x_1+x_2+x_3)^3=x_1^3+x_2^3+x_3^3+3x_1^2x_2+3x_1^2x_3\\
+3x_1x_2^3+3x_1x_3^2+3x_2^2x_3+3x_2x_3^2+6x_1x_2x_3\]&lt;/span> 中，每项都是&lt;span class="math">\(x_1^{n_1}x_2^{n_2}x_3^{n_3}\)&lt;/span>的形式，其中&lt;span class="math">\(n_1,n_2,n_3\)&lt;/span>都是非负整数，且&lt;span class="math">\(n_1+n_2+n_3=3\)&lt;/span>。项&lt;span class="math">\(x_1^{n_1}x_2^{n_2}x_3^{n_3}\)&lt;/span>得系数为 &lt;span class="math">\[\frac{3!}{n_1!n_2!n_3!}\]&lt;/span> 等同于在求范围内的排列数。&lt;/p>
&lt;blockquote>
&lt;p>定理8.1 设n为正整数，则 &lt;span class="math">\[(x_1+x_2+\dotsb+x_m)^n\\
=\sum_{n_1+n_2+\dotsb+n_m=n}C(n, n_1,n_2,\dotsb,n_m)x_1^{n_1}x_2^{n_2}\dotsb x_m^{n_m}\]&lt;/span> 其中&lt;span class="math">\(n_1,n_2,\dotsb,n_m\)&lt;/span>为非负整数。&lt;/p>
&lt;/blockquote>
&lt;p>证明：系数为多重集&lt;span class="math">\(x_i^{n_i}\)&lt;/span>的全排列。 &lt;span class="math">\[C(n,n_1)C(n-n_1,n_2)\dotsb C(n-n_1-\dotsb-n_{m-1},n_m)\\
\begin{aligned}
&amp;amp;=\frac{n!}{(n-n_1)!n_1!}\frac{(n-n_1)!}{(n-n_1-n_2)!n_2!}\dotsb\frac{(n-n_1-\dotsb-n_{m-1})!}{(n-n_1-\dotsb-n_m)!n_m!}\\
&amp;amp;=\frac{n!}{n_1!n_2!\dotsb n_m!(n-n_1-\dotsb-n_m)!}\\
&amp;amp;\because n=n_1+n_2+\dotsb+n_m\\
&amp;amp;\therefore 原式=\frac{n!}{n_1!n_2!\dotsb n_m!}
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>例题&lt;/strong>求展开&lt;span class="math">\((x_1+x_2+\dotsb+x_5)^7\)&lt;/span>，则&lt;span class="math">\(x_1^2x_3x_4^3x_5\)&lt;/span>的系数。 &lt;span class="math">\[\frac{(2+1+3+1)!}{2!1!3!1!}=420\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>例题&lt;/strong>展开&lt;span class="math">\((2x_1-3x_2+5x_3)^6\)&lt;/span>，则&lt;span class="math">\(x_1^3x_2x_3^2\)&lt;/span>的系数为 &lt;span class="math">\[\frac{(3+1+2)!}{3!1!2!}\cdot 2^3\cdot(-3)\cdot5^2=-36000\]&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理8.2 展开式&lt;span class="math">\((x_1+x_2+\dotsb+x_m)^n\)&lt;/span>的项数等于&lt;span class="math">\(C(n+m-1,n)\)&lt;/span>，而且这些项的系数之和等于&lt;span class="math">\(m^n\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：展开式的项&lt;span class="math">\(x_1^{n_1}x_2^{n_2}\dotsb x_m^{n_m},(n_1+n_2+\dotsb+n_m=n)\)&lt;/span> 与从m个元素中取n个允许重复的组合一一对应，故其项数为&lt;span class="math">\(C(n+m-1,n)\)&lt;/span>。当令&lt;span class="math">\(x_1=x_2=\dotsb=x_m=1\)&lt;/span>时，系数和为&lt;span class="math">\((1+1+\dotsb+1)^n=m^n\)&lt;/span>&lt;/p>
&lt;p>&lt;strong>例题&lt;/strong>多项式&lt;span class="math">\((x_1+x_2+x_3)^3\)&lt;/span>的展开式中恰有 &lt;span class="math">\[C(n+m-1,n)=C(3+3-1,3)=C(5,3)=10\]&lt;/span> 项。&lt;/p>
&lt;h3 id="stirling数">Stirling数&lt;/h3>
&lt;p>有两类Stirling数，称之为第一类Stirling数和第二类Stirling数&lt;/p>
&lt;blockquote>
&lt;p>定义： &lt;span class="math">\[[x_n]=x(x-1)\dotsb(x-n+1)\\
=s(n,n)x^n-s(n,n-1)x^{n-1}+\dotsb+(-1)^{n-1}s(n,1)x+(-1)^n s(n,0)\]&lt;/span> 称&lt;span class="math">\(s(n,0),s(n,1),\dotsb,s(n,n)\)&lt;/span>为&lt;strong>第一类Stirling数&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>定义可知：&lt;span class="math">\(s(n,0)=0 \quad s(n,n)=1\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>定义：n个有区别的球放到m个相同的盒子中，要求无一空盒，其不同的方案数用&lt;span class="math">\(S(n,m)\)&lt;/span>表示，称为&lt;strong>第二类Stirling数&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;h3 id="catalan数">Catalan数&lt;/h3>
&lt;blockquote>
&lt;p>定义：数 &lt;span class="math">\[C_n=\frac{1}{n+1}C(2n,n),n=0,1,2,\dotsb\]&lt;/span> 为第n个Catalan数。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">n&lt;/th>
&lt;th align="left">0&lt;/th>
&lt;th align="left">1&lt;/th>
&lt;th align="left">2&lt;/th>
&lt;th align="left">3&lt;/th>
&lt;th align="left">4&lt;/th>
&lt;th align="left">5&lt;/th>
&lt;th align="left">6&lt;/th>
&lt;th align="left">7&lt;/th>
&lt;th align="left">8&lt;/th>
&lt;th align="left">9&lt;/th>
&lt;th align="left">10&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">&lt;span class="math">\(C_n\)&lt;/span>&lt;/td>
&lt;td align="left">1&lt;/td>
&lt;td align="left">1&lt;/td>
&lt;td align="left">2&lt;/td>
&lt;td align="left">5&lt;/td>
&lt;td align="left">14&lt;/td>
&lt;td align="left">42&lt;/td>
&lt;td align="left">132&lt;/td>
&lt;td align="left">429&lt;/td>
&lt;td align="left">1430&lt;/td>
&lt;td align="left">4862&lt;/td>
&lt;td align="left">16796&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/blockquote></description></item><item><title>实变函数8之Radon-Nikodym定理</title><link>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B08%E4%B9%8Bradon-nikodym%E5%AE%9A%E7%90%86/</link><pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B08%E4%B9%8Bradon-nikodym%E5%AE%9A%E7%90%86/</guid><description/></item><item><title>实变函数7之黎曼积分</title><link>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B07%E4%B9%8B%E9%BB%8E%E6%9B%BC%E7%A7%AF%E5%88%86/</link><pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B07%E4%B9%8B%E9%BB%8E%E6%9B%BC%E7%A7%AF%E5%88%86/</guid><description/></item><item><title>实变函数6之控制收敛定理单调收敛定理与fatou引理</title><link>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B06%E4%B9%8B%E6%8E%A7%E5%88%B6%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86%E5%8D%95%E8%B0%83%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86%E4%B8%8Efatou%E5%BC%95%E7%90%86/</link><pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B06%E4%B9%8B%E6%8E%A7%E5%88%B6%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86%E5%8D%95%E8%B0%83%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86%E4%B8%8Efatou%E5%BC%95%E7%90%86/</guid><description/></item><item><title>实变函数5之不定积分</title><link>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B05%E4%B9%8B%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/</link><pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B05%E4%B9%8B%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/</guid><description/></item><item><title>实变函数4之可积函数序列与收敛</title><link>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B04%E4%B9%8B%E5%8F%AF%E7%A7%AF%E5%87%BD%E6%95%B0%E5%BA%8F%E5%88%97%E4%B8%8E%E6%94%B6%E6%95%9B/</link><pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B04%E4%B9%8B%E5%8F%AF%E7%A7%AF%E5%87%BD%E6%95%B0%E5%BA%8F%E5%88%97%E4%B8%8E%E6%94%B6%E6%95%9B/</guid><description/></item><item><title>实变函数3之积分的性质</title><link>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B03%E4%B9%8B%E7%A7%AF%E5%88%86%E7%9A%84%E6%80%A7%E8%B4%A8/</link><pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B03%E4%B9%8B%E7%A7%AF%E5%88%86%E7%9A%84%E6%80%A7%E8%B4%A8/</guid><description/></item><item><title>实变函数2之可积函数</title><link>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B02%E4%B9%8B%E5%8F%AF%E7%A7%AF%E5%87%BD%E6%95%B0/</link><pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B02%E4%B9%8B%E5%8F%AF%E7%A7%AF%E5%87%BD%E6%95%B0/</guid><description/></item><item><title>实变函数1之简单函数积分</title><link>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B01%E4%B9%8B%E7%AE%80%E5%8D%95%E5%87%BD%E6%95%B0%E7%A7%AF%E5%88%86/</link><pubDate>Thu, 09 Apr 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B01%E4%B9%8B%E7%AE%80%E5%8D%95%E5%87%BD%E6%95%B0%E7%A7%AF%E5%88%86/</guid><description>
&lt;ul>
&lt;li>&lt;a href="#简单函数积分定义">简单函数积分定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#简单函数的柯西序列">简单函数的柯西序列&lt;/a>&lt;/li>
&lt;li>&lt;a href="#习题">习题&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="简单函数的积分">简单函数的积分&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="简单函数积分定义">简单函数积分定义&lt;/h2>
&lt;blockquote>
&lt;p>简单函数:函数&lt;span class="math">\(f:X\rightarrow \mathbb{R}\)&lt;/span>称为简单函数（simple function），如果存在&lt;strong>有限个不相交&lt;/strong>的可测子集&lt;span class="math">\(\{E_1,E_2,\dotsb,E_m\}\)&lt;/span>和&lt;strong>有限个&lt;/strong>实数&lt;span class="math">\(\alpha_1,\alpha_2,\dotsb,\alpha_m\)&lt;/span>,使得&lt;span class="math">\(X=E_1\cup E_2\dotsb\cup E_m\)&lt;/span>和&lt;span class="math">\(\forall j,f=\alpha_j \quad on \quad E_j\)&lt;/span>。&lt;/p>
&lt;p>此时 &lt;span class="math">\(f\)&lt;/span> 可表示为： &lt;span class="math">\(f(x)=\sum\limits_{j=1}^m \alpha_j\chi_{E_j}(x)\)&lt;/span> ，&lt;span class="math">\(\chi_{E_j}\)&lt;/span>是可测子集 &lt;span class="math">\(E_j\)&lt;/span> 的特征函数(即示性函数)。&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../images/simple_function.jpg" alt="简单函数" />&lt;p class="caption">简单函数&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>注1:简单函数的取值&lt;span class="math">\(\alpha_1,\alpha_2,\dotsb,\alpha_m \in \mathbb{R}\)&lt;/span>没有要求两两不同&lt;/li>
&lt;li>注2:由定义可知,简单函数是可测的.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>定义:(可积简单函数在全空间&lt;span class="math">\(X\)&lt;/span>上的积分）&lt;/p>
&lt;p>测度空间 &lt;span class="math">\((X,\mathcal{F},\mu)\)&lt;/span> 上的简单函数 &lt;span class="math">\(f=\sum\limits_{i=1}^m \alpha_i\chi_{E_i}\)&lt;/span> 称为是&lt;strong>可积的&lt;/strong>(integrable)，如果当 &lt;span class="math">\(\alpha_i\neq 0\)&lt;/span> 时，有 &lt;span class="math">\(\mu(E_i)&amp;lt;\infty\)&lt;/span>。然后我们规定当 &lt;span class="math">\(\alpha_i=0,\mu(E_i)=\infty\)&lt;/span> 时，&lt;span class="math">\(\alpha_i\mu(E_i)=0\)&lt;/span>。此时，定义&lt;span class="math">\(f\)&lt;/span>积分的值为 &lt;span class="math">\[\sum\limits_{i=1}^m \alpha_i\mu(E_i)\]&lt;/span> 将这个值记做&lt;span class="math">\(\int f(x) d\mu(x)\)&lt;/span>,或者 &lt;span class="math">\(\int f d\mu\)&lt;/span>或者&lt;span class="math">\(\int f\)&lt;/span> ，即是&lt;span class="math">\(\int f(x) d\mu(x)=\int f d\mu=\int f=\sum\limits_{i=1}^m \alpha_i\mu(E_i)\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>注:如果加上积分空间,则&lt;span class="math">\(\int f\)&lt;/span>可以写成&lt;span class="math">\(\int_X f\)&lt;/span>。由上面定义知：这个定义即是要求积分是有限值，即 &lt;span class="math">\(\int f d\mu &amp;lt;\infty\)&lt;/span>(有限)。&lt;/p>
&lt;p>这个积分已经有勒贝格积分的意思了，对值域进行划分，然后统计定义域的测度。&lt;strong>从可积的定义来看相当于积分值小于&lt;span class="math">\(\infty\)&lt;/span>&lt;/strong>。&lt;/p>
&lt;p>注意到简单函数&lt;span class="math">\(f\)&lt;/span>可能有另外一种表示方式(拆分方式)，比如 &lt;span class="math">\(f=\sum\limits_{j=1}^n \beta_j\chi_{F_j}\)&lt;/span> ，我们来验证积分这个定义是well-defined：即是验证&lt;span class="math">\(f\)&lt;/span>在不同表示方式下的&lt;strong>积分值是唯一&lt;/strong>的，即是验证： &lt;span class="math">\[
\sum_{i=1}^m \alpha_i\chi_{E_i}=\sum_{j=1}^n \beta_j\chi_{F_j}\Rightarrow \sum_{i=1}^m \alpha_i\mu(E_i)=\sum_{j=1}^n \beta_j\mu(F_j)
\]&lt;/span>&lt;/p>
&lt;p>证明：当&lt;span class="math">\(E_i\cap F_j \neq \emptyset\)&lt;/span>时，在交集&lt;span class="math">\(E_i\cap F_j\)&lt;/span>上元素函数值相等即&lt;span class="math">\(\alpha_i=\beta_j\)&lt;/span>,此时记&lt;span class="math">\(\gamma_{ij}=\alpha_i=\beta_j\)&lt;/span> 然后每个&lt;span class="math">\(E_i\)&lt;/span>可表示为不相交的&lt;span class="math">\(E_i\cap F_j\)&lt;/span>的并（因为&lt;span class="math">\(F_j\)&lt;/span>互不相交）,即&lt;span class="math">\(E_i=\bigcup\limits_{j=1}^n(E_i\cap F_j)\)&lt;/span> 则有 &lt;span class="math">\[
\sum_{i=1}^m \alpha_i\mu(E_i)\overset{测度可列可加性}{=}\sum_{i=1}^m \alpha_i\sum_{j=1}^n\mu(E_i\cap F_j)\\
=\sum_{i=1}^m \sum_{j=1}^n \gamma_{ij}\mu(E_i\cap F_j)
\]&lt;/span> 同理也有 &lt;span class="math">\[
\sum_{j=1}^n \beta_j\mu(F_j)\overset{测度可列可加性}{=}\sum_{j=1}^n \beta_j\sum_{i=1}^m\mu(F_j\cap E_i)\\
=\sum_{i=1}^m \sum_{j=1}^n \gamma_{ij}\mu(E_i\cap F_j)
\]&lt;/span> 因为&lt;span class="math">\(m,n\)&lt;/span>是有限数，故双重求和符号可以交换次序。 即是&lt;span class="math">\(\sum\limits_{i=1}^m \alpha_i\mu(E_i)=\sum\limits_{j=1}^n \beta_j\mu(F_j)\)&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>命题1：在测度空间&lt;span class="math">\((X,\mathcal{F},\mu)\)&lt;/span>中，&lt;/p>
&lt;p>(a)&lt;span class="math">\(f\)&lt;/span>是简单函数，&lt;span class="math">\(E\in \mathcal{F}\)&lt;/span>是可测集。则&lt;span class="math">\(\chi_E\cdot f\)&lt;/span>也是简单函数；&lt;/p>
&lt;p>(b)&lt;span class="math">\(f\)&lt;/span>是简单函数，且可积分；&lt;span class="math">\(E\in \mathcal{F}\)&lt;/span>是可测集。则&lt;span class="math">\(\chi_E\cdot f\)&lt;/span>也可积分（隐含着也是简单函数）；&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>（a） 首先说明易证 &lt;span class="math">\(\chi_A\cdot\chi_B=\chi_{A\cap B}\)&lt;/span>。&lt;span class="math">\(f\)&lt;/span>是简单函数，因此&lt;span class="math">\(f\)&lt;/span>可以写成&lt;span class="math">\(f=\sum\limits_{i=1}^m\alpha_i\chi_{E_i}\)&lt;/span>，其中&lt;span class="math">\(E_i\cap E_j=\emptyset, X=\bigcup\limits_{i=1}^mE_i\)&lt;/span>（即两两互不相交且并为全集），那么&lt;span class="math">\(\chi_E\cdot f=\chi_E\cdot \sum\limits_{i=1}^m\alpha_i\chi_{E_i}=\sum\limits_{i=1}^m\alpha_i(\chi_E\cdot\chi_{E_i})=\sum\limits_{i=1}^m\alpha_i\chi_{E_i\cap E}\)&lt;/span>&lt;/p>
&lt;p>所以&lt;span class="math">\(\chi_E\cdot f\)&lt;/span>也是简单函数，且空间由&lt;span class="math">\(X\)&lt;/span>restrict到&lt;span class="math">\(E\)&lt;/span>。&lt;/p>
&lt;p>（b） 要证明简单函数可积，即要证明其积分&lt;span class="math">\(\int \chi_E\cdot fd\mu\)&lt;/span>有限。 &lt;span class="math">\[
\int \chi_E\cdot f d\mu=\sum_{i=1}^m \alpha_i \mu(E_i\cap E)\overset{测度单调性}{\leq} \sum_{i=1}^m \alpha_i \mu(E_i)\\
=\int f d\mu &amp;lt; \infty
\]&lt;/span> 故&lt;span class="math">\(\chi_E\cdot f\)&lt;/span>是可积的。&lt;/p>
&lt;blockquote>
&lt;p>定义2：可积简单函数在可测子集&lt;span class="math">\(E\)&lt;/span>上的积分。测度空间&lt;span class="math">\((X,\mathcal{F},\mu)\)&lt;/span>，设&lt;span class="math">\(E\)&lt;/span>是可测子集。定义可积简单函数&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(E\)&lt;/span>上的积分为： &lt;span class="math">\[
\int_E fd\mu=\int_X \chi_E\cdot f d\mu = \sum_{i=1}^m \alpha_i \mu(E_i\cap E)
\]&lt;/span> 其中&lt;span class="math">\(X=\bigcup\limits_{i=1}^m E_i\)&lt;/span>，且&lt;span class="math">\(E_i\cap E_j=\emptyset, \forall i\neq j\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>本章从简单函数开始定义积分，所有简单函数定义出来的积分&lt;strong>本质上都是在求和&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>命题2：特征函数&lt;span class="math">\(\chi_E\)&lt;/span>可积当且仅当&lt;span class="math">\(E\)&lt;/span>是可测集且&lt;span class="math">\(\mu(E)&amp;lt;\infty\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：必要性。特征函数是简单函数，若可积，则由定义隐含着(a)&lt;span class="math">\(E\)&lt;/span>是可测的；(b)&lt;span class="math">\(\int \chi_E d\mu&amp;lt;\infty\)&lt;/span>。而&lt;span class="math">\(\int \chi_E d\mu=\int_E d\mu=\mu(E)\)&lt;/span>。也就是&lt;span class="math">\(E\)&lt;/span>是可测的和&lt;span class="math">\(\mu(E)&amp;lt;\infty\)&lt;/span>&lt;/p>
&lt;p>充分性。若&lt;span class="math">\(E\)&lt;/span>是可测集且&lt;span class="math">\(\mu(E)&amp;lt;\infty\)&lt;/span>。显然有&lt;span class="math">\(\int \chi_E d\mu=\int_E d\mu=\mu(E)&amp;lt;\infty\)&lt;/span>,故特征函数&lt;span class="math">\(\chi_E\)&lt;/span>可积。&lt;/p>
&lt;blockquote>
&lt;p>定理1：（简单函数的基本积分性质）&lt;/p>
&lt;p>测度空间&lt;span class="math">\((X,\mathcal{F},\mu)\)&lt;/span>，设&lt;span class="math">\(f,g\)&lt;/span>是可积的简单函数，&lt;span class="math">\(\alpha,\beta \in \mathbb{R}\)&lt;/span>，默认测度都是一般的测度不是signed measure。&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>&lt;span class="math">\(\alpha f+\beta g\)&lt;/span> 是可积的简单函数，且&lt;span class="math">\(\int (\alpha f+\beta g)=\alpha \int f+\beta \int g\)&lt;/span>&lt;strong>即线性&lt;/strong>&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;span class="math">\(fg\)&lt;/span> 也是可积的简单函数&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;span class="math">\(|f|\)&lt;/span> 是可积的简单函数，且&lt;span class="math">\(|\int f|\leq \int |f|\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>如果&lt;span class="math">\(f\geq 0, a.e\)&lt;/span>，则&lt;span class="math">\(\int f\geq 0\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>如果&lt;span class="math">\(f \geq g, a.e\)&lt;/span>，则&lt;span class="math">\(\int f \geq \int g\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;span class="math">\(\int |f+g| \leq \int |f|+\int |g|\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>设可测子集 &lt;span class="math">\(E\)&lt;/span> 满足 &lt;span class="math">\(\mu(E)&amp;lt;\infty\)&lt;/span> ,且在&lt;span class="math">\(E\)&lt;/span>上有&lt;span class="math">\(m\leq f\leq M, a.e\)&lt;/span>。则&lt;span class="math">\(m\mu(E)\leq \int_E fd\mu\leq M\mu(E)\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>如果&lt;span class="math">\(f\geq 0,a.e\)&lt;/span> , &lt;span class="math">\(E\subseteq F \in \mathcal{F}\)&lt;/span> ，则&lt;span class="math">\(\int_E f \leq \int_F f\)&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>设&lt;span class="math">\(E=\bigcup\limits_{k=1}^\infty E_k\)&lt;/span>, &lt;span class="math">\(E,E_k\in \mathcal{F}\)&lt;/span>且&lt;span class="math">\(E_{k1}\cap E_{k2}=\emptyset\)&lt;/span>,则&lt;span class="math">\(\int_E f=\sum\limits_{k=1}^\infty\int_{E_k}f\)&lt;/span>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>注：性质(9)表明&lt;span class="math">\(S(E)=\int_E f d\mu\)&lt;/span> (看成是关于&lt;span class="math">\(E \)&lt;/span>的函数)是一个signed measure，因为有可列可加性质，且容易证&lt;span class="math">\(S(\emptyset)=0\)&lt;/span>&lt;/p>
&lt;p>证明：因为&lt;span class="math">\(f,g\)&lt;/span>是简单函数。故设&lt;span class="math">\(f=\sum\limits_{i=1}^n\alpha_i\chi_{E_i},g=\sum\limits_{j=1}^n\beta_j \chi_{F_j}\)&lt;/span>。注意区分在证明简单函数积分是well-defined的表达式，那是&lt;span class="math">\(f\)&lt;/span>的两种拆分，而此时是两个函数。其中&lt;span class="math">\(\{E_i\}\)&lt;/span>两两不交，&lt;span class="math">\(\{F_j\}\)&lt;/span>两两不交，且&lt;span class="math">\(\bigcup\limits_i^m E_i=\bigcup\limits_j^n F_j=X\)&lt;/span>。&lt;/p>
&lt;p>（1）由于&lt;span class="math">\(\{F_j\}\)&lt;/span>互不相交且所有&lt;span class="math">\(\{F_j\}\)&lt;/span>的并为全集，所以&lt;span class="math">\(\forall i, E_i=\bigcup\limits_{j=1}^n(E_i\cap F_j)\)&lt;/span>，故&lt;span class="math">\(\chi_{E_i}=\sum\limits_{j=1}^n\chi_{E_i\cap F_j}\)&lt;/span>，同理有&lt;span class="math">\(\chi_{F_j}=\sum\limits_{i=1}^m\chi_{E_i\cap F_j}\)&lt;/span>。注意到&lt;span class="math">\(\{E_i\cap F_j\}\)&lt;/span>是两两互不相交的。所以： &lt;span class="math">\[
\begin{aligned}
\alpha f+\beta g &amp;amp;= \sum_{i=1}^m \alpha \alpha_i \chi_{E_i}+\sum_{j=1}^n\beta\beta_j \chi_{F_j}\\
&amp;amp;=\sum_{i=1}^m \alpha \alpha_i \sum_{j=1}^n\chi_{E_i\cap F_j}+\sum_{j=1}^n\beta\beta_j\sum_{i=1}^m\chi_{F_j\cap E_i}\\
&amp;amp;\overset{有限求和可交换次序}{=}\sum_{i=1}^m\sum_{j=1}^n(\alpha\alpha_i+\beta\beta_j)\chi_{E_i\cap F_j}
\end{aligned}
\]&lt;/span> 因此&lt;span class="math">\(\alpha f+\beta g\)&lt;/span>是simple fuction，然后我们计算其积分 &lt;span class="math">\[
\begin{aligned}
\int_X\alpha f+ \beta g &amp;amp;= \sum_{i=1}^m\sum_{j=1}^n(\alpha\alpha_i+\beta\beta_j)\mu(E_i\cap F_j)\\
&amp;amp;=\sum_{i=1}^m \alpha \alpha_i \sum_{j=1}^n\mu(E_i\cap F_j)+\sum_{j=1}^n\beta\beta_j\sum_{i=1}^m\mu(E_i\cap F_j)\\
&amp;amp;\overset{测度可列可加性}{=}\alpha\sum_{i=1}^m\alpha_i\mu(\bigcup_{j=1}^n (E_i\cap F_j))+\beta\sum_{j=1}^n\beta_j\mu(\bigcup_{i=1}^m(E_i\cap F_j))\\
&amp;amp;=\alpha\sum_{i=1}^m\alpha_i\mu(E_i)+\beta\sum_{j=1}^n\beta_j\mu(F_j)\\
&amp;amp;=\alpha \int_X f+ \beta\int_X g&amp;lt;\infty(f,g可积)
\end{aligned}
\]&lt;/span> 可积简单函数的线性得证。&lt;/p>
&lt;p>（2）&lt;span class="math">\(fg=\sum\limits_{i=1}^m\sum\limits_{j=1}^n \alpha_i\beta_j\chi_{E_i\cap F_j}\)&lt;/span>，易得&lt;span class="math">\(fg\)&lt;/span>是simple function。&lt;/p>
&lt;p>而当&lt;span class="math">\(\alpha_i\neq 0\)&lt;/span>时，&lt;span class="math">\(\mu(E_i\cap F_j)\leq \mu(E_i)&amp;lt;\infty\)&lt;/span> (因为&lt;span class="math">\(f\)&lt;/span>可积，定义要求当&lt;span class="math">\(\alpha_i\neq 0\)&lt;/span>时，&lt;span class="math">\(\mu(E_i)&amp;lt;\infty\)&lt;/span>)。因此&lt;span class="math">\(\int_x fg = \sum\limits_{\alpha\neq 0,\beta\neq 0}\alpha_i\beta_j \mu(E_i\cap F_j)&amp;lt;\infty\)&lt;/span>，即&lt;span class="math">\(fg\)&lt;/span>可积。&lt;/p>
&lt;p>（3）&lt;span class="math">\(f\)&lt;/span>在每个&lt;span class="math">\(E_i\)&lt;/span>上的取值是&lt;span class="math">\(\alpha_i\)&lt;/span>，则&lt;span class="math">\(|f|\)&lt;/span>在&lt;span class="math">\(E_i\)&lt;/span>上的取值为&lt;span class="math">\(|\alpha_i|\)&lt;/span>。所以&lt;span class="math">\(|f|\)&lt;/span>只取有限个值，并且仍能够拆分成有限个不相交的部分，故&lt;span class="math">\(|f|\)&lt;/span>是simple function。因为&lt;span class="math">\(|\alpha_i|\neq 0\)&lt;/span>时，&lt;span class="math">\(\mu(E_i)&amp;lt;∞\)&lt;/span>，所以&lt;span class="math">\(|f|\)&lt;/span>是可积的。&lt;/p>
&lt;p>由于测度&lt;span class="math">\(\mu(E_i)≥0\)&lt;/span>，因此&lt;span class="math">\(|\alpha_i|\mu(E_i)≥\alpha_i\mu(E_i)\)&lt;/span>恒成立，因此 &lt;span class="math">\[
\int |f|=\sum_{i=1}^m|\alpha_i|\mu(E_i)≥|\sum_{i=1}^m\alpha_i\mu(E_i)|=|\int f|
\]&lt;/span>&lt;/p>
&lt;p>（4）由&lt;span class="math">\(f=\sum\limits_{i=1}^m \alpha_i \chi_{E_i}≥0, a.e\)&lt;/span>可知，对任一&lt;span class="math">\(\alpha_i≥0\)&lt;/span>，有&lt;span class="math">\(\mu(E_i)≥0\)&lt;/span>，对&lt;span class="math">\(\alpha_i&amp;lt;0\)&lt;/span>对应的&lt;span class="math">\(E_i\)&lt;/span>，则有&lt;span class="math">\(\mu(E_i)=0\)&lt;/span>(根据a.e)的定义可知。因此有&lt;span class="math">\(\int f = \sum\limits_{i=1}^m \alpha_i \mu(E_i)≥0\)&lt;/span>&lt;/p>
&lt;p>（5）令&lt;span class="math">\(h=f-g≥0\)&lt;/span>，根据（4）和（1）线性可得（5）。&lt;/p>
&lt;p>（6）&lt;span class="math">\(f，g\)&lt;/span>可积→由（1）可知&lt;span class="math">\(f+g\)&lt;/span>可积→由（3）可知&lt;span class="math">\(|f+g|\)&lt;/span>可积→由三角不等式可知&lt;span class="math">\(|f+g|≤|f|+|g|\)&lt;/span>→由（5）可知&lt;span class="math">\(\int |f+g|≤\int |f|+\int |g|\)&lt;/span>&lt;/p>
&lt;p>（7）平凡的&lt;/p>
&lt;p>（8）&lt;span class="math">\(E\subseteq F\)&lt;/span>等价于&lt;span class="math">\(\chi_{E}≤\chi_F\)&lt;/span>。又因为&lt;span class="math">\(f≥0,a.e\)&lt;/span>。故&lt;span class="math">\(\chi_E f≤\chi_F f, a.e\)&lt;/span>，根据（5）可有&lt;span class="math">\(\int \chi_E f≤\int \chi_F f\)&lt;/span>即为&lt;span class="math">\(\int_E f ≤ \int_F f\)&lt;/span>&lt;/p>
&lt;p>（9）简单函数积分的可列可加性。已知&lt;span class="math">\(f=\sum\limits_{i=1}^m \alpha_i\chi_{E_i}\)&lt;/span>，而&lt;span class="math">\(\chi_E\cdot f = \sum\limits_{i=1}^m\alpha_i\chi_{E_i\cap E}\)&lt;/span>。因此有 &lt;span class="math">\[
\begin{aligned}
\int_E f &amp;amp;= \sum_{i=1}^m\alpha_i\mu(E_i\cap E)\\
(拆分E)&amp;amp;=\sum_{i=1}^m\alpha_i\sum_{k=1}^∞\mu(E_i\cap E_k)\\
&amp;amp;=\sum_{k=1}^∞\sum_{i=1}^m\alpha_i\mu(E_i\cap E_k)\\
&amp;amp;=\sum_{j=1}^∞\int \chi_{E_k}\cdot f=\sum_{k=1}^∞\int_{E_k} f
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;h2 id="简单函数的柯西序列">简单函数的柯西序列&lt;/h2>
&lt;p>我们目的是定义更一般函数的积分，为此做准备，现在引入简单函数序列&lt;span class="math">\(\{f_n\}\)&lt;/span>的概念。&lt;/p>
&lt;blockquote>
&lt;p>定义3：&lt;strong>积分&lt;/strong>的&lt;strong>简单函数&lt;/strong>序列&lt;span class="math">\(\{f_n\}\)&lt;/span>被称为柯西序列，如果满足：&lt;span class="math">\(\int |f_n-f_m|→0,\)&lt;/span> as &lt;span class="math">\(n,m→\infty\)&lt;/span>,也就是对任意&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>，存在&lt;span class="math">\(N\)&lt;/span>，使得&lt;span class="math">\(m,n≥N\)&lt;/span>时，有&lt;span class="math">\(\int |f_n-f_m|&amp;lt;\varepsilon\)&lt;/span>，此时记作&lt;span class="math">\(\{f_n\}\)&lt;/span>柯西序列。&lt;/p>
&lt;/blockquote>
&lt;p>注1：就是定义了在&amp;quot;in mean&amp;quot;这种收敛方式下的柯西序列。？？&lt;/p>
&lt;p>注2：不要忘记，定义中的积分是对全空间&lt;span class="math">\(X\)&lt;/span>积分。&lt;/p>
&lt;blockquote>
&lt;p>引理1：可积的简单函数序列&lt;span class="math">\(\{f_n\}\)&lt;/span>若是柯西序列，则：存在几乎处处实值的可测函数&lt;span class="math">\(f\)&lt;/span>，使得&lt;span class="math">\(f_n→f\)&lt;/span>依测度收敛。&lt;/p>
&lt;/blockquote>
&lt;h2 id="习题">习题&lt;/h2>
&lt;p>1 测度空间&lt;span class="math">\((X,\mathcal{F},\mu)\)&lt;/span>。设&lt;span class="math">\(f\)&lt;/span>是简单函数，证明：&lt;span class="math">\(f\)&lt;/span>几乎处处为0，当且仅当，对任意的可测集&lt;span class="math">\(E\)&lt;/span>,有&lt;span class="math">\(\int_E f=0\)&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>&lt;span class="math">\(f\)&lt;/span>是简单函数，设&lt;span class="math">\(f\)&lt;/span>取值为&lt;span class="math">\(\alpha_i \in \mathbb{R}, i=\{1,2,\dotsb,m\}\)&lt;/span>，&lt;span class="math">\(X=\bigcup\limits_{i=1}^m E_i, E_i\cap E_j = \emptyset\)&lt;/span>&lt;/p>
&lt;p>&amp;quot;&lt;span class="math">\(\Rightarrow\)&lt;/span>&amp;quot;: &lt;span class="math">\(f=0,a.e\)&lt;/span>，则对&lt;span class="math">\(\alpha_i\neq 0,\mu(E_i)=0\)&lt;/span>。&lt;/p>
&lt;p>那么对任意可测集&lt;span class="math">\(E\)&lt;/span>, &lt;span class="math">\[
\int_E f=\int \chi_{E}f = \sum_{i=1}^m \alpha_i\mu(E\cap E_i)\\
≤\sum_{i=1}^m \alpha_i\mu(E_i)=\sum_{\alpha_i\neq 0}\alpha_i\mu(E_i)=0
\]&lt;/span> &amp;quot;&lt;span class="math">\(\Leftarrow\)&lt;/span>&amp;quot;：反证法。假设&lt;span class="math">\(f\)&lt;/span>不是几乎处处为0。因为&lt;span class="math">\(f\)&lt;/span>是简单函数只取有限个值，那么就存在&lt;span class="math">\(\alpha_k&amp;gt;0\)&lt;/span>且&lt;span class="math">\(\mu(E_k)&amp;gt;0\)&lt;/span>或&lt;span class="math">\(\alpha_k&amp;lt;0\)&lt;/span>且&lt;span class="math">\(\mu(E_k)&amp;lt;0\)&lt;/span>。不妨设是在&lt;span class="math">\(\alpha_k&amp;gt;0\)&lt;/span>且&lt;span class="math">\(\mu(E_k)&amp;gt;0\)&lt;/span>。那么取&lt;span class="math">\(E=E_k\)&lt;/span> ,&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(E_k\)&lt;/span>上的积分为 &lt;span class="math">\[
\int_{E_k} f=\int \chi_{E_k}f = \sum_{i=1}^m \alpha_i\mu(E_k\cap E_i)\\
=\alpha_k\mu(E_k\cap X)=\alpha_k\mu(E_k)&amp;gt;0
\]&lt;/span> 这与假设矛盾，故&lt;span class="math">\(f\)&lt;/span>几乎处处为0&lt;/p>
&lt;p>即如果&lt;span class="math">\(f\)&lt;/span>是非负、可积的简单函数，且 &lt;span class="math">\(\int f = 0\)&lt;/span>,证明 &lt;span class="math">\(f=0, a.e\)&lt;/span>&lt;/p>
&lt;/blockquote></description></item><item><title>数学-重要不等式</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6-%E9%87%8D%E8%A6%81%E4%B8%8D%E7%AD%89%E5%BC%8F/</link><pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6-%E9%87%8D%E8%A6%81%E4%B8%8D%E7%AD%89%E5%BC%8F/</guid><description>
&lt;h2 id="重要不等式">重要不等式&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#数学分析">数学分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#伯努利不等式">伯努利不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫总和不等式">切比雪夫总和不等式&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#切比雪夫不等式总和的积分形式">切比雪夫不等式总和的积分形式&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#闵可夫斯基不等式lps中的三角不等式k">闵可夫斯基不等式(&lt;span class="math">\(L^{p}(S)\)&lt;/span>中的三角不等式K)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#代数">代数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#排序不等式">排序不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#杨氏不等式">杨氏不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#柯西不等式">柯西不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#内斯比特不等式">内斯比特不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#平均数不等式">平均数不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#马勒不等式">马勒不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性矩阵不等式">线性矩阵不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#几何">几何&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三角不等式">三角不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#外森比克不等式">外森比克不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#阿达马不等式">阿达马不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#概率与统计">概率与统计&lt;/a>&lt;/li>
&lt;li>&lt;a href="#集中不等式">集中不等式&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#马尔可夫不等式">马尔可夫不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫不等式">切比雪夫不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#霍夫丁不等式">霍夫丁不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#efronstein不等式">Efron–Stein不等式&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#布尔不等式">布尔不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#琴生jensen不等式凸函数相关">琴生(Jensen)不等式(凸函数相关)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#测度论的版本">测度论的版本&lt;/a>&lt;/li>
&lt;li>&lt;a href="#概率论的版本">概率论的版本&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#赫尔德holder不等式">赫尔德(holder)不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#信息论">信息论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#吉布斯不等式相对熵">吉布斯不等式(相对熵)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#克拉夫特不等式">克拉夫特不等式&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="数学分析">数学分析&lt;/h2>
&lt;h3 id="伯努利不等式">伯努利不等式&lt;/h3>
&lt;p>对任意整数&lt;span class="math">\(n\geq 0\)&lt;/span>，和任意实数&lt;span class="math">\(x\geq -1\)&lt;/span>， &lt;span class="math">\[
(1+x)^{n}\geq 1+nx
\]&lt;/span> 如果&lt;span class="math">\({\displaystyle n\geq 0}\)&lt;/span>且是偶数，则不等式对任意实数&lt;span class="math">\(x\)&lt;/span>成立。&lt;/p>
&lt;p>证明：伯努利不等式可以用数学归纳法证明。&lt;/p>
&lt;h3 id="切比雪夫总和不等式">切比雪夫总和不等式&lt;/h3>
&lt;p>数学上的切比雪夫总和不等式以切比雪夫命名，可用以比较&lt;strong>两组数积的和&lt;/strong>及&lt;strong>两组数的线性和的积&lt;/strong>的大小： &lt;span class="math">\[
若a_{1}\geq a_{2}\geq \cdots \geq a_{n}且b_{1}\geq b_{2}\geq \cdots \geq b_{n}，则：\\
n\sum _{k=1}^{n}a_{k}b_{k}\geq \left(\sum_{k=1}^{n}a_{k}\right)\left(\sum_{k=1}^{n}b_{k}\right)\geq n\sum_{k=1}^{n}a_{k}b_{n+1-k}
\]&lt;/span> 上式同时除以&lt;span class="math">\(n^2\)&lt;/span>也可以写作 &lt;span class="math">\[
{\frac {1}{n}}\sum _{k=1}^{n}a_{k}b_{k}\geq \left({\frac {1}{n}}\sum_{k=1}^{n}a_{k}\right)\left({\frac{1}{n}}\sum_{k=1}^{n}b_{k}\right)\geq {\frac {1}{n}}\sum_{k=1}^{n}a_{k}b_{n+1-k}
\]&lt;/span> 它是由排序不等式而来。&lt;/p>
&lt;p>&lt;strong>证明&lt;/strong>：设&lt;span class="math">\(a_{1}\geq a_{2}\geq \cdots \geq a_{n}\)&lt;/span>且&lt;span class="math">\(b_{1}\geq b_{2}\geq \cdots \geq b_{n}\)&lt;/span>，由排序不等式可知，最大的和为顺序和： &lt;span class="math">\[
{\displaystyle a_{1}b_{1}+\cdots +a_{n}b_{n}}
\]&lt;/span> 于是： &lt;span class="math">\[
{\displaystyle a_{1}b_{1}+\cdots +a_{n}b_{n}=a_{1}b_{1}+a_{2}b_{2}+\cdots +a_{n}b_{n}}\\
{\displaystyle a_{1}b_{1}+\cdots +a_{n}b_{n}\geq a_{1}b_{2}+a_{2}b_{3}+\cdots +a_{n}b_{1}}\\
{\displaystyle a_{1}b_{1}+\cdots +a_{n}b_{n}\geq a_{1}b_{3}+a_{2}b_{4}+\cdots +a_{n}b_{2}}\\
\vdots\\
{\displaystyle a_{1}b_{1}+\cdots +a_{n}b_{n}\geq a_{1}b_{n}+a_{2}b_{1}+\cdots +a_{n}b_{n-1}}
\]&lt;/span> 将这&lt;span class="math">\(n\)&lt;/span>个不等式分边相加，同时对右边进行因式分解，便得到： &lt;span class="math">\[
{\displaystyle n(a_{1}b_{1}+\cdots +a_{n}b_{n})\geq (a_{1}+\cdots +a_{n})(b_{1}+\cdots +b_{n})}
\]&lt;/span> 两边都除以&lt;span class="math">\(n^{2}\)&lt;/span>，就得到切比雪夫综合不等式的第一个不等号： &lt;span class="math">\[
{\displaystyle {\frac {(a_{1}b_{1}+\cdots +a_{n}b_{n})}{n}}\geq {\frac {(a_{1}+\cdots +a_{n})}{n}}\cdot {\frac {(b_{1}+\cdots +b_{n})}{n}}}
\]&lt;/span> 同理，右边的不等号可由最小的和为逆序和推得。&lt;/p>
&lt;h4 id="切比雪夫不等式总和的积分形式">切比雪夫不等式总和的积分形式&lt;/h4>
&lt;p>若&lt;span class="math">\(f\)&lt;/span>和&lt;span class="math">\(g\)&lt;/span>是区间&lt;span class="math">\([0,1]\)&lt;/span>上的可积的实函数，并且两者都是&lt;strong>递增或两者都是递减&lt;/strong>的，则： &lt;span class="math">\[
{\displaystyle \int fg\geq \int f\int g}
\]&lt;/span> 上式可推广到&lt;strong>任意区间&lt;/strong>。&lt;/p>
&lt;h3 id="闵可夫斯基不等式lps中的三角不等式k">闵可夫斯基不等式(&lt;span class="math">\(L^{p}(S)\)&lt;/span>中的三角不等式K)&lt;/h3>
&lt;p>闵可夫斯基不等式（Minkowski inequality）表明&lt;span class="math">\(L^p\)&lt;/span>空间是一个&lt;strong>赋范向量空间&lt;/strong>。设&lt;span class="math">\(S\)&lt;/span>是一个度量空间，&lt;span class="math">\(1\leq p\leq \infty ,f,g\in L^{p}(S)\)&lt;/span>，那么&lt;span class="math">\(f+g\in L^{p}(S)\)&lt;/span>，我们有： &lt;span class="math">\[
\|f+g\|_{p}\leq \|f\|_{p}+\|g\|_{p}
\]&lt;/span> 如果&lt;span class="math">\(1&amp;lt;p&amp;lt;\infty\)&lt;/span>，等号成立当且仅当&lt;span class="math">\(\exists k\geq 0,f=kg\)&lt;/span>，或者&lt;span class="math">\(g=kf\)&lt;/span>.闵可夫斯基不等式是&lt;span class="math">\(L^{p}(S)\)&lt;/span>中的三角不等式。它可以用赫尔德不等式来证明。&lt;/p>
&lt;h2 id="代数">代数&lt;/h2>
&lt;h3 id="排序不等式">排序不等式&lt;/h3>
&lt;p>排序不等式是数学上的一条基础不等式。它可以推导出很多有名的不等式，其内容如下:&lt;/p>
&lt;p>如果 &lt;span class="math">\[
\begin{aligned}
&amp;amp;{\displaystyle x_{1}\leq x_{2}\leq \cdots \leq x_{n}}和{\displaystyle y_{1}\leq y_{2}\leq \cdots \leq y_{n}}是两组实数。而\\
&amp;amp;{\displaystyle x_{\sigma (1)},\ldots ,x_{\sigma (n)}}是{\displaystyle x_{1},\ldots ,x_{n}}的任意一个排列。
\end{aligned}
\]&lt;/span> 排序不等式指出: &lt;span class="math">\[
{\displaystyle x_{1}y_{1}+\cdots +x_{n}y_{n}\geq x_{\sigma (1)}y_{1}+\cdots +x_{\sigma (n)}y_{n}\geq x_{n}y_{1}+\cdots +x_{1}y_{n}}。
\]&lt;/span> 以文字可以说成是&lt;strong>顺序和不小于乱序和，乱序和不小于逆序和&lt;/strong>。这个不等式有一点强大在于，完全和符号无关。&lt;/p>
&lt;p>排序不等式可以用数学归纳法证明。关键在于下列结果：&lt;/p>
&lt;p>若&lt;span class="math">\(x_{i}\leq x_{j},\,y_{i}\leq y_{j}\)&lt;/span>，则有 &lt;span class="math">\[
{\displaystyle (x_{j}-x_{i})(y_{j}-y_{i})\geq 0}
\]&lt;/span> 移项得出 &lt;span class="math">\[
{\displaystyle x_{i}y_{i}+x_{j}y_{j}\geq x_{j}y_{i}+x_{i}y_{j}}。
\]&lt;/span> 重复以上步骤便可得出排序不等式。&lt;/p>
&lt;h3 id="杨氏不等式">杨氏不等式&lt;/h3>
&lt;p>杨氏不等式，指出：假设&lt;span class="math">\(a, b, p 和q\)&lt;/span>是正实数 ，且有&lt;span class="math">\(\frac{1}{p}+\frac{1}{q} = 1\)&lt;/span>，那么： &lt;span class="math">\[
{\displaystyle ab\leq {\frac {a^{p}}{p}}+{\frac {b^{q}}{q}}.}
\]&lt;/span> 等号成立当且仅当&lt;span class="math">\({\displaystyle a^{p}=b^{q}}\)&lt;/span>，因为这时 &lt;span class="math">\[{\displaystyle ab=a(b^{q})^{1 \over q}=aa^{p \over q}=a^{p}={a^{p} \over p}+{b^{q} \over q}}\]&lt;/span> 杨氏不等式是&lt;strong>加权算术－几何平均值不等式的特例&lt;/strong>，杨氏不等式是证明赫尔德不等式的一个快捷方法。该不等式以威廉·亨利·杨命名。&lt;/p>
&lt;p>&lt;strong>证明&lt;/strong>：我们知道函数&lt;span class="math">\({\displaystyle f(x)=e^{x}}\)&lt;/span>是一个凸函数， 因为它的二阶导数恒为正。 从而我们有： &lt;span class="math">\[{\displaystyle ab=e^{\ln(a)}e^{\ln(b)}=e^{{1 \over p}\ln(a^{p})+{1 \over q}\ln(b^{q})}\leq {1 \over p}e^{\ln(a^{p})}+{1 \over q}e^{\ln(b^{q})}={a^{p} \over p}+{b^{q} \over q}}\]&lt;/span> 这里我们使用了凸函数的一个性质：对任意&lt;span class="math">\(t\)&lt;/span>，若&lt;span class="math">\(0 &amp;lt; t &amp;lt;1\)&lt;/span>，则有： &lt;span class="math">\[{\displaystyle f(tx+(1-t)y)\leq tf(x)+(1-t)f(y)}\]&lt;/span>&lt;/p>
&lt;h3 id="柯西不等式">柯西不等式&lt;/h3>
&lt;p>柯西-施瓦茨不等式，又称施瓦茨不等式或柯西-布尼亚科夫斯基-施瓦茨不等式，是一条很多场合都用得上的不等式；例如线性代数的矢量，数学分析的无穷级数和乘积的积分，和概率论的方差和协方差。它被认为是最重要的数学不等式之一。它有一些推广，如赫尔德不等式。&lt;/p>
&lt;p>柯西-施瓦茨不等式叙述，对于一个内积空间所有向量&lt;span class="math">\(x\)&lt;/span>和&lt;span class="math">\(y\)&lt;/span>， &lt;span class="math">\[
\big| \langle x,y\rangle \big|^2 \leq \langle x,x\rangle \cdot \langle y,y\rangle\]&lt;/span> 其中&lt;span class="math">\(\langle\cdot,\cdot\rangle\)&lt;/span>表示内积，也叫点积。等价地，将两边开方，引用向量的范数，不等式可写为 &lt;span class="math">\[
|\langle x,y\rangle| \leq \|x\| \cdot \|y\|.\,\]&lt;/span> 另外，等式成立当且仅当&lt;span class="math">\(x\)&lt;/span>和&lt;span class="math">\(y\)&lt;/span>线性相关。&lt;/p>
&lt;p>柯西不等式有大量的特例形式与推广，这里不一一列举。&lt;/p>
&lt;h3 id="内斯比特不等式">内斯比特不等式&lt;/h3>
&lt;p>内斯比特不等式是数学的一条不等式，它说对任何正实数&lt;span class="math">\(a,b,c\)&lt;/span>，都有： &lt;span class="math">\[
{\displaystyle {\frac {a}{b+c}}+{\frac {b}{a+c}}+{\frac {c}{a+b}}\geq {\frac {3}{2}}.}
\]&lt;/span>&lt;/p>
&lt;h3 id="平均数不等式">平均数不等式&lt;/h3>
&lt;p>平均数不等式，或称平均值不等式、均值不等式，是数学上的一组不等式，也是基本不等式的推广。它是说：&lt;/p>
&lt;p>如果&lt;span class="math">\(x_{1},x_{2},\ldots ,x_{n}\)&lt;/span>是正数，则 &lt;span class="math">\[
H_n \le G_n \le A_n \le Q_n
\]&lt;/span> 当且仅当&lt;span class="math">\(x_{1}=x_{2}=\cdots =x_{n}\)&lt;/span>，等号成立。&lt;/p>
&lt;p>用文字描述即对这些正数：调和平均数 ≤ 几何平均数 ≤ 算术平均数 ≤ 平方平均数（方均根）&lt;/p>
&lt;h3 id="马勒不等式">马勒不等式&lt;/h3>
&lt;p>马勒不等式陈述说由两个无穷正项序列的对应项的和构成序列的几何均值大于或等于这两个无穷序列几何均值的和: &lt;span class="math">\[
{\displaystyle \prod _{k=1}^{n}(x_{k}+y_{k})^{1/n}\geq \prod_{k=1}^{n}x_{k}^{1/n}+\prod_{k=1}^{n}y_{k}^{1/n}}
\]&lt;/span> 其中, 对任何的&lt;span class="math">\(k, x_k, y_k &amp;gt; 0\)&lt;/span>.&lt;/p>
&lt;p>&lt;strong>证明&lt;/strong>：由均值不等式, 有: &lt;span class="math">\[
{\displaystyle \prod _{k=1}^{n}\left({x_{k} \over x_{k}+y_{k}}\right)^{1/n}\leq {1 \over n}\sum _{k=1}^{n}{x_{k} \over x_{k}+y_{k}},}
\]&lt;/span> 和 &lt;span class="math">\[
{\displaystyle \prod _{k=1}^{n}\left({y_{k} \over x_{k}+y_{k}}\right)^{1/n}\leq {1 \over n}\sum _{k=1}^{n}{y_{k} \over x_{k}+y_{k}}.}\]&lt;/span> 因此, &lt;span class="math">\[
{\displaystyle \prod _{k=1}^{n}\left({x_{k} \over x_{k}+y_{k}}\right)^{1/n}+\prod _{k=1}^{n}\left({y_{k} \over x_{k}+y_{k}}\right)^{1/n}\leq {1 \over n}n=1.}
\]&lt;/span> 两边同乘以分母项后即得结论.&lt;/p>
&lt;h3 id="线性矩阵不等式">线性矩阵不等式&lt;/h3>
&lt;p>线性矩阵不等式是凸优化中，具有形式： &lt;span class="math">\[
{\displaystyle \operatorname {LMI} (y):=A_{0}+y_{1}A_{1}+y_{2}A_{2}+\cdots +y_{m}A_{m}\geq 0\,}
\]&lt;/span> 的表达式, 其中,&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\({\displaystyle y=[y_{i}\,,~i\!=\!1,\dots ,m]}\)&lt;/span>是一个实向量&lt;/li>
&lt;li>&lt;span class="math">\({\displaystyle A_{0},A_{1},A_{2},\dots ,A_{m}}\)&lt;/span>是 &lt;span class="math">\(n\times n\)&lt;/span>的实对称矩阵&lt;span class="math">\({\mathbb {S}}^{n}\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(B\geq 0\)&lt;/span>是广义的不等式，意思是在&lt;span class="math">\(\mathbb S^{n}\)&lt;/span>的半正定子空间&lt;span class="math">\(\mathbb {S}_{+}\)&lt;/span>内，&lt;span class="math">\(B\)&lt;/span>是半正定矩阵。&lt;/li>
&lt;/ul>
&lt;p>线性矩阵不等式表示&lt;span class="math">\(y\)&lt;/span>的凸集限制条件。&lt;/p>
&lt;h2 id="几何">几何&lt;/h2>
&lt;h3 id="三角不等式">三角不等式&lt;/h3>
&lt;p>三角不等式是数学上的一个不等式，表示从A到B再到C的距离永不少于从A到C的距离；亦可以说是两项独立物件的量之和不少于其和的量。&lt;/p>
&lt;ul>
&lt;li>在三角形ABC中，这个式子用标量可以写作&lt;span class="math">\({\displaystyle {\overline {AB}}+{\overline {BC}}\geq {\overline {AC}}}\)&lt;/span>。用向量的写法，这个不等式可以写成：&lt;span class="math">\({\displaystyle \left|{\overrightarrow {AC}}\right|\leq \left|{\overrightarrow {AB}}\right|+\left|{\overrightarrow {BC}}\right|}\)&lt;/span>&lt;/li>
&lt;li>在实数中，此式依然成立：&lt;span class="math">\(\left|a+b\right|\leq \left|a\right|+\left|b\right|\)&lt;/span>。&lt;/li>
&lt;li>在&lt;span class="math">\({\displaystyle L^{p}(S)}\)&lt;/span>中是闵科夫斯基不等式。&lt;/li>
&lt;/ul>
&lt;h3 id="外森比克不等式">外森比克不等式&lt;/h3>
&lt;p>外森比克不等式（Weitzenböck's inequality）是有关三角形边长和面积的一个不等式。设三角形的边长为&lt;span class="math">\(a,b,c\)&lt;/span>，面积为&lt;span class="math">\(A\)&lt;/span>，则外森比克不等式声称 &lt;span class="math">\[a^2+b^2+c^2 \ge 4 \sqrt{3} A\]&lt;/span> 成立。当且仅当三角形为等边三角形，等号成立。佩多不等式是外森比克不等式的推广。&lt;/p>
&lt;h3 id="阿达马不等式">阿达马不等式&lt;/h3>
&lt;p>数学中的阿达马不等式从上限制了n维欧几里得空间中，由&lt;span class="math">\(n\)&lt;/span>支向量&lt;span class="math">\({\mathbf {v}}_{1}, {\mathbf {v}}_{2}, \ldots \mathbf {v} _{n}\)&lt;/span>标出的体积。&lt;/p>
&lt;p>这不等式的几何意义是当&lt;strong>向量为正交集时体积最大&lt;/strong>。&lt;/p>
&lt;h2 id="概率与统计">概率与统计&lt;/h2>
&lt;h3 id="集中不等式">集中不等式&lt;/h3>
&lt;p>集中不等式是数学中的一类不等式，描述了一个随机变量是否集中在某个取值附近。&lt;em>例如大数定律说明了一系列独立同分布随机变量的平均值在概率上趋近于它们的数学期望，这表示随着变量数目增大，平均值会集中在数学期望附近&lt;/em>。&lt;/p>
&lt;h4 id="马尔可夫不等式">马尔可夫不等式&lt;/h4>
&lt;p>在概率论中，马尔可夫不等式给出了随机变量的函数大于等于某正数的概率的上界。马尔可夫不等式把概率关联到数学期望，给出了随机变量的累积分布函数一个&lt;strong>宽泛但仍有用的界&lt;/strong>，例如不超过1/5的人口会有超过5倍于人均收入的收入。&lt;/p>
&lt;p>设&lt;span class="math">\(X\)&lt;/span>是一个非负随机变量，&lt;span class="math">\(\varepsilon&amp;gt;0\)&lt;/span>为正实数，则有 &lt;span class="math">\[
P(X&amp;gt;\varepsilon) \leq \frac{E(X)}{\varepsilon}
\]&lt;/span> &lt;strong>证明&lt;/strong>： &lt;span class="math">\[{\begin{aligned}
{\textrm {E}}(X)&amp;amp;=\int_{-\infty }^{\infty }xf(x)dx\\
&amp;amp;=\int_{0}^{\infty }xf(x)dx\\
&amp;amp;\geq \int_{\varepsilon}^{\infty }xf(x)dx\\
&amp;amp;\because x &amp;gt; \varepsilon, \forall x \in (\varepsilon,\infty)\\
&amp;amp;\geq \int_{\varepsilon}^{\infty }\varepsilon f(x)dx\\
&amp;amp;=\varepsilon\int _{\varepsilon}^{\infty }f(x)dx\\
&amp;amp;=\varepsilon{\textrm {P}}(X\geq \varepsilon).
\end{aligned}}\]&lt;/span> 离散情形下证明类似。&lt;/p>
&lt;p>对于单调增加函数的扩展版本:&lt;/p>
&lt;p>若&lt;span class="math">\(φ\)&lt;/span>是定义在非负实数上的单调增加函数，且其值非负，&lt;span class="math">\(X\)&lt;/span>是一个随机变量，&lt;span class="math">\(\varepsilon ≥ 0，且φ(\varepsilon) &amp;gt; 0\)&lt;/span>，则 &lt;span class="math">\[
{\displaystyle \mathbb {P} (|X|\geq \varepsilon)\leq {\frac {\mathbb {E} (\varphi (|X|))}{\varphi (\varepsilon)}}}
\]&lt;/span>&lt;/p>
&lt;h4 id="切比雪夫不等式">切比雪夫不等式&lt;/h4>
&lt;p>切比雪夫不等式（英语：Chebyshev's Inequality）显示了随机变量的“几乎所有”值都会“接近”平均。&lt;/p>
&lt;p>切比雪夫不等式对&lt;strong>任何分布形状的数据都适用&lt;/strong>，但是需要随机变量的期望、方差都存在。可表示为：对于任意&lt;span class="math">\(b&amp;gt;0\)&lt;/span>，有： &lt;span class="math">\[
{\displaystyle P(|X-E(X)|\geq \varepsilon)\leq {\frac {Var(X)}{\varepsilon^{2}}}}
\]&lt;/span> 反过来写就是 &lt;span class="math">\[
{\displaystyle P(|X-E(X)|\leq \varepsilon)\geq 1-{\frac {Var(X)}{\varepsilon^{2}}}}
\]&lt;/span>&lt;/p>
&lt;p>在概率论中，还有一种等价表述：&lt;/p>
&lt;p>设&lt;span class="math">\(X\)&lt;/span>为随机变量，期望值为&lt;span class="math">\(\mu\)&lt;/span>，标准差为&lt;span class="math">\(\sigma\)&lt;/span>。对于任何实数&lt;span class="math">\(k&amp;gt;0\)&lt;/span>， &lt;span class="math">\[
P(\left|X-\mu \right|\geq k\sigma )\leq {\frac {1}{k^{2}}}.
\]&lt;/span>&lt;/p>
&lt;p>证明：可从马尔可夫不等式直接证明：马氏不等式说明对任意随机变量&lt;span class="math">\(Y\)&lt;/span>和正数&lt;span class="math">\(a\)&lt;/span>有&lt;span class="math">\(\Pr(|Y|&amp;gt;a)\leq \operatorname {E}(|Y|)/a\)&lt;/span>。取&lt;span class="math">\(Y=(X-\mu )^{2}\)&lt;/span>及&lt;span class="math">\(a=(k\sigma )^{2}\)&lt;/span>可证。&lt;/p>
&lt;h4 id="霍夫丁不等式">霍夫丁不等式&lt;/h4>
&lt;p>霍夫丁不等式适用于&lt;strong>有界的随机变量&lt;/strong>。设有两两独立的一系列随机变量&lt;span class="math">\(X_{1},\dots ,X_{n}\)&lt;/span>。假设对所有的&lt;span class="math">\(1 \leq i\leq n\)&lt;/span>，&lt;span class="math">\(X_{i}\)&lt;/span>都是几乎有界的变量，即满足： &lt;span class="math">\[
{\displaystyle \mathbb {P} (X_{i}\in [a_{i},b_{i}])=1.}
\]&lt;/span> 那么这&lt;span class="math">\(n\)&lt;/span>个随机变量的经验期望： &lt;span class="math">\[
{\overline {X}}={\frac {X_{1}+\cdots +X_{n}}{n}}
\]&lt;/span> 满足以下的不等式: &lt;span class="math">\[
{\displaystyle \mathbb {P} ({\overline {X}}-\mathbb {E} [{\overline {X}}]\geq t)\leq \exp \left(-{\frac {2t^{2}n^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),}
\]&lt;/span> &lt;span class="math">\[
{绝对值：\displaystyle \mathbb {P} (|{\overline {X}}-\mathbb {E} [{\overline {X}}]|\geq t)\leq 2\exp \left(-{\frac {2t^{2}n^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),}
\]&lt;/span>&lt;/p>
&lt;h4 id="efronstein不等式">Efron–Stein不等式&lt;/h4>
&lt;p>Efron–Stein不等式给出了随机变量&lt;strong>方差的一个上限估计&lt;/strong>。设有两两独立的随机变量&lt;span class="math">\(X_{1}\dots X_{n}\)&lt;/span>和&lt;span class="math">\(X_{1}&amp;#39;\dots X_{n}&amp;#39;\)&lt;/span>，并且对所有的&lt;span class="math">\(i\)&lt;/span>，&lt;span class="math">\(X_{i}&amp;#39;\)&lt;/span>与&lt;span class="math">\(X_{i}\)&lt;/span>有着相同的分布。那么令 &lt;span class="math">\[{\displaystyle X=(X_{1},\dots ,X_{n}),X^{(i)}=(X_{1},\dots ,X_{i-1},X_{i}&amp;#39;,X_{i+1},\dots ,X_{n})}\]&lt;/span> 则有 &lt;span class="math">\[\mathrm {Var} (f(X))\leq {\frac {1}{2}}\sum_{i=1}^{n}E[(f(X)-f(X^{(i)}))^{2}].
\]&lt;/span>&lt;/p>
&lt;h3 id="布尔不等式">布尔不等式&lt;/h3>
&lt;p>布尔不等式（Boole's inequality），由乔治·布尔提出，指对于全部事件的概率不大于单个事件的概率总和。这是一个很平凡的不等式，但是很长时间内没有给出严格证明。&lt;/p>
&lt;p>对于事件&lt;span class="math">\(A_1、A_2、A_3、......\)&lt;/span>： &lt;span class="math">\[P(\bigcup_{{i}}A_{i})\leq \sum_{i}P(A_{i})\]&lt;/span> 在测度论上，布尔不等式满足&lt;span class="math">\(σ\)&lt;/span>次可加性。&lt;/p>
&lt;h3 id="琴生jensen不等式凸函数相关">琴生(Jensen)不等式(凸函数相关)&lt;/h3>
&lt;p>琴生不等式（Jensen's inequality）给出积分的&lt;strong>凸函数值和凸函数的积分值间&lt;/strong>的关系。琴生不等式有以下推论：过一个凸函数上任意两点所作割线一定在这两点间的函数图象的上方，即： &lt;span class="math">\[tf(x_{1})+(1-t)f(x_{2})\geq f\left(tx_{1}+(1-t)x_{2}\right),0\leq t\leq 1.\]&lt;/span>&lt;/p>
&lt;p>琴生不等式可以用测度论或概率论的语言给出。这两种方式都表明同一个很一般的结果。&lt;/p>
&lt;h4 id="测度论的版本">测度论的版本&lt;/h4>
&lt;p>假设&lt;span class="math">\(\mu\)&lt;/span>是集合&lt;span class="math">\(\Omega\)&lt;/span>的正测度，使得&lt;span class="math">\(\mu (\Omega )=1\)&lt;/span>。若&lt;span class="math">\(g\)&lt;/span>是勒贝格可积的实值函数，而&lt;span class="math">\(\varphi\)&lt;/span>是在&lt;span class="math">\(g\)&lt;/span>的值域上定义的凸函数，则 &lt;span class="math">\[
\varphi \left(\int_{{\Omega}}g\,d\mu \right)\leq \int_{\Omega }\varphi \circ g\,d\mu
\]&lt;/span>&lt;/p>
&lt;h4 id="概率论的版本">概率论的版本&lt;/h4>
&lt;p>以概率论的名词，&lt;span class="math">\(\mu\)&lt;/span>是个概率测度。函数&lt;span class="math">\(g\)&lt;/span>换作实值随机变数&lt;span class="math">\(X\)&lt;/span>（就纯数学而言，两者没有分别）。在&lt;span class="math">\(\Omega\)&lt;/span>空间上，任何函数相对于概率测度&lt;span class="math">\(\mu\)&lt;/span>的积分就成了期望值。这不等式就说，若&lt;span class="math">\(\varphi\)&lt;/span>是任一凸函数，则 &lt;span class="math">\[
\varphi \left(E(X)\right)\leq E(\varphi (X))
\]&lt;/span>&lt;/p>
&lt;h3 id="赫尔德holder不等式">赫尔德(holder)不等式&lt;/h3>
&lt;p>赫尔德不等式是数学分析的一条不等式，取名自奥托·赫尔德(Otto Hölder)。这是一条揭示&lt;span class="math">\(L^p\)&lt;/span>空间的相互关系的基本不等式：&lt;/p>
&lt;p>设&lt;span class="math">\(S\)&lt;/span>为测度空间，&lt;span class="math">\(1\leq p,q\leq \infty\)&lt;/span>，及&lt;span class="math">\({1 \over p}+{1 \over q}=1\)&lt;/span>，设&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(L^{p}(S)\)&lt;/span>内，&lt;span class="math">\(g\)&lt;/span>在&lt;span class="math">\(L^{q}(S)\)&lt;/span>内。则&lt;span class="math">\(f g\)&lt;/span>在&lt;span class="math">\(L^{1}(S)\)&lt;/span>内，且有 &lt;span class="math">\[
\|fg\|_{1}\leq \|f\|_{p}\|g\|_{q}。
\]&lt;/span> 若&lt;span class="math">\(S\)&lt;/span>取作&lt;span class="math">\(\{1,...,n\}\)&lt;/span>附计数测度，便得赫尔德不等式的特殊情形：对所有实数（或复数）&lt;span class="math">\(x_{1},...,x_{n};y_{1},...,y_{n}\)&lt;/span>，有 &lt;span class="math">\[
\sum_{{k=1}}^{n}|x_{k}y_{k}|\leq \left(\sum_{{k=1}}^{n}|x_{k}|^{p}\right)^{{1/p}}\left(\sum_{{k=1}}^{n}|y_{k}|^{q}\right)^{{1/q}}。
\]&lt;/span> 我们称p和q互为赫尔德共轭。(感觉会用于范数的证明)。&lt;/p>
&lt;p>当&lt;span class="math">\(p = q = 2\)&lt;/span>，便得到柯西-施瓦茨不等式。赫尔德不等式可以证明&lt;span class="math">\(L^p\)&lt;/span>空间上一般化的三角不等式，闵可夫斯基不等式。&lt;/p>
&lt;p>证明：一般通过杨氏不等式证明。&lt;/p>
&lt;h2 id="信息论">信息论&lt;/h2>
&lt;h3 id="吉布斯不等式相对熵">吉布斯不等式(相对熵)&lt;/h3>
&lt;p>若&lt;span class="math">\(\sum _{i=1}^{n}p_{i}=\sum _{i=1}^{n}q_{i}=1\)&lt;/span>，且&lt;span class="math">\(p_{i},q_{i}\in (0,1]\)&lt;/span>，则有： &lt;span class="math">\[
-\sum _{i=1}^{n}p_{i}\log p_{i}\leq -\sum_{i=1}^{n}p_{i}\log q_{i}
\]&lt;/span> 等号成立当且仅当&lt;span class="math">\(\forall i, p_{i}=q_{i}\)&lt;/span>。&lt;/p>
&lt;p>吉布斯不等式等价于： &lt;span class="math">\[
0\leq -\sum_{i=1}^{n}p_{i}\log q_{i}-(-\sum_{i=1}^{n}p_{i}\log p_{i})=\sum _{i=1}^{n}p_{i}\log(p_{i}/q_{i})=D_{\mathrm {KL} }(P\|Q)
\]&lt;/span> 文字解释就是：&lt;span class="math">\(0 \leq 交叉熵-熵 = 相对熵\)&lt;/span>。&lt;/p>
&lt;h3 id="克拉夫特不等式">克拉夫特不等式&lt;/h3>
&lt;p>在编码理论，克拉夫特不等式给出了一个码字长度集合存在唯一可解编码/单义可译码（uniquely decodable code）的必要条件。因为这个不等式在前缀码和树上面应用很多，所以在计算机科学和信息学中很常用。&lt;/p>
&lt;p>克拉夫特不等式对码字限制长度以保证前缀编码的可能性。这个不等式说明码字长度指数的倒数的分布和概率质量函数很相似。&lt;/p>
&lt;ul>
&lt;li>如果克拉夫特不等式中严格成立，相应的编码有冗余（redundancy）。&lt;/li>
&lt;li>如果克拉夫特不等式中等式成立，相应的编码被称作complete code。&lt;/li>
&lt;li>如果克拉夫特不等式不成立，相应的编码不是唯一可解编码（uniquely decipherable）。&lt;/li>
&lt;/ul>
&lt;p>设符号表中的原始符号为&lt;span class="math">\(S=\{\,s_{1},s_{2},\ldots ,s_{n}\,\}\)&lt;/span> 在大小为&lt;span class="math">\(r\)&lt;/span>的字符集上编码为唯一可解编码的码字长度为 &lt;span class="math">\[\ell_{1},\ell_{2},\ldots ,\ell_{n}.\]&lt;/span> 则 &lt;span class="math">\[
{\displaystyle \sum _{i=1}^{n}r^{-\ell_{i}}\leqslant 1.}
\]&lt;/span> 反之, 给定一个满足上述不等式的自然数集合&lt;span class="math">\(\ell_{1},\ell_{2},\ldots ,\ell_{n}\)&lt;/span>, 则在大小为&lt;span class="math">\(r\)&lt;/span>字符集上，存在一组唯一可解编码符合相应的码字长度。&lt;/p></description></item><item><title>数学-切比雪夫相关</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6-%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E7%9B%B8%E5%85%B3/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6-%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E7%9B%B8%E5%85%B3/</guid><description>
&lt;h2 id="切比雪夫相关概念">切比雪夫相关概念&lt;!-- omit in toc -->&lt;/h2>
&lt;p>巴夫尼提·列波维奇·切比雪夫（俄语：Пафну́тий Льво́вич Чебышёв ，1821年5月26日－1894年12月8日），俄罗斯数学家。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#概率论">概率论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫不等式">切比雪夫不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#数值分析">数值分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫多项式">切比雪夫多项式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫节点">切比雪夫节点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫总和不等式">切比雪夫总和不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫方程">切比雪夫方程&lt;/a>&lt;/li>
&lt;li>&lt;a href="#度量几何">度量几何&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫距离">切比雪夫距离&lt;/a>&lt;/li>
&lt;li>&lt;a href="#数论质数相关">数论(质数相关)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫定理">切比雪夫定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切比雪夫函数">切比雪夫函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录">附录&lt;/a>&lt;/li>
&lt;li>&lt;a href="#棣莫弗定理">棣莫弗定理&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概率论">概率论&lt;/h2>
&lt;h3 id="切比雪夫不等式">切比雪夫不等式&lt;/h3>
&lt;p>在概率论中，切比雪夫不等式（英语：Chebyshev's Inequality）显示了随机变量的“几乎所有”值都会“接近”平均切比雪夫不等式，对&lt;strong>任何分布形状的数据都适用&lt;/strong>。可表示为：对于任意&lt;span class="math">\(b&amp;gt;0\)&lt;/span>，有： &lt;span class="math">\[P(|X-E(X)|\geqslant b)\leq {\frac {Var(X)}{b^{2}}}\]&lt;/span>&lt;/p>
&lt;h2 id="数值分析">数值分析&lt;/h2>
&lt;h3 id="切比雪夫多项式">切比雪夫多项式&lt;/h3>
&lt;p>切比雪夫多项式是与棣莫弗定理有关，以递归方式定义的一系列&lt;strong>正交多项式&lt;/strong>序列。通常，第一类切比雪夫多项式以符号&lt;span class="math">\(T_n\)&lt;/span>表示，第二类切比雪夫多项式用&lt;span class="math">\(U_n\)&lt;/span>表示。切比雪夫多项式&lt;span class="math">\(T_n\)&lt;/span>或&lt;span class="math">\(U_n\)&lt;/span>代表n阶多项式。&lt;/p>
&lt;p>切比雪夫多项式在逼近理论中有重要的应用。这是因为第一类切比雪夫多项式的根（被称为切比雪夫节点）可以用于多项式插值。相应的插值多项式能最大限度地降低龙格现象，并且提供多项式在连续函数的最佳一致逼近。&lt;/p>
&lt;p>第一类Chebyshev多项式：由递推式 &lt;span class="math">\[T_0(x) = 1,\\ \tag{2.1}
T_1(x) = x, \\
T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x). \\\]&lt;/span> 所确立的一系列多项式称为第一类Chebyshev多项式.&lt;/p>
&lt;p>第二类Chebyshev多项式：由递推式 &lt;span class="math">\[U_0(x) = 1, \\ \tag{2.2}
U_1(x) = 2x, \\
U_{n+1}(x) = 2xU_n(x) - U_{n-1}(x). \\
\]&lt;/span> 所确立的一系列多项式称为第二类Chebyshev多项式.&lt;/p>
&lt;h3 id="切比雪夫节点">切比雪夫节点&lt;/h3>
&lt;p>对于一个插值区间&lt;span class="math">\([a, b]\)&lt;/span> 如果要在这个插值区间上选取&lt;span class="math">\(n\)&lt;/span>个点作为插值基点，使得上面的最大误差最小，则基点的选法如下: &lt;span class="math">\[x_i=\frac{b+a}{2}+\frac{b-a}{2}\cos\frac{(2i-1)\pi}{2n}\\
(i=1,2,\dotsb,n)\]&lt;/span> 这些节点称为切比雪夫（插值）节点。&lt;/p>
&lt;h3 id="切比雪夫总和不等式">切比雪夫总和不等式&lt;/h3>
&lt;p>用以比较两组数积的和及两组数的线性和的积的大小：&lt;/p>
&lt;p>若&lt;span class="math">\(a_{1}\geq a_{2}\geq \cdots \geq a_{n}\)&lt;/span>且&lt;span class="math">\(b_{1}\geq b_{2}\geq \cdots \geq b_{n}\)&lt;/span>，则： &lt;span class="math">\[n\sum _{k=1}^{n}a_{k}b_{k}\geq \left(\sum_{k=1}^{n}a_{k}\right)\left(\sum_{k=1}^{n}b_{k}\right)\geq n\sum_{k=1}^{n}a_{k}b_{n+1-k}.\]&lt;/span> 上式也可以写作 &lt;span class="math">\[{\frac {1}{n}}\sum _{k=1}^{n}a_{k}b_{k}\geq \left({\frac {1}{n}}\sum_{k=1}^{n}a_{k}\right)\left({\frac {1}{n}}\sum_{k=1}^{n}b_{k}\right)\geq {\frac {1}{n}}\sum_{k=1}^{n}a_{k}b_{n+1-k}\]&lt;/span> 它是由排序不等式而来。&lt;/p>
&lt;h3 id="切比雪夫方程">切比雪夫方程&lt;/h3>
&lt;p>切比雪夫方程（英语：Chebyshev equation）是指二阶线性常微分方程: &lt;span class="math">\[(1-x^{2}){d^{2}y \over dx^{2}}-x{dy \over dx}+p^{2}y=0\]&lt;/span> 其中&lt;span class="math">\(p\)&lt;/span>为一实常数。方程的解为幂级数: &lt;span class="math">\[y=\sum _{n=0}^{\infty }a_{n}x^{n}\]&lt;/span> 其中系数可通过以下递推关系式计算： &lt;span class="math">\[a_{n+2}={(n-p)(n+p) \over (n+1)(n+2)}a_{n}.\]&lt;/span> 级数在&lt;span class="math">\(x\in [-1,1]\)&lt;/span>上收敛（对递推关系式应用比值审敛法可得）。递推关系的初值a0与a1可为任意值，由此可得微分方程不同的特解。&lt;/p>
&lt;h2 id="度量几何">度量几何&lt;/h2>
&lt;h3 id="切比雪夫距离">切比雪夫距离&lt;/h3>
&lt;p>数学上，切比雪夫距离（Chebyshev distance）或是&lt;span class="math">\(L_∞\)&lt;/span>度量是向量空间中的一种度量，二个点之间的距离定义&lt;strong>为其各座标数值差的最大值&lt;/strong>。以&lt;span class="math">\((x_1,y_1)和(x_2,y_2)\)&lt;/span>二点为例，其切比雪夫距离为&lt;span class="math">\(max(|x_2-x_1|,|y_2-y_1|)\)&lt;/span>。&lt;/p>
&lt;p>若将国际象棋棋盘放在二维直角座标系中，格子的边长定义为1，座标的x轴及y轴和棋盘方格平行，原点恰落在某一格的中心点，则王从一个位置走到其他位置需要的步数恰为二个位置的切比雪夫距离，因此切比雪夫距离也称为&lt;strong>棋盘距离&lt;/strong>。&lt;/p>
&lt;p>范数定义：若二个向量或二个点&lt;span class="math">\(p\quad q\)&lt;/span>，其座标分别为&lt;span class="math">\(p_{i}\)&lt;/span>及&lt;span class="math">\(q_i\)&lt;/span>，则两者之间的切比雪夫距离定义如下： &lt;span class="math">\[D_{\rm {Chebyshev}}(p,q):=\max_{i}(|p_{i}-q_{i}|).\]&lt;/span> 这也等于以下&lt;span class="math">\(L_p\)&lt;/span>度量的极值： &lt;span class="math">\[\lim_{k\to \infty }{\bigg (}\sum_{i=1}^{n}\left|p_{i}-q_{i}\right|^{k}{\bigg )}^{1/k},\]&lt;/span> 因此切比雪夫距离也称为&lt;span class="math">\(L_∞\)&lt;/span>度量。&lt;/p>
&lt;h2 id="数论质数相关">数论(质数相关)&lt;/h2>
&lt;h3 id="切比雪夫定理">切比雪夫定理&lt;/h3>
&lt;p>对于&lt;span class="math">\(n&amp;gt;3\)&lt;/span>，n和2n之间至少有一个素数。&lt;/p>
&lt;h3 id="切比雪夫函数">切比雪夫函数&lt;/h3>
&lt;p>切比雪夫函数(Chebyshev function)重要的数论函数之一。它是切比雪夫为了证明素数定理而给出的。使函数&lt;span class="math">\(Ψ(x)\)&lt;/span>与对数函数建立了简单的联系，从而为证明素数定理和研究素数分布奠定了基础。&lt;/p>
&lt;h2 id="附录">附录&lt;/h2>
&lt;h3 id="棣莫弗定理">棣莫弗定理&lt;/h3>
&lt;p>棣莫弗公式是一个关于复数和三角函数的公式，。其内容为对任意复数x和整数n，下列性质成立： &lt;span class="math">\[(\cos(x)+i\sin(x))^{n}=\cos(nx)+i\sin(nx)\]&lt;/span> 为了方便起见，我们常常将&lt;span class="math">\(cos(x) + i sin(x)\)&lt;/span>合并为另一个三角函数&lt;span class="math">\(cis(x)\)&lt;/span>，也就是说： &lt;span class="math">\[\operatorname {cis} ^{n}(x)=\operatorname {cis} (nx)\]&lt;/span> 证明：欧拉公式。&lt;/p>
&lt;p>此定理可用来求单位复数的&lt;span class="math">\(n\)&lt;/span>次方根。设&lt;span class="math">\(|z|=1\)&lt;/span>，表为 &lt;span class="math">\[z=\cos(\theta)+i\sin(\theta)\]&lt;/span> 若&lt;span class="math">\(w^{n}=z\)&lt;/span>，按照棣莫弗公式： &lt;span class="math">\[w^{n}=(\cos \phi +i\sin \phi )^{n}=\cos n\phi +i\sin n\phi =\cos \theta +i\sin \theta =z\]&lt;/span> 于是得到 &lt;span class="math">\[n\phi =\theta +2k\pi （其中k\in \mathbb{Z} )\]&lt;/span> 也就是： &lt;span class="math">\[\phi =\frac{\theta+2k\pi}{n}\]&lt;/span> 当&lt;span class="math">\(k\)&lt;/span>取&lt;span class="math">\(0,1,\ldots ,n-1\)&lt;/span>，我们得到&lt;span class="math">\(n\)&lt;/span>个不同的根： &lt;span class="math">\[w=\cos({\dfrac {\theta +2k\pi }{n}})+i\sin({\dfrac {\theta +2k\pi }{n}}),k=0,1,\ldots ,n-1\]&lt;/span>&lt;/p></description></item><item><title>数学分析之投影内积和傅里叶级数</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E5%86%85%E7%A7%AF%E5%92%8C%E5%82%85%E9%87%8C%E5%8F%B6%E7%BA%A7%E6%95%B0/</link><pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E5%86%85%E7%A7%AF%E5%92%8C%E5%82%85%E9%87%8C%E5%8F%B6%E7%BA%A7%E6%95%B0/</guid><description>
&lt;h2 id="数学分析之投影内积和傅里叶级数">数学分析之投影内积和傅里叶级数&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#内积点积传统定义">内积（点积）传统定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#性质">性质&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#基本运算规则">基本运算规则&lt;/a>&lt;/li>
&lt;li>&lt;a href="#空间关系">空间关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#内积与投影">内积与投影&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#内积的推广">内积的推广&lt;/a>&lt;/li>
&lt;li>&lt;a href="#复数的内积">复数的内积&lt;/a>&lt;/li>
&lt;li>&lt;a href="#矩阵的内积">矩阵的内积&lt;/a>&lt;/li>
&lt;li>&lt;a href="#函数的内积">函数的内积&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从内积角度理解傅里叶级数">从内积角度理解傅里叶级数&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>点积是线性代数、矩阵论中常用的概念，我们最开始学习的点积就是向量空间中的点积，但是内积在其他领域比如复数、矩阵、函数中也是有定义的，本笔记将介绍内积及其拓展。&lt;/p>
&lt;h2 id="内积点积传统定义">内积（点积）传统定义&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>点积（Dot Product）又称数量积或标量积&lt;/strong>，是一种接受两个等长的数字序列（通常是坐标向量）、返回单个数字的代数运算。&lt;/p>
&lt;/blockquote>
&lt;p>在欧几里得几何中，两个笛卡尔坐标向量的点积常称为内积。点积的名称源自表示点乘运算的点号（&lt;span class="math">\(a\cdot b\)&lt;/span>），读作&lt;span class="math">\(a\ dot\ b\)&lt;/span>，标量积的叫法则是在强调其运算结果为标量而非向量。&lt;/p>
&lt;p>从代数角度看，先对两个数字序列中的&lt;strong>每组对应元素求积，再对所有积求和&lt;/strong>，结果即为点积。从几何角度看，点积则是&lt;strong>两个向量的长度与它们夹角余弦的积&lt;/strong>。这两种定义在笛卡尔坐标系中等价。&lt;/p>
&lt;p>代数角度，点积表示为： &lt;span class="math">\({\vec{a}}=[a_{1},a_{2},\cdots ,a_{n}]\)&lt;/span>和&lt;span class="math">\({\vec{b}}=[b_{1},b_{2},\cdots ,b_{n}]\)&lt;/span>的点积定义为： &lt;span class="math">\[
\vec{a}\cdot \vec{b} = \sum_{i=1}^n a_ib_i = a_1b_1 + a_2b_2 + \cdots + a_nb_n
\]&lt;/span> 点积还可以写为向量积的形式（注意这里的向量都是行向量，很多文章中默认向量是列向量）： &lt;span class="math">\[{\vec{a}}\cdot {\vec{b}}={\vec{a}}{\vec{b}}^{T}
\]&lt;/span> 几何角度，在欧几里得空间中，点积可以直观地定义为 &lt;span class="math">\[\vec{a} \cdot \vec{b} = |\vec{a}| \, |\vec{b}| \cos \theta \;\]&lt;/span> 这里&lt;span class="math">\(|\vec{x}|\)&lt;/span> 表示&lt;span class="math">\(\vec{x}\)&lt;/span>的模（长度），&lt;span class="math">\(\theta\)&lt;/span>表示两个向量之间的角度。&lt;/p>
&lt;h3 id="性质">性质&lt;/h3>
&lt;h4 id="基本运算规则">基本运算规则&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>满足交换律。&lt;span class="math">\({\vec {a}}\cdot {\vec {b}}={\vec {b}}\cdot {\vec {a}}\)&lt;/span>&lt;/li>
&lt;li>对向量加法满足分配律。&lt;span class="math">\(\vec{a} \cdot (\vec{b} + \vec{c}) = \vec{a} \cdot \vec{b} + \vec{a} \cdot \vec{c}\)&lt;/span>&lt;/li>
&lt;li>点积是双线性算子。&lt;span class="math">\({\vec {a}}\cdot (r{\vec {b}}+{\vec {c}})=r({\vec {a}}\cdot {\vec {b}})+({\vec {a}}\cdot {\vec {c}})\)&lt;/span>&lt;/li>
&lt;li>在乘以标量时满足：&lt;span class="math">\((c_1\vec{a}) \cdot (c_2\vec{b}) = (c_1c_2) (\vec{a} \cdot \vec{b})\)&lt;/span>&lt;/li>
&lt;li>不满足结合律！不满足结合律！不满足结合律！&lt;/li>
&lt;li>正交性：两个非零向量&lt;span class="math">\(\vec{a}\)&lt;/span>和&lt;span class="math">\(\vec{b}\)&lt;/span>是正交的，当且仅当&lt;span class="math">\({\vec{a}}\cdot {\vec{b}}=0\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;h4 id="空间关系">空间关系&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>线性空间+点积=内积空间。&lt;/li>
&lt;li>内积空间必为线性赋范空间，但线性赋范空间不一定是内积空间，因为可以通过点积定义范数，但是单单范数无法引入角度概念，定义不了内积。&lt;/li>
&lt;li>内积空间+完备性=希尔伯特空间&lt;/li>
&lt;/ol>
&lt;h4 id="内积与投影">内积与投影&lt;/h4>
&lt;p>欧氏空间中向量&lt;span class="math">\(\vec A\)&lt;/span>在向量&lt;span class="math">\(\vec B\)&lt;/span>上的标量投影是指 &lt;span class="math">\[
\vec A_B=|\vec A|\cos\theta
\]&lt;/span> 这里&lt;span class="math">\(\theta\)&lt;/span>是&lt;span class="math">\(\vec A\)&lt;/span>和&lt;span class="math">\(\vec B\)&lt;/span>的夹角。从点积的几何定义&lt;span class="math">\(\vec A\cdot\vec B=|\vec{A}||\vec{B}|\cos\theta\)&lt;/span>不难得出，两个向量的点积：&lt;span class="math">\(\vec A\cdot\vec B\)&lt;/span>可以理解为向量&lt;span class="math">\(\vec A\)&lt;/span>在向量&lt;span class="math">\(\vec B\)&lt;/span>上的投影再乘以&lt;span class="math">\(\vec B\)&lt;/span>的长度。&lt;/p>
&lt;p>![内积与投影.gif(/images/内积与投影.gif)&lt;/p>
&lt;h2 id="内积的推广">内积的推广&lt;/h2>
&lt;p>传统的内积定义环境都是实数线性空间，如欧几里得空间，然而内积不仅仅可以定义在实数线性空间上，还可以定义在其他空间，同时满足已有的内积性质。&lt;/p>
&lt;h3 id="复数的内积">复数的内积&lt;/h3>
&lt;p>复数内积与实数内积的区别就是做二者乘法的时候需要加&lt;strong>共轭&lt;/strong>，如果不加共轭，内积的性质就会变得完全不一样： &lt;span class="math">\[
\vec{a} \cdot \vec{b} =\sum _{i}{{a_{i}}\,{\overline {b_{i}}}}
\]&lt;/span> 其中，&lt;span class="math">\({\overline {b_{i}}}\)&lt;/span>表示&lt;span class="math">\(b_i\)&lt;/span>的共轭。写出向量乘积形式为： &lt;span class="math">\[
\vec{a} \cdot \vec{b} =\vec{b} ^{\mathsf {H}}\vec{a}
\]&lt;/span> 相应的，两个复向量的夹角&lt;span class="math">\(\theta\)&lt;/span>也定义为： &lt;span class="math">\[
\cos \theta ={\frac {\operatorname {Re} (\vec{a} \cdot \vec{b} )}{\|\vec{a}\| \|\vec{b}\|}}.
\]&lt;/span>&lt;/p>
&lt;h3 id="矩阵的内积">矩阵的内积&lt;/h3>
&lt;p>矩阵具有弗罗比尼乌斯内积，可以类比于向量的内积。它被定义为两个相同大小的矩阵&lt;span class="math">\(\mathbf {A}\)&lt;/span>和&lt;span class="math">\(\mathbf {B}\)&lt;/span>的对应元素的内积之和。&lt;/p>
&lt;p>复矩阵情况下： &lt;span class="math">\[
\mathbf {A} :\mathbf {B} =\sum_{i}\sum_{j}A_{ij}{\overline {B_{ij}}}=\mathrm {tr} (\mathbf {B} ^{\mathrm {H} }\mathbf {A} )=\mathrm {tr} (\mathbf {A} \mathbf {B} ^{\mathrm {H} })
\]&lt;/span> 实矩阵情况下： &lt;span class="math">\[
\mathbf {A} :\mathbf {B} =\sum_{i}\sum_{j}A_{ij}B_{ij}=\mathrm {tr} (\mathbf {B} ^{\mathrm {T} }\mathbf {A} )=\mathrm {tr} (\mathbf {A} \mathbf {B} ^{\mathrm {T} })=\mathrm {tr} (\mathbf {A} ^{\mathrm {T} }\mathbf {B} )=\mathrm {tr} (\mathbf {B} \mathbf {A} ^{\mathrm {T} })
\]&lt;/span>&lt;/p>
&lt;h3 id="函数的内积">函数的内积&lt;/h3>
&lt;p>函数内积是离散维度往连续维度的延拓。内积的核心过程是&lt;strong>对应元素求积再累加&lt;/strong>。对于可积且平方可积（平方可积是为了&lt;span class="math">\(f(x)=g(x)\)&lt;/span>情形）函数例如&lt;span class="math">\(f(x),g(x)\)&lt;/span>而言，对应元素求积好办，只要二者相乘&lt;span class="math">\(f(x)g(x)\)&lt;/span>就行，离散场景的求和进入到连续场景就是&lt;strong>求积分&lt;/strong>，因此我们可以将函数的内积定义为： &lt;span class="math">\[
&amp;lt;f(x),g(x)&amp;gt;=\int f(x)g(x)\mathrm{d}x
\]&lt;/span> 然而，到这里还没有结束。我们在内积的时累加指的是所有维度的累加，而函数的所有维度是什么呢？是它的定义域&lt;span class="math">\(\mathbf{D}\)&lt;/span>，因此我们还要在上述积分中添加积分域： &lt;span class="math">\[
&amp;lt;f(x),g(x)&amp;gt;=\int_{\mathbf{D}} f(x)g(x)\mathrm{d}x
\]&lt;/span> 注意，这里要求两个函数必须可积且平方可积。容易验证，函数内积的定义也满足传统内积的性质。&lt;/p>
&lt;h2 id="从内积角度理解傅里叶级数">从内积角度理解傅里叶级数&lt;/h2>
&lt;p>我们在笔记&lt;a href="线性代数与矩阵之理解向量、线性变换与矩阵乘法.md">线性代数与矩阵之理解向量、线性变换与矩阵乘法&lt;/a>了解到，空间中任意向量都可以拆解成空间中一组基来表示，如果这组基是标准正交基是最好了。那么，自变量&lt;span class="math">\(x\)&lt;/span>所有函数组成的集合是不是也是一种空间呢？答案是肯定的。&lt;/p>
&lt;p>既然所有的函数组成了一个空间，那么我们是否也能找到一组标准正交基呢？傅里叶用傅里叶级数和傅里叶变换给我们答案。我们可以对任意函数做傅里叶展开，得到表达式： &lt;span class="math">\[
f(x)=a_0+a_1 \cos x+b_1\sin x+a_2 \cos 2x+b_2\sin 2x+\dotsb
\]&lt;/span> 与之前的有限个标准正交向量组成的正交矩阵不同，这个空间是无限维，它的一组基是&lt;span class="math">\(1，\cos x，\sin x，\cos 2x，\sin 2x\dotsb\)&lt;/span>。我们计算基的任意两个分量，可验证其正交性： &lt;span class="math">\[
\int_0^{2\pi}\sin mx\cos nx \mathrm{d}x=0(n\neq m)
\]&lt;/span> 我们知道，在基分量上的系数等于在其上的投影大小，因此采用和标准正交基相似的内积运算可以得到傅里叶变换的参数。我们以&lt;span class="math">\(a_1\)&lt;/span>为例： &lt;span class="math">\[
\begin{aligned}
&amp;lt;f(x),\cos x&amp;gt;&amp;amp;=\int_0^{2\pi} f(x) \cos x\mathrm{d}x\\
&amp;amp;=\int_0^{2\pi} (a_0+a_1 \cos x+b_1\sin x+a_2 \cos 2x+b_2\sin 2x+\dotsb) \cos x\\
&amp;amp;=0+\int_0^{2\pi} a_1 \cos^2 x\mathrm{d}x+0+0+\dotsb\\
&amp;amp;=a_1\pi
\end{aligned}
\]&lt;/span> 可以得到&lt;span class="math">\(a_1=\frac{1}{\pi}&amp;lt;f(x),\cos x&amp;gt;=\frac{1}{\pi}\int_0^{2\pi} f(x) \cos x\mathrm{d}x\)&lt;/span>，同理可以求得其它参数。我们这样就得到了一组傅里叶基下的函数表示形式。&lt;strong>也就是说傅里叶级数的系数是函数在&lt;span class="math">\(\cos nx\)&lt;/span>与&lt;span class="math">\(\sin nx\)&lt;/span>上的投影&lt;/strong>。&lt;/p>
&lt;p>当函数&lt;span class="math">\(f(x)\)&lt;/span>是周期函数时，我们可以采用最小周期与函数相同的三角函数作为基的开始，然后其他的基周期分别为函数周期的&lt;span class="math">\(1/2,1/3,\dotsb,1/n,\dotsb\)&lt;/span>。当函数从周期函数变成非周期函数时，我们可以当成是周期无穷的周期函数，此时三角函数的间隔就会趋于0，函数的基也从离散无穷变成连续无穷，即由傅里叶级数变成傅里叶编号。&lt;/p></description></item><item><title>数学分析之泰勒级数与高阶中值定理</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0%E4%B8%8E%E9%AB%98%E9%98%B6%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86/</link><pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0%E4%B8%8E%E9%AB%98%E9%98%B6%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86/</guid><description>
&lt;h2 id="泰勒级数与高阶中值定理">泰勒级数与高阶中值定理&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#费马引理">费马引理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#费马引理证明">费马引理证明&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一阶微分中值定理">一阶微分中值定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#罗尔rolle中值定理">罗尔（Rolle）中值定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉格朗日langrange中值定理">拉格朗日（Langrange）中值定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#柯西cauchy中值定理">柯西（Cauchy）中值定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泰勒中值定理">泰勒中值定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#泰勒中值定理的一阶展开">泰勒中值定理的一阶展开&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="费马引理">费马引理&lt;/h2>
&lt;p>设函数&lt;span class="math">\(f(x)\)&lt;/span>在点&lt;span class="math">\(x_0\)&lt;/span>的某邻域&lt;span class="math">\(U(x_0)\)&lt;/span>内有定义，并且在&lt;span class="math">\(x_0\)&lt;/span>处可导，如果对任意的&lt;span class="math">\(x\in U(x_0)\)&lt;/span>，有 &lt;span class="math">\[f(x)\le f(x_0)或f(x)\ge f(x_0)\]&lt;/span> 那么&lt;span class="math">\(f^\prime(x_0)=0\)&lt;/span>。&lt;/p>
&lt;p>费马引理的一个推论是，函数&lt;span class="math">\(f\)&lt;/span>在定义域&lt;span class="math">\(A\)&lt;/span>内的最大值和最小值只能在&lt;strong>边界&lt;/strong>上，&lt;strong>不可导的点&lt;/strong>，或&lt;strong>驻点&lt;/strong>取得。&lt;/p>
&lt;h3 id="费马引理证明">费马引理证明&lt;/h3>
&lt;p>假设&lt;span class="math">\(x_0\)&lt;/span>是一个极大值（如果&lt;span class="math">\(x_{0}\)&lt;/span>是极小值，证明亦类似）。那么存在一个&lt;span class="math">\(\delta &amp;gt;0\)&lt;/span>，使得对于所有的&lt;span class="math">\(|x-x_{0}|&amp;lt;\delta\)&lt;/span>，都有&lt;span class="math">\(f(x_{0})\geq f(x)\)&lt;/span>。因此对于任何&lt;span class="math">\(h \in (0,\delta)\)&lt;/span>，有： &lt;span class="math">\[\frac{f(x_0+h) - f(x_0)}{h}\leq 0 \tag{1}\]&lt;/span> 由于当&lt;span class="math">\(h\)&lt;/span>从上方趋于0时，这个比值的极限存在且为 &lt;span class="math">\(f&amp;#39;(x_0)\)&lt;/span>，我们便有&lt;span class="math">\(f&amp;#39;(x_0) \le 0\)&lt;/span>。另一方面，当 &lt;span class="math">\(h \in (-\delta,0)\)&lt;/span>时，我们注意到： &lt;span class="math">\[\frac{f(x_0+h) - f(x_0)}{h} \geq 0\tag{2}\]&lt;/span> 当&lt;span class="math">\(h\)&lt;/span>从下方趋于0时，这个极限存在，且等于 &lt;span class="math">\(f&amp;#39;(x_0)\)&lt;/span>，我们又有&lt;span class="math">\(f&amp;#39;(x_0) \ge 0\)&lt;/span>。&lt;/p>
&lt;p>因此&lt;span class="math">\(f&amp;#39;(x_0) = 0\)&lt;/span>。&lt;/p>
&lt;h2 id="一阶微分中值定理">一阶微分中值定理&lt;/h2>
&lt;h3 id="罗尔rolle中值定理">罗尔（Rolle）中值定理&lt;/h3>
&lt;p>&lt;span class="math">\(y=f(x)\)&lt;/span>满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>在区间[a,b]上连续&lt;/li>
&lt;li>在区间（a，b）内可导&lt;/li>
&lt;li>&lt;span class="math">\(f(a)=f(b)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>&lt;span class="math">\(\Rightarrow\)&lt;/span>在(a,b)内至少存在一点&lt;span class="math">\(\xi\)&lt;/span>，使&lt;span class="math">\(f&amp;#39;(\xi)=0\)&lt;/span>&lt;/p>
&lt;p>罗尔（Rolle）中值定理证明：&lt;/p>
&lt;p>因&lt;span class="math">\(f(x)\)&lt;/span>在[a,b]上连续，故在[a,b]上取得最大值M和最小值m。&lt;/p>
&lt;p>若&lt;span class="math">\(M=m\)&lt;/span>，则&lt;span class="math">\(f(x)\equiv M,x\in [a,b],\)&lt;/span>，因此&lt;span class="math">\(\forall\xi\in(a,b),f&amp;#39;(\xi)=0\)&lt;/span>&lt;/p>
&lt;p>若&lt;span class="math">\(M&amp;gt;m\)&lt;/span>，则M和m中至少有一个与端点值不相同，不妨设极大值&lt;span class="math">\(M\neq f(a)\)&lt;/span>，则至少存在一点&lt;span class="math">\(\xi\in(a,b)\)&lt;/span>，使&lt;span class="math">\(f(\xi)=M\)&lt;/span>，则费马引理可知&lt;span class="math">\(f&amp;#39;(\xi)=0\)&lt;/span>（只有在驻点才可能取得极值）。&lt;/p>
&lt;p>注意：定理条件不全具备，结论不一定成立。&lt;/p>
（1）函数不连续 &lt;span class="math">\[f(x)=\begin{cases}
x,\quad 0\leq x &amp;lt;1 \\
0,\quad x=1
\end{cases}\]&lt;/span> &lt;img src="../images/f_uncontinus.png" alt="函数不连续" />
&lt;center>
图1 函数不连续
&lt;/center>
(2)函数不可导 &lt;span class="math">\[f(x)=\|x\|,x\in [-1,1]\]&lt;/span> &lt;img src="../images/f_not_deriv.png" alt="函数不可导" />
&lt;center>
图2 函数不可导
&lt;/center>
&lt;h3 id="拉格朗日langrange中值定理">拉格朗日（Langrange）中值定理&lt;/h3>
&lt;p>&lt;span class="math">\(y=f(x)\)&lt;/span>满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>在区间[a,b]上连续&lt;/li>
&lt;li>在区间（a，b）内可导&lt;/li>
&lt;/ol>
&lt;p>&lt;span class="math">\(\Rightarrow\)&lt;/span>至少存在一点&lt;span class="math">\(\xi\in(a,b)\)&lt;/span>，使&lt;span class="math">\(f&amp;#39;(\xi)=\frac{f(b)-f(a)}{b-a}\)&lt;/span>&lt;/p>
&lt;p>拉格朗日（Langrange）中值定理证明：&lt;/p>
&lt;p>可以把问题转为证明&lt;span class="math">\(f&amp;#39;(\xi)-\frac{f(b)-f(a)}{b-a}=0\)&lt;/span>。我们作辅助函数： &lt;span class="math">\[\phi(x)=f(x)-\frac{f(b)-f(a)}{b-a}x\]&lt;/span> 显然，&lt;span class="math">\(\phi(x)\)&lt;/span>在[a,b]上连续，在(a,b)内可导，且&lt;span class="math">\(\phi(a)=\frac{bf(a)-af(b)}{b-a}=\phi(b)\)&lt;/span>。&lt;/p>
&lt;p>&lt;span class="math">\(\phi(x)\)&lt;/span>函数可由罗尔定理只至少存在一点&lt;span class="math">\(\xi\in(a,b)\)&lt;/span>，使&lt;span class="math">\(\phi&amp;#39;(\xi)=0\)&lt;/span>，即定理结论整理。证毕。&lt;/p>
&lt;h3 id="柯西cauchy中值定理">柯西（Cauchy）中值定理&lt;/h3>
&lt;p>&lt;span class="math">\(f(x)\)&lt;/span>及&lt;span class="math">\(F(x)\)&lt;/span>满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>在闭区间[a,b]上连续&lt;/li>
&lt;li>在开区间(a,b)内可导&lt;/li>
&lt;li>在开区间(a,b)内&lt;span class="math">\(F(x)\neq0\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>&lt;span class="math">\(\Rightarrow\)&lt;/span>至少存在一点&lt;span class="math">\(\xi\in(a,b)\)&lt;/span>，使&lt;span class="math">\(\frac{f(b)-f(a)}{F(b)-F(a)}=\frac{f&amp;#39;(\xi)}{F&amp;#39;(\xi)}\)&lt;/span>&lt;/p>
&lt;p>证明同拉格朗日中值定理。&lt;/p>
&lt;img src="../images/mean_value.png" alt="中值定理之间关系" />
&lt;center>
图3 中值定理之间关系
&lt;/center>
&lt;h2 id="泰勒中值定理">泰勒中值定理&lt;/h2>
&lt;p>若&lt;span class="math">\(f(x)\)&lt;/span>在包含&lt;span class="math">\(x_0\)&lt;/span>的某开区间（a，b）内具有直到n+1阶导数，则当&lt;span class="math">\(x\in(a,b)\)&lt;/span>时，有 &lt;span class="math">\[f(x)=f(x_0)+f&amp;#39;(x_0)(x-x_0)+\frac{f&amp;#39;&amp;#39;(x_0)}{2!}(x-x_0)^2+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)(1)\]&lt;/span> &lt;span class="math">\[R_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{(n+1)}（2）\]&lt;/span> 其中，&lt;span class="math">\(\xi\)&lt;/span>在&lt;span class="math">\(x\)&lt;/span>和&lt;span class="math">\(x_0\)&lt;/span>之间(&lt;span class="math">\(x\in [min(x,x_0),max(x,x_0)]\)&lt;/span>)。&lt;/p>
&lt;p>公式（1）为泰勒n+1阶展开式。公式（2）为n+1阶泰勒余式，称为拉格朗日余项。&lt;/p>
&lt;h3 id="泰勒中值定理的一阶展开">泰勒中值定理的一阶展开&lt;/h3>
&lt;p>如果&lt;span class="math">\(f(x)\in C^2,P=1,\xi在x_0和x之间\)&lt;/span>，那么 &lt;span class="math">\[f(x)=f(x_0)+f&amp;#39;(x_0)(x-x_0)+\frac{f&amp;#39;&amp;#39;(\xi)}{2!}(x-x_0)^2,\xi\in\]&lt;/span> 变量&lt;span class="math">\(x\)&lt;/span>变成高维（&lt;span class="math">\(0\leq \alpha \leq 1\)&lt;/span>）： &lt;span class="math">\[f(\textbf{x}+\boldsymbol{\delta})=f(\textbf{x})+\textbf{g}(\textbf{x})^T\boldsymbol{\delta}+\frac{1}{2}\boldsymbol{\delta}^T\textbf{H}(\textbf{x}+\alpha\boldsymbol{\delta})\boldsymbol{\delta}\]&lt;/span> 其中，&lt;span class="math">\(\textbf{g}(\textbf{x})\)&lt;/span>为一阶导函数向量（梯度），&lt;span class="math">\(\textbf{H}(\textbf{x})\)&lt;/span>为多元函数的海森矩阵。&lt;/p></description></item><item><title>数学分析之水平集</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E6%B0%B4%E5%B9%B3%E9%9B%86/</link><pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E6%B0%B4%E5%B9%B3%E9%9B%86/</guid><description>
&lt;h2 id="数学分析之水平集">数学分析之水平集&lt;!-- omit in toc -->&lt;/h2>
&lt;p>水平集是数学中的中一个重要概念，这个概念和函数图像有着直观的联系，同时水平集也和函数的连续性，拟凸性，等高线图有着直接的联系。&lt;/p>
&lt;h2 id="水平集">水平集&lt;/h2>
&lt;p>在数学领域中, 一个具有&lt;span class="math">\(n\)&lt;/span>变量的实值函数&lt;span class="math">\(f\)&lt;/span>的&lt;strong>水平集&lt;/strong>是具有以下形式的集合 &lt;span class="math">\[
\{(x_1,...,x_n)|f(x_1,...,x_n)=c\}
\]&lt;/span>&lt;/p>
&lt;p>其中&lt;span class="math">\(c\)&lt;/span>是常数， 即使得函数值具有给定常数的变量集合。当具有&lt;strong>两个变量&lt;/strong>时, 称为&lt;strong>水平曲线(等高线)&lt;/strong>, 如果有&lt;strong>三个变量, 称为水平曲面&lt;/strong>, 更多变量时, 水平集被叫做&lt;strong>水平超曲面&lt;/strong>。&lt;/p>
&lt;h3 id="例子">例子&lt;/h3>
&lt;p>例如, 指定一个半径&lt;span class="math">\(r\)&lt;/span>, 圆的方程可以定义为一个等高线： &lt;span class="math">\[
r^2=x^2 + y^2
\]&lt;/span> 如果取&lt;span class="math">\(r=5\)&lt;/span>, 那么等高值为&lt;span class="math">\(c=5^2=25\)&lt;/span>。&lt;/p>
&lt;p>所有使得&lt;span class="math">\(x^2 + y^2=25\)&lt;/span>的点&lt;span class="math">\((x,y)\)&lt;/span>构成了它的等高线。 这就是说他们属于等高线的水平集。 如果&lt;span class="math">\(x^2 + y^2\)&lt;/span>小于 25 这个点&lt;span class="math">\((x,y)\)&lt;/span>就在等高线的内部;如果大于25，这个点就在等高线外部。&lt;/p>
&lt;h2 id="水平集与梯度">水平集与梯度&lt;/h2>
&lt;blockquote>
&lt;p>定理： 函数&lt;span class="math">\(f\)&lt;/span>在一点处的梯度与在该点处&lt;span class="math">\(f\)&lt;/span>的水平集垂直。&lt;/p>
&lt;/blockquote>
&lt;p>这个定理是十分不寻常的。为更好的理解定理的含义，设想两个旅行者在一座山峰的同一位置。其中一个人很大胆，决定从坡度最大的地方走。另一个人比较保守；他不想向上爬，也不想走下去，选择了一条在同一高度的路。上面的定理就是说，这两个旅行者相互离开的方向是互相垂直的。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Level_grad.png" alt="Level_grad.png" />&lt;p class="caption">Level_grad.png&lt;/p>
&lt;/div>
&lt;h2 id="上水平集和下水平集">上水平集和下水平集&lt;/h2>
&lt;blockquote>
&lt;p>集合&lt;span class="math">\(\{ (x_1,...,x_n) | f(x_1,...,x_n) ≤ c \}\)&lt;/span>被称为&lt;span class="math">\(f\)&lt;/span>的下水平集或子水平集;&lt;/p>
&lt;p>集合&lt;span class="math">\(\{ (x_1,...,x_n) | f(x_1,...,x_n) \geq c \}\)&lt;/span>被称为&lt;span class="math">\(f\)&lt;/span>的上水平集。&lt;/p>
&lt;/blockquote>
&lt;p>上下水平集与&lt;strong>最大最小值定理&lt;/strong>相关：&lt;/p>
&lt;blockquote>
&lt;p>定义：对于一个拓扑空间&lt;span class="math">\(X\)&lt;/span>，如果&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(f:X→R\)&lt;/span>是下半连续的，则其任意下水平集是闭的。&lt;/li>
&lt;li>&lt;span class="math">\(f:X→R\)&lt;/span>是上半连续的，则其任意上水平集是闭的。&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>通过半连续性，我们可以知道一个映射是否可以取到最值&lt;/strong>： &amp;gt;定理：设&lt;span class="math">\(X\)&lt;/span>是非空紧集，&lt;span class="math">\(f:X→R\)&lt;/span>则： &amp;gt; &amp;gt;- &lt;span class="math">\(f\)&lt;/span>下半连续&lt;span class="math">\(\Rightarrow\)&lt;/span>函数有最小值 &amp;gt;- &lt;span class="math">\(f\)&lt;/span>上半连续&lt;span class="math">\(\Rightarrow\)&lt;/span>函数有最大值&lt;/p>
&lt;p>具体可以参考&lt;a href="https://en.wikipedia.org/wiki/Extreme_value_theorem#Extension_to_semi-continuous_functions">魏尔斯特拉斯极值定理&lt;/a>&lt;/p>
&lt;p>上下水平集与&lt;strong>拟凸拟凹函数&lt;/strong>有关：&lt;/p>
&lt;blockquote>
&lt;p>&lt;span class="math">\(f:R^n→R, \mathop{dom}f\)&lt;/span>是凸集，&lt;span class="math">\(\alpha-\)&lt;/span>下水平集是凸集，则&lt;span class="math">\(f(x)\)&lt;/span>为&lt;strong>拟凸函数&lt;/strong>。即 &lt;span class="math">\[f:R^n→R,s.t.\, S_\alpha=\{x\in \mathop{dom} f| f(x)\geq \alpha\}为凸集\]&lt;/span> &lt;span class="math">\(f\)&lt;/span>是拟凸函数，我们称&lt;span class="math">\(-f\)&lt;/span>为&lt;strong>拟凹&lt;/strong>函数,拟凹函数的&lt;span class="math">\(\alpha-\)&lt;/span>&lt;strong>上水平集是凸集&lt;/strong>。&lt;/p>
&lt;/blockquote></description></item><item><title>数学分析之连续性</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E8%BF%9E%E7%BB%AD%E6%80%A7/</link><pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E8%BF%9E%E7%BB%AD%E6%80%A7/</guid><description>
&lt;h2 id="数学分析之连续性">数学分析之连续性&lt;!-- omit in toc -->&lt;/h2>
&lt;p>设有一个&lt;span class="math">\(f:\mathcal R^n→\mathcal R^m\)&lt;/span>的映射。&lt;/p>
&lt;blockquote>
&lt;p>点的连续性：&lt;span class="math">\(f\)&lt;/span>称为在&lt;span class="math">\(x_0\in \mathcal R^n\)&lt;/span>处&lt;strong>连续&lt;/strong>，若&lt;span class="math">\(\forall \varepsilon&amp;gt;0, \exists \delta&amp;gt;0\)&lt;/span>，&lt;span class="math">\(s.t. \forall x\in \mathcal R^n\)&lt;/span>，若&lt;span class="math">\(\Vert x-x_0\Vert&amp;lt;\delta\)&lt;/span>，则有&lt;span class="math">\(\Vert f(x)-f(x_0)\Vert &amp;lt;\varepsilon\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>如果我们用开球（又称邻域）&lt;span class="math">\(B(x_0;r)=\{x\in \mathcal{R^n} \big\vert \Vert x-x_0\Vert &amp;lt;r\}\)&lt;/span>，来描述连续性： &amp;gt;&lt;span class="math">\(\forall \varepsilon&amp;gt;0, \exists \delta&amp;gt;0有f(B(x_0;\delta))\subset B(f(x_0);\varepsilon)\)&lt;/span> ---------------------------------------- &amp;gt;映射的连续性：&lt;span class="math">\(f:\mathcal R^n→\mathcal R^m\)&lt;/span>称为连续映射，若&lt;span class="math">\(\forall x_0 \in \mathcal R^n\)&lt;/span>，&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>出连续。&lt;/p>
&lt;h2 id="连续与开集的关系">连续与开集的关系&lt;/h2>
&lt;p>设开集为&lt;span class="math">\(U\)&lt;/span>，若对于&lt;span class="math">\(\forall U \underset{open}{\subset} \mathcal{R^m}\)&lt;/span>&lt;/p>
&lt;p>&lt;strong>&lt;span class="math">\(f\)&lt;/span>连续&lt;span class="math">\(\Rightarrow\)&lt;/span>开集的原像是开集&lt;/strong>&lt;/p>
&lt;p>考虑&lt;span class="math">\(U\)&lt;/span>的原像&lt;span class="math">\(f^{-1}(U),\forall x_0\in f^{-1}(U),f(x_0)\in U\)&lt;/span>。&lt;/p>
&lt;p>由于&lt;span class="math">\(U\)&lt;/span>是开集，对于其中的点&lt;span class="math">\(f(x_0)，\exist \varepsilon&amp;gt;0. s.t. B(f(x_0);\varepsilon)\subset U\)&lt;/span>&lt;/p>
&lt;p>又因为&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>出连续，则&lt;span class="math">\(\exist \delta&amp;gt;0\)&lt;/span>使得&lt;span class="math">\(f(B(x_0;\delta))\subset B(f(x_0);\varepsilon)\Rightarrow B(x_0;\delta)\subset f^{-1}(B(f(x_0);\varepsilon))\subset f^{-1}(U)\)&lt;/span>。对于&lt;span class="math">\(U\)&lt;/span>中每一点都成立，所以&lt;span class="math">\(f^{-1}(U)\)&lt;/span>为&lt;span class="math">\(\mathcal{R^n}\)&lt;/span>中开集。&lt;/p>
&lt;p>&lt;strong>开集的原像是开集&lt;span class="math">\(\Rightarrow f\)&lt;/span>连续&lt;/strong>&lt;/p>
&lt;p>&lt;span class="math">\(\forall x_0 \in \mathcal{R^n}\)&lt;/span>，先证&lt;span class="math">\(f\)&lt;/span>在任一点&lt;span class="math">\(x_0\)&lt;/span>出连续。&lt;/p>
&lt;p>&lt;span class="math">\(\forall \varepsilon&amp;gt;0\)&lt;/span>，考虑&lt;span class="math">\(B(f(x_0);\varepsilon)\)&lt;/span>,由于条件开集的原像为开集，开球的原像&lt;span class="math">\(f^{-1}(B(f(x_0);\varepsilon))\)&lt;/span>也是开集，即&lt;span class="math">\(x_0\in f^{-1}(B(f(x_0);\varepsilon))且\exist\delta&amp;gt;0\)&lt;/span>，s.t. &lt;span class="math">\(B(x_0;\delta)\subset f^{-1}(B(f(x_0);\varepsilon))\Rightarrow f(B(x_0;\delta))\subset B(f(x_0);\varepsilon)\Rightarrow f\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>出连续。由于对任意&lt;span class="math">\(x_0\)&lt;/span>都成立，所以映射&lt;span class="math">\(f\)&lt;/span>连续。&lt;/p>
&lt;p>总结：&lt;strong>&lt;span class="math">\(f:\mathcal{R^n}→\mathcal{R^m}\)&lt;/span>连续映射&lt;span class="math">\(\Leftrightarrow\)&lt;/span>在&lt;span class="math">\(f\)&lt;/span>下，开集的原像为开集&lt;/strong>。这样我们可以&lt;strong>仅通过开集的概念来验证映射的连续性&lt;/strong>。&lt;/p></description></item><item><title>数学分析-三角函数与三角级数的正交性</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E4%B8%89%E8%A7%92%E5%87%BD%E6%95%B0%E4%B8%8E%E4%B8%89%E8%A7%92%E7%BA%A7%E6%95%B0%E7%9A%84%E6%AD%A3%E4%BA%A4%E6%80%A7/</link><pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E4%B8%89%E8%A7%92%E5%87%BD%E6%95%B0%E4%B8%8E%E4%B8%89%E8%A7%92%E7%BA%A7%E6%95%B0%E7%9A%84%E6%AD%A3%E4%BA%A4%E6%80%A7/</guid><description>
&lt;h2 id="三角函数与三角级数的正交性">三角函数与三角级数的正交性&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#三角恒等式和差化积与积化和差">三角恒等式——和差化积与积化和差&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三角级数与三角函数系的正交性">三角级数与三角函数系的正交性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#应用傅里叶级数展开">应用——傅里叶级数展开&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="三角恒等式和差化积与积化和差">三角恒等式——和差化积与积化和差&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">积化和差&lt;/th>
&lt;th align="center">和差化积&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\({\displaystyle \sin \alpha \cos \beta ={\sin(\alpha +\beta )+\sin(\alpha -\beta ) \over 2}}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\({\displaystyle \sin \alpha +\sin \beta =2\sin {\frac {\alpha +\beta }{2}}\cos {\frac {\alpha -\beta }{2}}}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\({\displaystyle \cos \alpha \sin \beta ={\sin(\alpha +\beta )-\sin(\alpha -\beta ) \over 2}}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\({\displaystyle \sin \alpha -\sin \beta =2\cos {\alpha +\beta \over 2}\sin {\alpha -\beta \over 2}}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\({\displaystyle \cos \alpha \cos \beta ={\cos(\alpha +\beta )+\cos(\alpha -\beta ) \over 2}}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\({\displaystyle \cos \alpha +\cos \beta =2\cos {\frac {\alpha +\beta }{2}}\cos {\frac {\alpha -\beta }{2}}}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\({\displaystyle \sin \alpha \sin \beta =-{\cos(\alpha +\beta )-\cos(\alpha -\beta ) \over 2}}\)&lt;/span>&lt;/td>
&lt;td align="center">&lt;span class="math">\({\displaystyle \cos \alpha -\cos \beta =-2\sin {\alpha +\beta \over 2}\sin {\alpha -\beta \over 2}}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="三角级数与三角函数系的正交性">三角级数与三角函数系的正交性&lt;/h2>
&lt;blockquote>
&lt;p>定理1： 组成三角级数的函数系 &lt;span class="math">\[1, \sin x, \cos x, \sin 2x, \cos 2x, \dotsb, \sin nx, \cos nx\]&lt;/span> 在区间&lt;span class="math">\([-\pi , \pi]或[0, 2\pi]\)&lt;/span>上正交，即其中任意两个&lt;strong>不同&lt;/strong>函数之积在区间&lt;span class="math">\([-\pi , \pi]或[0, 2\pi]\)&lt;/span>积分为0.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>证明&lt;/strong>：&lt;/p>
可以用积化和差公式证明，&lt;span class="math">\(\sin kx \cos nx={1 \over 2}[\sin (kx+nx)+\sin (kx-nx)]\)&lt;/span>，其中&lt;span class="math">\(k,n\)&lt;/span>为整数，且&lt;span class="math">\(k \neq n\)&lt;/span>。 $$
&lt;p>&lt;span class="math">\[
由于对任意**非0**整数$n$，有$\sin nx, \cos nx$在区间$[-\pi , \pi]或[0, 2\pi]$积分为0，所以上式右侧积分为0，即
\]&lt;/span> &lt;em>0^{2}kx nx x = 0 &lt;span class="math">\[
同理可以证明
\]&lt;/span> &lt;/em>0^{2}kx nx x = 0\ &lt;em>0^{2}kx nx x = 0\ &lt;/em>0^{2}kx nx x = 0 &lt;span class="math">\[
需要指出的是三角函数系中相同函数的乘积在在区间$[-\pi , \pi]或[0, 2\pi]$积分不为0，即
\]&lt;/span> &lt;em>0^{2}nx nx x 0\ &lt;/em>0^{2}nx nx x 0\ $$ 我们从积化和差公式中也可以窥出原因，即出现了直流项。&lt;/p>
&lt;h2 id="应用傅里叶级数展开">应用——傅里叶级数展开&lt;/h2>
&lt;blockquote>
&lt;p>狄利克雷定理 (傅里叶级数):&lt;/p>
&lt;p>设&lt;span class="math">\(f(x)\)&lt;/span>是周期为&lt;span class="math">\(2\pi\)&lt;/span>的周期函数，并满足狄利克雷条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>在一个周期内连续或只有有限个第一类间断点；&lt;/li>
&lt;li>在一个周期内只有有限个极限点；&lt;/li>
&lt;/ol>
&lt;p>则&lt;span class="math">\(f(x)\)&lt;/span>的傅里叶级数收敛，且有 &lt;span class="math">\[
\frac{a_0}{2}+\sum_{n=1}^\infty(a_n \cos nx + b_n \sin nx)=\\
\begin{cases}
f(x), x为连续点\\
\frac{f(x^+)+f(x^-)}{2}, x为间断点
\end{cases}
\]&lt;/span> 其中，&lt;span class="math">\(a_n, b_n\)&lt;/span>为&lt;span class="math">\(f(x)的傅里叶系数\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>可以发现傅里叶级数比幂级数的条件要宽松很多，对可导没有要求。&lt;/p>
&lt;p>另外，对奇函数，偶函数可以有对应的正弦级数和余弦级数。&lt;/p></description></item><item><title>数学分析-等幂求和公式</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E7%AD%89%E5%B9%82%E6%B1%82%E5%92%8C%E5%85%AC%E5%BC%8F/</link><pubDate>Mon, 02 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E7%AD%89%E5%B9%82%E6%B1%82%E5%92%8C%E5%85%AC%E5%BC%8F/</guid><description>
&lt;h2 id="等幂求和公式">等幂求和公式&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="一阶求和公式">一阶求和公式&lt;/h2>
&lt;p>一阶求和公式就是我们熟知的高斯求和公式 &lt;span class="math">\[
S_1(n) = 1+2+3+\dotsb+n=\frac{n(n+1)}{2}
\]&lt;/span>&lt;/p>
&lt;h2 id="二阶求和公式">二阶求和公式&lt;/h2>
&lt;p>&lt;span class="math">\[
S_2(n) = 1^2+2^2+3^2+\dotsb+n^2=\frac{n(n+1)(2n+1)}{6}
\]&lt;/span>&lt;/p>
&lt;p>二阶求和公式在高中也学过，但当时使用的是猜测加数学归纳法，公式的推导过程不够直观。下面，我们用一个直观的构造法来推导二阶求和公式。&lt;/p>
&lt;p>构造直到&lt;span class="math">\(2n\)&lt;/span>的平方和： &lt;span class="math">\[
\begin{aligned}
S_2(2n)&amp;amp;=1^2+2^2+3^2+\dotsb+n^2+(n+1)^2+\dotsb+(n+n)^2\\
&amp;amp;=\underbrace{1^2+2^2+3^2+\dotsb+n^2}_{S_2(n)}+(n^2+2n+1^2)+\dotsb+(n^2+2n\times n+n^2)\\
&amp;amp;=S_2(n)+n\times n^2 + 2n\underbrace{(1+2+3+\dotsb+n)}_{S_1(n)}+\underbrace{(1^2+2^2+\dotsb+n^2)}_{S_2(n)}\\
&amp;amp;=2S_2(n)+n^3+2n\times \frac{n(n+1)}{2}\\
&amp;amp;=2S_2(n)+2n^3+n^2 \dotsb(1)\\
\end{aligned}
\]&lt;/span> 另一个方面，我们可以将&lt;span class="math">\(S_2(2n)\)&lt;/span>根据奇偶性拆分： &lt;span class="math">\[
\begin{aligned}
S_2(2n)&amp;amp;=1^2+2^2+3^2+\dotsb+n^2+(n+1)^2+\dotsb+(n+n)^2\\
&amp;amp;=1^2+3^2+\dotsb+(2n-1)^2+2^2+4^2+\dotsb+(2n)^2\\
&amp;amp;=(2\times 1-1)^2+(2\times 2-1)^2+\dotsb+(2\times n-1)^2+4\underbrace{(1^2+2^2+\dotsb+n^2)}_{S_(n)}\\
&amp;amp;=(2^2\times 1^2 - 2\times 2\times 1 + 1^2)+(2^2\times 2^2 - 2\times 2\times 2 + 1^2)+\dotsb\\
&amp;amp;+(2^2\times n^2 - 2\times 2\times n + 1^2)+4S_2(n)\\
&amp;amp;=4\underbrace{(1^2+2^2+\dotsb+n^2)}_{S_2(n)}-4(1+2+\dotsb+n)+n+4S_2(n)\\
&amp;amp;=8S_2(n)-4\frac{n(n+1)}{2}+n=8S_2(n)-2n^2-n\dotsb(2)
\end{aligned}
\]&lt;/span> 显然公式&lt;span class="math">\((1)\)&lt;/span>等于公式&lt;span class="math">\((2)\)&lt;/span>，因此有： &lt;span class="math">\[
2S_2(n)+2n^3+n^2=8S_2(n)-2n^2-n\\
\Rightarrow S_2(n)=\frac{n(n+1)(2n+1)}{6}
\]&lt;/span>&lt;/p>
&lt;h2 id="三阶求和公式">三阶求和公式&lt;/h2>
&lt;p>&lt;span class="math">\[
S_3(n)=\left[{\frac {n(n+1)}{2}}\right]^{{2}}
\]&lt;/span> 三阶求和公式也需要一个构造，这个构造利用了对称性，比二阶求和公式好懂一些。 &lt;span class="math">\[
S_3(n)=1^3 +2^3 +3^3 +\dotsb+n^3\\
S_3(n)=n^3+(n-1)^3+(n-2)^3+\dotsb+1^3
\]&lt;/span> 两者相加有： &lt;span class="math">\[
\begin{aligned}
2S_3(n)&amp;amp;=n^3+1^3+(n-1)^3+2^3+(n-2)^3+3^3+\dotsb+1^3+n^3\\
&amp;amp;根据立方和公式a^3+b^3=(a+b)(a^2-ab+b^2)有：\\
2S_3(n)&amp;amp;=(n+1)(n^2-n+1)+[n+1]((n-1)^2-2(n-1)+2^2)\\
&amp;amp;+[n+1]((n-2)^2-3(n-2)+3^2)+\dotsb+[n+1](1^2-n+n^2)\\
&amp;amp;=[n+1](2\underbrace{(1^2+2^2+\dotsb+n^2)}_{S_2(n)}-(1\times n+2(n-1)+3(n-2)+\dotsb+n\times (n-(n-1)))\\
&amp;amp;=[n+1](2S_2(n)-n\underbrace{(1+2+\dotsb+n)}_{S_1(n)}+2\times 1+3\times 2+\dotsb+n\times(n-1))\\
&amp;amp;=[n+1](2S_2(n)-nS_1(n)+1^2+1+2^2+2+\dotsb+(n-1)^2+(n-1))\\
&amp;amp;=[n+1](2S_2(n)-nS_1(n)+(S_2(n)-n^2)+(S_1(n)-n))\\
&amp;amp;=[n+1](3S_2(n)-(n-1)S_1(n)-n^2-n)\\
&amp;amp;代入S_1(n),S_2(n)结果可得：\\
&amp;amp;=[n+1](\frac{n(n+1)(2n+1)}{2}-\frac{(n-1)n(n+1)}{2}-n(n+1))\\
&amp;amp;={1\over 2}(n+1)^2[n(2n+1)-(n-1)n-2n]=\frac{1}{2}n^2(n+1)^2\\
\Rightarrow S_3(n)&amp;amp;=\frac{1}{4}n^2(n+1)^2=[\frac{n(n+1)}{2}]^2
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;h2 id="等幂求和公式推断">等幂求和公式推断&lt;/h2>
&lt;p>我们根据1~3阶的求和公式发现，&lt;span class="math">\(m\)&lt;/span>阶&lt;span class="math">\(n\)&lt;/span>个连续自然数的和是&lt;strong>一个和&lt;span class="math">\(n\)&lt;/span>相关的&lt;span class="math">\(m+1\)&lt;/span>阶多项式(且没有0阶项)&lt;/strong>，因此可以猜测，&lt;span class="math">\(m\)&lt;/span>阶&lt;span class="math">\(n\)&lt;/span>个连续自然数和的形式为: &lt;span class="math">\[
a_{m+1} n^{m+1}+a_{m} n^{m}+\dotsb+a_1 n^1
\]&lt;/span> 这样我们可以先找出&lt;span class="math">\(m+1\)&lt;/span>个初始条件，然后以待定系数法求得求和公式。以2阶求和公式为例： &lt;span class="math">\[
\begin{aligned}
1^2 = 1 &amp;amp;= 1^3 a_3+ 1^2 a_2 + 1^1 a_1 \\
1^2 +2^2 = 5&amp;amp;= 2^3 a_3+ 2^2 a_2 + 2^1 a_1\\
1^2 +2^2 + 3^2= 14&amp;amp;= 3^3 a_3+ 3^2 a_2 + 3^1 a_1
\end{aligned}
\]&lt;/span> 根据高斯消元法或者克莱姆法则可知： &lt;span class="math">\[
\begin{cases}
a_1=\frac{1}{3}\\
a_2=\frac{1}{2}\\
a_3=\frac{1}{6}\\
\end{cases}
\]&lt;/span> 即2阶求和公式为&lt;span class="math">\(S_2(n)=\frac{1}{3}n^3+\frac{1}{2}n^2+\frac{1}{6}n\)&lt;/span>。&lt;/p>
&lt;p>当然这只是通过推断得出的结论，没有直接的证明。下面直接给出等幂求和公式 &lt;span class="math">\[
S_m(n)={1 \over {m+1}}\sum_{{i=0}}^{m}{m+1 \choose {i}}B_{i}(n+1)^{{m+1-i}}\tag{3}
\]&lt;/span> 其中&lt;span class="math">\(B_i\)&lt;/span>是伯努利数。关于伯努利数请参考维基百科&lt;a href="https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%95%B0">https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%95%B0&lt;/a>.&lt;/p></description></item><item><title>数学分析-基础</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E5%9F%BA%E7%A1%80/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E5%9F%BA%E7%A1%80/</guid><description>
&lt;h2 id="数学分析基础">数学分析基础&lt;!-- omit in toc -->&lt;/h2>
&lt;div class="figure">
&lt;img src="../images/数学分析定理关系.jpg" alt="数学分析定理关系" />&lt;p class="caption">数学分析定理关系&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>&lt;a href="#实数完备性定理">实数完备性定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#确界定理">确界定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单调有界定理">单调有界定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#闭区间套定理">闭区间套定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#列紧性定理致密性定理">列紧性定理(致密性定理)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#柯西收敛准则">柯西收敛准则&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有限覆盖定理紧性">有限覆盖定理（紧性）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#聚点定理">聚点定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#导集内核闭包">导集内核闭包&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从完备性列紧紧看定理体系">从完备性，列紧，紧看定理体系&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="实数完备性定理">实数完备性定理&lt;/h2>
&lt;p>详细请看&lt;a href="../文档/实变函数/实数完备性基本定理的相互证明(30个).pdf">实数完备性基本定理的相互证明(30个).pdf&lt;/a>&lt;/p>
&lt;h2 id="确界定理">确界定理&lt;/h2>
&lt;blockquote>
&lt;p>上（下）确界的两种定义方式：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>严格定义：若&lt;span class="math">\(\beta\)&lt;/span>是数集&lt;span class="math">\(S\)&lt;/span>的一个上（下）界，并且有&lt;span class="math">\(\forall \varepsilon&amp;gt;0，\exists x_\varepsilon ∈ S\)&lt;/span>，满足&lt;span class="math">\(x_\varepsilon&amp;gt;\beta-\varepsilon(x_\varepsilon&amp;lt;\beta+\varepsilon)\)&lt;/span>，则称&lt;span class="math">\(\beta\)&lt;/span>是数集&lt;span class="math">\(S\)&lt;/span>的上（下）确界。&lt;/li>
&lt;li>简化定义：上确界是最小上界，下确界是最大下界。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>确界原理可以被看做&lt;strong>公理&lt;/strong>，它是实数的&lt;strong>连续性或完备性&lt;/strong>的体现，即实数包含了数轴上所有的点，没有空隙。数集&lt;span class="math">\(S\)&lt;/span>的上确界常被记作&lt;span class="math">\(\sup S\)&lt;/span>，下确界记作&lt;span class="math">\(\inf S\)&lt;/span>。&lt;/p>
&lt;blockquote>
&lt;p>有上界的非空数集必有上确界。有下界的非空数集必有下确界。&lt;/p>
&lt;p>空集的上确界是&lt;span class="math">\(-∞\)&lt;/span>，下确界是&lt;span class="math">\(+∞\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="单调有界定理">单调有界定理&lt;/h2>
&lt;blockquote>
&lt;p>单调有界定理:单调有界数列必有极限。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>考虑有上界的单调递增序列，确定定理指出，有上界必有上确界。&lt;/p>
&lt;p>首先指出有上确界&lt;span class="math">\(a，\forall a_n和\varepsilon&amp;gt;0，a_n≤a&amp;lt;a+\varepsilon\)&lt;/span>。&lt;/p>
&lt;p>其次根据上确界的定义，&lt;span class="math">\(\forall \varepsilon&amp;gt;0，\exists a_N\)&lt;/span>，使得&lt;span class="math">\(a_N&amp;gt;a-\varepsilon\)&lt;/span>。由于序列递增，所以当&lt;span class="math">\(n&amp;gt;N\)&lt;/span>时，&lt;span class="math">\(a_n≥a_N&amp;gt;a-\varepsilon\)&lt;/span>。&lt;/p>
&lt;p>综上，当&lt;span class="math">\(n&amp;gt;N\)&lt;/span>时，&lt;span class="math">\(\forall \varepsilon&amp;gt;0\)&lt;/span>， &lt;span class="math">\[a_n&amp;lt;a+\varepsilon\Rightarrow a_n-a&amp;lt;\varepsilon\\
a_n≥a_N&amp;gt;a-\varepsilon\Rightarrow a-a_n&amp;lt;\varepsilon\]&lt;/span> 即&lt;span class="math">\(|a_n-a|&amp;lt;\varepsilon\)&lt;/span>，上确界&lt;span class="math">\(a\)&lt;/span>为序列的极限。同理对于有下界的递减数列同样可得极限为下确界。&lt;/p>
&lt;blockquote>
&lt;p>推论：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>如果单调序列的一个子序列收敛，则这个单调序列收敛。&lt;/li>
&lt;li>如果单调序列的一个子序列趋向无穷，则这个序列发散。&lt;/li>
&lt;li>一个单调序列要么收敛要么发散。&lt;/li>
&lt;li>单调序列收敛的&lt;strong>充分必要&lt;/strong>条件是序列有界。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h2 id="闭区间套定理">闭区间套定理&lt;/h2>
&lt;blockquote>
&lt;p>设&lt;span class="math">\(I_n=[a_n,b_n],n∈N^+\)&lt;/span>，为一列闭区间，满足：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(I_1\supset I_2\supset I_3\supset\dotsb\supset I_n\supset I_{n+1} \dotsb\)&lt;/span>&lt;/li>
&lt;li>区间长度&lt;span class="math">\(|I_n|=b_n-a_n → 0(n→∞)\)&lt;/span>，&lt;/li>
&lt;/ol>
&lt;p>则存在&lt;strong>唯一一点&lt;/strong>&lt;span class="math">\(\xi满足\xi\in\bigcap\limits_{n=1}^∞ I_n\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>即&lt;span class="math">\(\lim_{n→∞}a_n=\lim_{n→∞}b_n=\xi\)&lt;/span>。&lt;strong>R中长度趋于0的区间套有且只有一个公共点。&lt;/strong> &lt;img src="../images/闭区间套定理.png" alt="闭区间套定理" />&lt;/p>
&lt;p>证明：由区间的包含关系可知，左端点组成的数列&lt;span class="math">\(\{a_n\}\)&lt;/span>递增，右端点组成的数列&lt;span class="math">\(\{b_n\}\)&lt;/span>递减，并且&lt;span class="math">\(\{a_n\}\)&lt;/span>有上界&lt;span class="math">\(b_1\)&lt;/span>，&lt;span class="math">\(\{b_n\}\)&lt;/span>有下界&lt;span class="math">\(a_1\)&lt;/span>。由&lt;strong>单调有界定理&lt;/strong>可知，下面两式有极限： &lt;span class="math">\[\lim_{n→∞}a_n=a\\
\lim_{n→∞}b_n=b\]&lt;/span> 由于&lt;span class="math">\(a_n\leq b_n\)&lt;/span>恒成立，所以&lt;span class="math">\(a\leq b\)&lt;/span>，因此由不等式： &lt;span class="math">\[a_n≤a≤b≤b_n(n∈N^+)\\
\Rightarrow 0≤b-a≤b_n-a_n=|I_n|\]&lt;/span> 由区间长度&lt;span class="math">\(|I_n|→0(n→∞)\)&lt;/span>可知，&lt;span class="math">\(a=b\)&lt;/span>。此时： &lt;span class="math">\[a_n≤a=b≤b_n，对\forall n∈N^+成立，即a ∈ I_n\]&lt;/span> 由此可得：&lt;span class="math">\(a ∈ \bigcap\limits_{n=1}^∞ I_n\)&lt;/span>，&lt;span class="math">\(a\)&lt;/span>的唯一性可有极限的唯一性(Hausdorff空间极限都唯一)推得。&lt;/p>
&lt;h2 id="列紧性定理致密性定理">列紧性定理(致密性定理)&lt;/h2>
&lt;blockquote>
&lt;p>列紧性定理(致密性定理)：任意有界数列中必可造出收敛子列。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>方法1：区间套定理。&lt;/p>
&lt;p>设&lt;span class="math">\(\{x_n\}\)&lt;/span>为一有界数列，有&lt;span class="math">\(a≤x_n≤b\)&lt;/span>，将区间&lt;span class="math">\([a,b]\)&lt;/span>分成&lt;span class="math">\([a,(a+b)/2]\)&lt;/span>和&lt;span class="math">\([(a+b)/2,b]\)&lt;/span>两部分，显然至少一个区间包含无穷多项，取那个区间的下界记作&lt;span class="math">\(a_1\)&lt;/span>，上界记作&lt;span class="math">\(b_1\)&lt;/span>。在该区间任取一项记作&lt;span class="math">\(c_1\)&lt;/span>。依次取下去得到数列&lt;span class="math">\(\{a_n\}\)&lt;/span>和&lt;span class="math">\(\{b_n\}\)&lt;/span>和闭区间列&lt;span class="math">\(\{[a_n,b_n]\}\)&lt;/span>，且&lt;span class="math">\(\lim\limits_{n→∞}(b_n-a_n)=\lim\limits_{n→∞}\frac{b-a}{2^n}=0\)&lt;/span>。根据区间套定理，&lt;span class="math">\(\lim\limits_{n→∞}a_n=\lim\limits_{n→∞}b_n=\xi\)&lt;/span>，由于每一个区间包含无穷多项，因而可以取到完整的子列&lt;span class="math">\(\{c_n\}\)&lt;/span>，并且有&lt;span class="math">\(a_n≤c_n≤b_n\)&lt;/span>，根据夹逼定理有&lt;span class="math">\(\lim\limits_{n→∞}c_n=\xi\)&lt;/span>。&lt;/p>
&lt;p>方法2：此外还可以依赖&lt;strong>单调有界定理&lt;/strong>，以下重点说明。&lt;/p>
&lt;blockquote>
&lt;p>引理：每一个无限实数序列必包含一个单调子序列。&lt;/p>
&lt;p>如果对于一个正整数&lt;span class="math">\(n\)&lt;/span>，有如果&lt;span class="math">\(n&amp;lt;m\)&lt;/span>，那么&lt;span class="math">\(a_n&amp;gt;a_m\)&lt;/span>，则称&lt;span class="math">\(a_n\)&lt;/span>为一个“峰”。即峰值比以后任意一个数值都要大。&lt;/p>
&lt;p>如果峰的个数有无限多个，&lt;span class="math">\(n_{1}&amp;lt;n_{2}&amp;lt;n_{3}&amp;lt;\dots &amp;lt;n_{j}&amp;lt;\dots\)&lt;/span>，那么我们构造峰的子序列&lt;span class="math">\(\{a_{n_j}\}\)&lt;/span>。由于峰值递减，所以子序列&lt;span class="math">\(\{a_{n_j}\}\)&lt;/span>为一&lt;strong>单调递减&lt;/strong>子序列。如果峰的个数有限，那么设&lt;span class="math">\(N\)&lt;/span>为&lt;strong>最后一个峰&lt;/strong>的序号，取&lt;span class="math">\(n_1=N+1\)&lt;/span>，&lt;span class="math">\(n_1\)&lt;/span>不是峰，那意味着&lt;span class="math">\(\exists n_2&amp;gt;n_1\)&lt;/span>，使得&lt;span class="math">\(a_{n_2}≥a_{n_1}\)&lt;/span>。同样的，&lt;span class="math">\(n_2\)&lt;/span>也不是峰，那么&lt;span class="math">\(\exists n_3&amp;gt;n_2\)&lt;/span>，使得&lt;span class="math">\(a_{n_3}≥a_{n_2}\)&lt;/span>。重复这个过程，我们就可以取得一个&lt;strong>单调递增&lt;/strong>序列&lt;span class="math">\(\{a_{n_1},a_{n_2},a_{n_3},\dotsb\}\)&lt;/span>。&lt;/p>
&lt;p>引理证毕。&lt;/p>
&lt;/blockquote>
&lt;p>我们已知序列&lt;span class="math">\(\{a_n\}\)&lt;/span>中必存在一个单调子序列。并且原序列是有界的，所以子序列是一个单调有界子序列，根据&lt;strong>单调有界定理&lt;/strong>，此子序列收敛（存在极限）。&lt;/p>
&lt;p>定理证毕。&lt;/p>
&lt;h2 id="柯西收敛准则">柯西收敛准则&lt;/h2>
&lt;blockquote>
&lt;p>柯西基本序列：对给定数列&lt;span class="math">\(x_n\)&lt;/span>，如&lt;span class="math">\(\forall \varepsilon&amp;gt;0，\exists N ∈ \mathbb{N^+}\)&lt;/span>，当&lt;span class="math">\(m，n ∈ \mathbb{N^+}，且m，n&amp;gt;N\)&lt;/span>时，都有 &lt;span class="math">\[|x_m-x_n|&amp;lt;\varepsilon\]&lt;/span> 则称&lt;span class="math">\(x_n\)&lt;/span>为基本列。或者叙述为：&lt;/p>
&lt;span class="math">\(\forall \varepsilon &amp;gt; 0, \exists N ∈ \mathbb{N^+}，当n&amp;gt;N时，\forall p ∈ \mathbb{N^+}，有\)&lt;/span> &lt;span class="math">\[|x_{n+p}-x_n|&amp;lt;\varepsilon\]&lt;/span> &lt;img src="../images/柯西基本序列.png" alt="柯西基本序列" />
&lt;center>
柯西基本序列示意
&lt;/center>
&lt;/blockquote>
&lt;blockquote>
&lt;p>柯西收敛准则：&lt;span class="math">\(\{a_n\}\)&lt;/span>是基本列&lt;span class="math">\(\Leftrightarrow\)&lt;/span>数列&lt;span class="math">\(\{a_n\}\)&lt;/span>收敛 。柯西收敛准则还体现了实数的完备性。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;p>&lt;strong>柯西基本列&lt;span class="math">\(\Rightarrow\)&lt;/span>收敛&lt;/strong>。设&lt;span class="math">\(\{a_n\}\)&lt;/span>为基本列.&lt;/p>
&lt;p>(1)先证明&lt;span class="math">\(\{a_n\}\)&lt;/span>有界，取&lt;span class="math">\(\varepsilon_0=1\)&lt;/span>，根据基本列定义，&lt;span class="math">\(\exists N ∈ \mathbb{N}^+，n&amp;gt;N\)&lt;/span>时： &lt;span class="math">\[|a_n-a_{N+1}|&amp;lt;\varepsilon_0=1\\
|a_n|=|a_n-a_{N+1}+a_{N+1}|≤|a_n-a_{N+1}|+|a_{N+1}|\\
≤1+|a_{N+1}|\]&lt;/span> 取&lt;span class="math">\(M=\max\{|a_1|,|a_2|,\dots,|a_N|,1+|a_{N+1}|\},\)&lt;/span>则对&lt;span class="math">\(\forall n\)&lt;/span>，有&lt;span class="math">\(|a_n|≤M\)&lt;/span>，即数列&lt;span class="math">\(\{a_n\}\)&lt;/span>有界。&lt;/p>
&lt;p>(2)由列紧性定理可知，数列&lt;span class="math">\(\{a_n\}\)&lt;/span>有界则存在收敛子序列&lt;span class="math">\(\{a_{i_n}\}\)&lt;/span>。设&lt;span class="math">\(\lim_{n→∞}a_{i_n}=a\)&lt;/span>，则&lt;span class="math">\(\forall \varepsilon&amp;gt;0，\exists N_1 ∈ \mathbb{N^+}\)&lt;/span>，当&lt;span class="math">\(i_n&amp;gt;N_1\)&lt;/span>时: &lt;span class="math">\[|a_{i_n}-a|&amp;lt;\frac{\varepsilon}{2}\]&lt;/span> 由&lt;span class="math">\(\{a_n\}\)&lt;/span>是基本列可知，&lt;span class="math">\(\exists N_2 ∈ \mathbb{N}^+\)&lt;/span>，&lt;span class="math">\(m，n&amp;gt;N_2\)&lt;/span>时： &lt;span class="math">\[|a_m-a_n|&amp;lt;\frac{\varepsilon}{2}\]&lt;/span> 我们取&lt;span class="math">\(i_k&amp;gt;max(N_1,N_2)\)&lt;/span>则 &lt;span class="math">\[|a_n-a|=|a_n-a_{i_k}+a_{i_k}-a|≤|a_n-a_{i_k}|+|a_{i_k}-a|&amp;lt;\varepsilon\]&lt;/span> 所以&lt;span class="math">\(\lim_{n→∞}a_n=a\)&lt;/span>，得证。&lt;/p>
&lt;p>&lt;strong>收敛&lt;span class="math">\(\Rightarrow\)&lt;/span>柯西基本列&lt;/strong>。设&lt;span class="math">\(\{a_n\}\)&lt;/span>收敛于&lt;span class="math">\(a\)&lt;/span>，则&lt;span class="math">\(\forall\varepsilon&amp;gt;0，\exists N ∈ \mathbb{N^+}，n&amp;gt;N\)&lt;/span>，有&lt;span class="math">\(|a_n-a|&amp;lt;\varepsilon/2\)&lt;/span>。当&lt;span class="math">\(m,n&amp;gt;N\)&lt;/span>时 &lt;span class="math">\[|a_m-a_n|=|a_m-a+a-a_n|≤|a_m-a|+|a_n-a|&amp;lt;\varepsilon/2+\varepsilon/2=\varepsilon\]&lt;/span> 所以&lt;span class="math">\(\{a_n\}\)&lt;/span>是柯西基本列。&lt;/p>
&lt;p>&lt;strong>更重要的是其他6个定理可由柯西收敛准则直接推出&lt;/strong>。&lt;/p>
&lt;h2 id="有限覆盖定理紧性">有限覆盖定理（紧性）&lt;/h2>
&lt;blockquote>
&lt;p>覆盖定义：设&lt;span class="math">\(Ф\)&lt;/span>是拓扑空间&lt;span class="math">\(X\)&lt;/span>的&lt;strong>子集族&lt;/strong>，称&lt;span class="math">\(Ф\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的一个覆盖，如果对任意&lt;span class="math">\(x∈X，x\)&lt;/span>至少包含在&lt;span class="math">\(Ф\)&lt;/span>的一个成员之中。&lt;/p>
&lt;blockquote>
&lt;p>等价定义：给定集合&lt;span class="math">\(X\)&lt;/span>若有一族开区间&lt;span class="math">\(\{I_\lambda,\lambda ∈ \Lambda\}\)&lt;/span>，使&lt;span class="math">\(A\subset\bigcup\limits_{\lambda ∈ \Lambda}I_\lambda\)&lt;/span>，则称开区间族&lt;span class="math">\(\{I_\lambda\}\)&lt;/span>是&lt;span class="math">\(X\)&lt;/span>的一个开覆盖。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;hr />
&lt;blockquote>
&lt;p>有限覆盖定理（海涅-博雷尔定理）不同表述：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>若开区间所成的区间集&lt;span class="math">\(\{I_\lambda\}\)&lt;/span>覆盖闭区间&lt;span class="math">\([a,b]\)&lt;/span>，则可以从&lt;span class="math">\(\{I_\lambda\}\)&lt;/span>中选出&lt;strong>有限个&lt;/strong>区间覆盖&lt;span class="math">\([a,b]\)&lt;/span>。区间集必须为开区间集，否则集合不能成立。&lt;/li>
&lt;li>R中的有界闭集是紧的。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>证明：&lt;strong>反证法&lt;/strong>。&lt;/p>
&lt;p>设&lt;span class="math">\([a,b]\)&lt;/span>不能被&lt;span class="math">\(\{I_\lambda\}\)&lt;/span>中的有限个开区间覆盖。将&lt;span class="math">\([a,b]\)&lt;/span>二等分，必至少有一个区间&lt;span class="math">\([a_1,b_1]\)&lt;/span>不能被有限覆盖。&lt;/p>
&lt;p>&lt;span class="math">\([a_1,b_1]\)&lt;/span>二等分，必至少有一个闭区间&lt;span class="math">\([a_2,b_2]\)&lt;/span>不能被有限覆盖。如此继续操作得到比区间套&lt;span class="math">\(\{ [a_n,b_n]\}\)&lt;/span>，且其中每一个闭区间都不能被&lt;strong>有限的覆盖&lt;/strong>。&lt;/p>
&lt;p>由&lt;strong>闭区间套定理&lt;/strong>可知，&lt;span class="math">\(\exists \eta ∈ \bigcap\limits_{n=1}^∞[a_n,b_n]\)&lt;/span>，且&lt;span class="math">\(\lim_{n→∞}a_n=\lim_{n→∞}b_n=\eta\)&lt;/span>.&lt;/p>
&lt;p>因为&lt;span class="math">\(\eta ∈ [a,b]\)&lt;/span>，所以根据覆盖定义在&lt;span class="math">\(\{I_\lambda\}\)&lt;/span>中至少有一个开区间&lt;span class="math">\((\alpha,\beta)\)&lt;/span>覆盖了&lt;span class="math">\(\eta\)&lt;/span>。由数列极限定义给出，对于&lt;span class="math">\(\forall \eta-\alpha&amp;gt;0，\exists N ∈ \mathbb N^+\)&lt;/span>，当&lt;span class="math">\(n&amp;gt;N\)&lt;/span>，&lt;span class="math">\(|a_n-\eta|&amp;lt;\eta-\alpha\)&lt;/span>；同理有&lt;span class="math">\(|b_n-\eta|&amp;lt;\beta-\eta\)&lt;/span>，所以： &lt;span class="math">\[|a_n-\eta|&amp;lt;\eta-\alpha\Rightarrow \eta-a_n&amp;lt;\eta-\alpha\Rightarrow a_n&amp;gt;\alpha\\
|b_n-\eta|&amp;lt;\beta-\eta\Rightarrow b_n-\eta&amp;lt;\beta-\eta\Rightarrow b_n&amp;lt;\beta\]&lt;/span> 即&lt;span class="math">\([a_n,b_n]\subset(\alpha,\beta)\)&lt;/span>可以被一个开区间覆盖。与上文说的每一个&lt;span class="math">\([a_n,b_n]\)&lt;/span>都不能被有限的覆盖矛盾。&lt;/p>
&lt;p>所以反命题不成立，原命题得证。&lt;/p>
&lt;p>我觉得联系聚点的概念更容易理解有限覆盖定理。&lt;/p>
&lt;h2 id="聚点定理">聚点定理&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>邻域&lt;/strong>：设&lt;span class="math">\(\delta&amp;gt;0\)&lt;/span>，开区间&lt;span class="math">\((a-\delta,a+\delta)\)&lt;/span>称为&lt;span class="math">\(a\)&lt;/span>的&lt;span class="math">\(\delta\)&lt;/span>邻域，记作&lt;span class="math">\(U(a,\delta)\)&lt;/span>,&lt;span class="math">\(\delta\)&lt;/span>称作该邻域的半径。 &lt;strong>去心邻域&lt;/strong>：设&lt;span class="math">\(\delta&amp;gt;0\)&lt;/span>，&lt;span class="math">\(U(a,\delta)-{a}\)&lt;/span>称为&lt;span class="math">\(a\)&lt;/span>的去心&lt;span class="math">\(\delta\)&lt;/span>邻域，记作&lt;span class="math">\(U^\circ(a,\delta)\)&lt;/span> &lt;strong>聚点&lt;/strong>：设X是数集，实数&lt;span class="math">\(a\)&lt;/span>满足，&lt;span class="math">\(\forall \delta&amp;gt;0\)&lt;/span>，满足&lt;span class="math">\(U^\circ(a,\delta)\cap X \neq \emptyset\)&lt;/span>，则称&lt;span class="math">\(a\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>的聚点。&lt;/p>
&lt;blockquote>
&lt;p>等价描述为：如果点&lt;span class="math">\(a\)&lt;/span>的任何邻域 &lt;span class="math">\[U^\circ(a,\delta)=\{x|0&amp;lt;|x-a|&amp;lt;\delta\}\]&lt;/span> 都含有X中无穷多个点，则称&lt;span class="math">\(a\)&lt;/span>为&lt;span class="math">\(X\)&lt;/span>的聚点。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>借助这些概念我们有如下定理：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>聚点定理&lt;/strong>：任何有界无穷点集至少有一个聚点。&lt;/p>
&lt;/blockquote>
&lt;p>证明：反证法（利用有限覆盖定理）。设有界无限点集&lt;span class="math">\(S\)&lt;/span>无聚点，则由&lt;span class="math">\(S\)&lt;/span>有界可知：&lt;/p>
&lt;p>存在实数&lt;span class="math">\(a，b\)&lt;/span>使得&lt;span class="math">\(S \subset [a,b]\)&lt;/span>。因为&lt;span class="math">\(S\)&lt;/span>无聚点，所以&lt;span class="math">\([a,b]\)&lt;/span>中的点都不是&lt;span class="math">\(S\)&lt;/span>的聚点。&lt;span class="math">\(\forall x ∈ [a,b], \exists \delta_x\)&lt;/span>，使得&lt;span class="math">\(U^\circ(x,\delta_x)\)&lt;/span>仅含有&lt;strong>有限&lt;/strong>个&lt;span class="math">\(S\)&lt;/span>中的点。&lt;/p>
&lt;p>记&lt;span class="math">\(F=\{U^\circ(x,\delta_x)|x ∈ [a,b]\}\)&lt;/span>，则&lt;span class="math">\(F\)&lt;/span>为&lt;span class="math">\(S\)&lt;/span>的一开覆盖。由&lt;strong>有限覆盖定理&lt;/strong>可知，存在&lt;span class="math">\(S\)&lt;/span>的有限个数的子覆盖。而每个开区间邻域内只有有限个点，有限个覆盖&lt;span class="math">\(\times\)&lt;/span>有限个点&amp;lt;&lt;span class="math">\(∞\)&lt;/span>，不可能得到无穷点集，矛盾。所以反命题不成立。得证。&lt;/p>
&lt;p>此外，我们再来写一个证明来完整闭环。聚点定理&lt;span class="math">\(\Rightarrow\)&lt;/span>致密性定理。&lt;/p>
&lt;p>设数列&lt;span class="math">\(\{a_n\}\)&lt;/span>有界，显然可以看做一无穷点集，根据聚点定理，至少存在一个聚点&lt;span class="math">\(\xi\)&lt;/span>。依次从&lt;span class="math">\(\xi\)&lt;/span>的&lt;span class="math">\(1/i\)&lt;/span>邻域中取一项，记作&lt;span class="math">\(x_i\)&lt;/span>，根据聚点定义，可以无限取下去构成子列&lt;span class="math">\(\{x_n\}\)&lt;/span>，且有&lt;span class="math">\(|x_n-\xi|&amp;lt;1/n\)&lt;/span>，易证&lt;span class="math">\(\lim\limits_{n→ ∞}x_n=\xi\)&lt;/span>。得证。&lt;/p>
&lt;h3 id="导集内核闭包">导集内核闭包&lt;/h3>
&lt;blockquote>
&lt;p>内核：设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中的点集，由&lt;span class="math">\(E\)&lt;/span>的所有内点组成的集合称为&lt;span class="math">\(E\)&lt;/span>的内核，记为&lt;span class="math">\(E^\circ\)&lt;/span>。&lt;/p>
&lt;p>导集：设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中的点集，由&lt;span class="math">\(E\)&lt;/span>的所有聚点组成的集合称为&lt;span class="math">\(E\)&lt;/span>的导集，记为&lt;span class="math">\(E&amp;#39;\)&lt;/span>。&lt;/p>
&lt;p>闭包：设&lt;span class="math">\(E\)&lt;/span>是&lt;span class="math">\(R^n\)&lt;/span>中的点集，称&lt;span class="math">\(E^\circ\cup E&amp;#39;\)&lt;/span>为&lt;span class="math">\(E\)&lt;/span>的闭包，记为&lt;span class="math">\(\overline{E}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>关于导集、内核、闭包有以下定理： &amp;gt;如果&lt;span class="math">\(A \subset B\)&lt;/span>，则&lt;span class="math">\(A^\circ \subset B^\circ，A&amp;#39;\subset B&amp;#39;，\overline{A}\subset\overline{B}\)&lt;/span>。即导集、内核、闭包的运算具有单调性。&lt;/p>
&lt;h2 id="从完备性列紧紧看定理体系">从完备性，列紧，紧看定理体系&lt;/h2>
&lt;div class="figure">
&lt;img src="../images/实数完备列紧与紧.jpg" alt="实数完备列紧与紧" />&lt;p class="caption">实数完备列紧与紧&lt;/p>
&lt;/div></description></item><item><title>博弈论1-纳什均衡存在性</title><link>https://surprisedcat.github.io/studynotes/%E5%8D%9A%E5%BC%88%E8%AE%BA1-%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E5%AD%98%E5%9C%A8%E6%80%A7/</link><pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%8D%9A%E5%BC%88%E8%AE%BA1-%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E5%AD%98%E5%9C%A8%E6%80%A7/</guid><description>
&lt;h2 id="博弈论1-纳什均衡存在性">博弈论1-纳什均衡存在性&lt;!-- omit in toc -->&lt;/h2>
&lt;p>对于有限非合作博弈的纳什均衡的存在性具有普遍性，其证明主要用到了布劳尔(Brouwer)不动点定理和角谷静夫(Kakutani)不动点定理，这两个定理的证明用到了复杂的数学工具，在这里不给出证明。&lt;/p>
&lt;h2 id="不动点定理">不动点定理&lt;/h2>
&lt;p>所有的纳什均衡存在性定理证明都采用了不动点定理，这是因为，纳什均衡的概念在数学上就是一个不动点的概念。在给出存在性定理及其证明之前，我们先来说&lt;strong>明不动点的概念和给出不动点定理&lt;/strong>。&lt;/p>
&lt;p>什么是“不动点”呢？考虑一个&lt;strong>方程(不是函数)&lt;/strong>&lt;span class="math">\(f(x)=x\)&lt;/span>，其中&lt;span class="math">\(x\)&lt;/span>为方程的解。&lt;/p>
&lt;p>我们将&lt;span class="math">\(f(\cdot)\)&lt;/span>视为一种“变换”，即&lt;span class="math">\(f(\cdot)\)&lt;/span>是将&lt;span class="math">\(x\)&lt;/span>对应为&lt;span class="math">\(y=f(x)\)&lt;/span>的变换，其中&lt;span class="math">\(x\)&lt;/span>和&lt;span class="math">\(y\)&lt;/span>分别是属于集合&lt;span class="math">\(X\)&lt;/span>和&lt;span class="math">\(Y\)&lt;/span>的两个元素，&lt;span class="math">\(x\in X\)&lt;/span>，&lt;span class="math">\(y\in Y\)&lt;/span>。如果&lt;span class="math">\(X=Y\)&lt;/span>，则方程&lt;span class="math">\(f(x)=x\)&lt;/span>的几何意义就是：变换&lt;span class="math">\(f(\cdot)\)&lt;/span>将&lt;span class="math">\(x\)&lt;/span>变为自己，即&lt;span class="math">\(x\)&lt;/span>在&lt;span class="math">\(f(\cdot)\)&lt;/span>变换下是不变的，故称&lt;span class="math">\(f(x)=x\)&lt;/span>的&lt;strong>解&lt;/strong>为变换&lt;span class="math">\(f(\cdot)\)&lt;/span>的不动点。&lt;/p>
&lt;p>一般地，我们可以将所有的方程都写为&lt;span class="math">\(f(x,y)=0\)&lt;/span>的形式，比如&lt;span class="math">\(f(x)=x\)&lt;/span>可以写成&lt;span class="math">\(f(x)-x=0\)&lt;/span>。&lt;strong>这样求不动点问题就和方程求根问题联系起来&lt;/strong>。&lt;/p>
&lt;p>对于这样一种非常一般地的问题，数学家们感到十分高兴的是居然在不太严格的条件下&lt;span class="math">\(f(x,y)=0\)&lt;/span>存在解，即不动点是&lt;strong>较为广泛地存在的&lt;/strong>。&lt;/p>
&lt;p>譬如，图1表明不动点是曲线&lt;span class="math">\(f(\cdot)\)&lt;/span>与&lt;span class="math">\(45\degree\)&lt;/span>线的交点。当函数&lt;span class="math">\(f(x)\)&lt;/span>定义在&lt;span class="math">\(x\in[0,1]\)&lt;/span>区间上且因变量&lt;span class="math">\(y=f(x)\)&lt;/span>的值域也为&lt;span class="math">\([0,1]\)&lt;/span>区间时，如果&lt;span class="math">\(f(x)\)&lt;/span>是连续的，则必然存在不动点。&lt;/p>
&lt;img src="../images/不动点示意图.png" alt="不动点示意图.png" />
&lt;center>
图1 [0,1]区间上的自变换函数的不动点
&lt;/center>
&lt;p>那么，这种现象到底具有多大的一般性意义呢？数学家Brouwer在很久以前就注意到这一现象，他得出了如下的一般性定理，即著名的Brouwer不动点定理。&lt;/p>
&lt;blockquote>
&lt;p>定理1(Brouwer不动点定理)：设&lt;span class="math">\(f(x)\)&lt;/span>是定义在集合&lt;span class="math">\(X\subset \mathbf{R^n}\)&lt;/span>上的实函数，且&lt;span class="math">\(f(x)\in X, \forall x\in X\)&lt;/span>。如果&lt;span class="math">\(f(x)\)&lt;/span>是&lt;strong>连续&lt;/strong>的，&lt;span class="math">\(X\)&lt;/span>为一&lt;strong>非空的有界凸闭集(非空紧凸集)&lt;/strong>，则至少存在一个&lt;span class="math">\(x^\star \in X\)&lt;/span>使&lt;span class="math">\(f(x^\star)=x^\star\)&lt;/span>。即&lt;span class="math">\(f(x)\)&lt;/span>至少存在一个不动点&lt;span class="math">\(^{[1]}\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>有意思的是，Brouwer不动点定理存在很强的几何直观&lt;span class="math">\(^{[2]}\)&lt;/span>，但其数学证明却十分艰深，需要动用代数拓扑这类就是职业数学家也感到望而生畏的超级抽象数学工具&lt;span class="math">\(^{[3]}\)&lt;/span>。在此，我们不给出Brouwer不动点定理的证明。&lt;/p>
&lt;p>&lt;strong>直接用来证明纳什存在性定理的不动点定理还不是Brouwer不动点定理，而是角谷静夫(Kakutani)不动点定理&lt;/strong>，而后者的证明只是前者的一个相对简单的运用。&lt;/p>
&lt;p>我们所以要引用角谷静夫不动点定理，是因为在纳什均衡存在性证明中所遇到的&lt;strong>反应函数一般是多个因变量函数&lt;/strong>，即所谓&lt;strong>对应(correspondence)&lt;/strong>，而角谷静夫不动点定理正好描述的是对应的一种性质。角谷静夫不动点定理是Brouwer不动点定理的推广，但其自身的证明要用到Brouwer不动点定理。我们在这里不打算给出这两个不动点定理的证明，因为这类证明只是一种纯数学过程，但我们将给出纳什存在性定理的一种证明，因为了解存在性定理的证明过程有助于我们更好地理解纳什均衡。&lt;/p>
&lt;p>为了解读角谷静夫不动点定理，我们先来准备一下一些有关的数学概念。&lt;/p>
&lt;h3 id="角谷静夫不动点定理">角谷静夫不动点定理&lt;/h3>
&lt;p>对于任一有限集&lt;span class="math">\(M\)&lt;/span>，我们用&lt;span class="math">\(R^M\)&lt;/span>表示形如&lt;span class="math">\(x=(x_m)_{m\in M}\)&lt;/span>的所有&lt;span class="math">\(M\)&lt;/span>维向量组成的集合，其中对&lt;span class="math">\(M\)&lt;/span>中每一个&lt;span class="math">\(m\)&lt;/span>，第&lt;span class="math">\(m\)&lt;/span>个分量&lt;span class="math">\(x_m\)&lt;/span>是实数域&lt;span class="math">\(\mathcal R\)&lt;/span>的一个元素。为方便计，我们也可将&lt;span class="math">\(R^M\)&lt;/span>等价地理解为&lt;span class="math">\(M\)&lt;/span>到&lt;span class="math">\(\mathcal R\)&lt;/span>上的&lt;strong>所有函数&lt;/strong>组成的集合，这时&lt;span class="math">\(R^M\)&lt;/span>中&lt;span class="math">\(x\)&lt;/span>的&lt;span class="math">\(m\)&lt;/span>分量&lt;span class="math">\(x_m\)&lt;/span>也可被记为&lt;span class="math">\(x(m)\)&lt;/span>。&lt;/p>
&lt;h2 id="纳什存在性定理及其证明">纳什存在性定理及其证明&lt;/h2>
&lt;h2 id="其它的纳什均衡存在性定理">其它的纳什均衡存在性定理&lt;/h2>
&lt;h2 id="注释">注释&lt;/h2>
&lt;p>【1】 这个定理的表述中隐含了&lt;span class="math">\(X\)&lt;/span>为一个度量空间，所谓度量空间，即在空间&lt;span class="math">\(X\)&lt;/span>上定义了一个“距离”函数&lt;span class="math">\(||\cdot||\)&lt;/span>，使得对任意的&lt;span class="math">\(x_1,x_2\in X\)&lt;/span>都有&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>三角不等式&lt;span class="math">\(||x_1+x_2||\leq ||x_1||+||x_2||\)&lt;/span>，(意思是三角形的两边之和大于第三边)&lt;/li>
&lt;li>&lt;span class="math">\(||x||\geq 0, \forall x\in X\)&lt;/span>；同时还有&lt;span class="math">\(||x||=0\)&lt;/span>当且仅当&lt;span class="math">\(x=0\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>当然，这种定义又要求在空间&lt;span class="math">\(X\)&lt;/span>上首先定义了一种加法“+”和“零”元素。一般地，度量空间的形式化定义为：集合&lt;span class="math">\(X\)&lt;/span>上的“距离”指&lt;span class="math">\(X\times X\)&lt;/span>到实数轴&lt;span class="math">\(\mathcal R\)&lt;/span>上的一个函数 ，满足：对&lt;span class="math">\(X\)&lt;/span>中任意的&lt;span class="math">\(x,y\)&lt;/span>和&lt;span class="math">\(z\)&lt;/span>，有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>(对称性)&lt;span class="math">\(\delta(x,y)=\delta(y,x)\)&lt;/span>&lt;/li>
&lt;li>(非负性)&lt;span class="math">\(\delta(x,y)=0\)&lt;/span>当且仅当&lt;span class="math">\(x=y\)&lt;/span>&lt;/li>
&lt;li>(三角不等式)&lt;span class="math">\(\delta(x,y)+\delta(y,z)\geq \delta(x,z)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>【2】 一些几何例子比如：取两张一样大小的白纸，在上面画好垂直的坐标系以及纵横的方格。将一张纸平铺在桌面，而另外一张随意揉成一个形状（但不能撕裂），放在第一张白纸之上，不超出第一张的边界。那么第二张纸上一定有一点正好就在第一张纸的对应点的正上方。一个更简单的说法是：将一张白纸平铺在桌面上，再将它揉成一团（不撕裂），放在原来白纸所在的地方，那么只要它不超出原来白纸平铺时的边界，那么白纸上一定有一点在水平方向上没有移动过。这个断言的根据就是布劳威尔不动点定理在二维欧几里得空间（欧几里得平面）的情况，因为把纸揉皱是一个连续的变换过程。&lt;/p>
&lt;p>另一个例子是大商场等地方可以看到的平面地图，上面标有“您在此处”的红点。如果标注足够精确，那么这个点就是把实际地形射到地图的连续函数的不动点。&lt;/p>
&lt;p>三维空间中的情况：如果我们用一个密封的锅子煮水，那么总有一个水分子在煮开前的某一刻和煮开后的某一刻处于同样的位置。地球绕着它的自转轴自转。自转轴在自转过程中是不变的，也就是自转运动的不动点。&lt;/p>
&lt;p>【3】某些数学家声称已找到Brouwer不动点定理的初等证明，但从严格的数学证明所要求的严密程度看，这类“证明”，并非真正数学意义上的证明，同时，它们还十分繁锁。&lt;/p></description></item><item><title>机器学习-七种回归分析</title><link>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%83%E7%A7%8D%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/</link><pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%83%E7%A7%8D%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/</guid><description>
&lt;h2 id="七种回归分析">七种回归分析&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#什么是回归分析">什么是回归分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#为什么要用回归分析--预测">为什么要用回归分析？--&amp;gt;预测&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归分析的种类">回归分析的种类&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性回归">线性回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#着重强调">着重强调&lt;/a>&lt;/li>
&lt;li>&lt;a href="#logistic回归">Logistic回归&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#它是如何工作的">它是如何工作的&lt;/a>&lt;/li>
&lt;li>&lt;a href="#注意点">注意点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#何时适用">何时适用&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#polynomial_regression">Polynomial_Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#stepwise_regression">Stepwise_Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#ridge_regression">Ridge_Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lasso_regression">Lasso_Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#elasticnet_regression">ElasticNet_Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#如何选择正确的回归分析算法">如何选择正确的回归分析算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#参考文献">参考文献&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="什么是回归分析">什么是回归分析&lt;/h2>
&lt;p>回归分析是一种预测模型来研究因变量和自变量（们）之间的关系。例如，研究开快车和路上交通事故直接的关系。回归分析也是一种重要的数据建模与分析工具。我们想找到一条曲线/直线，尽可能的通过各个点，使线和点之间的总误差尽可能小。&lt;/p>
&lt;h2 id="为什么要用回归分析--预测">为什么要用回归分析？--&amp;gt;预测&lt;/h2>
&lt;p>如上所述，回归分析是一种估计两个或多个变量之间关系的。比如，我们想基于现在的经济状况估计一个公司的销售情况。根据近期的公司数据，销量的增长大约是经济增长的2.5倍。这样，我们可以依靠现有和过去的信息预测未来公司的销量。&lt;/p>
&lt;p>使用回归分析的优点有两种：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>发现自变量和因变量之间的显著关系&lt;/li>
&lt;li>发现各个自变量对因变量的影响程度&lt;/li>
&lt;/ol>
&lt;h2 id="回归分析的种类">回归分析的种类&lt;/h2>
&lt;p>有许多种回归分析可以用来做预测？这些技术通常由三个维度驱动（自变量的数量，因变量的类型和回归曲线的形状），如下所示：&lt;/p>
&lt;img src="../images/regression_classification.png" alt="回归分析的种类" />
&lt;center>
图1 回归分析的种类
&lt;/center>
&lt;p>如果你富有创造性，也可以创造出新的回归方式，综合上面多种参数。在此之前，让我们来了解最常用的回归方式:&lt;/p>
&lt;h3 id="线性回归">线性回归&lt;/h3>
&lt;p>线性回归是最广为人知的建模技术，也是人们最先选择的回归预测模型。在这个技术中，自变量可以是离散的，也可以是连续的，但是&lt;strong>因变量必须是连续的&lt;/strong>。而且双方是一种线性关系。&lt;/p>
&lt;p>线性回归使用最佳的拟合&lt;strong>直线&lt;/strong>（也就是回归线）在&lt;strong>因变量（Y）&lt;/strong>和一个或多个&lt;strong>自变量（X(s)）&lt;/strong>之间建立一种&lt;strong>线性&lt;/strong>关系。&lt;/p>
&lt;p>这种关系可以用&lt;span class="math">\(Y=a+b*x+e\)&lt;/span>来表示。其中，a是截距，b是直线的斜率，e是误差项。这个公式可以通过给定的一些预测参数来预测目标值。&lt;/p>
&lt;img src="../images/linear_regression.png" alt="图2 线性回归" />
&lt;center>
图2 线性回归
&lt;/center>
&lt;p>单线性回归和多线性回归的区别是：多线性回归有多余1个自变量，而单线性回归只有一个自变量。现在的问题是“如何最适曲线？”。&lt;/p>
&lt;p>如何最适曲线？&lt;/p>
&lt;p>答案是寻找最小均方误差，这是回归分析最常用的方式。 &lt;span class="math">\[min\|Xw-y\|^2_2\]&lt;/span> 我们可以通过R-square来衡量模型的性能。更多的细节可以参考这篇文章&lt;a href="https://www.analyticsvidhya.com/blog/2015/01/model-performance-metrics-classification/">《11 Important Model Evaluation Metrics for Machine Learning Everyone should know》&lt;/a>&lt;/p>
&lt;h3 id="着重强调">着重强调&lt;/h3>
&lt;ul>
&lt;li>自变量和因变量之间必须要有线性关系！&lt;/li>
&lt;li>多变量回归容易收到&lt;strong>多重共线性，自相关和异方差性影响&lt;/strong>&lt;/li>
&lt;li>线性回归对&lt;strong>异常值很敏感&lt;/strong>，它会对线性回归和最终的预测值产生糟糕的影响&lt;/li>
&lt;li>多重共线性会增加系数估计的方差，使模型对微小的变动十分敏感。结果会导致系数估计不稳定&lt;/li>
&lt;li>在多个独立自变量的场景下，我们可以使用前向选择，后向消除和逐步法来选择最显著的独立变量&lt;/li>
&lt;/ul>
&lt;h3 id="logistic回归">Logistic回归&lt;/h3>
&lt;p>Logistic回归是用来区分Event=Success或Event=Failure概率的方法。我们在因变量是&lt;strong>二元变量&lt;/strong>（0/1，True/False，Yes/No）的回归分析中可以使用Logistic回归。&lt;/p>
&lt;p>Logistic回归也是从统计学中借鉴来的，尽管名字里有回归俩字儿，但它不是一个需要预测连续结果的回归算法。与之相反，Logistic 回归是二分类任务的首选方法。它输出一个 0 到 1 之间的离散二值结果。简单来说，它的结果不是 1 就是 0。癌症检测算法可看做是 Logistic回归问题的一个简单例子，这种算法输入病理图片并且应该辨别患者是患有癌症（1）或没有癌症（0）。&lt;/p>
&lt;h4 id="它是如何工作的">它是如何工作的&lt;/h4>
&lt;p>Logistic 回归通过使用其固有的 logistic 函数估计概率，来衡量因变量（我们想要预测的标签）与一个或多个自变量（特征）之间的关系。&lt;/p>
&lt;p>然后这些概率必须二值化才能真地进行预测。这就是 logistic 函数的任务，也称为 Sigmoid 函数。Sigmoid 函数是一个 S 形曲线，它可以将任意实数值映射到介于 0 和 1 之间的值，但并不能取到 0或1。然后使用阈值分类器将 0 和 1 之间的值转换为 0 或 1。&lt;/p>
&lt;p>下面的图片说明了 logistic 回归得出预测所需的所有步骤。&lt;/p>
&lt;img src="../images/logistic_regression.png" alt="logistic回归步骤图" />
&lt;center>
图3 logistic回归步骤图
&lt;/center>
&lt;p>下面是 logistic 函数（sigmoid 函数）的图形表示：&lt;/p>
&lt;img src="../images/sigmoid.png" alt="sigmoid 函数" />
&lt;center>
图4 Sigmoid函数
&lt;/center>
&lt;h4 id="注意点">注意点&lt;/h4>
&lt;ol style="list-style-type: decimal">
&lt;li>常被用于分类问题。&lt;/li>
&lt;li>Logistic回归不需要自变量和因变量有线性关系，因为是非线性的log变换。&lt;/li>
&lt;li>为了避免过拟合和欠拟合，我们应该包括所有重要的变量。确保这一过程的一个好方法是使用逐步的方式来估计Logistic回归。&lt;/li>
&lt;/ol>
&lt;h4 id="何时适用">何时适用&lt;/h4>
&lt;p>就像我已经提到的那样，Logistic 回归通过线性边界将你的输入分成两个「区域」，每个类别划分一个区域。因此，你的数据应当是线性可分的，如下图所示的数据点：&lt;/p>
&lt;img src="../images/Logistic回归线性可分.png" alt="Logistic回归线性可分" />
&lt;center>
图5 Logistic回归线性可分
&lt;/center>
&lt;p>换句话说：当 Y 变量只有两个值时（例如，当你面临分类问题时），您应该考虑使用逻辑回归。注意，你也可以将 Logistic 回归用于多类别分类。其他常见的分类算法有朴素贝叶斯、决策树、随机森林、支持向量机、k-近邻等等。我们将在其他文章中讨论它们，但别被这些机器学习算法的数量吓到。请注意，最好能够真正了解 4 或 5 种算法，并将精力集中在特征处理上，这也是未来工作的主题。&lt;/p>
&lt;h3 id="polynomial_regression">Polynomial_Regression&lt;/h3>
&lt;p>多项式回归。如果回归方程中自变量的阶数大于1，为多项式形式，则为多项式回归。&lt;/p>
&lt;blockquote>
&lt;p>e.g. &lt;span class="math">\(y=a+bx^2\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这种回归得到是曲线，而不像线性回归是直线。多项式回归可以通过提高多项式的阶数来更好的拟合曲线。但是如果阶数（特征）设置的太多，特别是当特征数和训练点数相仿时，&lt;strong>拟合就趋近于插值&lt;/strong>。对于训练集，曲线将会特别好的符合，但是无法泛化的测试集中。这种现象被称为&lt;strong>过拟合&lt;/strong>。过拟合可以通过降低多项式级数（减少特征数）或者正规化来解决。&lt;/p>
&lt;p>欠拟合，拟合与过拟合可以由图6表示。&lt;/p>
&lt;embed src="../images/underfitting-overfitting.webp" />
&lt;center>
图6 欠拟合，拟合与过拟合
&lt;/center>
&lt;h3 id="stepwise_regression">Stepwise_Regression&lt;/h3>
&lt;p>逐步回归。与平时所说的 regression analysis 不太相同，stepwise regression 可以算是一种 feature extraction 的方法。&lt;/p>
&lt;p>举个例子，假如我们的数据中有一个因变量，但却有十几或几十个自变量。为了便于对变量数过多的数据进行处理，避免 “curse of dimensionality” 中可能出现的种种问题，我们总是会对数据进行降维，根据在特定领域中的知识或是理论假设，选择其中一些可能更有意义的变量进行后续分析。但不是任何情况下我们都掌握这些先验信息，所以基于数据本身的特征提取方法应运而生。&lt;/p>
&lt;p>在 stepwise regression 中，提取哪些变量主要基于的假设是：在线性条件下，哪些变量组合能够解释更多的因变量变异，则将其保留。&lt;/p>
&lt;p>具体操作方法有三种：&lt;/p>
&lt;p>Forward selection: 首先模型中只有一个单独解释因变量变异最大的自变量，之后尝试将加入另一自变量，看加入后整个模型所能解释的因变量变异是否显著增加（这里需要进行检疫，可以用 F-test， t-test 等等）；这一过程反复迭代，直到没有自变量再符合加入模型的条件。&lt;/p>
&lt;p>Backward elimination: 与 Forward selection 相反，此时，所有变量均放入模型，之后尝试将其中一个自变量从模型中剔除，看整个模型解释因变量的变异是否有显著变化，之后将使解释量减少最少的变量剔除；此过程不断迭代，直到没有自变量符合剔除的条件。&lt;/p>
&lt;p>Bidirectional elimination: 这种方法相当于将前两种结合起来。可以想象，如果采用第一种方法，每加入一个自变量，可能会使已存在于模型中的变量单独对因变量的解释度减小，当其的作用很小（不显著）时，则可将其从模型中剔除。而第三种方法就做了这么一件事，不是一味的增加变量，而是增加一个后，对整个模型中的所有变量进行检验，剔除作用不显著的变量。最终尽可能得到一个最优的变量组合。&lt;/p>
&lt;p>可以想象，这样得到的变量组合，基于当前数据，应该是可以最大程度的解释因变量的变异，但其反面的作用就是会使模型有偏，即所谓的 overfitting 问题；另外，鉴于算法是基于变量解释度来进行特征提取的，当两个变量对因变量的影响相近时，则不免受到较大的噪声影响，使特征提取结果不稳定。&lt;/p>
&lt;h3 id="ridge_regression">Ridge_Regression&lt;/h3>
&lt;p>当使用最小二乘法计算线性回归模型参数的时候，如果数据集合矩阵（也叫做设计矩阵(design matrix)）X，存在多重共线性，那么最小二乘法对输入变量中的噪声非常的敏感，其解会极为不稳定。为了解决这个问题，就有了这一节脊回归（Ridge Regression ）。&lt;/p>
&lt;p>当设计矩阵&lt;span class="math">\(X\)&lt;/span>存在多重共线性的时候（数学上称为病态矩阵），最小二乘法求得的参数&lt;span class="math">\(w\)&lt;/span>在数值上会非常的大，而一般的线性回归其模型是&lt;span class="math">\(y=w^Tx\)&lt;/span>，显然，就是因为&lt;span class="math">\(w\)&lt;/span>在数值上非常的大，所以，如果输入变量&lt;span class="math">\(x\)&lt;/span>有一个微小的变动，其反应在输出结果上也会变得非常大，这就是对输入变量总的噪声非常敏感的原因。&lt;/p>
&lt;p>如果能限制参数&lt;span class="math">\(w\)&lt;/span>的增长，使&lt;span class="math">\(w\)&lt;/span>不会变得特别大，那么模型对输入&lt;span class="math">\(w\)&lt;/span>中噪声的敏感度就会降低。这就是脊回归和套索回归（Ridge Regression and Lasso Regrission）的基本思想。&lt;/p>
&lt;p>为了限制模型参数&lt;span class="math">\(w\)&lt;/span>的数值大小，就在模型原来的目标函数上加上一个惩罚项，这个过程叫做正则化（Regularization）。&lt;/p>
&lt;p>如果惩罚项是参数的&lt;span class="math">\(l_2\)&lt;/span>范数，就是脊回归(Ridge Regression)&lt;/p>
&lt;p>如果惩罚项是参数的&lt;span class="math">\(l_1\)&lt;/span>范数，就是套索回归（Lasso Regrission）&lt;/p>
&lt;p>正则化同时也是防止过拟合有效的手段，这在“多项式回归”中有详细的说明。&lt;/p>
&lt;p>所谓脊回归，就是对于一个线性模型，&lt;strong>在原来的损失函数加入参数的&lt;span class="math">\(l_2\)&lt;/span>范数的惩罚项&lt;/strong>，其损失函数为如下形式: &lt;span class="math">\[J_w=\min_w{\|X-w\|^2+\alpha\|w\|^2},\alpha&amp;gt;0\]&lt;/span> 由于&lt;span class="math">\(w\)&lt;/span>最小化整个式子，如果&lt;span class="math">\(w\)&lt;/span>本身特别大，第二项就会很大，这样就限制了&lt;span class="math">\(w\)&lt;/span>的大小。&lt;/p>
&lt;p>&lt;span class="math">\(α\)&lt;/span>的数值越大，那么正则项，也是惩罚项的作用就越明显；&lt;span class="math">\(α\)&lt;/span>的数值越小，正则项的作用就越弱。极端情况下，&lt;span class="math">\(α=0\)&lt;/span>则和原来的损失函数是一样的，如果&lt;span class="math">\(α=∞\)&lt;/span>，则损失函数只有正则项，此时其最小化的结果必然是&lt;span class="math">\(w=0\)&lt;/span>。&lt;/p>
&lt;p>之前，我们根据线性回归判断参数的解为： &lt;span class="math">\[w=(X^TX)^{-1}X^Ty\]&lt;/span> 同理，脊回归的损失函数为： &lt;span class="math">\[\|Xw-y\|^2+\alpha\|w\|^2=(Xw-y)^T(Xw-y)+\alpha w^T w\]&lt;/span> 对于参数&lt;span class="math">\(w\)&lt;/span>求导之后，极值为0： &lt;span class="math">\[X^T(Xw-y)+X^T(Xw-y)+2\alpha w=0\]&lt;/span> 解得： &lt;span class="math">\[w=(X^T X+\alpha I)^{-1}X^Ty\]&lt;/span>&lt;/p>
&lt;h3 id="lasso_regression">Lasso_Regression&lt;/h3>
&lt;p>Lasso回归全称是(Least Absolute Shrinkage and Selection Operator)，它使用L1范数作为惩罚项。 &lt;span class="math">\[\|Xw-y\|^2+\alpha|w|\]&lt;/span>&lt;/p>
&lt;h3 id="elasticnet_regression">ElasticNet_Regression&lt;/h3>
&lt;p>弹性网络是一种使用 L1，L2范数作为先验正则项训练的线性回归模型.这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 Lasso一样，但是它仍然保持一些像Ridge的正则性质。我们可利用 l1_ratio 参数控制L1和L2的凸组合。弹性网络是一不断叠代的方法。&lt;/p>
&lt;h2 id="如何选择正确的回归分析算法">如何选择正确的回归分析算法&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>数据探索是构建预测模型的必然部分。这是选择正确的模型（例如确定变量之间的关系和影响）之前的第一步。&lt;/li>
&lt;li>为了比较不同模型的适用性，我们需要不同维度的分析，例如参数的统计特性，R-square, Adjusted r-square, AIC, BIC和误差项等。另一个方法是使用Mallow’s Cp 准则。 This essentially checks for possible bias in your model, by comparing the model with all possible submodels (or a careful selection of them).&lt;/li>
&lt;li>Cross-validation is the best way to evaluate models used for prediction. Here you divide your data set into two group (train and validate). A simple mean squared difference between the observed and predicted values give you a measure for the prediction accuracy.&lt;/li>
&lt;li>If your data set has multiple confounding variables, you should not choose automatic model selection method because you do not want to put these in a model at the same time.&lt;/li>
&lt;li>It’ll also depend on your objective. It can occur that a less powerful model is easy to implement as compared to a highly statistically significant model.&lt;/li>
&lt;li>Regression regularization methods(Lasso, Ridge and ElasticNet) works well in case of high dimensionality and multicollinearity among the variables in the data set.&lt;/li>
&lt;/ol>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>[1] 7 Regression Techniques you should know!&lt;a href="https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/">https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/&lt;/a>&lt;/p></description></item><item><title>优化理论之-凸优化-拟凸函数</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B-%E5%87%B8%E4%BC%98%E5%8C%96-%E6%8B%9F%E5%87%B8%E5%87%BD%E6%95%B0/</link><pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B-%E5%87%B8%E4%BC%98%E5%8C%96-%E6%8B%9F%E5%87%B8%E5%87%BD%E6%95%B0/</guid><description>
&lt;h2 id="拟凸函数">拟凸函数&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#下水平集与上水平集">下水平集与上水平集&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拟凸函数引入">拟凸函数引入&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拟凸函数与凸函数联系">拟凸函数与凸函数联系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#扩展拟凹函数拟线性函数">扩展：拟凹函数，拟线性函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拟凸函数与jensen不等式">拟凸函数与Jensen不等式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拟凸优化问题">拟凸优化问题&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>我也有&lt;strong>全局最小值&lt;/strong>呦~~&lt;/p>
&lt;h2 id="下水平集与上水平集">下水平集与上水平集&lt;/h2>
&lt;blockquote>
&lt;p>&lt;span class="math">\(\alpha-\)&lt;/span>下水平集(sub-level set)：&lt;span class="math">\(f:R^n→R, C_\alpha-=\{x|x\in \mathop{dom}f \cap f(x)\leq \alpha\}\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math">\(\alpha-\)&lt;/span>上水平集(super-level set)：&lt;span class="math">\(f:R^n→R, S_\alpha=\{x|x\in \mathop{dom}f \cap f(x)\geq \alpha\}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>如果函数&lt;span class="math">\(f\)&lt;/span>是&lt;strong>凸&lt;/strong>函数，则&lt;span class="math">\(f\)&lt;/span>的&lt;span class="math">\(\alpha-\)&lt;/span>&lt;strong>下水平集是凸集&lt;/strong>。对应的，如果函数&lt;span class="math">\(f\)&lt;/span>是&lt;strong>凹&lt;/strong>函数，则&lt;span class="math">\(f\)&lt;/span>的&lt;span class="math">\(\alpha-\)&lt;/span>&lt;strong>上水平集是凸集&lt;/strong>。&lt;/p>
&lt;p>逆命题不成立，例如：&lt;span class="math">\(f(x)=-e^x\)&lt;/span>的&lt;span class="math">\(\alpha-\)&lt;/span>下水平集是凸集，但是&lt;span class="math">\(f(x)=-e^x\)&lt;/span>是凹函数。&lt;/p>
&lt;h2 id="拟凸函数引入">拟凸函数引入&lt;/h2>
&lt;blockquote>
&lt;p>标准定义（定义1）&lt;span class="math">\(f:R^n→R, \mathop{dom}f\)&lt;/span>是凸集，&lt;span class="math">\(\alpha-\)&lt;/span>下水平集是凸集，则&lt;span class="math">\(f(x)\)&lt;/span>为拟凸函数。即 &lt;span class="math">\[f:R^n→R,s.t.\, S_\alpha=\{x\in \mathop{dom} f| f(x)\geq \alpha\}为凸集\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>解释:对于任意的&lt;span class="math">\(\alpha\)&lt;/span>一下的值(像),其对应的原像集合都是凸集。根据第一节中的例子，我们发现&lt;strong>凹函数也可能是拟凸的&lt;/strong>。&lt;/p>
&lt;img src="../images/Quasiconvex_function.png" alt="Quasiconvex_function.png" />
&lt;center>
这个函数不是凸的，但是是拟凸的
&lt;/center>
&lt;p>观察上图二维图像：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(f(x)\)&lt;/span>既不是非增也不是非减&lt;/li>
&lt;li>有全局最小值时（可能是点也可能是集合），&lt;strong>左侧非增，右侧非减&lt;/strong>。&lt;/li>
&lt;/ol>
&lt;h3 id="拟凸函数与凸函数联系">拟凸函数与凸函数联系&lt;/h3>
&lt;p>凸函数&lt;span class="math">\(\Rightarrow\)&lt;/span>拟凸函数&lt;/p>
&lt;p>拟凸函数&lt;strong>不一定&lt;/strong>是凸函数，甚至可能是凹函数。&lt;/p>
&lt;h2 id="扩展拟凹函数拟线性函数">扩展：拟凹函数，拟线性函数&lt;/h2>
&lt;blockquote>
&lt;p>&lt;span class="math">\(f\)&lt;/span>是拟凸函数，我们称&lt;span class="math">\(-f\)&lt;/span>为&lt;strong>拟凹&lt;/strong>函数,拟凹函数的&lt;span class="math">\(\alpha-\)&lt;/span>&lt;strong>上水平集是凸集&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;hr />
&lt;blockquote>
&lt;p>拟线性函数：既是拟凸也是拟凹。等同于&lt;span class="math">\(f:R^n→R,s.t.\, S_\alpha=\{x\in \mathop{dom} f| f(x)=\alpha\}为凸集\)&lt;/span>（类似线性函数是既凸又凹）&lt;/p>
&lt;/blockquote>
&lt;p>一般情况下，拟线性函数是一个非增或非减函数。&lt;/p>
&lt;h2 id="拟凸函数与jensen不等式">拟凸函数与Jensen不等式&lt;/h2>
&lt;h2 id="拟凸优化问题">拟凸优化问题&lt;/h2></description></item><item><title>优化理论之KKT与拉格朗日</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8Bkkt%E4%B8%8E%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8Bkkt%E4%B8%8E%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5/</guid><description>
&lt;h2 id="拉格朗日乘子法lagrange-multiplier-和kkt条件">拉格朗日乘子法（Lagrange Multiplier) 和KKT条件&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#应用场景及使用">应用场景及使用&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉格朗日乘子法与等式条件">拉格朗日乘子法与等式条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kkt条件与强对偶">KKT条件与强对偶&lt;/a>&lt;/li>
&lt;li>&lt;a href="#原始问题">原始问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶问题">对偶问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#原始问题与对偶问题的关系">原始问题与对偶问题的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kkt条件">KKT条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总结">总结&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="应用场景及使用">应用场景及使用&lt;/h2>
&lt;p>拉格朗日乘子法（Lagrange Multiplier) 和KKT条件是求解有约束优化问题非常重要的两个求取方法。对于等式约束的优化问题，可以应用拉格朗日乘子法去求取最优值；如果含有不等式约束，可以应用KKT条件去求取。当然，这两个方法求出的点只满足必要条件（也就是常说的极值点不一定是最值点，但最值点一定是极值点），当目标函数是凸函数时可转化为充分必要条件。（凸函数对应min类优化问题）&lt;/p>
&lt;p>通常我们需要求解的最优化问题有如下几类：&lt;/p>
&lt;p>（1）无约束优化问题，形如 &lt;span class="math">\[\min f(x)\]&lt;/span> （2）有等式约束的优化问题，形如 &lt;span class="math">\[\min f(x)\\
s.t.\quad h_i(x)=0;i=1,2,\dotsb\]&lt;/span> （3）有不等式约束的优化问题，形如 &lt;span class="math">\[\min f(x)\\
s.t.\quad h_i(x)=0;i=1,2,\dotsb\\
\qquad g_j(x)\le 0;j=1,2,\dotsb\\\]&lt;/span>&lt;/p>
&lt;p>对于第(1)类的优化问题，常常使用的方法就是Fermat引理，即使用求取&lt;span class="math">\(f(x)\)&lt;/span>的导数，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。&lt;/p>
&lt;p>对于第(2)类问题，利用拉格朗日系数将约束与目标函数按如下形式组合： &lt;span class="math">\[L(a,x)=f(x)+\sum_{i=1}^n a_i h_i(x),其中x是向量\]&lt;/span> 然后求取最优值， &lt;span class="math">\(L(a,x)对x\)&lt;/span>求导取零，联立&lt;span class="math">\(h_i(x)=0(a_i≠0)\)&lt;/span>进行求取，这个在高等数学里面有讲，但是没有讲为什么这么做就可以，在后面，将简要介绍其思想。&lt;/p>
&lt;p>对于第(3)类问题，同样将所有条件和目标函数写成一个式子： &lt;span class="math">\[L(a,b,x)=f(x)+\sum_{i=1}^m a_i g_i(x)+\sum_{j=1}^n b_j h_j(x)\]&lt;/span> 利用KKT条件求解最优值，&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(L(a,b,x)\)&lt;/span>对&lt;span class="math">\(x\)&lt;/span>求导为0&lt;/li>
&lt;li>&lt;span class="math">\(h_j(x)=0;j=1,2,\dotsb\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(g_i(x)\le 0;i=1,2,\dotsb\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(a_i \ge 0;b_j \neq 0\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(a_i g_i(x)=0\)&lt;/span>(互补松弛)&lt;/li>
&lt;/ol>
&lt;p>其中第4个式子是SVM很多重要性质的来源，如支持向量的概念。&lt;/p>
&lt;h2 id="拉格朗日乘子法与等式条件">拉格朗日乘子法与等式条件&lt;/h2>
&lt;p>为什么拉格朗日乘子法（Lagrange Multiplier)能够得到最优值？&lt;/p>
&lt;p>设我们的目标函数为&lt;span class="math">\(z=f(x)\)&lt;/span>，&lt;span class="math">\(x\)&lt;/span>为向量，将&lt;span class="math">\(z\)&lt;/span>投影到自变量&lt;span class="math">\(x\)&lt;/span>构成的平面上，即形成等高线。如下图，目标函数是&lt;span class="math">\(f(x,y)\)&lt;/span>，这里&lt;span class="math">\(x，y\)&lt;/span>是标量，虚线是等高线，箭头为导数方向，显然范围越小的等高线越接近无约束条件下的最值。现在假设我们的约束&lt;span class="math">\(g(x)=0\)&lt;/span>，&lt;span class="math">\(x\)&lt;/span>是向量，在&lt;span class="math">\(x\)&lt;/span>构成的平面或者曲面上是一条曲线，假设&lt;span class="math">\(g(x)\)&lt;/span>与等高线相交，交点就是同时满足等式约束条件和目标函数的可行域的值。但肯定不是最优值，&lt;strong>因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与约束函数的交点的值更大或者更小&lt;/strong>，也就是存在两种变化方向的可能性。而只有当等高线与约束条件&lt;strong>相切&lt;/strong>时，可能取值最优值，即此时单向不可更优。如下图1所示，等高线与约束函数的曲线在该点的法向量必须有相同方向，所以最优点满足：&lt;span class="math">\(f(x)的梯度=a_i∗g_i(x)的梯度;i=1,…,n\)&lt;/span>&lt;/p>
&lt;img src="../images/拉格朗日乘子法.jpg" alt="拉格朗日乘子法原理" />
&lt;center>
图1 拉格朗日乘子法原理
&lt;/center>
&lt;h2 id="kkt条件与强对偶">KKT条件与强对偶&lt;/h2>
&lt;p>KKT条件是满足强对偶条件的优化问题的必要条件，需要结合拉格朗日对偶（Lagrange duality）进行理解。&lt;/p>
&lt;h3 id="原始问题">原始问题&lt;/h3>
&lt;p>假设&lt;span class="math">\(f(x),c_i(x),h_j(x)\)&lt;/span>是定义在&lt;span class="math">\(R^n\)&lt;/span>上的连续可微函数（为什么要求连续可微呢，后面再说，这里不用多想），考虑约束最优化问题： &lt;span class="math">\[\min_{x\in R^n} f(x)\\
s.t.\quad c_i(x)\le 0;i=1,2,\dotsb,k\\
\qquad h_j(x)=0;j=1,2,\dotsb,l\\\]&lt;/span> 称为约束最优化问题的原始问题。&lt;/p>
&lt;p>现在&lt;strong>如果不考虑约束条件&lt;/strong>，原始问题就是： &lt;span class="math">\[\min_{x\in R^n} f(x)\]&lt;/span> 因为假设其连续可微，利用高中的知识，对&lt;span class="math">\(f(x)\)&lt;/span>求导数，然后令导数为0，就可解出最优解，很easy. 那么，问题来了（呵呵。。。），偏偏有约束条件，好烦啊，要是能想办法把&lt;strong>约束条件去掉&lt;/strong>就好了，bingo! 拉格朗日函数就是干这个的。&lt;/p>
&lt;p>引进&lt;strong>广义拉格朗日函数&lt;/strong>（generalized Lagrange function）: &lt;span class="math">\[L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_i c_i(x)+\sum_{j=1}^l\beta_j h_j(x)\\
x=(x^{(1)},x^{(2)},\dotsb,x^{(n)})^T\in R^n\]&lt;/span> 不要怕这个式子，也不要被拉格朗日这个高大上的名字给唬住了，让我们慢慢剖析！这里&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>是拉格朗日乘子（名字高大上，其实就是上面函数中的参数而已），特别要求&lt;span class="math">\(\alpha_i&amp;gt;0\)&lt;/span>。&lt;/p>
&lt;p>现在，&lt;strong>如果把&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>看作是关于&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的函数&lt;/strong>，要求其最大值，即 &lt;span class="math">\[\max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\]&lt;/span> 再次注意&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>是一个关于&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的函数，经过我们优化（不要管什么方法），就是确定&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的值使得&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>取得最大值（此过程中把&lt;span class="math">\(x\)&lt;/span>看做常量），确定了&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的值，就可以得到&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>的最大值，因为&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>&lt;strong>已经确定&lt;/strong>，显然最大值&lt;span class="math">\(\max\limits_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\)&lt;/span>就是&lt;strong>只和&lt;span class="math">\(x\)&lt;/span>有关的函数&lt;/strong>，定义这个函数为： &lt;span class="math">\[\theta_p(x)=\max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\]&lt;/span>&lt;/p>
&lt;p>对于原式 &lt;span class="math">\[L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_i c_i(x)+\sum_{j=1}^l\beta_j h_j(x)\]&lt;/span> 通过&lt;span class="math">\(x\)&lt;/span>是否满足约束条件两方面来分析这个函数:&lt;/p>
&lt;p>(1).&lt;strong>考虑某个&lt;span class="math">\(x\)&lt;/span>违反了原始的约束&lt;/strong>，即&lt;span class="math">\(c_i(x)&amp;gt;0\)&lt;/span>或者&lt;span class="math">\(h_j(x)\neq 0\)&lt;/span>，那么： &lt;span class="math">\[\theta_p(x)=\max\limits_{\alpha,\beta,\alpha_i&amp;gt;0}[f(x)+\sum_{i=1}^k\alpha_i c_i(x)\\
+\sum_{j=1}^l\beta_j h_j(x)]=+\infty\]&lt;/span> 注意中间的最大化式子就是确定&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的之后的结果，若&lt;span class="math">\(c_i(x)&amp;gt;0\)&lt;/span>，则令&lt;span class="math">\(\alpha_i\rightarrow+\infty\)&lt;/span>，如果&lt;span class="math">\(h_j(x)\neq 0\)&lt;/span>，很容易取值&lt;span class="math">\(\beta_j\)&lt;/span>使得&lt;span class="math">\(\beta_j h_j(x)\rightarrow +\infty\)&lt;/span>。&lt;/p>
&lt;p>(2).&lt;strong>考虑&lt;span class="math">\(x\)&lt;/span>满足原始的约束&lt;/strong>，&lt;span class="math">\(c_i(x)\)&lt;/span>都是小于等于0的。如果&lt;span class="math">\(c_i(x)\)&lt;/span>严格小于0，那么我们必要让&lt;span class="math">\(\alpha_i\)&lt;/span>等于0，这时才能取最大值。等于0的情况就无所谓了，所以对于满足原始约束的情况，&lt;span class="math">\(\alpha_ic_i(x)=0，\)&lt;/span>即互补松弛条件。因此，&lt;span class="math">\(\theta_p(x)=\max\limits_{\alpha,\beta,\alpha_i&amp;gt;0}[f(x)]=f(x)\)&lt;/span>，注意中间的最大化是确定&lt;span class="math">\(\alpha_i,\beta_j\)&lt;/span>的过程，&lt;span class="math">\(f(x)\)&lt;/span>就是个常量，常量的最大值显然是本身。&lt;/p>
&lt;p>通过上面两条分析可以得出： &lt;span class="math">\[\theta_p(x)=
\begin{cases}
f(x), &amp;amp;x\text{满足原始问题约束}\\
+\infty, &amp;amp;\text{其他}
\end{cases}\]&lt;/span> 那么在满足约束条件下： &lt;span class="math">\[\min_x\theta_p(x)=\min_x \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\\
=\min_x f(x)\quad\]&lt;/span> 即&lt;span class="math">\(\min\limits_x\theta_p(x)\)&lt;/span>与原始优化问题等价,所以常用&lt;span class="math">\(\min\limits_x\theta_p(x)\)&lt;/span>代表原始问题，下标&lt;span class="math">\(p\)&lt;/span>表示原始问题，定义&lt;strong>原始问题的最优值&lt;/strong>： &lt;span class="math">\[p^\ast=\min_x\theta_p(x)\]&lt;/span> 原始问题讨论就到这里，做一个总结：&lt;strong>通过拉格朗日这位大神的办法重新定义一个无约束问题（大家都喜欢无拘无束），这个无约束问题等价于原来的约束优化问题，从而将约束问题无约束化！&lt;/strong>&lt;/p>
&lt;p>到这里，如果原问题是可解的，那么我们就可以使用求导的方式，求解问题。&lt;/p>
&lt;h3 id="对偶问题">对偶问题&lt;/h3>
&lt;p>现在我们先将&lt;span class="math">\(x\)&lt;/span>看做参数，定义关于&lt;span class="math">\(\alpha,\beta\)&lt;/span>的函数 &lt;span class="math">\[θ_D(\alpha,\beta)=\min_x L(x,\alpha,\beta)\]&lt;/span> 注意等式右边是关于&lt;span class="math">\(x\)&lt;/span>的函数的最小化，&lt;span class="math">\(x\)&lt;/span>确定以后，最小值就只与&lt;span class="math">\(\alpha,\beta\)&lt;/span>有关，所以是一个关于&lt;span class="math">\(\alpha,\beta\)&lt;/span>的函数。&lt;/p>
&lt;p>考虑极大化&lt;span class="math">\(θ_D(\alpha,\beta)=\min\limits_x L(x,\alpha,\beta)\)&lt;/span>，即 &lt;span class="math">\[\max_{\alpha,\beta:\alpha_i \ge0}θ_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i \ge0}\min\limits_x L(x,\alpha,\beta)\]&lt;/span> 这就是原始问题的对偶问题，再把原始问题写出来： &lt;span class="math">\[\min_x\theta_p(x)=\min_x \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\]&lt;/span> 形式上可以看出很对称，只不过原始问题是先固定&lt;span class="math">\(L(x,\alpha,\beta)\)&lt;/span>中的&lt;span class="math">\(x\)&lt;/span>，优化出参数&lt;span class="math">\(\alpha,\beta\)&lt;/span>，再优化最优&lt;span class="math">\(x\)&lt;/span>，而对偶问题是先固定&lt;span class="math">\(\alpha,\beta\)&lt;/span>，优化出最优&lt;span class="math">\(x\)&lt;/span>，然后再确定参数&lt;span class="math">\(\alpha,\beta\)&lt;/span>。&lt;/p>
&lt;p>定义对偶问题的最优值： &lt;span class="math">\[d^\ast=\max_{\alpha,\beta:\alpha_i \ge0}θ_D(\alpha,\beta)\]&lt;/span>&lt;/p>
&lt;h3 id="原始问题与对偶问题的关系">原始问题与对偶问题的关系&lt;/h3>
&lt;blockquote>
&lt;p>定理：若原始问题与对偶问题都有最优值，则 &lt;span class="math">\[d^\ast=\max_{\alpha,\beta:\alpha_i \ge0}\min\limits_x L(x,\alpha,\beta)\\
\le \min_x \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)=p^\ast\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明：对于任意的&lt;span class="math">\(\alpha,\beta,x\)&lt;/span>，有 &lt;span class="math">\[θ_D(\alpha,\beta)=\min_xL(x,\alpha,\beta) \le L(x,\alpha,\beta)\\
\le \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)=\theta_p(x)\\
\Rightarrow θ_D(\alpha,\beta)\le \theta_p(x)恒成立\]&lt;/span> 由于原始问题与对偶问题都有最优值，所以 &lt;span class="math">\[\max_{\alpha,\beta:\alpha_i \ge0}θ_D(\alpha,\beta) \le \min_x \theta_p(x)\]&lt;/span> 即 &lt;span class="math">\[d^\ast=\max_{\alpha,\beta:\alpha_i \ge0}\min\limits_x L(x,\alpha,\beta)\\
\le \min_x \max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)=p^\ast\]&lt;/span> 也就是说&lt;strong>原始问题的最优值不小于对偶问题的最优值&lt;/strong>，但是我们要通过对偶问题来求解原始问题，就必须使得&lt;strong>原始问题的最优值与对偶问题的最优值相等&lt;/strong>，于是可以得出下面的推论：&lt;/p>
&lt;blockquote>
&lt;p>推论：设&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>分别是原始问题和对偶问题的可行解，如果&lt;span class="math">\(d^\ast=p^\ast\)&lt;/span>，那么&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>分别是原始问题和对偶问题的最优解。&lt;/p>
&lt;/blockquote>
&lt;p>所以，当原始问题和对偶问题的最优值相等：&lt;span class="math">\(d^\ast=p^\ast\)&lt;/span>时，可以用&lt;strong>求解对偶问题来求解原始问题&lt;/strong>（当然是对偶问题求解比直接求解原始问题简单的情况下），但是到底满足什么样的条件才能使得&lt;span class="math">\(d^\ast=p^\ast\)&lt;/span>呢，这就是下面要阐述的&lt;strong>KKT&lt;/strong>条件。&lt;/p>
&lt;h3 id="kkt条件">KKT条件&lt;/h3>
&lt;p>强对偶性：存在&lt;span class="math">\(a^∗,b^∗,x^\ast\)&lt;/span>使得&lt;span class="math">\(d^∗=p^∗\)&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>定理：对于原始问题和对偶问题，假设函数&lt;span class="math">\(f(x)\)&lt;/span>和&lt;span class="math">\(c_i(x)\)&lt;/span>是凸函数，&lt;span class="math">\(h_j(x)\)&lt;/span>是仿射函数（即由一阶多项式构成的函数，&lt;span class="math">\(f(x)=Ax+b\)&lt;/span>,&lt;span class="math">\(A\)&lt;/span>是矩阵，&lt;span class="math">\(x,b\)&lt;/span>是向量）；并且假设不等式约束&lt;span class="math">\(c_i(x)\)&lt;/span>是严格可行的，即存在&lt;span class="math">\(x\)&lt;/span>，对所有&lt;span class="math">\(i\)&lt;/span>有&lt;span class="math">\(c_i(x)&amp;lt;0\)&lt;/span>，则存在&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>，使得&lt;span class="math">\(x^\ast\)&lt;/span>是原始问题的最优解，&lt;span class="math">\(\alpha^\ast,\beta^\ast\)&lt;/span>是对偶问题的最优解，并且 &lt;span class="math">\[d^\ast=p^\ast=L(x^\ast,\alpha^\ast,\beta^\ast)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>如果简单点说要求&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>分别是原始问题和对偶问题的最优解的&lt;strong>充分必要条件&lt;/strong>是&lt;span class="math">\(x^\ast,\alpha^\ast,\beta^\ast\)&lt;/span>满足下面的Karush-Kuhn-Tucker(KKT)条件: &lt;span class="math">\[
\nabla_x L(x^\ast,\alpha^\ast,\beta^\ast)=0\\
\alpha_i^\ast\ge 0;i=1,2,\dotsb,k\\
h_j(x^\ast)=0;j=1,2,\dotsb,l\\
\alpha_i^\ast c_i(x^\ast)=0;i=1,2,\dotsb,k\]&lt;/span>&lt;/p>
&lt;p>关于KKT 条件的理解：前面三个条件是由解析函数的知识，对于各个变量的偏导数为0（这就解释了一开始为什么假设三个函数连续可微，如果不连续可微的话，这里的偏导数存不存在就不能保证），后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束。&lt;/p>
&lt;p>特别注意,最后一个条件称为&lt;strong>KKT对偶互补条件&lt;/strong>。当&lt;span class="math">\(\alpha_i^\ast&amp;gt;0\)&lt;/span>时，由KKT对偶互补条件可知：&lt;span class="math">\(c_i(x^\ast)=0\)&lt;/span>。&lt;/p>
&lt;p>证明：关于互补松弛条件，我们可以回顾一下这样一个式子： &lt;span class="math">\[\begin{aligned}
\theta_p(x)&amp;amp;=\max_{\alpha,\beta:\alpha_i \ge0}L(x,\alpha,\beta)\\
&amp;amp;=f(x)+\max_{\alpha,:\alpha_i \ge0}\sum_{i=1}^k\alpha_i c_i(x)+\max_{\beta}\sum_{j=1}^l\beta_j h_j(x)\\
&amp;amp;=f(x)
\end{aligned}\]&lt;/span> 显然&lt;span class="math">\(\sum\limits_{i=1}^k\alpha_i c_i(x)=0\)&lt;/span>时取得最大，因此这也是隐含的&lt;span class="math">\(d^∗=p^∗\)&lt;/span>时的条件。 这个知识点会在 SVM 的推导中用到。&lt;/p>
&lt;p>最后说明一下&lt;/p>
&lt;p>slater条件：存在&lt;span class="math">\(x\)&lt;/span>，使得不等式约束&lt;span class="math">\(c(x)&amp;lt;=0\)&lt;/span>严格成立(不取等号)。&lt;/p>
&lt;p>slater条件性质： slater条件是原问题可以等价于对偶问题的一个充分条件，满足他就说明存在最优解。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>一句话，某些条件下，把原始的约束问题通过拉格朗日函数转化为无约束问题，如果原始问题求解棘手，在满足KKT的条件下用求解对偶问题来代替求解原始问题，使得问题求解更加容易。&lt;/p>
&lt;p>其他参考资料： &lt;a href="http://xiaoyc.com/duality-theory-for-optimization/#citem_30">优化问题中的对偶性理论&lt;/a>，强烈推荐看一看&lt;/p></description></item><item><title>优化理论之共轭梯度法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95/</guid><description/></item><item><title>优化理论之内点法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%86%85%E7%82%B9%E6%B3%95/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%86%85%E7%82%B9%E6%B3%95/</guid><description/></item><item><title>优化理论之牛顿法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%89%9B%E9%A1%BF%E6%B3%95/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%89%9B%E9%A1%BF%E6%B3%95/</guid><description>
&lt;h2 id="牛顿法">牛顿法&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#基础牛顿法">基础牛顿法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#牛顿法求解函数根">牛顿法求解函数根&lt;/a>&lt;/li>
&lt;li>&lt;a href="#牛顿法求根的几何解释">牛顿法求根的几何解释&lt;/a>&lt;/li>
&lt;li>&lt;a href="#函数最优值与求根">函数最优值与求根&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#一元函数情况">一元函数情况&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多元函数的情况">多元函数的情况&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#实现算法">实现算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#牛顿法修正">牛顿法修正&lt;/a>&lt;/li>
&lt;li>&lt;a href="#goldstein-price修正法g-p法">Goldstein-Price修正法(G-P法)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#负曲率方向法">负曲率方向法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#goldfeld修正">Goldfeld修正&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gill-murray改进cholesky分解法的稳定牛顿法">Gill-Murray改进Cholesky分解法的稳定牛顿法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#信赖域法">信赖域法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#信赖域算法的基本思想">信赖域算法的基本思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基本信赖域法">基本信赖域法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#levenbergmarquardt法">Levenberg–Marquardt法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dogleg法">Dogleg法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#拟牛顿法">拟牛顿法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dfp算法">DFP算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#sr1算法">SR1算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bfgs算法">BFGS算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#l-bfgs算法">L-BFGS算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#共轭梯度法">共轭梯度法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总结">总结&lt;/a>&lt;/li>
&lt;li>&lt;a href="#参考文献">参考文献&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>牛顿法和梯度下降法都是求解非线性&lt;strong>无约束&lt;/strong>最优化问题的常用方法，但是效果速度比梯度法更好（但是计算量增加了，需要二阶信息）。牛顿法是&lt;strong>迭代算法&lt;/strong>，每一步需要求解目标函数的 Hessian 矩阵的逆矩阵，计算过程复杂。虽然牛顿法收敛速度很快，但是有两个很难处理的问题。一是要保证Hessian 矩阵正定。二是求Hessian 矩阵是很耗计算量的。因此，有许多&lt;strong>修正牛顿法&lt;/strong>来保证矩阵正定或是在矩阵不正定时提供候选方向。同时为了简化计算，诞生了&lt;strong>拟牛顿法&lt;/strong>通过正定矩阵近似 Hessian 矩阵的逆矩阵或 Hessian 矩阵，简化计算过程。DFP方法、BFGS方法和L-BFGS方法都属于拟牛顿法。&lt;/p>
&lt;p>牛顿法、修正的牛顿法和拟牛顿法解决的问题可以用如下&lt;strong>无约束最小化&lt;/strong>问题概括： &lt;span class="math">\[\mathop{min}\limits_{x}\quad f(x)\]&lt;/span> 其中，&lt;span class="math">\(x=(x_1，x_2，\ldots，x_N)^T\in\mathbb{R}^N\)&lt;/span>。由于本文不讨论收敛性，没有特殊说明时，假设&lt;span class="math">\(f\)&lt;/span>为&lt;strong>凸函数，且两阶连续可微&lt;/strong>，此外，记极小问题的解是&lt;span class="math">\(x^\ast\)&lt;/span>。&lt;/p>
&lt;h2 id="基础牛顿法">基础牛顿法&lt;/h2>
&lt;h3 id="牛顿法求解函数根">牛顿法求解函数根&lt;/h3>
&lt;p>牛顿法最核心的功能是&lt;strong>迭代求函数根&lt;/strong>，其他函数优化功能都是从这里衍生出去的。我们知道并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以&lt;strong>迭代求解&lt;/strong>。 原理是利用泰勒公式。我们将任一函数在&lt;span class="math">\(x_0\)&lt;/span>处展开，且展开到一阶，即： &lt;span class="math">\[f(x)=f(x_0)+(x–x_0)f&amp;#39;(x_0)+\underbrace{\frac{1}{2}(x-x_0)^2f&amp;#39;&amp;#39;(\varepsilon)}_{余项}\]&lt;/span> 其中，&lt;span class="math">\(\varepsilon\)&lt;/span>处于&lt;span class="math">\(x,x_0\)&lt;/span>之间，由于后面的余项是&lt;span class="math">\((x-x_0)\)&lt;/span>的高阶数，&lt;strong>先忽略&lt;/strong>。因此要求解方程&lt;span class="math">\(f(x)=0\)&lt;/span>，即是&lt;span class="math">\(f(x_0)+(x–x_0)f&amp;#39;(x_0)=0\)&lt;/span>，解为： &lt;span class="math">\[x=x_1=x_0–f(x_0)/f&amp;#39;(x_0)\]&lt;/span> 因为这是利用泰勒公式的一阶展开，且忽略了余项，余项的存在导致&lt;span class="math">\(f(x)与f(x_0)+(x–x_0)f&amp;#39;(x_0)\)&lt;/span>在&lt;span class="math">\(x_1\)&lt;/span>并不是完全相等，而是近似相等。因此，这里求得的&lt;span class="math">\(x_1\)&lt;/span>并不能让&lt;span class="math">\(f(x)=0\)&lt;/span>，只能说&lt;span class="math">\(f(x_1)\)&lt;/span>的值比&lt;span class="math">\(f(x_0)\)&lt;/span>更接近&lt;span class="math">\(f(x)=0\)&lt;/span>，于是乎，迭代求解的想法就很自然了，可以进而推出 &lt;span class="math">\[x_{n+1}=x_n–f(x_n)/f&amp;#39;(x_n)\]&lt;/span> 通过迭代，这个式子必然在&lt;span class="math">\(f(x^∗)=0\)&lt;/span>的时候收敛. 整个过程如下图：&lt;/p>
&lt;img src="../images/newton_root.gif" alt="newton_root.gif" />
&lt;center>
图1牛顿法求根
&lt;/center>
&lt;h3 id="牛顿法求根的几何解释">牛顿法求根的几何解释&lt;/h3>
&lt;p>牛顿法求根其实是利用了切线的性质。如果我们查看过点&lt;span class="math">\(x_0\)&lt;/span>处的切线方程： &lt;span class="math">\[
y-f(x_0)=f&amp;#39;(x_0)(x-x_0)\\
\]&lt;/span> 当&lt;span class="math">\(y=0\)&lt;/span>时，有： &lt;span class="math">\[
-f(x_0)=f&amp;#39;(x_0)(x-x_0)\\
\Rightarrow x=x_0-\frac{f(x_0)}{f&amp;#39;(x_0)}
\]&lt;/span> 这个式子和牛顿法求根的迭代公式一致。说白了，牛顿法求根就是利用当前点所在直线与&lt;span class="math">\(y=0\)&lt;/span>的交点，这个交点&lt;span class="math">\(x_1\)&lt;/span>会比&lt;span class="math">\(x_0\)&lt;/span>更接近让函数值为0。&lt;/p>
&lt;h3 id="函数最优值与求根">函数最优值与求根&lt;/h3>
&lt;h4 id="一元函数情况">一元函数情况&lt;/h4>
&lt;p>从简单的一元函数开始，我们先列出函数的二阶泰勒展开式： &lt;span class="math">\[f(x)=f(x_0)+f&amp;#39;(x_0)(x-x_0)+\frac{1}{2}f&amp;#39;&amp;#39;(x_0)(x-x_0)^2+R_2(x)\]&lt;/span>&lt;/p>
&lt;p>然后，我们再来讨论我们的问题，求函数最优值。我们知道一个可微函数的最值点，一定是它的驻点（&lt;em>或边界点，假设定义域范围暂不考虑&lt;/em>）。那我们就要求驻点，即&lt;span class="math">\(f&amp;#39;(x)=0\)&lt;/span>的点。我们如果对&lt;span class="math">\(f(x)\)&lt;/span>和其二阶泰勒展开式求导（忽略余项）： &lt;span class="math">\[f&amp;#39;(x)=f&amp;#39;(x_0)+f&amp;#39;&amp;#39;(x_0)(x-x_0)\]&lt;/span> 根据驻点性质，要求&lt;span class="math">\(f&amp;#39;(x)=0\)&lt;/span>，则根据上一节&lt;strong>牛顿法求解函数根&lt;/strong>可知： &lt;span class="math">\[x_1=x_0-\frac{f&amp;#39;(x_0)}{f&amp;#39;&amp;#39;(x_0)}\]&lt;/span> 这样我们就得到了下一点的位置&lt;span class="math">\(x_1\)&lt;/span>。&lt;span class="math">\(f&amp;#39;(x_1)\)&lt;/span>比&lt;span class="math">\(f&amp;#39;(x_0)\)&lt;/span>更接近0。接下来重复这个过程，直到到达导数为0的点，由此得到求极值的牛顿法的迭代公式： &lt;span class="math">\[x_{n+1}=x_n-\frac{f&amp;#39;(x_n)}{f&amp;#39;&amp;#39;(x_n)}\]&lt;/span> 给定初始迭代点&lt;span class="math">\(x_0\)&lt;/span>，反复用上面的公式进行迭代，直到达到导数为0的点或者达到最大迭代次数。&lt;/p>
&lt;h4 id="多元函数的情况">多元函数的情况&lt;/h4>
&lt;p>下面推广到多元函数的情况，根据多元函数的泰勒展开公式（&lt;em>参考矩阵论教材&lt;/em>），我们对目标函数在&lt;span class="math">\(x_0\)&lt;/span>点处做二阶泰勒展开，有（自变量皆为列向量，忽略余项）： &lt;span class="math">\[f(x)=f(x_0)+\nabla f(x_0)^T(x-x_0)+\frac{1}{2}(x-x_0)^T\nabla^2f(x_0)(x-x_0)\]&lt;/span> 其中，&lt;span class="math">\(\nabla^2f(x_0)\)&lt;/span>为多元实值函数&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(x_0\)&lt;/span>处的海森矩阵（Hessian Matrix），我们以后用&lt;span class="math">\(H\)&lt;/span>表示。那么如果函数梯度为0，且海森矩阵&lt;span class="math">\(H\)&lt;/span>可逆，则有： &lt;span class="math">\[\nabla f(x)=\vec{0} \Rightarrow \\
\nabla f(x_0)+\nabla^2f(x_0)(x-x_0)=\vec{0}\Rightarrow \\
x_1=x_0-(\nabla^2f(x_0))^{-1}\nabla f(x_0)\Rightarrow \\
x_1=x_0-H^{-1}[f(x_0)]\nabla f(x_0)\]&lt;/span> 注意矩阵左右乘不可颠倒,&lt;span class="math">\(H^{-1}[f(x_0)]\)&lt;/span>，为海森矩阵的逆矩阵。&lt;strong>特别地，若是极小值点则Hessian是正定矩阵&lt;/strong>。同理，我们可以得到如下递推公式： &lt;span class="math">\[x_{n+1}=x_n-H^{-1}[f(x_n)]\nabla f(x_n) \\
可简写为：x_{n+1}=x_n-H^{-1}_n{g_n}\]&lt;/span> 最终会到达函数的驻点处。其中&lt;span class="math">\(-H^{-1}g\)&lt;/span>称为&lt;strong>牛顿方向&lt;/strong>。迭代终止的条件是梯度的模接近于0，或者函数值下降小于指定阈值。&lt;/p>
&lt;p>注意，我们目前默认牛顿法的步长是1，这样可以保证2阶收敛速度，但是并不是对所有函数都有效。现不加证明的指出，带步长的牛顿法是收敛的。&lt;/p>
&lt;h3 id="实现算法">实现算法&lt;/h3>
&lt;p>1.给定初始值&lt;span class="math">\(x_0\)&lt;/span>和精度阈值&lt;span class="math">\(\epsilon\)&lt;/span>，设置&lt;span class="math">\(k = 0\)&lt;/span>&lt;/p>
&lt;p>2.计算梯度&lt;span class="math">\(g_k\)&lt;/span>和矩阵&lt;span class="math">\(H_k\)&lt;/span>&lt;/p>
&lt;p>3.如果&lt;span class="math">\(\|g_k\|&amp;lt;\epsilon\)&lt;/span>即在此点处梯度的值接近于0，则达到极值点处，停止迭代&lt;/p>
&lt;p>4.计算搜索方向&lt;span class="math">\(d_k=-H^{-1}_k{g_k}\)&lt;/span>&lt;/p>
&lt;p>5.计算新的迭代点&lt;span class="math">\(x_{k+1}=x_k-\alpha H^{-1}_k{g_k}\)&lt;/span>(一般可以取&lt;span class="math">\(\alpha=1\)&lt;/span>)&lt;/p>
&lt;p>6.令&lt;span class="math">\(k = k + 1\)&lt;/span>，返回步骤2&lt;/p>
&lt;p>其中&lt;span class="math">\(\epsilon\)&lt;/span>是一个人工设定的接近于0的常数，和梯度下降法一样，需要这个参数的原因是保证&lt;span class="math">\(x_{k+1}\)&lt;/span>在&lt;span class="math">\(x_k\)&lt;/span>的邻域内，从而可以忽略泰勒展开的高次项。如果目标函数是二次函数，Hessian矩阵是一个常数矩阵，对于任意给定的初始点，牛顿法只需要一步迭代就可以收敛到极值点。&lt;/p>
&lt;p>牛顿法收敛速度快，具有二阶收敛性。但是和梯度下降法一样，牛顿法寻找的也是导数为0的点，这不一定是极值点，因此会面临局部极小值和鞍点问题。另外，它需要Hessian矩阵，计算量大，并且Hessian矩阵并不是一直能够求出。因此出现了一些&lt;strong>降低计算量&lt;/strong>得&lt;strong>拟牛顿法&lt;/strong>。&lt;/p>
&lt;p>除此之外，牛顿法在每次迭代时序列&lt;span class="math">\(x_i\)&lt;/span>可能不会收敛到一个最优解，它甚至不能保证函数值会按照这个序列递减。解决第一个问题可以通过&lt;strong>调整牛顿方向的步长&lt;/strong>来实现，目前常用的方法有两种：直线搜索和可信区域法。加了步长搜索的牛顿法也叫&lt;strong>阻尼牛顿法&lt;/strong>。&lt;/p>
&lt;h2 id="牛顿法修正">牛顿法修正&lt;/h2>
&lt;p>牛顿法的一大困难在于某一k步的Hessian 矩阵&lt;span class="math">\(H_k\)&lt;/span>可能不正定。这时二次型模型不一定有极小点，甚至没有平稳点。此时，牛顿方向&lt;span class="math">\(-H_k^{-1}g_k\)&lt;/span>不一定会让目标函数值下降。对应的各种改进方案应运而生，各位接着往下看。&lt;/p>
&lt;h3 id="goldstein-price修正法g-p法">Goldstein-Price修正法(G-P法)&lt;/h3>
&lt;p>针对&lt;span class="math">\(H_k\)&lt;/span>可能不正定的问题，如其不正定，就用“最速下降方向”来作为搜索方向。 &lt;span class="math">\[d_{k+1}=\begin{cases}-H_kg_k，H_k正定\\-g_k，H_k非正定
\end{cases}\]&lt;/span> 确定方向后，采用下列精确一维搜索（Armijo-Goldstein准则）（PS：其他准则也可以） &lt;span class="math">\[1:f(\vec x_k+\alpha\vec d_k)≤f(\vec x_k)+\alpha c\nabla f(\vec x_k)\vec d_k\]&lt;/span> &lt;span class="math">\[2:f(\vec x_k+\alpha\vec d_k)≥f(\vec x_k)+\alpha (1-c)\nabla f(\vec x_k)\vec d_k\]&lt;/span> 其中，&lt;span class="math">\(c ∈ (0,0.5)\)&lt;/span>&lt;/p>
&lt;p>在一定条件下，G-P法全局收敛。但当海森矩阵&lt;span class="math">\(H\)&lt;/span>非正定情况较多时，收敛速度降为接近线性（梯度法）。&lt;/p>
&lt;h3 id="负曲率方向法">负曲率方向法&lt;/h3>
&lt;p>在鞍点处，我们有梯度为0，同时H矩阵不定，G-P等修正牛顿法无法在鞍点处继续，需要额外的方式：采用&lt;strong>负曲率方向&lt;/strong>作为搜索方向，可使目标函数值下降。&lt;/p>
&lt;p>负曲率方向，即&lt;span class="math">\(d^T_k∇_2f(x_k)d_k&amp;lt;0\)&lt;/span>。这里涉及到负曲率方向的定义，在张贤达的《矩阵分析与应用》中说到，当矩阵H是非线性函数&lt;span class="math">\(f(x)\)&lt;/span>的Hessian矩阵时，称满足&lt;span class="math">\(p^HHp&amp;gt;0\)&lt;/span> 的向量&lt;span class="math">\(p\)&lt;/span>为函数&lt;span class="math">\(f\)&lt;/span>的正曲率方向，满足&lt;span class="math">\(p^HHp&amp;lt;0\)&lt;/span>的向量&lt;span class="math">\(p\)&lt;/span>为函数&lt;span class="math">\(f\)&lt;/span>的负曲率方向。标量&lt;span class="math">\(p^HHp\)&lt;/span>称为函数&lt;span class="math">\(f\)&lt;/span>沿着方向&lt;span class="math">\(p\)&lt;/span>的曲率。沿着负曲率方向进行一维搜索必能使目标函数值下降。&lt;/p>
&lt;p>因此，在鞍点时，我们只要找到任意负曲率方向，使函数值继续减小，就可以摆脱鞍点。&lt;/p>
&lt;h3 id="goldfeld修正">Goldfeld修正&lt;/h3>
&lt;p>与上面的Goldstein-Price修正的思路不同，Goldfeld在1966年也提出了一种方法，他的方法虽然还是在搜索方向&lt;span class="math">\(d_k\)&lt;/span> 上动手，但是当&lt;span class="math">\(H_k\)&lt;/span> 不正定时，他不是用最速下降方向&lt;span class="math">\(-g_k\)&lt;/span>来作为搜索方向，而是将&lt;span class="math">\(d_k\)&lt;/span>修正成下降方向——用下面的式子： &lt;span class="math">\[{d_k} = - B_k^{ - 1}{g_k}\]&lt;/span> 其中，&lt;span class="math">\({B_k} = {H_k} + {E_k}\)&lt;/span>是一个正定矩阵，&lt;span class="math">\(E_k\)&lt;/span>称为修正矩阵。在&lt;span class="math">\(E_k\)&lt;/span>满足一定条件的时候，（Goldfeld修正）牛顿法具有整体收敛性。&lt;/p>
&lt;p>接下来我们使用&lt;span class="math">\(B_k\)&lt;/span>表示经过各种修正后的海森矩阵。&lt;/p>
&lt;p>我们可以设&lt;span class="math">\(E_k=v_kI\)&lt;/span>，使得使得&lt;span class="math">\(H_k+v_kI\)&lt;/span>正定。比较理想的是，&lt;span class="math">\(v_k\)&lt;/span>为使&lt;span class="math">\(H_k+v_kI\)&lt;/span>正定的最小&lt;span class="math">\(v\)&lt;/span>。特征值的估计可以用盖尔圆盘定理或者Rayleigh商方法。&lt;/p>
&lt;h3 id="gill-murray改进cholesky分解法的稳定牛顿法">Gill-Murray改进Cholesky分解法的稳定牛顿法&lt;/h3>
&lt;p>前一个小节里说过加上一个&lt;span class="math">\(v_kI\)&lt;/span>使修正的Hessian矩阵正定，本节具体介绍一种高效稳定方法。Gill-Murray改进Cholesky分解法的稳定牛顿法可以说是&lt;strong>修正牛顿法中的集大成&lt;/strong>。基本思想是当&lt;span class="math">\(H_k\)&lt;/span>不定矩阵时，采用修改Cholesky分解强迫矩阵正定；当&lt;span class="math">\(g_k\)&lt;/span>趋于0时，采用负曲率方向使函数值下降。如果矩阵正定且负曲率方向不存在，则是最优解。其大体流程如下&lt;/p>
&lt;blockquote>
&lt;ol style="list-style-type: decimal">
&lt;li>给定初始点&lt;span class="math">\(x_0,\varepsilon&amp;gt;0,k=1\)&lt;/span>&lt;/li>
&lt;li>计算&lt;span class="math">\(g_k,H_k\)&lt;/span>&lt;/li>
&lt;li>对&lt;span class="math">\(H_k\)&lt;/span>进行修改cholesky分解强迫矩阵正定，&lt;span class="math">\(L_kD_kL_k^\ast=H_k+E_k\)&lt;/span>&lt;/li>
&lt;li>若&lt;span class="math">\(||g_k||&amp;gt;\varepsilon\)&lt;/span>（梯度不为0的计算机表达），则解方程&lt;span class="math">\(L_kD_kL_k^\ast d_k=-g_k\Rightarrow d_k=-(H_k+E_k)^{-1}g_k\)&lt;/span>，转步骤6&lt;/li>
&lt;li>若&lt;span class="math">\(||g_k||&amp;gt;\varepsilon\approx0\)&lt;/span>，构造负曲率方向，如果构造不出负曲率方向，&lt;strong>得到局部最优&lt;/strong>。&lt;/li>
&lt;li>一维搜 索步长，可用精确法或不精确准则得到步长&lt;span class="math">\(\alpha_k\)&lt;/span>，令&lt;span class="math">\(x_{k+1}=x_k+\alpha_k d_k\)&lt;/span>&lt;/li>
&lt;li>若第5步满足终止条件，则OK；否则&lt;span class="math">\(k=k+1\)&lt;/span>，转2.&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>可以证明该方法总体收敛。现在还有其中最重要的一步：&lt;strong>修正的Cholesky分解&lt;/strong>，请看笔记《海森矩阵校正》，更加详细的关于修正的Cholesky分解可见教程《Modified Cholesky Decomposition and Applications》&lt;/p>
&lt;h3 id="信赖域法">信赖域法&lt;/h3>
&lt;p>信赖域和line search同为最优化算法的基础算法，但是，从“Trust Region”这个名字你就可以看出，它是没有line search过程的，它是直接在一个region中“search”。 在一维搜索中，从&lt;span class="math">\(x_k\)&lt;/span>点移动到下一个点的过程，可以描述为：&lt;span class="math">\(x_k + α_k d_k\)&lt;/span>，此处&lt;span class="math">\(α_k d_k\)&lt;/span>就是在&lt;span class="math">\(d_k\)&lt;/span>方向上的位移，可以记&lt;span class="math">\(s_k\)&lt;/span>，总的来说，&lt;strong>线搜索把复杂问题改编成一系列简单的一维优化问题&lt;/strong>。而信赖域算法是根据一定的原则，直接确定位移&lt;span class="math">\(s_k\)&lt;/span>，同时，与一维搜索不同的是，它并没有先确定搜索方向&lt;span class="math">\(d_k\)&lt;/span>。如果根据“&lt;strong>某种原则&lt;/strong>”确定的位移能使目标函数值充分下降，则扩大信赖域；若不能使目标函数值充分下降，则缩小信赖域。如此迭代下去，直到收敛。总的来说，&lt;strong>信赖域方法是把复杂问题转化为一系列相对简单的局部优化问题&lt;/strong>。&lt;/p>
&lt;h4 id="信赖域算法的基本思想">信赖域算法的基本思想&lt;/h4>
&lt;p>在每次迭代中给出一个信赖域，这个信赖域一般是当前迭代点 的一个小邻域。然后，在这个邻域内求解一个子问题，得到&lt;strong>试探步长（trial step）&lt;span class="math">\(s_k\)&lt;/span>&lt;/strong>，接着用某一&lt;strong>评价函数&lt;/strong>来决定是否接受该试探步以及确定下一次迭代的信赖域。 如果试探步长被接受，则：&lt;span class="math">\(x_{k+1}=x_k+s_k\)&lt;/span>，否则，&lt;span class="math">\(x_{k+1}=x_k\)&lt;/span>。 新的信赖域的大小取决于试探步的好坏，&lt;em>粗略地说，如果试探步较好，在下一步信赖域扩大或者保持不变，否则减小信赖域&lt;/em>。&lt;/p>
&lt;h4 id="基本信赖域法">基本信赖域法&lt;/h4>
&lt;p>设当前点&lt;span class="math">\(x_k\)&lt;/span>的邻域定义为：&lt;span class="math">\(\Omega_k\)&lt;/span>。目标函数在极值点附近近似一个二次函数，因此对于无约束优化问题，利用二次逼近，构造如下信赖域子问题： 设当前点&lt;span class="math">\(x_k\)&lt;/span>的邻域定义为：&lt;span class="math">\(\Omega_k=\{x ∈ R^n|\Vert x-x_k\Vert≤\Delta_k\}\)&lt;/span>，其中，&lt;span class="math">\(\Delta_k\)&lt;/span>称为信赖域半径。 目标函数在极值点附近&lt;strong>近似一个二次函数&lt;/strong>(有点类似插值法)，因此对于无约束优化问题，利用二次逼近，构造如下&lt;strong>信赖域子问题&lt;/strong>： &lt;span class="math">\[\min q^{(k)}(s)=f(x_k)+g_k^Ts+\frac{1}{2}(s^TB_ks)\\
s.t. \quad \|s\|_2≤\Delta_k\]&lt;/span> 其中，&lt;span class="math">\(s=x-s_k\)&lt;/span>，&lt;span class="math">\(g_k\)&lt;/span>是目标函数&lt;span class="math">\(f(x)\)&lt;/span>在当前迭代点&lt;span class="math">\(x_k\)&lt;/span>处的梯度，&lt;span class="math">\(B_k ∈ R^{n*n}\)&lt;/span>对称，是&lt;span class="math">\(f(x)\)&lt;/span>在&lt;span class="math">\(x_k\)&lt;/span>处Hessian矩阵或Hessian矩阵的近似。&lt;/p>
&lt;p>设&lt;span class="math">\(s_k\)&lt;/span>是信赖域子问题的解。目标函数&lt;span class="math">\(f(x)\)&lt;/span>在第k步的实际下降量（真实下降量）： &lt;span class="math">\[Ared_k=f(x_k)-f(x_k+s_k)\]&lt;/span> 二次模型函数&lt;span class="math">\(q^{(k)}(s)\)&lt;/span>的下降量（预测下降量）： &lt;span class="math">\[Pred_k=q^{(k)}(0)-q^{(k)}(s_k)&amp;gt;0\]&lt;/span> 定义比值： &lt;span class="math">\[r=\frac{Ared_k}{Pred_k}\]&lt;/span> 它衡量了二次模型与目标函数的逼近程度，&lt;span class="math">\(r_k\)&lt;/span>越接近于1，表明接近程度越好。因此，我们也用这个量来确定下次迭代的信赖域半径。&lt;/p>
&lt;ul>
&lt;li>越接近与1，表明接近程度越好。这时可以增大&lt;span class="math">\(\Delta_k\)&lt;/span>以扩大信赖域；&lt;/li>
&lt;li>&lt;span class="math">\(r_k&amp;gt;0\)&lt;/span>但是不接近于1，保持&lt;span class="math">\(\Delta_k\)&lt;/span>不变；&lt;/li>
&lt;li>如果&lt;span class="math">\(r_k\)&lt;/span>接近于0，减小&lt;span class="math">\(\Delta_k\)&lt;/span>，缩小信赖域。 &lt;a href="https://www.codelast.com/%E5%8E%9F%E5%88%9B%E4%BF%A1%E8%B5%96%E5%9F%9Ftrust-region%E7%AE%97%E6%B3%95%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/">https://www.codelast.com/%E5%8E%9F%E5%88%9B%E4%BF%A1%E8%B5%96%E5%9F%9Ftrust-region%E7%AE%97%E6%B3%95%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="levenbergmarquardt法">Levenberg–Marquardt法&lt;/h4>
&lt;p>&lt;a href="https://www.codelast.com/%e5%8e%9f%e5%88%9blm%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%9e%e7%8e%b0/">https://www.codelast.com/%e5%8e%9f%e5%88%9blm%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%9e%e7%8e%b0/&lt;/a>&lt;/p>
&lt;h4 id="dogleg法">Dogleg法&lt;/h4>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/99392484">https://zhuanlan.zhihu.com/p/99392484&lt;/a> &lt;a href="https://zhuanlan.zhihu.com/p/101124802">https://zhuanlan.zhihu.com/p/101124802&lt;/a>&lt;/p>
&lt;h2 id="拟牛顿法">拟牛顿法&lt;/h2>
&lt;p>拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。&lt;/p>
&lt;p>拟牛顿法的本质思想是&lt;strong>改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度&lt;/strong>。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。&lt;/p>
&lt;h3 id="dfp算法">DFP算法&lt;/h3>
&lt;h3 id="sr1算法">SR1算法&lt;/h3>
&lt;h3 id="bfgs算法">BFGS算法&lt;/h3>
&lt;h3 id="l-bfgs算法">L-BFGS算法&lt;/h3>
&lt;div class="figure">
&lt;img src="../images/拟牛顿法的迭代公式.jpg" alt="拟牛顿法的迭代公式" />&lt;p class="caption">拟牛顿法的迭代公式&lt;/p>
&lt;/div>
&lt;h2 id="共轭梯度法">共轭梯度法&lt;/h2>
&lt;h2 id="总结">总结&lt;/h2>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>[1] &lt;a href="https://zhuanlan.zhihu.com/p/37588590">https://zhuanlan.zhihu.com/p/37588590&lt;/a>&lt;/p>
&lt;p>[2] &lt;a href="https://blog.csdn.net/itplus/article/details/21896619">https://blog.csdn.net/itplus/article/details/21896619&lt;/a>&lt;/p>
&lt;p>[3] &lt;a href="https://blog.csdn.net/itplus/article/details/21896453">https://blog.csdn.net/itplus/article/details/21896453&lt;/a>&lt;/p></description></item><item><title>优化理论之非线性方法-梯度法与牛顿法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95-%E6%A2%AF%E5%BA%A6%E6%B3%95%E4%B8%8E%E7%89%9B%E9%A1%BF%E6%B3%95/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95-%E6%A2%AF%E5%BA%A6%E6%B3%95%E4%B8%8E%E7%89%9B%E9%A1%BF%E6%B3%95/</guid><description>
&lt;h2 id="梯度下降法gradient-descent与牛顿法newtons-method求解最小值">梯度下降法(gradient descent)与牛顿法(newton’s method)求解最小值&lt;!-- omit in toc -->&lt;/h2>
&lt;p>梯度下降法与牛顿法是求解最小值/优化问题的两种经典算法。本文的目标是介绍两种算法的推导思路与流程，并且从初学者的角度就一些容易混淆的话题如：梯度下降法(gradient descent)与最速下降法(steepest descent)的联系与区别、牛顿求根迭代方法(Newton–Raphson method) 与牛顿法求解最小值算法的联系(来自 Andrew Ng 机器学习课程第四讲)进行说明。本文的内容将对高斯牛顿法(Gauss–Newton algorithm) ,Levenberg-Marquardt算法(LM算法)等非线性最小二乘问题解法起到引出作用。&lt;/p>
&lt;h2 id="梯度下降法">梯度下降法&lt;/h2>
&lt;p>已知多元函数&lt;span class="math">\(f(x_1,x_2,\dots,x_n)\)&lt;/span>在定义域上可微，如果将&lt;span class="math">\(f(\mathbf{x})\)&lt;/span>在&lt;span class="math">\(\mathbf{x}\)&lt;/span>处一阶泰勒展开(tayler expansion),可得到：(说明：为了编辑方便下文中统一以&lt;span class="math">\(x={\begin{bmatrix} x_1,x_2,\dots,x_n \end{bmatrix}}^T\)&lt;/span>代替&lt;span class="math">\(\mathbf{x}\)&lt;/span> )。 &lt;span class="math">\[f(x+\epsilon) = f(x)+\epsilon ^T\nabla_x f + O(||\epsilon||)\approx f(x)+\epsilon ^T\nabla_x f\]&lt;/span> 其中&lt;span class="math">\(\nabla_x f={\begin{bmatrix} \frac{\partial f}{\partial x_1},\dots,\frac{\partial f}{\partial x_n} \end{bmatrix}}^T\)&lt;/span>为&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(x\)&lt;/span>处的梯度向量。&lt;/p>
&lt;p>这个式子我们可以解读为当&lt;span class="math">\(x\)&lt;/span>增加&lt;span class="math">\(\epsilon\)&lt;/span>时，&lt;span class="math">\(f(x)\)&lt;/span>增加&lt;span class="math">\(\epsilon ^T\nabla_x f\)&lt;/span>，即&lt;span class="math">\(\epsilon\)&lt;/span>与梯度&lt;span class="math">\(\nabla_x f\)&lt;/span>的內积。如果我们限定&lt;span class="math">\(\epsilon\)&lt;/span>的模长为定值，其方向怎样才能获得&lt;span class="math">\(f(x+\epsilon)\)&lt;/span>的最小值呢？答案当然是与&lt;span class="math">\(\nabla_x f\)&lt;/span>方向相反的时候，此时&lt;span class="math">\(\epsilon ^T\nabla_x f\)&lt;/span>获得最小值。&lt;/p>
&lt;p>我们也可以利用直觉上较好理解的爬山的例子来解释梯度下降法。假设你位于山上某一点坐标为&lt;span class="math">\(\mathbf{\theta}(\theta_1,\theta_2)\)&lt;/span>，那么在此处(注意，是在这一点)下山最快的方向当然是沿着此处的梯度方向。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/steepest_descent_1.png" alt="steepest_descent_1.png" />&lt;p class="caption">steepest_descent_1.png&lt;/p>
&lt;/div>
&lt;p>所以说，将整个故事串起来，梯度下降法的思路可以总结如下：欲求多元函数&lt;span class="math">\(f(x)\)&lt;/span>的最小值，可以采用如下步骤：&lt;/p>
&lt;p>给定初始值&lt;span class="math">\(x_0\)&lt;/span>。 按照如下方式“下山”：&lt;span class="math">\(x_{i+1} = x_i-\eta\nabla_{x_i} f\)&lt;/span>。其中&lt;span class="math">\(\eta&amp;gt;0\)&lt;/span>，在机器学习领域，&lt;span class="math">\(\eta\)&lt;/span>也被称之为学习率(learning rate)。 直到&lt;span class="math">\(x\)&lt;/span>满足收敛条件为止。如&lt;span class="math">\(\|f(x_{i+1}) – f(x_i)\|&amp;lt;\epsilon\)&lt;/span>或&lt;span class="math">\(||\nabla_{x_i}f||\approx 0\)&lt;/span>。&lt;/p>
&lt;p>学习率的重要性：&lt;/p>
&lt;p>学习率作为控制下降步长的参数，影响函数下降的速度。学习率是我们根据经验确定的一个参数，因此在机器学习领域中这样的参数也被成为超参数(hyperparameter)。学习率的选取不能过大或者过小，如下图，不同的学习率导致函数不同的收敛速度，甚至可能导致函数不收敛。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/different_learning_rate.png" alt="different_learning_rate.png" />&lt;p class="caption">different_learning_rate.png&lt;/p>
&lt;/div>
&lt;h3 id="梯度下降法的优势">梯度下降法的优势&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>时间复杂度低，在每一个迭代中，只需要计算梯度，不需要对二阶导数矩阵（即海森矩阵(Hessian Matrix)）进行计算。&lt;/li>
&lt;li>空间复杂度低，因为梯度向量为一个&lt;span class="math">\(n\times 1\)&lt;/span>的向量，比起Hessian Matrix来，占用存储空间小n倍。在实际应用中，&lt;span class="math">\(\mathbf{x}\)&lt;/span>的维度可能非常高。&lt;/li>
&lt;/ol>
&lt;h3 id="梯度下降法的局限">梯度下降法的局限&lt;/h3>
&lt;p>对于部分求解函数，梯度下降法可能会出现下降非常缓慢的情形。其收敛速度也较其他方法低（其他文献分析其收敛速度为线性，本文不作推导）。如下图，梯度下降法的路径出现了z字型。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Banana-SteepDesc.gif" alt="Banana-SteepDesc.gif" />&lt;p class="caption">Banana-SteepDesc.gif&lt;/p>
&lt;/div>
&lt;p>究其原因，我认为，某一点的梯度只能作为这一点的一个极小的领域处的最快下降方向，一旦梯度变化较快，梯度下降法会出现因为学习率不合适而出现”zigzag”现象。而且，如果我们将梯度下降法与下文的牛顿法做对比，你会发现，一直沿着梯度方向下降的速度不一定是最快的。如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Newton_optimization_vs_grad_descent.png" alt="Newton_optimization_vs_grad_descent" />&lt;p class="caption">Newton_optimization_vs_grad_descent&lt;/p>
&lt;/div>
&lt;h3 id="最速下降法steepest-decent-与-梯度下降法gradient-descent的联系">最速下降法(steepest decent) 与 梯度下降法(gradient descent)的联系&lt;/h3>
&lt;p>总结一下就是 梯度下降法是最速下降法的一种特例。在最速下降法中，对于某一范数下&lt;span class="math">\(\epsilon\)&lt;/span>的取值根据以下原则：&lt;/p>
&lt;p>&lt;span class="math">\[\bigtriangleup \epsilon_{nsd}=argmin_v(\nabla f(x)^T\epsilon\mid \|\epsilon\|=1)\]&lt;/span>&lt;/p>
&lt;p>当我们指定的范数为欧几里得范数时，最速下降法给出的下降方向就是梯度的负方向，即梯度下降法给出的方向。&lt;/p>
&lt;p>在wikipedia中说明，梯度下降法也被称为最速下降法(Gradient descent is also known as steepest descent )。&lt;/p>
&lt;h2 id="牛顿法">牛顿法&lt;/h2>
&lt;p>如同根据一阶泰勒展开推导出梯度下降法一样，根据二阶泰勒展开可以推导出牛顿优化法(newton’s method in optimization)。将&lt;span class="math">\(f(\mathbf{x})\)&lt;/span>在&lt;span class="math">\(\mathbf{x}\)&lt;/span>处一阶泰勒展开(tayler expansion),可得到： &lt;span class="math">\[f(x+\epsilon) = f(x)+\epsilon ^T\nabla_x f + \frac{1}{2}\epsilon ^TH\epsilon +O(||\epsilon||^2)\\
\approx f(x)+\epsilon ^T\nabla_x f + \frac{1}{2}\epsilon ^TH\epsilon\]&lt;/span>&lt;/p>
&lt;p>如果我们将&lt;span class="math">\(x\)&lt;/span>看做固定的已知量，将&lt;span class="math">\(f\)&lt;/span>看做关于&lt;span class="math">\(\epsilon\)&lt;/span>的函数，那么欲求&lt;span class="math">\(f(\epsilon | x)\)&lt;/span>的最小值，必要条件(注意：不是充要条件)是&lt;span class="math">\(\frac{\partial f}{\partial \epsilon}=0\)&lt;/span>其中 &lt;span class="math">\[H = \begin{bmatrix}\frac{\partial^2f}{\partial x_1 \partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial^2f}{\partial x_1 \partial x_n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{\partial^2f}{\partial x_n \partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial^2f}{\partial x_n \partial x_n}\end{bmatrix}\]&lt;/span> 称之为&lt;span class="math">\(f\)&lt;/span>的&lt;span class="math">\(Hessian\)&lt;/span>矩阵。因为二阶连续混合编导数具备性质 &lt;span class="math">\[\frac{\partial^2f}{\partial x_i \partial x_j} = \frac{\partial^2f}{\partial x_j \partial x_i}\]&lt;/span> 因此&lt;span class="math">\(Hessian\)&lt;/span>矩阵为对称矩阵。根据矩阵求导法则，可以得到 &lt;span class="math">\[\frac{\partial f}{\partial \epsilon}={\nabla_x f}^T+\epsilon^TH=0\\
\epsilon = -H^{-1}{\nabla_x f}\]&lt;/span>&lt;/p>
&lt;p>可见，牛顿法的思路是将函数f在x处展开为多元二次函数，再通过求解二次函数最小值的方法得到本次迭代的下降方向&lt;span class="math">\(\epsilon\)&lt;/span>。那么问题来了，多元二次函数在梯度为0的地方一定存在最小值么？直觉告诉我们是不一定的。以 一元二次函数&lt;span class="math">\(g(x)=ax^2+bx+c\)&lt;/span>为例，我们知道当&lt;span class="math">\(a&amp;gt;0\)&lt;/span>时，&lt;span class="math">\(g(x)\)&lt;/span>可以取得最小值，否则&lt;span class="math">\(g(x)\)&lt;/span>不存在最小值。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/shape-of-the-graph.png" alt="shape-of-the-graph.png" />&lt;p class="caption">shape-of-the-graph.png&lt;/p>
&lt;/div>
&lt;p>推广到多元的情况，可以得出二次项矩阵必须是正定(positive definite)的，对应上式即&lt;span class="math">\(Hessian\)&lt;/span>为正定矩阵时，函数&lt;span class="math">\(f(\epsilon | x)\)&lt;/span>的最小值才存在。&lt;/p>
&lt;p>因此，牛顿法首先需要计算&lt;span class="math">\(Hessian\)&lt;/span>矩阵并且判断其正定性，当&lt;span class="math">\(Hessian\)&lt;/span>矩阵正定，此时其所有特征值均&amp;gt;0,当然&lt;span class="math">\(Hessian\)&lt;/span>矩阵也是可逆的，最小值存在。&lt;/p>
&lt;p>需要指出的是，当多元函数 f 本身就是二次函数并且存在最小值时，牛顿法可以一步解出最小值。&lt;/p>
&lt;h3 id="牛顿法的优点">牛顿法的优点&lt;/h3>
&lt;p>因为目标函数在接近极小值点附近接近二次函数，因此在极小值点附近，牛顿法的收敛速度较梯度下降法快的多。其他文献分析其收敛速度为2次收敛，本文不给出推导。下图是牛顿法应用在Rosenbrock函数上的效果：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Rosenbrock_newton.png" alt="Rosenbrock_newton.png" />&lt;p class="caption">Rosenbrock_newton.png&lt;/p>
&lt;/div>
&lt;h3 id="牛顿法的缺点">牛顿法的缺点&lt;/h3>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(Hessian\)&lt;/span>矩阵的计算难度非常的大。因此在高维度应用案例中，通常不会计算&lt;span class="math">\(Hessian\)&lt;/span>矩阵。因此牛顿法也产生了很多变种，主要的思想就是采用其他矩阵近似&lt;span class="math">\(Hessian\)&lt;/span>矩阵，降低计算复杂度。&lt;/li>
&lt;li>牛顿法当&lt;span class="math">\(Hessian\)&lt;/span>矩阵为正定矩阵时，最小值才存在。牛顿法经常会因为&lt;span class="math">\(Hessian\)&lt;/span>矩阵不正定而发散(diverage)。因此 牛顿法并不是非常的稳定。&lt;/li>
&lt;/ol>
&lt;h3 id="牛顿法求根公式与牛顿优化法之间的联系">牛顿法求根公式与牛顿优化法之间的联系&lt;/h3>
&lt;p>在说道牛顿优化方法的时候，上过《计算方法》这门课的同学经常会说，牛顿法不是用来求根的么？实际上，牛顿优化法还真可以用牛顿求根法推导得出。我看到的材料是 Andrew Ng在《机器学习课程》中给出的一种推导。在牛顿求根公式中，&lt;span class="math">\(f(x)=0\)&lt;/span>的解由迭代式 &lt;span class="math">\[x_{i+1}=x_{i}-\frac{f(x_i)}{f\prime(x_i)}\]&lt;/span> 给出。在牛顿优化法中，我们欲求得梯度&lt;span class="math">\(g(x)=f&amp;#39;(x)=0\)&lt;/span>对应的&lt;span class="math">\(x\)&lt;/span>。&lt;/p>
&lt;p>因此&lt;span class="math">\(x\)&lt;/span>可以根据求根公式 &lt;span class="math">\[x_{i+1}=x_{i}-\frac{f\prime(x_i)}{f\prime\prime(x_i)}\]&lt;/span> 求出。推广到多元函数上，&lt;span class="math">\({1}/{f\prime\prime(x_i)}\)&lt;/span>演变为&lt;span class="math">\(H^{-1}\)&lt;/span>，&lt;span class="math">\(f\prime(x_i)\)&lt;/span>演变为&lt;span class="math">\(\nabla_x f(x_i)\)&lt;/span>因此 &lt;span class="math">\[x_{i+1}=x_{i}-H^{-1}{\nabla_x f(x_i)}\]&lt;/span> 与根据二阶泰勒展开并求&lt;span class="math">\(f(\epsilon | x)\)&lt;/span>的最小值得到的结论一致。&lt;/p>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;a href="https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0">gradient descent in a nutshell – towardsdatascience.com&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s method in Optimization-wikipedia&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://people.rit.edu/lxwast/Linwei_Wangs_HomePage/CIS820_files/C3_GD.pdf">Gradient Descent Method – Rochester Institute of Technology&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml2-2008/graddescent.pdf">Using Gradient Descent in Optimization and Learning – University Collage London&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://math.stackexchange.com/questions/1659452/difference-between-gradient-descent-method-and-steepest-descent">Difference between Gradient Descent method and Steepest Descent – stack exchange&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.quora.com/In-optimization-why-is-Newtons-method-much-faster-than-gradient-descent">In optimization, why is Newton’s method much faster than gradient descent?&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>优化理论之非线性方法-高斯牛顿法与莱文贝格-马夸特方法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95-%E9%AB%98%E6%96%AF%E7%89%9B%E9%A1%BF%E6%B3%95%E4%B8%8E%E8%8E%B1%E6%96%87%E8%B4%9D%E6%A0%BC-%E9%A9%AC%E5%A4%B8%E7%89%B9%E6%96%B9%E6%B3%95/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95-%E9%AB%98%E6%96%AF%E7%89%9B%E9%A1%BF%E6%B3%95%E4%B8%8E%E8%8E%B1%E6%96%87%E8%B4%9D%E6%A0%BC-%E9%A9%AC%E5%A4%B8%E7%89%B9%E6%96%B9%E6%B3%95/</guid><description>
&lt;h2 id="高斯-牛顿法guass-newton-algorithm与莱文贝格-马夸特方法levenbergmarquardt-algorithm求解非线性最小二乘问题">高斯-牛顿法(Guass-Newton Algorithm)与莱文贝格-马夸特方法(Levenberg–Marquardt algorithm)求解非线性最小二乘问题&lt;!-- omit in toc -->&lt;/h2>
&lt;p>众所周知，最小二乘法通过最小化误差平方和获得最佳函数。有时候你可能产生疑问，为什么不能通过其他方式获得最优函数，比如说最小化误差的绝对值的和？本文中，我将会从概率的角度解释最小二乘法的依据（参考自andrew ng 《机器学习课程》 第三讲）。最小二乘问题可以分为线性最小二乘和非线性最小二乘两类，本文的目标是介绍两种经典的最小二乘问题解法：高斯牛顿法与莱文贝格-马夸特方法。实际上，后者是对前者以及梯度下降法的综合。&lt;/p>
&lt;h2 id="最小二乘法的概率解释probabilistic-interpretation">最小二乘法的概率解释(probabilistic interpretation)&lt;/h2>
&lt;p>以线性回归为例，假设最佳函数为 &lt;span class="math">\(y=\mathbf{\theta}^T\mathbf{x}\)&lt;/span> (&lt;span class="math">\(\theta,x\)&lt;/span>为向量), 对于每对观测结果&lt;span class="math">\((x^{(i)},y^{(i)})\)&lt;/span>，都有 &lt;span class="math">\[y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}\]&lt;/span> 其中 &lt;span class="math">\(\epsilon\)&lt;/span>为误差，基于一种合理的假设（中心极限定理），我们可以认为误差的分布服从正态分布(又称高斯分布)，即 &lt;span class="math">\(\epsilon \sim N(0,\sigma^2)\)&lt;/span> ，那么，我们可以认为&lt;span class="math">\(y^{(i)} \sim N(\theta^Tx^{(i)},\sigma^2)\)&lt;/span>,根据正态分布的概率公式 &lt;span class="math">\[P(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\]&lt;/span> 在统计学中，将所有的&lt;span class="math">\(P(y|x)\)&lt;/span>累乘作为&lt;span class="math">\(\theta\)&lt;/span>的似然函数,用以衡量&lt;span class="math">\(\theta\)&lt;/span>的或然性(likelihood) &lt;span class="math">\[L(\theta)=P(y|x;\theta)=\prod _{i=0}^m \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\]&lt;/span> 最佳的参数&lt;span class="math">\(\theta\)&lt;/span>应该是使得所有数据出现的概率最大的那个，这个过程称之为最大似然估计(maximum likelihood estimation)。为了数学计算上的便利，采用单调函数:log函数&lt;span class="math">\(l(\theta)=log(L(\theta))\)&lt;/span>，称为对数似然函数代表&lt;span class="math">\(L(\theta)\)&lt;/span>: &lt;span class="math">\[l(\theta)=log\prod_{i=0}^m \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\ =\sum_{i=0}^mlog\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\=mlog\frac{1}{\sqrt{2\pi}\sigma} – \frac{1}{2\sigma^2}\sum_{i=0}^m(y^{(i)}-\theta^Tx^{(i)})^2\]&lt;/span> 需要指出的是，在Andrew ng的讲解认为&lt;span class="math">\(\sigma\)&lt;/span>不影响&lt;span class="math">\(\theta\)&lt;/span>的决定，因此，最大化&lt;span class="math">\(l(\theta)\)&lt;/span>等同于最小化&lt;span class="math">\(\sum_{i=0}^m(y^{(i)}-\theta^Tx^{(i)})^2\)&lt;/span>。最小二乘法通过最小化误差平方和得到最佳函数的方法存在概率论方面的基础。&lt;/p>
&lt;h2 id="高斯-牛顿法guass-newton-algorithm">高斯-牛顿法(Guass-Newton Algorithm)&lt;/h2>
&lt;p>高斯-牛顿法是在牛顿法基础上进行修改得到的，用来(仅用于)解决非线性最小二乘问题。高斯-牛顿法相较牛顿法的最大优点是不需要计算二阶导数矩阵(Hessian矩阵)，当然，这项好处的代价是其仅适用于最小二乘问题。如下是其推导过程：&lt;/p>
&lt;p>最小二乘方法的目标是令残差的平方和最小： &lt;span class="math">\[f(\theta) = \frac{1}{2}\sum_{i=0}^mr(\mathbf{x_i})^2\]&lt;/span> 采用牛顿法求解&lt;span class="math">\(f(\theta)\)&lt;/span>的最小值，需要计算其梯度向量与Hessian矩阵。 &lt;span class="math">\[\nabla_\theta f =\frac{\partial f}{\partial \theta}=\sum r_i\frac{\partial r_i}{\partial \theta}\\
=\begin{bmatrix}
r(x_1)&amp;amp;r(x_2)&amp;amp;\dots&amp;amp;r(r_m)\end{bmatrix} \begin{bmatrix}
\nabla_\theta r(x_1)^T \\
\nabla_\theta r(x_2)^T \\
\vdots\\
\nabla_\theta r(x_m)^T \\
\end{bmatrix}\]&lt;/span>&lt;/p>
&lt;p>其中 &lt;span class="math">\[J_r(\theta)=\begin{bmatrix}\frac{\partial r_j}{\partial\theta_i}\end{bmatrix}_{j=1,\dots,m;i=1,\dots,n}=\begin{bmatrix}
\nabla_\theta r(x_1)^T \\
\nabla_\theta r(x_2)^T \\
\vdots\\
\nabla_\theta r(x_m)^T \\\
\end{bmatrix}\]&lt;/span> 称为&lt;span class="math">\(r\)&lt;/span>的雅各比(Jacobian)矩阵。因此上式可以写作 &lt;span class="math">\[\nabla_\theta f = r^TJ_r=J_r^Tr\]&lt;/span> 其中&lt;span class="math">\(r=\begin{bmatrix}r(x_1)&amp;amp;r(x_2)&amp;amp;\dots&amp;amp;r_m\end{bmatrix}^T\)&lt;/span>。再看Hessian矩阵的计算： &lt;span class="math">\[H = \begin{bmatrix}\frac{\partial^2f}{\partial \theta^2} \end{bmatrix}=\sum\begin{bmatrix}r_i\frac{\partial^2r_i}{\partial\theta^2}+(\frac{\partial r_i}{\partial \theta})(\frac{\partial r_i}{\partial \theta})^T \end{bmatrix}\]&lt;/span> 观察二阶导数项&lt;span class="math">\(r_i\frac{\partial^2r_i}{\partial\theta^2}\)&lt;/span>，因为残差 &lt;span class="math">\(r_i\approx 0\)&lt;/span>,因此我们可以认为此项接近于0而舍去。所以Hessian矩阵可以近似写成： &lt;span class="math">\[H\approx\sum\begin{bmatrix}(\frac{\partial r_i}{\partial\theta})(\frac{\partial r_i}{\partial\theta})^T\end{bmatrix}=J_r^TJ_r\]&lt;/span> 这里我们可以看到高斯-牛顿法相对于牛顿法的不同就是在于采用了近似的Hessian矩阵降低了计算的难度，但是同时，舍去项仅适用于最小二乘问题中残差较小的情形。&lt;/p>
&lt;p>将梯度向量，Hessian矩阵(近似)带入牛顿法求根公式，得到高斯-牛顿法的迭代式： &lt;span class="math">\[\theta_i = \theta_{i-1}-{(J_r^TJ_r)}^{-1}J_r^Tr\]&lt;/span> 只需要计算出&lt;span class="math">\(m\times n\)&lt;/span>的Jacobian矩阵便可以进行高斯-牛顿法的迭代，计算已经算是非常简便的了。&lt;/p>
&lt;h2 id="levenberg-marquart-算法">Levenberg-Marquart 算法&lt;/h2>
&lt;p>与牛顿法一样，当初始值距离最小值较远时，高斯-牛顿法的并不能保证收敛。并且当&lt;span class="math">\(J_r^TJ_r\)&lt;/span>近似奇异的时候，高斯牛顿法也不能正确收敛。Levenberg-Marquart 算法是对上述缺点的改进。L-M方法是对梯度下降法与高斯-牛顿法进行线性组合以充分利用两种算法的优势。通过在Hessian矩阵中加入阻尼系数&lt;span class="math">\(\lambda\)&lt;/span>来控制每一步迭代的步长以及方向： &lt;span class="math">\[(H+\lambda I)\epsilon = -J_r^Tr\]&lt;/span>&lt;/p>
&lt;ul>
&lt;li>当&lt;span class="math">\(\lambda\)&lt;/span>增大时，&lt;span class="math">\(H+\lambda I\)&lt;/span>趋向于&lt;span class="math">\(\lambda I\)&lt;/span>，因此&lt;span class="math">\(\epsilon\)&lt;/span>趋向于 &lt;span class="math">\(-\lambda J_r^Tr\)&lt;/span>，也就是梯度下降法给出的迭代方向；&lt;/li>
&lt;li>当&lt;span class="math">\(\lambda\)&lt;/span>减小时，&lt;span class="math">\(H+\lambda I\)&lt;/span>趋向于&lt;span class="math">\(H\)&lt;/span>，&lt;span class="math">\(\epsilon\)&lt;/span>趋向于&lt;span class="math">\(-H^{-1}J_r^Tr\)&lt;/span>，也就是高斯-牛顿法给出的方向。&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="math">\(\lambda\)&lt;/span>的大小通过如下规则调节，也就是L-M算法的流程：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/L-Mprocessure.png" alt="L-Mprocessure.png" />&lt;p class="caption">L-Mprocessure.png&lt;/p>
&lt;/div>
&lt;ol style="list-style-type: decimal">
&lt;li>初始化 &lt;span class="math">\(\theta_0\)&lt;/span>，&lt;span class="math">\(\lambda_0\)&lt;/span>。&lt;/li>
&lt;li>计算当前点&lt;span class="math">\(\theta_i\)&lt;/span>处的残差向量&lt;span class="math">\(r_i\)&lt;/span>与雅各比矩阵&lt;span class="math">\(J_r\)&lt;/span>。&lt;/li>
&lt;li>通过求解&lt;span class="math">\((H_i+\lambda I)\epsilon = -J_r^Tr_i\)&lt;/span>求解迭代方向&lt;span class="math">\(\epsilon\)&lt;/span>。&lt;/li>
&lt;li>计算&lt;span class="math">\(\theta_i^\prime=\theta_i+\epsilon\)&lt;/span>点处的残差向量&lt;span class="math">\(r_i^\prime\)&lt;/span>。&lt;/li>
&lt;li>如果&lt;span class="math">\(\|r_i\prime\|^2&amp;gt;\|r_i\|^2\)&lt;/span>?,即残差没有下降，则更新&lt;span class="math">\(\lambda = \beta\lambda\)&lt;/span>，增大&lt;span class="math">\(\lambda\)&lt;/span>重新回到第三步重新求解新的&lt;span class="math">\(\epsilon\)&lt;/span>。如果残差下降，则更新&lt;span class="math">\(\theta_{i+1} = \theta_i+\epsilon\)&lt;/span> ，到第二步，并且降低&lt;span class="math">\(\lambda=\alpha\lambda\)&lt;/span>，增大迭代步长。&lt;/li>
&lt;/ol>
&lt;p>在曲线拟合实践中，&lt;span class="math">\(\alpha\)&lt;/span>通常选取 0.1，&lt;span class="math">\(\beta\)&lt;/span>选取10。&lt;/p>
&lt;p>相比于高斯-牛顿法，L-M算法的优势在于非常的鲁棒，很多情况下即使初始值距离(局部)最优解非常远，仍然可以保证求解成功。作为一种阻尼最小二乘解法，LMA(Levenberg-Marquart Algorithm)的收敛速度要稍微低于GNA(Guass-Newton Algorithm)。L-M算法作为求解非线性最小二乘问题最流行的算法广泛被各类软件包实现，例如google用于求解优化问题的库Ceres Solver。后续，我会通过最小二乘圆拟合的案例给出L-M算法的实现细节。&lt;/p>
&lt;h2 id="最小二乘法的改进">最小二乘法的改进&lt;/h2>
&lt;p>传统的最小二乘法是针对线性函数与线性场景，典型的场景如下图：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Linear_regression.svg" alt="Linear_regression.svg" />&lt;p class="caption">Linear_regression.svg&lt;/p>
&lt;/div>
&lt;p>其需要优化的方程是： &lt;span class="math">\[
\min\|X\theta-y\|^2
\]&lt;/span> 其中，&lt;span class="math">\(\theta\)&lt;/span>表示参数向量，参数的个数与特征的个数一样（算上常数项的），&lt;span class="math">\(X\)&lt;/span>表示&lt;span class="math">\(m\)&lt;/span>个观察到的数据，每个数据有&lt;span class="math">\(n\)&lt;/span>个特征（算上常数项的），因此&lt;span class="math">\(X\)&lt;/span>是一个&lt;span class="math">\(m\times n\)&lt;/span>维矩阵。&lt;span class="math">\(y\)&lt;/span>是实际观察到的结果向量。详细表示起来则是： &lt;span class="math">\[
X=\begin{bmatrix}
——(x^{(1)})^T——\\
——(x^{(2)})^T——\\
\vdots\\
——(x^{(m)})^T——\\
\end{bmatrix},
x^{(i)}=\begin{bmatrix}
x^{(i)}_0(=1)\\
x^{(i)}_1\\
\vdots\\
x^{(i)}_{n-1}\\
\end{bmatrix},
\theta=\begin{bmatrix}
\theta_0\\
\theta_1\\
\vdots\\
\theta_{n-1}\\
\end{bmatrix},
y=\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_m\\
\end{bmatrix}
\]&lt;/span> 根据矩阵计算结果： &lt;span class="math">\[
\theta=(X^TX)^{-1}X^Ty
\]&lt;/span> 显然，&lt;span class="math">\(X\theta\)&lt;/span>只能表示直线结果，对于曲线结果无能为力。针对曲线场景，通常有两种改进思路，第一个就是将特征&lt;span class="math">\(x_j\)&lt;/span>的高阶项加入其中，举个简单例子，比如除了常数项，只有一个特征的场景： &lt;span class="math">\[
y=\theta_0+\theta_0 x_1+\theta_2 x^2_1+\theta_3 x^3_1
\]&lt;/span> 这样通过高阶多项式就可以拟合曲线，且根据拉格朗日插值法，只有阶数够多，那么就能够很好的拟合任意曲线。然而，高阶项数都是超参数，而且如果让每个特征都有高阶参数，那么参数数量就是几何数量级增长，难以控制参数规模。比如，有两个特征时，引入二次项就会有&lt;span class="math">\(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2+\theta_5x_1x_2\)&lt;/span>共6项，如果到三次方项，那会有更多参数。我们知道，当参数少时会有欠拟合现象，偏差大；参数过多就是过拟合，方差大，因此引入高次方项后，情况变得难以控制。同时，高次函数拟合会出现龙格现象。&lt;/p>
&lt;blockquote>
&lt;p>在数值分析领域中，龙格现象是在一组等间插值点上使用具有高次多项式的多项式插值时出现的区间边缘处的振荡问题。&lt;/p>
&lt;/blockquote>
&lt;div class="figure">
&lt;img src="../images/曲线拟合.png" alt="曲线拟合" />&lt;p class="caption">曲线拟合&lt;/p>
&lt;/div>
&lt;center>
左图：一阶函数欠拟合；中图：二阶函数适当拟合；右图：5阶函数过拟合
&lt;/center>
&lt;p>第二个改进思路就是使用&lt;strong>局部加权线性回归算法&lt;/strong>(Locally Weighted Linear Regression, LWLR)。它的核心思想就是弯曲点附近的局部曲线，接近整体曲线。我们知道传统的线性规划的代价函数为 &lt;span class="math">\[\sum_i (y^{(i)} − θ^T x^{(i)})^2\]&lt;/span> 而局部加权线性回归算法需要体现加权特性，它在不同线性函数之间进行加权，代价函数修改为： &lt;span class="math">\[\sum_i w^{(i)}(y^{(i)} − θ^T x^{(i)})^2\]&lt;/span> 这里&lt;span class="math">\(w^{(i)}\)&lt;/span>是非负的加权值，可以令&lt;span class="math">\(w^{(i)}=\exp{(-\frac{(x^{(i)}-x)^2}{2\tau^2})}\)&lt;/span>。虽然&lt;span class="math">\(w^{(i)}\)&lt;/span>有高斯分布形式，但是和高斯分布没有直接联系。显然，当&lt;span class="math">\(|x^{(i)}-x|\)&lt;/span>比较小时，&lt;span class="math">\(|x^{(i)}-x|\rightarrow 0\)&lt;/span>，此时&lt;span class="math">\(w^{(i)}\rightarrow 1\)&lt;/span>；当&lt;span class="math">\(|x^{(i)}-x|\)&lt;/span>比较大时，&lt;span class="math">\(w^{(i)}\rightarrow 0\)&lt;/span>，权重较小。&lt;/p>
&lt;p>局部加权线性回归的优化函数写成矩阵形式为： &lt;span class="math">\[
J(\theta)=(X\theta-y)^TW(X\theta-y)
\]&lt;/span> 多了一个权重矩阵&lt;span class="math">\(W\)&lt;/span>，它是一个对角矩阵，对角元素&lt;span class="math">\(w_{ii}={1\over 2}w^{(i)}\)&lt;/span>。根据矩阵求导公式可得： &lt;span class="math">\[
\begin{aligned}
\nabla J(\theta)&amp;amp;=\nabla (X\theta-y)^TW(X\theta-y)\\
&amp;amp;=2\frac{\partial{(X\theta-y)}}{\partial{\theta}}W(X\theta-y)\\
&amp;amp;=2X^TW(X\theta-y)
\end{aligned}
\]&lt;/span> 当取极值时，&lt;span class="math">\(\nabla J(\theta)=0\)&lt;/span>，即&lt;span class="math">\(2X^TW(X\theta-y)=0\)&lt;/span>，所以有： &lt;span class="math">\[
2X^TW(X\theta-y)=0\\
X^TWX\theta=X^TWy\\
\theta=(X^TWX)^{-1}X^TWy
\]&lt;/span> 关于矩阵求导的细节，请查阅网页资料&lt;a href="../网页资料/线性代数与矩阵-矩阵求导表.html">线性代数与矩阵-矩阵求导表&lt;/a>&lt;/p>
&lt;p>此外，整个代价函数中只有一个超参数&lt;span class="math">\(\tau\)&lt;/span>，称为&lt;strong>带宽参数&lt;/strong>。它控制着加权参数随着对&lt;span class="math">\(x\)&lt;/span>的远离，衰减的速度。当&lt;span class="math">\(\tau\)&lt;/span>值比较大时，&lt;span class="math">\(w^{(i)}\)&lt;/span>衰减得慢，容易出现欠拟合；当&lt;span class="math">\(\tau\)&lt;/span>值比较小时，&lt;span class="math">\(w^{(i)}\)&lt;/span>衰减得快，容易出现过拟合。从词义上来看，带宽参数意味着能够容纳周围参数的多少，容纳的越多函数越平缓，当&lt;span class="math">\(\tau\rightarrow \infty\)&lt;/span>时，&lt;span class="math">\(w^{(i)}\rightarrow 1\)&lt;/span>此时，带宽无限大，容纳了所有数据点，局部加权线性回归算法退化为传统线性规划算法。当&lt;span class="math">\(\tau\)&lt;/span>很小时，&lt;span class="math">\(w^{(i)}\)&lt;/span>衰减很快，只有在当前点才有效，此时局部加权线性回归算法退化为插值算法。这个算法的问题在于，对于每一个要预测的点，都要重新依据整个数据集计算一个线性回归模型出来，使得算法代价极高。局部加权线性回归效果如下&lt;/p>
&lt;div class="figure">
&lt;img src="../images/局部加权线性规划.png" alt="局部加权线性规划.png" />&lt;p class="caption">局部加权线性规划.png&lt;/p>
&lt;/div>
&lt;center>
上图：带宽参数过大，欠拟合；中图：适当拟合；下图：带宽参数过小，过拟合
&lt;/center>
&lt;p>在局部加权线性回归算法中，参数数量&lt;span class="math">\(w^{(i)}\)&lt;/span>是随着数据量增加而线性增加的，这个参数不固定的监督学习算法成为&lt;strong>非参数学习算法&lt;/strong>；相对的参数固定的监督学习算法，比如传统的线性规划就称作&lt;strong>参数学习算法&lt;/strong>。&lt;/p>
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;a href="http://ece.eng.umanitoba.ca/undergraduate/ECE4850T02/Lecture%20Slides/MLRegression.pdf">maximum likelihood regression-university of manitoba&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://open.163.com/special/opencourse/machinelearning.html">过拟合与欠拟合-网易公开课：斯坦福大学机器学习课程&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml2-2008/graddescent.pdf">Using Gradient Descent for Optimization and Learning&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mads.lanl.gov/presentations/Leif_LM_presentation_m.pdf">Numerical Optimization using the Levenberg-Marquardt Algorithm-Los Alamos National Laboratory&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://people.cas.uab.edu/~mosya/cl/">Circular and Linear Regression Fitting Circles and Lines by Least Squares – Nikolai Chernov – UAB&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>优化理论之对偶单纯形法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%AF%B9%E5%81%B6%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%AF%B9%E5%81%B6%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</guid><description>
&lt;h2 id="对偶单纯行法">对偶单纯行法&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#前期准备">前期准备&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶单纯形法的基本思想">对偶单纯形法的基本思想&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="前期准备">前期准备&lt;/h2>
&lt;p>线性规划标准形式 &lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\sum_{j=1}^n c_jx_j\\
\mathop{s.t.}\; &amp;amp;\sum_{j=1}^n \alpha_{ij}x_j=b_i,\ &amp;amp;i=1,\dotsb,m\\
&amp;amp;x_j\geq 0,\ &amp;amp;j=1,\dotsb,n\\
\end{aligned}
\]&lt;/span> 用矩阵表示为： &lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\boldsymbol c^T \boldsymbol x\\
\mathop{s.t.}\; &amp;amp;\boldsymbol A \boldsymbol x = \boldsymbol b\\
&amp;amp;\boldsymbol x \geq \boldsymbol 0\\
\end{aligned}\tag{2.1}
\]&lt;/span>&lt;/p>
&lt;h2 id="对偶单纯形法的基本思想">对偶单纯形法的基本思想&lt;/h2>
&lt;p>前面用单纯形法求解问题&lt;span class="math">\((2.1)\)&lt;/span>时，往往需要引进人工变量，通过解一阶段问题求初始基本可行解。现在利用&lt;strong>对偶性质&lt;/strong>给出一种&lt;strong>不需引进人工变量&lt;/strong>的求解方法，这就是&lt;strong>对偶单纯形法&lt;/strong>。为介绍这种方法的基本思想，先引入&lt;strong>对偶可行的基本解&lt;/strong>的概念。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>对偶可行的基本解&lt;/strong>:设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>是&lt;span class="math">\((2.1)\)&lt;/span>式的一个基本解（不一定是可行解），它对应的基矩阵为&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>，若其对应的单纯性乘子&lt;span class="math">\(\boldsymbol{w^T=c^T_B B^{-1}}\)&lt;/span>是&lt;span class="math">\((2.1)\)&lt;/span>式的对偶问题的&lt;strong>可行解&lt;/strong>，即对所有&lt;span class="math">\(j\)&lt;/span>，&lt;span class="math">\(\boldsymbol{w^T p_j}-c_j\leq 0\)&lt;/span>成立，则称&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>为&lt;strong>原问题的‘对偶可行’的基本解&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>显然，&lt;strong>对偶可行的基本解不一定是原问题的可行解&lt;/strong>。当对偶可行的基本解&lt;strong>是&lt;/strong>原问题的可行解时，由于&lt;strong>判别数均小于或等于零&lt;/strong>，因此它就是原问题的&lt;strong>最优解&lt;/strong>。即对偶可行的基本解如果是原问题的可行解，那么它是最优解。&lt;/p>
&lt;p>对偶单纯形法的基本思想是，&lt;strong>从原问题的一个对偶可行的基本解出发&lt;/strong>，求改进的对偶可行的基本解，当得到的对偶可行的基本解是原问题的可行解时，就达到最优解。那么现在有两个问题&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>如何先找到一个对偶可行的基本解（&lt;strong>解扩充问题&lt;/strong>）&lt;/li>
&lt;li>如何改进这个对偶可行的基本解使得其在原问题可行（&lt;strong>改进对偶问题&lt;/strong>）&lt;/li>
&lt;/ol></description></item><item><title>优化理论之最优化条件</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E6%9C%80%E4%BC%98%E5%8C%96%E6%9D%A1%E4%BB%B6/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E6%9C%80%E4%BC%98%E5%8C%96%E6%9D%A1%E4%BB%B6/</guid><description>
&lt;h2 id="优化理论最优化条件">优化理论最优化条件&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#最优解的存在性">最优解的存在性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束问题最优化条件">无约束问题最优化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束问题最优化条件">有约束问题最优化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化一阶条件">有约束数值优化一阶条件&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#可行方向法">可行方向法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#切锥下的一阶条件">切锥下的一阶条件&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#kkt点">KKT点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化二阶条件">有约束数值优化二阶条件&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二阶必要条件">二阶必要条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二阶充分条件">二阶充分条件&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;p>我们在本文主要谈论最小化： &lt;span class="math">\[f:\mathbb{R}^n→R，f为连续函数。对于f的可行域\Omega，求\\
\min_{x ∈ \Omega} f(x)\]&lt;/span>&lt;/p>
&lt;h2 id="最优解的存在性">最优解的存在性&lt;/h2>
&lt;p>我们需要指出的是，函数并没有指出在可行域内有最优解。即使有最优解，也未必在可行域内。因此，我们需要明确哪些情况最优解是一定存在的。&lt;/p>
&lt;p>根据基本的维尔思特拉斯极值定理有&lt;/p>
&lt;blockquote>
&lt;p>定理1：维尔思特拉斯极值定理：每一定义在紧集上的连续函数，都可以在定义域内取到极值点。这也表明：紧集上的连续函数都是有界的。&lt;/p>
&lt;/blockquote>
&lt;p>在此基础上，我们介绍一个更深的定理，但是现阶段不太会用到：&lt;/p>
&lt;blockquote>
&lt;p>定理2： 若函数&lt;span class="math">\(f\)&lt;/span>在n维闭区域&lt;span class="math">\(S\)&lt;/span>上连续并且向正无穷发散(coercive，即如果&lt;span class="math">\(lim||x||⇒∞,f(x)=+∞\)&lt;/span>)，那么&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(S\)&lt;/span>上一定有全局最小值。&lt;/p>
&lt;/blockquote>
&lt;p>以上定理只能保证最小值&lt;strong>存在&lt;/strong>，没有建立最小值和极小值之间的关系。但是，对于一类特殊的函数，它在一定区域内的极小值一定是最小值，这类函数即&lt;strong>凸函数&lt;/strong>。凸函数为定义在凸区间上的一种函数，它满足任意两点的连线位于抽象的函数曲面之下；而凸区间则满足任意两点连线仍然在区间中。定义在凸区间内的严格凸函数有唯一的极小值，该极小值为该函数在该区间上的最小值。&lt;/p>
&lt;h2 id="无约束问题最优化条件">无约束问题最优化条件&lt;/h2>
&lt;p>本节研究无约束问题：（基于费马引理） &lt;span class="math">\[\min f(x),x\in R^n（1）\]&lt;/span> 的最优性条件，包括一阶条件和二阶条件。&lt;/p>
&lt;p>应该指出，实际上我们只是求一个局部（局部严格）极小点，而非总体最小点。尽管我们也会考虑总体最小值的情况，但是一般来说这是一个相当困难的任务。在很多实际应用中，求局部最小值已经满足了问题的要求。因此本文章所指的极小值是指局部极小值。仅当问题具有某种凸性质，局部最小值才是总体最小值。&lt;/p>
&lt;p>设&lt;span class="math">\(f\)&lt;/span>的一阶导数和二阶导数存在，且分别表示为： &lt;span class="math">\[g(x)=\nabla f(x),G(x)=\nabla^2f(x),\]&lt;/span> 则我们有以下三个定理：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理1：&lt;/strong>（一阶必要条件） 设&lt;span class="math">\(f:D\subset R^n\rightarrow R^1\)&lt;/span>在开集D上连续可微，若&lt;span class="math">\(x^\ast\in D\)&lt;/span>是（1）上的局部极小点，则 &lt;span class="math">\[g(x^\ast)=0(2)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>证明1：&lt;/strong> 反证法&lt;/p>
&lt;p>假定&lt;span class="math">\(g(x^\ast)\neq0\)&lt;/span>，即存在非0梯度，那么取下降的负梯度&lt;span class="math">\(d=-g(x^\ast)\)&lt;/span>，则 &lt;span class="math">\[g(x^\ast)^Td=-g(x^\ast)^Tg(x^\ast)&amp;lt;0\]&lt;/span> 由于d是下降法向，从而在以&lt;span class="math">\(x^\ast\)&lt;/span>为中心邻域存在&lt;span class="math">\(\delta&amp;gt;0\)&lt;/span>，使得 &lt;span class="math">\[f(x^\ast+\alpha d)&amp;lt;f(x^\ast),\forall\alpha\in(0,\delta).\]&lt;/span> 那我们可以取&lt;span class="math">\(\overline{\delta}=\delta\|d\|\)&lt;/span>，因&lt;span class="math">\(\alpha&amp;lt;\delta\)&lt;/span>，故有： &lt;span class="math">\[\|\alpha d\|=\alpha\|d\|=\overline{\delta}\]&lt;/span> 因此，存在&lt;span class="math">\(x^\ast+\alpha d\in N(x^\ast)\)&lt;/span>，使得 &lt;span class="math">\[f(x^\ast+\alpha d)&amp;lt;f(x^\ast)，\]&lt;/span> 这与&lt;span class="math">\(x^\ast\)&lt;/span>是局部极小点矛盾。&lt;/p>
&lt;p>证明1完。&lt;/p>
&lt;p>我们也可以用费马引理来更简单的说明这个定理：&lt;/p>
&lt;blockquote>
&lt;p>引理1：费马引理：设函数&lt;span class="math">\(f(x)\)&lt;/span>在点&lt;span class="math">\(x_0\)&lt;/span>的某邻域&lt;span class="math">\(U(x_0)\)&lt;/span>内有定义，并且在&lt;span class="math">\(x_0\)&lt;/span>处可导，如果对任意的&lt;span class="math">\(x\in U(x_0)\)&lt;/span>，有 &lt;span class="math">\[f(x)\le f(x_0)或f(x)\ge f(x_0)\]&lt;/span> 那么&lt;span class="math">\(f^\prime(x_0)=0\)&lt;/span>。&lt;/p>
&lt;p>推论1：函数&lt;span class="math">\(f\)&lt;/span>在定义域&lt;span class="math">\(\Omega\)&lt;/span>内的最大值和最小值只能在&lt;strong>边界上，不可导的点，或驻点&lt;/strong>取得。&lt;/p>
&lt;/blockquote>
&lt;p>因此，对于连续可微函数，我们只需要在驻点和端点查找最小值即可。驻点即为一阶导数为0的点。&lt;/p>
&lt;p>如果只是一阶导数是0，那么到底能不能确定这个点是极小值点呢？这就需要二阶导数。如果二阶导数的海森矩阵半正定那就是极小值点，半负定就是极大值点，不定就是鞍点。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理2：&lt;/strong>（二阶必要条件）设&lt;span class="math">\(f:D\subset R^n\rightarrow R^1\)&lt;/span>在开集&lt;span class="math">\(D\)&lt;/span>上二阶连续可微，若&lt;span class="math">\(x^\ast\in D\)&lt;/span>是（1）的局部极小点，则： &lt;span class="math">\[g(x^\ast)=0\quad G(x^\ast)\geq0 (3)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>证明2：&lt;/strong> 反证法&lt;/p>
&lt;p>(3)的前半部分已经在&lt;strong>证明1&lt;/strong>得证。为了证明第二部分，即半正定，我们先假设&lt;span class="math">\(G(x^\ast)\)&lt;/span>不定，并设&lt;span class="math">\(N_\delta(x^\ast)\)&lt;/span>是&lt;span class="math">\(x^\ast\)&lt;/span>的邻域。有连续性可知，对所有&lt;span class="math">\(x\in N_\delta(x^\ast)\)&lt;/span>，&lt;span class="math">\(G(x)\)&lt;/span>不定。今选择&lt;span class="math">\(\epsilon\)&lt;/span>和向量d，使得&lt;span class="math">\(x^\ast+\epsilon d\in N_\delta(x^\ast)\)&lt;/span>，且满足&lt;span class="math">\(d^TG(x^\ast+\epsilon d)d&amp;lt;0\)&lt;/span>。利用&lt;span class="math">\(g(x^\ast)=0\)&lt;/span>，有&lt;span class="math">\(f(x+\epsilon d)\)&lt;/span>在&lt;span class="math">\(x^\ast\)&lt;/span>处的泰勒展开式： &lt;span class="math">\[f(x^\ast+\epsilon d)=f(x^\ast)+\frac{1}{2}\epsilon^2d^TG(x^\ast+\theta\epsilon d)d\]&lt;/span> 其中，&lt;span class="math">\(0\leq\theta\leq1\)&lt;/span>，从而，&lt;span class="math">\(f(x^\ast+\epsilon d)&amp;lt;f(x^\ast)\)&lt;/span>。这与&lt;span class="math">\(x^\ast\)&lt;/span>是局部极小点矛盾。所以&lt;span class="math">\(G(x^\ast)\)&lt;/span>在邻域内必须大于等于0，即半正定。&lt;/p>
&lt;p>满足&lt;span class="math">\(g(x^\ast)=0\)&lt;/span>的点&lt;span class="math">\(x^\ast\)&lt;/span>称为函数&lt;span class="math">\(f\)&lt;/span>的平稳点或驻点。如果&lt;span class="math">\(g(x^\ast)=0\)&lt;/span>，则有可能是极小点，也可能是极大点，也可能不是极值点，即鞍点。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理3：&lt;/strong>（二阶充分条件）设&lt;span class="math">\(f:D\subset R^n\rightarrow R^1\)&lt;/span>在开集&lt;span class="math">\(D\)&lt;/span>上二阶连续可微，则&lt;span class="math">\(x^\ast\in D\)&lt;/span>是&lt;span class="math">\(f\)&lt;/span>的一个严格局部极小点的充分条件是： &lt;span class="math">\[g(x^\ast)=0 且G(x^\ast)是正定矩阵(4)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>一般地，目标函数的平稳点不一定是极小值点。但若目标函数是凸函数，则其平稳点就是其极小点，且为最小点。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理4：&lt;/strong>（凸充分条件）设&lt;span class="math">\(f:D\subset R^n\rightarrow R^1\)&lt;/span>在开集&lt;span class="math">\(D\)&lt;/span>上为凸函数，且&lt;span class="math">\(f\in C^1\)&lt;/span>，则&lt;span class="math">\(x^\ast\)&lt;/span>是总体极小点的充分必要条件是：&lt;span class="math">\(g(x^\ast)=0\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="有约束问题最优化条件">有约束问题最优化条件&lt;/h2>
&lt;p>有约束条件的可行域不再是整个空间&lt;span class="math">\(R^n\)&lt;/span>，我们记可行域是&lt;span class="math">\(\Omega\)&lt;/span>，即 &lt;span class="math">\[\min_{\vec x} f(\vec x)\\
subject\quad to:\quad \vec x ∈ \Omega\]&lt;/span> 我们假定&lt;span class="math">\(f\)&lt;/span>是连续可微函数，并且可行域是非空闭集。&lt;/p>
&lt;h3 id="有约束数值优化一阶条件">有约束数值优化一阶条件&lt;/h3>
&lt;h4 id="可行方向法">可行方向法&lt;/h4>
&lt;p>我们在研究无约束优化一阶条件的基础上，特别想知道有哪些结论可以直接用到有约束情况下的。首先我们考虑可行方向：&lt;/p>
&lt;blockquote>
&lt;p>定义1：可行方向：对于可行域&lt;span class="math">\(\Omega \subset R^n\)&lt;/span>，有一个点&lt;span class="math">\(\vec x∈ \Omega\)&lt;/span>，我们说向量&lt;span class="math">\(\vec d\)&lt;/span>是&lt;strong>可行方向&lt;/strong>，如果存在一个步长&lt;span class="math">\(\bar t&amp;gt;0，\forall t ∈[0,\bar t]，\vec x+t\vec d ∈ \Omega\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>对于存在可行方向的点，我们存在以下定理&lt;/p>
&lt;blockquote>
&lt;p>定理5：有可行方向的有约束一阶条件：对于一个局部最优点&lt;span class="math">\(\vec x\)&lt;/span>，在这个点所有存在的可行方向&lt;span class="math">\(f&amp;#39;(\vec x+t\vec d)\)&lt;/span>，我们都有&lt;span class="math">\(f&amp;#39;(\vec x+t\vec d)≥0\)&lt;/span>.&lt;/p>
&lt;/blockquote>
&lt;p>显然这个定理是无约束条件的类比版，只不过可行方向受到可行域限制。对于凸多边形，可行方向法是充分的，我觉得单纯性法就是可行方向法的一个应用。但是对于很多曲线，可行方向法就不适用了，例如： &lt;span class="math">\[\Omega=\{(x_1,x_2):x_1^2+x_2^2=1\}\]&lt;/span> 每个点的可行方向都是&lt;span class="math">\(\vec 0\)&lt;/span>，这就没法做了。&lt;/p>
&lt;h4 id="切锥下的一阶条件">切锥下的一阶条件&lt;/h4>
&lt;p>为了克服可行方向法的弊端，我们使引入切锥。切锥是通过序列极限定义的：&lt;/p>
&lt;blockquote>
&lt;p>定义2：切锥：给定非空闭集&lt;span class="math">\(C \subset R^n\)&lt;/span>。如果在&lt;span class="math">\(C\)&lt;/span>内，存在收敛到&lt;span class="math">\(\vec x\)&lt;/span>的序列&lt;span class="math">\(\{\vec x^k\}\)&lt;/span>和收敛到&lt;span class="math">\(0\)&lt;/span>的正数序列&lt;span class="math">\(\{\tau^k\}\)&lt;/span>，使得向量&lt;span class="math">\(\vec d^k=(\vec x^k-x)/\tau^k→\vec d\)&lt;/span>。那么&lt;span class="math">\(\vec d\)&lt;/span>就是点&lt;span class="math">\(\vec x\)&lt;/span>的切线方向。点&lt;span class="math">\(\vec x\)&lt;/span>所有的切线方向组成的锥，就是切锥。 &lt;span class="math">\[T_c(\vec x)=\{\vec d ∈ R^n:\exists\{\vec x^k\}∈ C\}和\{\tau^k\}→0,d=\lim_{k→ ∞}\frac{\vec x^k-\vec x}{\tau^k}\}\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>这个定义主要在说什么呢？假设有一个闭集，然后有一点&lt;span class="math">\(\vec x\)&lt;/span>在闭集内，与此同时闭集内还有一组逐渐收敛于&lt;span class="math">\(\vec x点的序列。在点\)&lt;/span>x附近（neighborhood）所有可以收敛的方向都是这个点的tangent direction。看完这个定义会发现，点$x在该闭集上的tangent cone一般来讲是从该点作切线，并包含闭集内部的向外延伸的锥。下图是几个例子：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/切锥.jpg" alt="切锥" />&lt;p class="caption">切锥&lt;/p>
&lt;/div>
&lt;p>可行方向的约束作用在一些情况下略显严格，会导致空集的产生（例如非凸集合）。因此，我们需要适当放宽这个约束。因此，我们考虑用切锥来替代。在约束条件是&lt;strong>凸集&lt;/strong>的情况下，切锥等同于可行方向，而对于非凸情况，切锥避免了空集的产生。因此，我们可以放宽&lt;strong>定理5&lt;/strong>：&lt;/p>
&lt;blockquote>
&lt;p>定理6：切锥下的有约束一阶条件：对于一个局部最优点&lt;span class="math">\(\vec x\)&lt;/span>，有 &lt;span class="math">\[\nabla f(\vec x)^T\vec d≥0,\forall d ∈ T(\vec x|\Omega)\]&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>显然，无约束一阶条件是定理6的特殊情况。&lt;/p>
&lt;h3 id="kkt点">KKT点&lt;/h3>
&lt;p>详见《KKT与拉格朗日》&lt;/p>
&lt;h3 id="有约束数值优化二阶条件">有约束数值优化二阶条件&lt;/h3>
&lt;p>我们知道，满足&lt;span class="math">\(p^T∇f(x^⋆)&amp;gt;0\)&lt;/span>的方向&lt;span class="math">\(p\)&lt;/span>可以使函数值增大，满足&lt;span class="math">\(p^T∇f(x^⋆)&amp;lt;0\)&lt;/span>的方向&lt;span class="math">\(p\)&lt;/span>可以使函数值减小，但对于&lt;span class="math">\(p^T∇f(x^⋆)=0\)&lt;/span>的方向&lt;span class="math">\(p\)&lt;/span>，我们不知道它到底使函数值增大还是减小。如下图所示，&lt;span class="math">\(p^T∇f(x^⋆)=0\)&lt;/span>可以使函数值增大、减小或不变。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/IXMRBAL.png" alt="IXMRBAL.png" />&lt;p class="caption">IXMRBAL.png&lt;/p>
&lt;/div>
&lt;p>将这些模棱两可的方向的集合称为“关键锥”（critical cone） &lt;span class="math">\[\mathcal{C}(x^{\star},\gamma)=\{w\in\mathcal{F}(x)|\nabla d_j(x^{\star})^Tw=0,\forall j\in\mathcal{A}(x^{\star})\text{ with }\lambda_j&amp;gt;0\}\]&lt;/span> 其中 &lt;span class="math">\[\mathcal{F}(x)=\left\{p|p \text{ 满足 }\begin{cases}\nabla c_i(x)^Tp=0\quad \forall i \\ \nabla d_j(x)^Tp\ge 0 \quad\forall d_j\in\mathcal{A}(x)\end{cases}\right\}\]&lt;/span>&lt;/p>
&lt;h4 id="二阶必要条件">二阶必要条件&lt;/h4>
&lt;ul>
&lt;li>D1. 满足KKT条件&lt;/li>
&lt;li>D2. &lt;span class="math">\(p^T\nabla^2\mathcal{L}(x^{\star},\gamma)p\ge 0 \quad \forall p\in\mathcal{C}(x^{\star},\gamma)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;h4 id="二阶充分条件">二阶充分条件&lt;/h4>
&lt;ul>
&lt;li>E1. 满足KKT条件&lt;/li>
&lt;li>E2. &lt;span class="math">\(p^T\nabla^2\mathcal{L}(x^{\star},\gamma)p\gt 0 \quad \forall p\in\mathcal{C}(x^{\star},\gamma)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>类似于无约束的二阶条件，只不过约束问题的二阶条件是“有选择地”正定（或半正定）即可。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/最优化二阶条件.png" alt="最优化二阶条件" />&lt;p class="caption">最优化二阶条件&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>在Case I中，&lt;span class="math">\(f(x)\)&lt;/span>的斜率变化快于&lt;span class="math">\(d_1(x)\)&lt;/span>，所以拉格朗日函数的二次型严格大于零&lt;/li>
&lt;li>在Case II中，&lt;span class="math">\(f(x)\)&lt;/span>的斜率变化等于&lt;span class="math">\(d_1(x)\)&lt;/span>，所以拉格朗日函数的二次型等于零&lt;/li>
&lt;li>在Case III中，&lt;span class="math">\(f(x)\)&lt;/span>的斜率变化慢于&lt;span class="math">\(d_1(x)\)&lt;/span>，所以拉格朗日函数的二次型严格小于零&lt;/li>
&lt;/ul></description></item><item><title>优化理论之线性规划的对偶</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%AF%B9%E5%81%B6/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%AF%B9%E5%81%B6/</guid><description>
&lt;h2 id="线性规划的对偶原理">线性规划的对偶原理&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#对偶问题的表达">对偶问题的表达&lt;/a>&lt;/li>
&lt;li>&lt;a href="#1对称形式的对偶">（1）对称形式的对偶&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2非对称形式的对偶">（2）非对称形式的对偶&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3一般情形">（3）一般情形&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶问题的对偶是原问题">对偶问题的对偶是原问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶规划的一般规则重要">对偶规划的一般规则（重要）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶定理">对偶定理&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="对偶问题的表达">对偶问题的表达&lt;/h2>
&lt;p>线性规划中的对偶可以概括为&lt;strong>三种形式&lt;/strong>：&lt;/p>
&lt;h3 id="对称形式的对偶">（1）对称形式的对偶&lt;/h3>
&lt;p>原问题 &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax≥b}（注意这里是≥）\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.1}
\]&lt;/span> 对偶问题 &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{w^Tb}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{w^TA≤c^T}（注意这里是≤）\\
&amp;amp;\boldsymbol{w^T≥0}
\end{aligned}\tag{1.2}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{A=(p_1,p_2,\dotsb,p_n)}\)&lt;/span>是&lt;span class="math">\(m×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{b}=(b_1,b_2,\dotsb,b_m)^T\)&lt;/span>是&lt;strong>m维列&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{c^T}=(c_1,c_2,\dotsb,c_n)\)&lt;/span>是&lt;strong>n维行&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{x}=(x_1,x_2,\dotsb,x_n)^T\)&lt;/span>是原问题的变量组成的&lt;strong>n维列&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{w^T}=(w_1,w_2,\dotsb,w_m)\)&lt;/span>是由对偶问题变量组成的&lt;strong>m维行&lt;/strong>向量。&lt;/p>
&lt;p>在原问题&lt;span class="math">\((1.1)\)&lt;/span>中，目标函数是&lt;span class="math">\(\boldsymbol{c^T}\)&lt;/span>和&lt;span class="math">\(x\)&lt;/span>的内积，&lt;span class="math">\(\boldsymbol{Ax≥b}\)&lt;/span>包含m个不等式约束，若其中每个不等式约束记作&lt;span class="math">\(\boldsymbol{A_ix≥}b_i\)&lt;/span>，&lt;span class="math">\(\boldsymbol{A_i}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>的第i行，变量&lt;span class="math">\(\boldsymbol{x}\)&lt;/span>有非负限制。&lt;/p>
&lt;p>在对偶问题中&lt;span class="math">\((1.2)\)&lt;/span>中，目标函数是&lt;span class="math">\(\boldsymbol{b^T}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w}\)&lt;/span>的内积，&lt;span class="math">\(\boldsymbol{w^TA≤c^T}\)&lt;/span>包含n个不等式约束，每个约束条件记作&lt;span class="math">\(\boldsymbol{w^Tp_j}≤c_j\)&lt;/span>，对偶变量&lt;span class="math">\(w_i\)&lt;/span>也有非负限制。&lt;/p>
&lt;p>根据对称对偶的定义，原问题中约束条件&lt;span class="math">\(\boldsymbol{A_ix≥}b_i\)&lt;/span>个数，敲好等于对偶变量的个数；原问题中变量的个数，恰好等于对偶问题中约束条件&lt;span class="math">\(\boldsymbol{w^Tp_j}≤c_j\)&lt;/span>的个数。&lt;/p>
&lt;p>&lt;span class="math">\[
\begin{aligned}
对称形式：&amp;amp;原问题约束条件\rightarrow 对偶变量\\
&amp;amp;原问题变量\rightarrow 对偶约束\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>对于对偶问题引入理解，&lt;a href="../文档/优化理论/为何引入对偶问题.pdf">为何引入对偶问题&lt;/a>从经济学和数学两个方面给出了解释。（但是强对偶性即原问题最优解=对偶问题最优解，并没有被证明）。以下非对称形式对可以转换成对称形式，因此引入他们的根本理由都可以通过对称问题的对偶理解。&lt;/p>
&lt;h3 id="非对称形式的对偶">（2）非对称形式的对偶&lt;/h3>
&lt;p>考虑具有&lt;strong>等式约束条件&lt;/strong>的线性规划问题（这个反而是线性规划的标准形式） &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax=b}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.3}
\]&lt;/span> 我们已经引入了对称形式的对偶问题，现在我们可以通过一个trick将其&lt;strong>转换成对称形式&lt;/strong>：&lt;span class="math">\(=\Leftrightarrow ≥ \&amp;amp;\&amp;amp; ≤\)&lt;/span> &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax≥b}\\
\quad&amp;amp;\boldsymbol{Ax≤b\Rightarrow -Ax≥-b}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.4}
\]&lt;/span> 其中对前两行限制条件做一下变形： &lt;span class="math">\[
\left . \begin{aligned}
\quad&amp;amp;\boldsymbol{Ax≥b}\\
\quad&amp;amp;\boldsymbol{-Ax≥-b}\end{aligned} \right \}\Rightarrow \begin{bmatrix}\boldsymbol{A} \\ \boldsymbol{-A}\end{bmatrix}\boldsymbol{x}≥\begin{bmatrix}\boldsymbol{b} \\ \boldsymbol{-b}\end{bmatrix}
\]&lt;/span> 现在一个有&lt;span class="math">\(2m\)&lt;/span>个限制条件，因此对偶问题有&lt;span class="math">\(2m\)&lt;/span>个变量记为&lt;span class="math">\(\boldsymbol{[w&amp;#39;^T,w&amp;#39;&amp;#39;^T]},其中\boldsymbol{w&amp;#39;^T,w&amp;#39;&amp;#39;^T}\)&lt;/span>都是m维行向量。依照对称形式对偶问题的模样，我们可以写出&lt;span class="math">\((1.4)\)&lt;/span>的对偶形式 &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{[w&amp;#39;^T,w&amp;#39;&amp;#39;^T]}\begin{bmatrix}\boldsymbol{b} \\ \boldsymbol{-b}\end{bmatrix}=\boldsymbol{(w&amp;#39;^T-w&amp;#39;&amp;#39;^T)b}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{[w&amp;#39;^T,w&amp;#39;&amp;#39;^T]}\begin{bmatrix}\boldsymbol{A} \\ \boldsymbol{-A}\end{bmatrix}=\boldsymbol{(w&amp;#39;^T-w&amp;#39;&amp;#39;^T)A}≤\boldsymbol{c}\\
&amp;amp;\boldsymbol{w&amp;#39;^T,w&amp;#39;&amp;#39;^T≥0}
\end{aligned}
\]&lt;/span> 如果我们令&lt;span class="math">\(\boldsymbol{w^T=w&amp;#39;^T-w&amp;#39;&amp;#39;^T}\)&lt;/span>，我们可以得到和对称形式结构类似的对偶问题，变量数量也从&lt;span class="math">\(2m\rightarrow m\)&lt;/span>，但是&lt;strong>非负限制条件没有了&lt;/strong>： &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{w^Tb}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{w^TA}≤\boldsymbol{c}\\
\end{aligned}\tag{1.5}
\]&lt;/span> 到此，&lt;span class="math">\((1.5)\)&lt;/span>是&lt;span class="math">\((1.3)\)&lt;/span>的对偶问题。&lt;strong>他们的结构很相似，唯一的不同在于原问题的m个等式约束对应的对偶问题的m个变量没有正负号限制，它们称为非对称对偶&lt;/strong>。&lt;/p>
&lt;h3 id="一般情形">（3）一般情形&lt;/h3>
&lt;p>实际问题中，线性规划问题常常同时含有&lt;span class="math">\(≥，≤，=\)&lt;/span>多种限制条件，它们组成的原问题为： &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{A_1x≥b_1}\\
\quad&amp;amp;\boldsymbol{A_2x=b_2}\\
\quad&amp;amp;\boldsymbol{A_3x≤b_3}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.6}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{A_1}\)&lt;/span>是&lt;span class="math">\(m_1×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{A_2}\)&lt;/span>是&lt;span class="math">\(m_2×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{A_3}\)&lt;/span>是&lt;span class="math">\(m_3×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{b_1,b_2,b_3}\)&lt;/span>分别是&lt;span class="math">\(m_1,m_2,m_3\)&lt;/span>维列向量，&lt;span class="math">\(\boldsymbol{c^T}\)&lt;/span>是n维行向量，&lt;span class="math">\(\boldsymbol{x}\)&lt;/span>是n维列向量。&lt;/p>
&lt;p>显然，我们可以使用&lt;strong>添加松弛变量的方式将一般形式&lt;span class="math">\((1.6)\)&lt;/span>转换为非对称形式&lt;span class="math">\((1.3)\)&lt;/span>&lt;/strong>，即 &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{A_1 x-x_s=b_1}\\
\quad&amp;amp;\boldsymbol{A_2x=b_2}\\
\quad&amp;amp;\boldsymbol{A_3 x+-x_t=b_3}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{x_s}\)&lt;/span>是由&lt;span class="math">\(m_1\)&lt;/span>个松弛变量组成的&lt;span class="math">\(m_1\)&lt;/span>维列向量，&lt;span class="math">\(\boldsymbol{x_t}\)&lt;/span>是由&lt;span class="math">\(m_3\)&lt;/span>个松弛变量组成的&lt;span class="math">\(m_3\)&lt;/span>维列向量，如果用矩阵方式描述上述问题可写为 &lt;span class="math">\[
\begin{aligned}
\min \quad &amp;amp; \boldsymbol{c^Tx+0\cdot x_s+0\cdot x_t}\\
\mathop{s.t.} \quad &amp;amp; \begin{bmatrix}\boldsymbol{A_1} &amp;amp; \boldsymbol{-I_{m_1}} &amp;amp; \boldsymbol{0}\\
\boldsymbol{A_2} &amp;amp; \boldsymbol{0} &amp;amp; \boldsymbol{0}\\
\boldsymbol{A_3} &amp;amp; \boldsymbol{0} &amp;amp; \boldsymbol{I_{m_3}}\\ \end{bmatrix}
\begin{bmatrix}\boldsymbol{x}\\ \boldsymbol{x_s}\\ \boldsymbol{x_t}\end{bmatrix}=
\begin{bmatrix}\boldsymbol{b_1}\\ \boldsymbol{b_2}\\ \boldsymbol{b_3}\end{bmatrix},\\
\quad &amp;amp; \boldsymbol{x,x_s,x_t≥0},
\end{aligned}\tag{1.7}
\]&lt;/span> 按照非对称形式的对偶套路，&lt;span class="math">\((1.7)\)&lt;/span>的对偶问题可以描述为： &lt;span class="math">\[
\begin{aligned}
\max \quad &amp;amp; \boldsymbol{w^T b}=\boldsymbol{w^T_1 b_1+w^T_2 b_2+w^T_3 b_3}\\
\mathop{s.t.} \quad &amp;amp; \boldsymbol{[w^T_1,w^T_2,w^T_3]}
\begin{bmatrix}\boldsymbol{A_1} &amp;amp; \boldsymbol{-I_{m_1}} &amp;amp; \boldsymbol{0}\\
\boldsymbol{A_2} &amp;amp; \boldsymbol{0} &amp;amp; \boldsymbol{0}\\
\boldsymbol{A_3} &amp;amp; \boldsymbol{0} &amp;amp; \boldsymbol{I_{m_3}}\\ \end{bmatrix} ≤ \boldsymbol{[c^T,0,0]}
\end{aligned}
\]&lt;/span> 我们把矩阵的内容展开可得 &lt;span class="math">\[
\begin{aligned}
\max \quad &amp;amp; \boldsymbol{w^T_1 b_1+w^T_2 b_2+w^T_3 b_3}\\
\mathop{s.t.} \quad &amp;amp; \boldsymbol{w^T_1A_1+w^T_2A_2+w^T_3A_3≤c^T},\\
\quad &amp;amp; \boldsymbol{w_1≥0},\\
\quad &amp;amp; \boldsymbol{w_3≤0},\\
\quad &amp;amp; \boldsymbol{w_2}无限制,\\
\end{aligned}\tag{1.8}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{w_1,w_2,w_3}\)&lt;/span>分别是由对偶变量组成的&lt;span class="math">\(m_1\)&lt;/span>维，&lt;span class="math">\(m_2\)&lt;/span>维，&lt;span class="math">\(m_3\)&lt;/span>维行向量。定义&lt;span class="math">\((1.8)\)&lt;/span>式为&lt;span class="math">\((1.6)\)&lt;/span>式的对偶问题。&lt;/p>
&lt;p>由&lt;span class="math">\((1.8)\)&lt;/span>式可知，原问题中的约束&lt;span class="math">\(\boldsymbol{A_1 x≥b_1}\)&lt;/span>所对应的对偶变量&lt;span class="math">\(\boldsymbol{w_1}\)&lt;/span>有&lt;strong>非负限制&lt;/strong>，&lt;span class="math">\(\boldsymbol{A_2 x=b_2}\)&lt;/span>所对应的对偶变量&lt;span class="math">\(\boldsymbol{w_2}\)&lt;/span>&lt;strong>无正负限制&lt;/strong>，&lt;span class="math">\(\boldsymbol{A_3 x≤b_3}\)&lt;/span>所对应的对偶变量&lt;span class="math">\(\boldsymbol{w_3}\)&lt;/span>有&lt;strong>非正限制&lt;/strong>。从矩阵形式的对偶描述我们可以发现对偶变量的正负限制来自于&lt;strong>松弛变量的添加&lt;/strong>。&lt;/p>
&lt;h2 id="对偶问题的对偶是原问题">对偶问题的对偶是原问题&lt;/h2>
&lt;p>&lt;strong>原问题和对偶问题是相对的&lt;/strong>，显然原问题的对偶问题也是线性规划，它也有对偶问题，显然它的对偶问题就是原来对偶中的原问题。因此互相对偶的两个问题中，任何一个问题均可以作为原问题，而把另一个问题作为对偶问题。&lt;/p>
&lt;h2 id="对偶规划的一般规则重要">对偶规划的一般规则（重要）&lt;/h2>
&lt;p>为了方便讨论，我们下面所说的约束均指 &lt;span class="math">\[
\begin{aligned}
\boldsymbol{A_ix}&amp;amp;≥b_i\quad 及\quad\boldsymbol{w^T p_j}&amp;amp;≤c_j\\
(&amp;amp;=)&amp;amp;(=)\\
(&amp;amp;≥)&amp;amp;(≥)\\
\end{aligned}
\]&lt;/span> 型约束，不包含变量非负或非正限制。我们总结的一般规则有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>若原问题是极大化问题，那么对偶问题就是极小化问题；若原问题是极小化问题，那么对偶问题就是极大化问题。（&lt;strong>极小化&lt;span class="math">\(\leftrightarrow\)&lt;/span>极大化&lt;/strong>）&lt;/li>
&lt;li>在原问题和对偶问题中，约束右端变量向量与目标函数中系数向量恰好对换。（&lt;strong>约束向量&lt;span class="math">\(\leftrightarrow\)&lt;/span>系数向量&lt;/strong>）&lt;/li>
&lt;li>对于极小化问题的“&lt;span class="math">\(≥\)&lt;/span>”型约束（极大化问题中的“&lt;span class="math">\(≤\)&lt;/span>”型约束），相应的对偶变量有非负限制；对于极小化问题中的“&lt;span class="math">\(≤\)&lt;/span>”型约束（极大化问题中的“&lt;span class="math">\(≥\)&lt;/span>”型约束），相应的对偶变量有非正限制；对于原问题的“&lt;span class="math">\(=\)&lt;/span>”型约束，对应的对偶变量无正负限制。（&lt;strong>这一点可以通过矩阵形式添加的松弛变量体现出来&lt;/strong>）&lt;/li>
&lt;li>对于极小化问题的具有非负限制的变量（极大化问题的具有非正限制的变量），在其对偶问题中， 相应的约束为”&lt;span class="math">\(≥\)&lt;/span>“型不等式； 对小极小化问题的具有非正限制的变量（极大化问题的具有非负限制的变量），在其对偶问题中，相应的约束为“&lt;span class="math">\(≥\)&lt;/span>”型不等式；对于原问题中无正负限制的变量，在其对偶问题中，相应的约束为等式。（&lt;strong>根据对偶性，从第3点可以推出&lt;/strong>）&lt;/li>
&lt;/ol>
&lt;p>例子&lt;span class="math">\((1.9)\)&lt;/span>见《最优化理论与算法（第2版）》p126.&lt;/p>
&lt;h2 id="对偶定理">对偶定理&lt;/h2>
&lt;p>下面研究对偶的基本性质，由于不同形式的对偶问题可以相互转化，&lt;strong>在此仅叙述并证明关于对称对偶的几个重要定理，其结论对于其他形式的对偶仍成立&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理1.1&lt;/strong> 设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是&lt;span class="math">\((1.1),(1.2)\)&lt;/span>式的可行解,则&lt;span class="math">\(\boldsymbol{c^Tx^{(0)}}≥\boldsymbol{w^{(0)T}b}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>证明:利用对偶问题和原问题的定义有&lt;span class="math">\(\boldsymbol{Ax^{(0)}≥b}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}≥0}\)&lt;/span>,则有 &lt;span class="math">\[
\boldsymbol{w^{(0)T}Ax^{(0)}≥w^{(0)T}b}\tag{1.10}
\]&lt;/span> 同时,原问题和对偶问题还需要满足&lt;span class="math">\(\boldsymbol{w^{(0)T}A≤c^T}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{x^{(0)T}≥0}\)&lt;/span>，则有 &lt;span class="math">\[
\boldsymbol{c^{(0)T}x≥w^{(0)T}Ax^{(0)}}\tag{1.11}
\]&lt;/span> 由&lt;span class="math">\((1.10),(1.11)\)&lt;/span>可以推出&lt;span class="math">\(\boldsymbol{c^{(0)T}x≥w^{(0)T}b}\)&lt;/span>&lt;strong>恒成立&lt;/strong>。&lt;/p>
&lt;p>上述定理表明，就原问题和对偶问题的可行解而言，对于对偶中的两个问题，每一个问题的&lt;strong>任何一个可行解处的目标函数值都给出另一个问题的目标函数值的界&lt;/strong>。极小化问题给出极大化问题的目标函数值的上界；极大化问题给出极小化问题的目标函数值的下界。&lt;/p>
&lt;p>由定理1.1可以得到以下几个重要推论：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>推论1：&lt;/strong> 若&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的可行解，且&lt;span class="math">\(\boldsymbol{c^Tx^{(0)}=w^{(0)T}b}\)&lt;/span>，则&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解。(&lt;span class="math">\(\boldsymbol{c^{(0)T}x≥w^{(0)T}b}\)&lt;/span>&lt;strong>恒成立&lt;/strong>因此相等的情况分别是二者的最优解)&lt;/p>
&lt;p>&lt;strong>推论2：&lt;/strong> 对偶规划&lt;span class="math">\((1.1)\)&lt;/span>和&lt;span class="math">\((1.2)\)&lt;/span>有最优解的充要条件是它们同时有可行解。（最优解必然是可行解；可行解里面必然能选出至少一个最优解）&lt;/p>
&lt;p>&lt;strong>推论3：&lt;/strong> 若原问题&lt;span class="math">\((1.1)\)&lt;/span>的目标函数值在可行域上&lt;strong>无下界&lt;/strong>， 则对偶问题&lt;span class="math">\((1.2)\)&lt;/span>&lt;strong>无可行解&lt;/strong>；反之，若对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的目标函数值在可行域上&lt;strong>无上界&lt;/strong>， 则原问题&lt;span class="math">\((1.1)\)&lt;/span>&lt;strong>无可行解&lt;/strong>。（&lt;span class="math">\((1.1)\)&lt;/span>无下界说明下界值为&lt;span class="math">\(-\infty\)&lt;/span>，没有比&lt;span class="math">\(-\infty\)&lt;/span>更小的值，所以对偶问题&lt;span class="math">\((1.2)\)&lt;/span>无解，反之同理）&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>推论1&lt;/code>粗略的阐明了原问题和对偶问题最优解的关系，下面我们通过&lt;code>定理1.2&lt;/code>详细证明一下，&lt;code>定理1.2&lt;/code>可以说是对偶理论的&lt;strong>核心&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理1.2：&lt;/strong>设原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>中&lt;strong>有一个问题存在最优解， 则另一个问题也存在最优解&lt;/strong>，且两个问题的目标函数的&lt;strong>最优值相等&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：设原问题&lt;span class="math">\((1.1)\)&lt;/span>存在最优解，引进松弛变量，可以把原问题写成等价形式： &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax-v=b}\\
&amp;amp;\boldsymbol{x≥0}\\
&amp;amp;\boldsymbol{v≥0}
\end{aligned}\tag{1.12}
\]&lt;/span> 由于线性规则&lt;span class="math">\((1.12)\)&lt;/span>存在最优解， 因此能够用单纯形方法（包括使用能避免循环发生的摄动法）求出它的一个最优基本可行解，不妨设这个最优解是 &lt;span class="math">\[
\boldsymbol{y^{(0)}}=\begin{bmatrix}
\boldsymbol{x^{(0)}}\\
\boldsymbol{v^{(0)}}
\end{bmatrix}
\]&lt;/span> 相应的最优基是&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>。由单纯形法步骤可知，这时所有判别数&lt;span class="math">\(z_j-c_j\)&lt;/span>均非正， 即 &lt;span class="math">\[\boldsymbol{w^{(0)T}p_j}-c_j≤0\quad\forall j, \tag{1.13}\]&lt;/span> 其中&lt;span class="math">\(\boldsymbol{w^{(0)T}}=\boldsymbol{c_B^T B^{-1}}\)&lt;/span>，&lt;span class="math">\(\boldsymbol{c_B}\)&lt;/span>是目标函数中基变量（包括松弛变量中的基变量）的系数组成的向量。考虑所有原来变量（不包括松弛变量）在基&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>下的判别数，把它们所满足的条件&lt;span class="math">\((1.13)\)&lt;/span>用矩阵形式同时写出，得到 &lt;span class="math">\[
\boldsymbol{w^{(0)T}A-c^T≤0}\Rightarrow\\
\boldsymbol{w^{(0)T}A≤c^T} \tag{1.14}
\]&lt;/span> 对于松弛变量对应的矩阵列向量&lt;span class="math">\(\boldsymbol{p_j}\)&lt;/span>，只有松弛变量对应元素位置为-1（&lt;span class="math">\(\boldsymbol{Ax-v=b}\)&lt;/span>），其他位置为0，而&lt;span class="math">\((1.13)\)&lt;/span>对任意&lt;span class="math">\(j\)&lt;/span>成立；同时，松弛变量对应&lt;span class="math">\(\boldsymbol{c}\)&lt;/span>中系数皆为0，所以对任意松弛变量在基&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>下对应的判别数所满足的条件&lt;span class="math">\((1.13)\)&lt;/span>用矩阵形式表示，得到 &lt;span class="math">\[
\boldsymbol{w^{(0)T}(-I_m)≤0}\Rightarrow\\
\boldsymbol{w^{(0)T}≥0} \tag{1.15}
\]&lt;/span> 我们惊喜的发现：在单纯形法中构造出来的&lt;span class="math">\(\boldsymbol{w^{(0)T}}=\boldsymbol{c_B^T B^{-1}}\)&lt;/span>是满足对偶问题&lt;span class="math">\((1.2)\)&lt;/span>约束&lt;span class="math">\((1.14),(1.15)\)&lt;/span>的&lt;strong>可行解&lt;/strong>！！！&lt;/p>
&lt;p>根据单纯性法步骤，有&lt;span class="math">\(\boldsymbol{w^{(0)T}}=\boldsymbol{c_B^T B^{-1}}\)&lt;/span>，&lt;span class="math">\(\boldsymbol{y_B^{(0)}=B^{-1} b}\)&lt;/span>，因此有： &lt;span class="math">\[
\boldsymbol{w^{(0)T}b=c^T_B \underbrace{B^{-1} b}_{y_B^{(0)}}=c^T_B y_B^{(0)}}
\]&lt;/span> 由于非基变量取值为零及目标函数中松弛变量的系数为零，因此 &lt;span class="math">\[
\boldsymbol{c^T_B y_B^{(0)}=c^Tx^{(0)}}
\]&lt;/span> 这里&lt;span class="math">\(\boldsymbol{y^{(0)}_B}\)&lt;/span>表示&lt;span class="math">\(\boldsymbol{y^{(0)}}\)&lt;/span>中基变量的取值，其余非基变量都是0.根据&lt;code>定理1.1&lt;/code>的&lt;code>推论1&lt;/code>:&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>是对偶问题&lt;span class="math">\((1.2)\)&lt;/span>最优解，且原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的目标函数的最优值相等。类似的，可以证明，如果对偶问题&lt;span class="math">\((1.2)\)&lt;/span>存在最优解，则原问题&lt;span class="math">\((1.1)\)&lt;/span>也存在最优解，且两个问题目标函数的最优值相等。（定理1.2也常用凸集分离定理证明）&lt;/p>
&lt;p>由上述定理的证明过程可以得到下面一个推论：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>推论4&lt;/strong>：若线性规划&lt;span class="math">\((1.1)\)&lt;/span>存在一个对应基&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>的最优基本可行解， 则单纯形乘子&lt;span class="math">\(\boldsymbol{w^{(0)T}}=\boldsymbol{c_B^T B^{-1}}\)&lt;/span>是对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的一个最优解。&lt;/p>
&lt;/blockquote>
&lt;p>根据这个推论，我们能够&lt;strong>从原问题的最优单纯形表中直接获得对偶问题的一个最优解&lt;/strong>。&lt;/p></description></item><item><title>优化理论之线性规划的对偶的互补松弛定理</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%AF%B9%E5%81%B6%E7%9A%84%E4%BA%92%E8%A1%A5%E6%9D%BE%E5%BC%9B%E5%AE%9A%E7%90%86/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%AF%B9%E5%81%B6%E7%9A%84%E4%BA%92%E8%A1%A5%E6%9D%BE%E5%BC%9B%E5%AE%9A%E7%90%86/</guid><description>
&lt;h2 id="线性规划的对偶的互补松弛定理">线性规划的对偶的互补松弛定理&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#前情提要">前情提要&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对称形式对偶问题定义">对称形式对偶问题定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非对称形式对偶问题定义">非对称形式对偶问题定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对偶定理">对偶定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#互补松弛性质">互补松弛性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#对称形式">对称形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非对称形式">非对称形式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#强互补松弛定理">强互补松弛定理&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="前情提要">前情提要&lt;/h2>
&lt;h3 id="对称形式对偶问题定义">对称形式对偶问题定义&lt;/h3>
&lt;p>原问题 &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax≥b}（注意这里是≥）\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.1}
\]&lt;/span> 对偶问题 &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{w^Tb}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{w^TA≤c^T}（注意这里是≤）\\
&amp;amp;\boldsymbol{w^T≥0}
\end{aligned}\tag{1.2}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{A=(p_1,p_2,\dotsb,p_n)}\)&lt;/span>是&lt;span class="math">\(m×n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol{b}=(b_1,b_2,\dotsb,b_m)^T\)&lt;/span>是&lt;strong>m维列&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{c^T}=(c_1,c_2,\dotsb,c_n)\)&lt;/span>是&lt;strong>n维行&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{x}=(x_1,x_2,\dotsb,x_n)^T\)&lt;/span>是原问题的变量组成的&lt;strong>n维列&lt;/strong>向量，&lt;span class="math">\(\boldsymbol{w^T}=(w_1,w_2,\dotsb,w_m)\)&lt;/span>是由对偶问题变量组成的&lt;strong>m维行&lt;/strong>向量。&lt;/p>
&lt;h3 id="非对称形式对偶问题定义">非对称形式对偶问题定义&lt;/h3>
&lt;p>原问题 &lt;span class="math">\[
\begin{aligned}
\min\quad&amp;amp;\boldsymbol{c^Tx}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{Ax=b}\\
&amp;amp;\boldsymbol{x≥0}
\end{aligned}\tag{1.3}
\]&lt;/span> 对偶问题 &lt;span class="math">\[
\begin{aligned}
\max\quad&amp;amp;\boldsymbol{w^Tb}\\
\mathop{s.t.}\quad&amp;amp;\boldsymbol{w^TA}≤\boldsymbol{c}\\
\end{aligned}\tag{1.5}
\]&lt;/span>&lt;/p>
&lt;h3 id="对偶定理">对偶定理&lt;/h3>
&lt;p>&lt;strong>定理1.1&lt;/strong> 设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是&lt;span class="math">\((1.1),(1.2)\)&lt;/span>式的可行解,则&lt;span class="math">\(\boldsymbol{c^Tx^{(0)}}≥\boldsymbol{w^{(0)T}b}\)&lt;/span>&lt;/p>
&lt;p>&lt;strong>推论1：&lt;/strong> 若&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的可行解，且&lt;span class="math">\(\boldsymbol{c^Tx^{(0)}=w^{(0)T}b}\)&lt;/span>，则&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解。(&lt;span class="math">\(\boldsymbol{c^{(0)T}x≥w^{(0)T}b}\)&lt;/span>&lt;strong>恒成立&lt;/strong>因此相等的情况分别是二者的最优解)&lt;/p>
&lt;p>&lt;strong>定理1.2：&lt;/strong>设原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>中&lt;strong>有一个问题存在最优解， 则另一个问题也存在最优解&lt;/strong>，且两个问题的目标函数的&lt;strong>最优值相等&lt;/strong>。&lt;/p>
&lt;h2 id="互补松弛性质">互补松弛性质&lt;/h2>
&lt;p>利用对偶定理可以证明原问题和对偶问题的最优解满足重要的互补松弛关系。&lt;strong>互补松弛定理可以了解到变量在限制条件中是否取等号或者是取上下限值&lt;/strong>。&lt;/p>
&lt;h3 id="对称形式">对称形式&lt;/h3>
&lt;blockquote>
&lt;p>&lt;strong>定理1.3&lt;/strong>：设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的可行解，那么&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>都是最优解的充要条件是，对所有&lt;span class="math">\(i和j\)&lt;/span>，下列关系成立：&lt;/p>
&lt;blockquote>
&lt;ol style="list-style-type: decimal">
&lt;li>如果&lt;span class="math">\(x^{(0)}_j&amp;gt;0\)&lt;/span>，就有&lt;span class="math">\(\boldsymbol{w^{(0)T}p_j}=c_j\)&lt;/span>;&lt;/li>
&lt;li>如果&lt;span class="math">\(\boldsymbol{w^{(0)T}p_j}&amp;lt;c_j\)&lt;/span>，就有&lt;span class="math">\(x^{(0)}_j=0\)&lt;/span>;&lt;/li>
&lt;li>如果&lt;span class="math">\(w^{(0)}_i&amp;gt;0\)&lt;/span>，就有&lt;span class="math">\(\boldsymbol{A_i x^{(0)}}=b_i\)&lt;/span>;&lt;/li>
&lt;li>如果&lt;span class="math">\(\boldsymbol{A_i x^{(0)}}&amp;gt;b_i\)&lt;/span>，就有&lt;span class="math">\(w^{(0)}_i=0\)&lt;/span>;&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>其中&lt;span class="math">\(\boldsymbol{p_j}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>的第&lt;span class="math">\(j\)&lt;/span>列，&lt;span class="math">\(\boldsymbol{A_i}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>的第&lt;span class="math">\(i\)&lt;/span>行。&lt;/p>
&lt;/blockquote>
&lt;p>证明：先证必要性。最优解&lt;span class="math">\(\Rightarrow\)&lt;/span> (1,2,3,4)&lt;/p>
&lt;p>设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解。由于&lt;span class="math">\(\boldsymbol{c^T≥w^{(0)T}A}\)&lt;/span>以及&lt;span class="math">\(\boldsymbol{x^{(0)}≥0}\)&lt;/span>，则有 &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}≥\boldsymbol{w^{(0)T}A x^{(0)}}\tag{1.16}
\]&lt;/span> 由于&lt;span class="math">\(\boldsymbol{Ax^{(0)}≥b}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)T}≥0}\)&lt;/span>，则 &lt;span class="math">\[
\boldsymbol{w^{(0)T}A x^{(0)}}≥\boldsymbol{w^{(0)T}b}\tag{1.17}
\]&lt;/span> 由于&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解，根据&lt;code>定理1.2&lt;/code>必有： &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}=\boldsymbol{w^{(0)T}b}\tag{1.18}
\]&lt;/span> 用&lt;span class="math">\((1.18)\)&lt;/span>夹逼&lt;span class="math">\((1.16),(1.17)\)&lt;/span>可得 &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}=\boldsymbol{w^{(0)T}A x^{(0)}}=\boldsymbol{w^{(0)T}b} \tag{1.19}\\
\]&lt;/span> &lt;span class="math">\[
\boldsymbol{(c^T-w^{(0)T}A) x^{(0)}}=0 \tag{1.20}\\
\]&lt;/span> &lt;span class="math">\[
\boldsymbol{w^{(0)T}(A x^{(0)}-b)}=0\tag{1.21}\\
\]&lt;/span> 由于&lt;span class="math">\(\boldsymbol{c^T-w^{(0)T}A≥0}，\boldsymbol{x^{(0)}≥0}\)&lt;/span>，因此对于两项分量非0时都为正，因此对于每一个分量都有 &lt;span class="math">\[
(c_j-\boldsymbol{w^{0T}p_j})x_j^{(0)}=0,\quad j=1,2,\dotsb,n
\]&lt;/span> 故&lt;code>定理1.3&lt;/code>中关系（1）（2）成立。&lt;/p>
&lt;p>由于&lt;span class="math">\(\boldsymbol{Ax^{(0)}-b≥0}，\boldsymbol{w^{(0)}≥0}\)&lt;/span>，因此对于两项分量非0时都为正，因此对于每一个分量都有 &lt;span class="math">\[
w_i^{(0)}(\boldsymbol{A_ix^{(0)}}-b_i)=0,\quad i=1,2,\dotsb,m
\]&lt;/span> 故&lt;code>定理1.3&lt;/code>中关系（3）（4）成立。必要性得证。&lt;/p>
&lt;p>再证充分性。最优解&lt;span class="math">\(\Leftarrow\)&lt;/span> (1,2,3,4)&lt;/p>
&lt;p>设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的可行解，且关系（1）（2）（3）（4）成立。&lt;/p>
&lt;p>由于关系（1）和（2）成立，则对每一个&lt;span class="math">\(j\)&lt;/span>，有 &lt;span class="math">\[
(c_j-\boldsymbol{w^{0T}p_j})x_j^{(0)}=0,\quad j=1,2,\dotsb,n\tag{1.22}
\]&lt;/span> 由此可推出&lt;span class="math">\(\boldsymbol{(w^{(0)T}A-c^T)x^{(0)}=0}\)&lt;/span>，即 &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}=\boldsymbol{w^{(0)T}A x^{(0)}}\tag{1.23}
\]&lt;/span> 由于关系（3）（4）成立，则对于每一个&lt;span class="math">\(i\)&lt;/span>，有 &lt;span class="math">\[
w_i^{(0)}(\boldsymbol{A_ix^{(0)}}-b_i)=0,\quad i=1,2,\dotsb,m\tag{1.24}
\]&lt;/span> 由此可以看出&lt;span class="math">\(\boldsymbol{w^{(0)T}(A x^{(0)}-b)}=0\)&lt;/span>，即 &lt;span class="math">\[
\boldsymbol{w^{(0)T}A x^{(0)}}=\boldsymbol{w^{(0)T}b} \tag{1.25}
\]&lt;/span> 由&lt;span class="math">\((1.23)\)&lt;/span>和&lt;span class="math">\((1.25)\)&lt;/span>可以得到 &lt;span class="math">\[
\boldsymbol{c^T x^{(0)}}=\boldsymbol{w^{(0)T}b}
\]&lt;/span> 由&lt;code>定理1.1&lt;/code>的&lt;code>推论1&lt;/code>可知，&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.1)\)&lt;/span>和对偶问题&lt;span class="math">\((1.2)\)&lt;/span>的最优解。&lt;/p>
&lt;h3 id="非对称形式">非对称形式&lt;/h3>
&lt;p>对于非对称形式的对偶规划，由于在原问题中约束条件是&lt;span class="math">\(\boldsymbol{Ax=b}\)&lt;/span>，而对偶变量无正负 限制。因此互补松弛性质叙述如下&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>定理1.4&lt;/strong>：设&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>分别是原问题&lt;span class="math">\((1.3)\)&lt;/span>和对偶问题&lt;span class="math">\((1.5)\)&lt;/span>的可行解，那么&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>和&lt;span class="math">\(\boldsymbol{w^{(0)}}\)&lt;/span>都是最优解的充要条件是，对于所有&lt;span class="math">\(j\)&lt;/span>，下列关系成立：&lt;/p>
&lt;blockquote>
&lt;ol style="list-style-type: decimal">
&lt;li>如果&lt;span class="math">\(x^{(0)}_j&amp;gt;0\)&lt;/span>，就有&lt;span class="math">\(\boldsymbol{w^{(0)T}p_j}=c_j\)&lt;/span>;&lt;/li>
&lt;li>如果&lt;span class="math">\(\boldsymbol{w^{(0)T}p_j}&amp;lt;c_j\)&lt;/span>，就有&lt;span class="math">\(x^{(0)}_j=0\)&lt;/span>;&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="强互补松弛定理">强互补松弛定理&lt;/h2>
&lt;p>暂时略&lt;/p></description></item><item><title>优化理论之单纯形法</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</guid><description>
&lt;h2 id="单纯行法">单纯行法&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#核心思想">核心思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单纯形方法原理">单纯形方法原理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一般计算步骤">一般计算步骤&lt;/a>&lt;/li>
&lt;li>&lt;a href="#收敛性">收敛性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#使用表格形式的单纯形方法">使用表格形式的单纯形方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#退化情形">退化情形&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="核心思想">核心思想&lt;/h2>
&lt;p>由优化理论可知，如果线性规划中的最优解，那么这个最优解必是最优基本可行解。因此线性规划的核心就变成了求&lt;strong>最优基本可行解&lt;/strong>。这构成了单纯形法的基础。&lt;/p>
&lt;p>单纯形法核心思路：&lt;/p>
&lt;ul>
&lt;li>找到一个基本可行解&lt;/li>
&lt;li>求一个使目标函数值有所改善的基本可行解&lt;/li>
&lt;li>不断改进的基本可行解直到最优解&lt;/li>
&lt;/ul>
&lt;p>因此，大方向可以分为两个：1.找一个初时基本可行；2.改进基本可行解。对于第一个问题，用二阶段法或者大M法，第二个问题则需要考虑退化情形。&lt;/p>
&lt;h2 id="单纯形方法原理">单纯形方法原理&lt;/h2>
&lt;p>考虑线性规划问题 &lt;span class="math">\[
\begin{aligned}
\min\ &amp;amp;f \overset{\mathop{def}}{=} \boldsymbol{c}^T\boldsymbol{x}\\
\mathop{s.t.}\ &amp;amp;\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b},\\
&amp;amp;\boldsymbol{x}\geq 0
\end{aligned}\tag{1.1}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>是&lt;span class="math">\(m×n\)&lt;/span>矩阵，秩为&lt;span class="math">\(m\)&lt;/span>。&lt;span class="math">\(\boldsymbol{c}^T\)&lt;/span>是n维行向量，&lt;span class="math">\(\boldsymbol{b}≥0\)&lt;/span>是m维列向量(每个分量都大于等于0)。我们以列向量来表示&lt;span class="math">\(\boldsymbol{A}\)&lt;/span> &lt;span class="math">\[
\boldsymbol{A}=(\boldsymbol{p_1},\boldsymbol{p_2},\dotsb,\boldsymbol{p_n})
\]&lt;/span> 显然&lt;span class="math">\(\boldsymbol{A}\boldsymbol{x}\)&lt;/span>可表示为： &lt;span class="math">\[
\boldsymbol{A}\boldsymbol{x}=\sum_{j=1}^n x_j\boldsymbol{p_j}
\]&lt;/span> &lt;span class="math">\(x_j\)&lt;/span>可以看成是列向量&lt;span class="math">\(\boldsymbol{p_j}\)&lt;/span>线性组合的系数。现将&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>分解成&lt;span class="math">\((\boldsymbol{B},\boldsymbol{N})\)&lt;/span>（可能经过列调换），使得其中&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>是基矩阵，&lt;span class="math">\(\boldsymbol{N}\)&lt;/span>是非基矩阵。在此分解下，我们假设能得到一个基本可行解： &lt;span class="math">\[
\boldsymbol{x^{(0)}}=\begin{bmatrix}
\boldsymbol{B^{-1}b}\\
\boldsymbol{0}\\
\end{bmatrix}
\]&lt;/span> 这个基本可行解对应了线性规划的某个极点，在点&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>处的目标函数值是 &lt;span class="math">\[
f_0=\boldsymbol{c}^T\boldsymbol{x^{(0)}}=(\boldsymbol{c_B^T,c_N^T})\begin{bmatrix}
\boldsymbol{B^{-1}b}\\
\boldsymbol{0}\\
\end{bmatrix}\\
=\boldsymbol{c_B^T B^{-1}b}\tag{1.2}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{c^T_B}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{c}^T\)&lt;/span>中与&lt;strong>基变量&lt;/strong>对应的分量组成的m维行向量，&lt;span class="math">\(\boldsymbol{c^T_N}\)&lt;/span>是&lt;span class="math">\(\boldsymbol{c}^T\)&lt;/span>中与&lt;strong>非基变量&lt;/strong>对应的分量组成的n-m维行向量（因为为了让&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>的秩为m，可能对&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>中的列变量进行过变换，对应的&lt;span class="math">\(\boldsymbol{x,c}\)&lt;/span>都要变换）。&lt;/p>
&lt;p>现在我们从这个基本可行解&lt;span class="math">\(\boldsymbol{x^{(0)}}\)&lt;/span>出发，利用非基变量得出一个改进的基本可行解。设 &lt;span class="math">\[
\boldsymbol{x}=\begin{bmatrix}
\boldsymbol{x_B}\\
\boldsymbol{x_N}
\end{bmatrix}
\]&lt;/span> 是任一个可行解，则由&lt;span class="math">\(\boldsymbol{Ax=b}\)&lt;/span>可得： &lt;span class="math">\[
\boldsymbol{x_B}=\boldsymbol{B^{-1}b-B^{-1}Nx_N}\tag{1.3}
\]&lt;/span> 在此点处的目标函数值是 &lt;span class="math">\[
\begin{aligned}
f&amp;amp;=\boldsymbol{c^Tx}=(\boldsymbol{c^T_B,c^T_N})\begin{bmatrix}\boldsymbol{x_B}\\\boldsymbol{x_N}\end{bmatrix}\\
&amp;amp;=\boldsymbol{c^T_B x_B}+\boldsymbol{c^T_N x_N}\\
&amp;amp;=\boldsymbol{c^T_B(B^{-1}b-B^{-1}Nx_N)}+\boldsymbol{c^T_N x_N}\\
&amp;amp;=\underbrace{\boldsymbol{c^T_B B^{-1}b}}_{基本可行解函数值}-\boldsymbol{(c^T_B B^{-1}N-c^T_N)x_N}\\
&amp;amp;=f_0 - \sum_{j\in R}(\underbrace{\boldsymbol{c^T_B B^{-1}p_j}}_{z_j}-c_j)x_j,\ R为非基变量下标集\\
&amp;amp;=f_0 - \sum_{j ∈ R}(z_j-c_j)x_j
\end{aligned}\tag{1.4}
\]&lt;/span> 由于&lt;span class="math">\(\boldsymbol{c^T_B}\)&lt;/span>是m维行向量，&lt;span class="math">\(\boldsymbol{B^{-1}}\)&lt;/span>是&lt;span class="math">\(m×m\)&lt;/span>维矩阵，&lt;span class="math">\(\boldsymbol{p_j}\)&lt;/span>是m维列向量，因此它们的积是一个数字。由&lt;span class="math">\((1.4)\)&lt;/span>可知，适当的选择非基变量的值&lt;span class="math">\(x_j\)&lt;/span>，可以使得 &lt;span class="math">\[
\sum_{j ∈ R}(z_j-c_j)x_j&amp;gt;0\tag{1.5}
\]&lt;/span> 例如，当&lt;span class="math">\(z_j-c_j&amp;lt;0\Rightarrow x_j=0\)&lt;/span>；当&lt;span class="math">\(z_j-c_j≥0\Rightarrow x_j&amp;gt;0\)&lt;/span>。&lt;strong>从而得到使目标函数的值减少的新的基本可行解&lt;/strong>。为了方便，我们不妨令&lt;span class="math">\(n-m-1\)&lt;/span>个非基变量取0，一个非基变量，比如&lt;span class="math">\(x_k\)&lt;/span>大于0。需要注意的是，这个&lt;span class="math">\(x_k\)&lt;/span>的系数&lt;span class="math">\(z_j-c_j\)&lt;/span>应该是大于0的。&lt;/p>
&lt;p>根据&lt;span class="math">\((1.4)\)&lt;/span>，&lt;strong>正&lt;/strong>系数&lt;span class="math">\(z_j-c_j\)&lt;/span>越大，目标函数下降越快，我们不妨用&lt;strong>贪心法&lt;/strong>，选择&lt;span class="math">\(x_k\)&lt;/span>，使 &lt;span class="math">\[
z_k-c_k=\max_{j\in R}\{z_j-c_j\}&amp;gt;0\tag{1.6}
\]&lt;/span> &lt;span class="math">\(x_k\)&lt;/span>由0变成正数后，得到方程组&lt;span class="math">\(\boldsymbol{Ax=b}\)&lt;/span>的另一个解： &lt;span class="math">\[
\begin{aligned}
\boldsymbol{x_B}&amp;amp;=\boldsymbol{B^{-1}b-B^{-1}Nx_N}\\
&amp;amp;=\boldsymbol{\underbrace{B^{-1}b}_{\bar b}-\underbrace{B^{-1}p_k}_{y_k}}x_k\\
&amp;amp;=\boldsymbol{\bar b}- \boldsymbol{y_k}x_k
\end{aligned}\tag{1.7}
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{\bar b, y_k}\)&lt;/span>都是m维列向量，&lt;span class="math">\(x_k\)&lt;/span>是标量。我们知道标准形式下的可行解要求&lt;span class="math">\(\boldsymbol{x_B}≥0\)&lt;/span>，若我们把新得到的解&lt;span class="math">\(\boldsymbol{x_B}\)&lt;/span>按分量写出，即 &lt;span class="math">\[
\boldsymbol{x_B}=\begin{bmatrix}x_{B_1}\\x_{B_2}\\ \vdots \\x_{B_m}\end{bmatrix}=\begin{bmatrix}\bar{b}_1\\\bar{b}_2\\ \vdots \\\bar{b}_m\end{bmatrix}-\begin{bmatrix}y_{k_1}\\y_{k_2}\\ \vdots \\y_{k_m}\end{bmatrix}x_k ≥0 \tag{1.8}
\]&lt;/span> &lt;span class="math">\[
\boldsymbol{x_N}=(0,0,\dotsb,0,x_k,0,\dotsb,0)^T\tag{1.9}
\]&lt;/span> 在新得到的点，目标函数的函数值为 &lt;span class="math">\[
f=f_0-(z_k-c_k)x_k\tag{1.10}
\]&lt;/span> 再来，我们分析怎样确定&lt;span class="math">\(x_k\)&lt;/span>的取值。根据&lt;span class="math">\((1.8)\)&lt;/span>的可行性限制条件(非负)，我们不难得出，对&lt;span class="math">\(\forall i ∈ \{1,2,\dotsb,m\}且y_{k_i}&amp;gt;0\)&lt;/span>.： &lt;span class="math">\[
x_{B_i}=\bar b_i -y_{k_i}x_k≥0\Rightarrow x_k≤\frac{\bar b_i}{y_{k_i}}
\]&lt;/span> 因为若&lt;span class="math">\(y_{k_i}\)&lt;/span>为负，&lt;span class="math">\(x_k\)&lt;/span>为正，这一分量不会成为破坏非负限制条件的分量。为了保证非负限制条件，我们必须有 &lt;span class="math">\[
x_k≤\min\big\{\frac{\bar b_i}{y_{k_i}}\big |y_{k_i}&amp;gt;0\big\}
\]&lt;/span> 同时，为了让&lt;span class="math">\(f\)&lt;/span>尽可能减少，我们希望&lt;span class="math">\(x_k\)&lt;/span>尽量取大的值，所以有 &lt;span class="math">\[
x_k=\min\big\{\frac{\bar b_i}{y_{k_i}}\big |y_{k_i}&amp;gt;0\big\}=\frac{\bar b_r}{y_{k_r}}\tag{1.11}
\]&lt;/span> 认为第&lt;span class="math">\(r\)&lt;/span>项即为最小值。回顾一下刚刚的步骤，在目标函数中，因为&lt;span class="math">\(x_i\)&lt;/span>是未定变量，因此我们先简单的使用贪心法，选择正系数最大的&lt;span class="math">\((z_k-c_k)\)&lt;/span>；接下来，再判断&lt;span class="math">\((z_k-c_k)\)&lt;/span>对应的变量&lt;span class="math">\(x_k\)&lt;/span>的取值范围，得到其最大值。&lt;/p>
&lt;p>根据&lt;span class="math">\((1.8)\)&lt;/span>和&lt;span class="math">\((1.11)\)&lt;/span>，我们得到了一个改进可行解，其&lt;span class="math">\(x_r\rightarrow 0, x_k\rightarrow\frac{\bar b_r}{y_{k_r}}\)&lt;/span>，即 &lt;span class="math">\[
\boldsymbol{x_B&amp;#39;}=(x_{B_1},x_{B_2},\dotsb,x_{B_{r-1}},0,x_{B_{r+1}},\dotsb,x_{B_m},0,\dotsb,x_k,\dotsb,0)^T
\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>这个可行解一定是基本可行解&lt;/strong>。这是因为原来的基矩中， &lt;span class="math">\[
\boldsymbol{B}=(\boldsymbol{p_{B_1},p_{B_2},\dotsb,p_{B_m}})
\]&lt;/span> 中的m个列是线性无关的，其中不包含原属于&lt;span class="math">\(\boldsymbol{N}\)&lt;/span>中的列&lt;span class="math">\(\boldsymbol{p_{k}}\)&lt;/span>。根据&lt;span class="math">\((1.7)\)&lt;/span>，有&lt;span class="math">\(\boldsymbol{y_{k}}=\boldsymbol{B^{-1}p_k}\Rightarrow \boldsymbol{p_k}=\boldsymbol{By_k}=\sum_{i=1}^m y_{k_i}\boldsymbol{p_{B_i}}\)&lt;/span>，即&lt;span class="math">\(\boldsymbol{p_k}\)&lt;/span>是线性无关向量组&lt;span class="math">\((\boldsymbol{p_{B_1},p_{B_2},\dotsb,p_{B_m}})\)&lt;/span>的线性组合，且至少存在一个非0系数&lt;span class="math">\(y_{k_r}\)&lt;/span>.根据&lt;strong>替换定理&lt;/strong>，用&lt;span class="math">\(\boldsymbol{p_{k}}\)&lt;/span>来替代&lt;span class="math">\(\boldsymbol{p_{B_r}}\)&lt;/span>得到如下向量组依然是&lt;strong>线性无关的&lt;/strong>。 &lt;span class="math">\[
\boldsymbol{B}=(\boldsymbol{p_{B_1},p_{B_2},\dotsb,p_{B_r}\rightarrow p_{k},\dotsb,p_{B_m}})
\]&lt;/span> 这样可以组成新的基矩阵&lt;span class="math">\(\boldsymbol{B&amp;#39;}\)&lt;/span>，并得到新的基解&lt;span class="math">\(\boldsymbol{x_B&amp;#39;}\)&lt;/span>，而从&lt;span class="math">\((1.8)和(1.11)\)&lt;/span>，我们确定&lt;span class="math">\(\boldsymbol{x_B&amp;#39;}\)&lt;/span>满足每一个分量非负的条件，因此其必然是一个基本可行解。&lt;/p>
&lt;p>经过上述转换，&lt;span class="math">\(x_k\)&lt;/span>由原来的非基变量变成了基变量，而原来的基变量&lt;span class="math">\(x_{B_r}\)&lt;/span>变成了非基变量，同时目标函数值比原来减少了&lt;span class="math">\((z_k-c_k)x_k&amp;gt;0\)&lt;/span>。重复以上过程，进一步改进基本可行解，直到式&lt;span class="math">\((1.6)\)&lt;/span>的结果只能为非正数，以致任何一个非基变量取正值都无法再使得目标函数值降低，此时即为最优解。&lt;/p>
&lt;blockquote>
&lt;p>总结定理1：若在极小化问题中，对于某个基本可行解，所有&lt;span class="math">\((z_j-c_j)≤0\)&lt;/span>，则这个这个基本可行解是最优解；&lt;/p>
&lt;p>若在极大化问题中，对于某个基本可行解，所有&lt;span class="math">\((z_j-c_j)≥0\)&lt;/span>，则这个这个基本可行解是最优解；&lt;/p>
&lt;p>其中，&lt;span class="math">\(z_j-c_j=\boldsymbol{c_B^T B^{-1} p_j}-c_j,\ j=1,\dotsb,n\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>在线性规划中，通常称&lt;span class="math">\(z_j-c_j\)&lt;/span>为&lt;strong>判别数或检验数&lt;/strong>。&lt;/p>
&lt;h2 id="一般计算步骤">一般计算步骤&lt;/h2>
&lt;ol start="0" style="list-style-type: decimal">
&lt;li>首先，我们要获得一个初始基本可行解（可通过直接计算，大M法，两阶段法获得），设初始基矩阵为&lt;span class="math">\(\boldsymbol{B}\)&lt;/span>&lt;/li>
&lt;li>在限制条件&lt;span class="math">\(\boldsymbol{Ax}=\bigl[\boldsymbol{B}\ \boldsymbol{N}\bigr] \bigl[ \begin{smallmatrix} \boldsymbol{x_B} \\ \boldsymbol{x_N} \end{smallmatrix} \bigr]=\boldsymbol{b}\)&lt;/span>中，令非基变量&lt;span class="math">\(\boldsymbol{x_N=0}\)&lt;/span>，则&lt;span class="math">\(\boldsymbol{Bx_B=b}\Rightarrow \boldsymbol{x_B}=\boldsymbol{B^{-1}b}=\boldsymbol{\bar b}\)&lt;/span>，计算目标函数值&lt;span class="math">\(f=\boldsymbol{c^T_Bx_B}\)&lt;/span>&lt;/li>
&lt;li>对于此基矩阵的解集，求&lt;strong>单纯形乘子&lt;/strong>&lt;span class="math">\(\boldsymbol{w^T=c_B^T B^{-1}}\)&lt;/span>（m维行向量）。对于所有非基变量，计算判别数&lt;span class="math">\(\boldsymbol{c_B^T B^{-1}p_j}-c_j=\boldsymbol{w^T p_j}-c_j=z_j-c_j\)&lt;/span>，求得&lt;span class="math">\(z_k-c_k=\max\limits_{j\in R}\{z_j-c_j\}\)&lt;/span>。若&lt;span class="math">\(z_k-c_k≤0\)&lt;/span>，停止计算，目前基本可行解是最优解。否则，进行步骤（3）&lt;/li>
&lt;li>&lt;span class="math">\(\boldsymbol{x_B&amp;#39;}=\boldsymbol{\underbrace{B^{-1}b}_{\bar b}-\underbrace{B^{-1}p_k}_{y_k}}x_k\)&lt;/span>。若&lt;span class="math">\(\boldsymbol{y_k}≤0\)&lt;/span>，即每一个分量都不大于0，则停止计算，问题不存在有限最优解；否则，进行步骤（4）&lt;/li>
&lt;li>确定下标&lt;span class="math">\(r\)&lt;/span>，使得&lt;span class="math">\(x_k=\frac{\bar b_r}{y_{k_r}}=\min\big\{\frac{\bar b_i}{y_{k_i}}\big |y_{k_i}&amp;gt;0\big\}\)&lt;/span>。&lt;span class="math">\(x_{B_r}\)&lt;/span>为离基变量，&lt;span class="math">\(x_k\)&lt;/span>为进基变量，用&lt;span class="math">\(\boldsymbol{p_k}\)&lt;/span>替代变量&lt;span class="math">\(p_{B_r}\)&lt;/span>得到新的基矩阵&lt;span class="math">\(\boldsymbol{B&amp;#39;\rightarrow B}\)&lt;/span>，返回步骤（1）&lt;/li>
&lt;/ol>
&lt;h2 id="收敛性">收敛性&lt;/h2>
&lt;p>对于非退化情形，单纯形法有优秀的结论：&lt;/p>
&lt;blockquote>
&lt;p>定理2：对于非退化问题，单纯形方法经过有限次迭代后，&lt;strong>能达到最优解或得出无界的结论&lt;/strong>。在此条件下，单纯形法是收敛的。&lt;/p>
&lt;/blockquote>
&lt;p>证明：&lt;/p>
&lt;div class="figure">
&lt;img src="../images/单纯形法收敛性非退化.png" alt="单纯形法收敛性非退化" />&lt;p class="caption">单纯形法收敛性非退化&lt;/p>
&lt;/div>
&lt;p>对于非退化情形，每次迭代都有 &lt;span class="math">\[
\boldsymbol{x_B=B^{-1}b=\bar b}&amp;gt;0
\]&lt;/span> 此时能保证&lt;span class="math">\(x_r=\frac{b_r}{y_{k_r}}&amp;gt;0\)&lt;/span>。因此经迭代，目标函数值减小，并且由此可知，各次迭代得到的基本可行解互不相同。由于基本可行解的个数有限，因此经有限次迭代必能达到最优解。&lt;strong>对于退化情形，我们在后面将要证明，如果最优解存在，只要采取一定的措施，也能做到有限步收敛&lt;/strong>。&lt;/p>
&lt;h2 id="使用表格形式的单纯形方法">使用表格形式的单纯形方法&lt;/h2>
&lt;p>用单纯形方法求解线性规划问题的过程，实际上就是解线性方程组。只是在每次迭代中，要按一定规则选择自由未知量，以便能够得到改进的基本可行解。这个求解过程可以通过变换单纯形表来实现。具体做法可见《最优化理论与算法（第2版）》第3.1.4节。&lt;/p>
&lt;h2 id="退化情形">退化情形&lt;/h2>
&lt;p>对于退化情形，即存在基变量为0的情形（基变量为0是否一定退化有待验证），经过单纯形法多次迭代后，可能出现循环解现象（退化常见而循环不常见）。因此，我们可以采用&lt;strong>摄动法&lt;/strong>、Bland法，字典序法。&lt;/p></description></item><item><title>优化理论之线性规划</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/</guid><description>
&lt;h2 id="线性规划的标准形式和基本性质">线性规划的标准形式和基本性质&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="标准形式">标准形式&lt;/h2>
&lt;p>&lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\sum_{j=1}^n c_jx_j\\
\mathop{s.t.}\; &amp;amp;\sum_{j=1}^n \alpha_{ij}x_j=b_i,\ &amp;amp;i=1,\dotsb,m\\
&amp;amp;x_j\geq 0,\ &amp;amp;j=1,\dotsb,n\\
\end{aligned}\tag{1.1}
\]&lt;/span> 用矩阵表示为： &lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\boldsymbol c^T \boldsymbol x\\
\mathop{s.t.}\; &amp;amp;\boldsymbol A \boldsymbol x = \boldsymbol b\\
&amp;amp;\boldsymbol x \geq \boldsymbol 0\\
\end{aligned}\tag{1.2}
\]&lt;/span> 其中&lt;span class="math">\(\boldsymbol{A}\)&lt;/span>是&lt;span class="math">\(m\times n\)&lt;/span>矩阵，&lt;span class="math">\(\boldsymbol c^T\)&lt;/span>是n维行向量，&lt;span class="math">\(\boldsymbol b\)&lt;/span>是m维列向量，且&lt;span class="math">\(\boldsymbol b \geq 0\)&lt;/span>(每个分量都大于等于0)&lt;/p>
&lt;h2 id="从非标准形式转化为标准形式">从非标准形式转化为标准形式&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>如果存在&lt;span class="math">\(\boldsymbol b\)&lt;/span>的某一分量小于0，可以把方程两端都乘以-1&lt;/li>
&lt;li>在标准形式中，&lt;span class="math">\(x_j\)&lt;/span>都是非负的，这在实际生活中有着现实意义，但是处理数据时，难免有负数变量，因此可以令&lt;span class="math">\(x_j=x_j&amp;#39;-x_j&amp;#39;&amp;#39;\)&lt;/span>，其中&lt;span class="math">\(x_j&amp;#39;\geq 0,x_j&amp;#39;&amp;#39;\geq 0\)&lt;/span>。即用非负变量替换&lt;span class="math">\(x_j\)&lt;/span>&lt;/li>
&lt;li>当变量有上下界，不符合标准形式的要求时，也可做变量替换。当&lt;span class="math">\(x_j\geq l_j\)&lt;/span>,可令&lt;span class="math">\(x_j&amp;#39;=x_j-l_j\)&lt;/span>,则取&lt;span class="math">\(x_j&amp;#39;\geq 0\)&lt;/span>。当&lt;span class="math">\(x_j\leq u_j\)&lt;/span>时，可令&lt;span class="math">\(x_j&amp;#39;=u_j-x_j\)&lt;/span>，则取&lt;span class="math">\(x_j&amp;#39;\geq 0\)&lt;/span>&lt;/li>
&lt;li>当存在小于等于时，使用正松弛法；当存在大于等于号时用负松弛法。 &lt;span class="math">\[
\begin{aligned}
\min \; &amp;amp;\sum_{j=1}^n c_jx_j\\
\mathop{s.t.}\; &amp;amp;\sum_{j=1}^n \alpha_{ij}x_j=b_i,\ &amp;amp;i=1,\dotsb,m\\
&amp;amp;x_j\geq 0,\ &amp;amp;j=1,\dotsb,n\\
&amp;amp; 小于等于限制转化为（松弛变量）\Rightarrow\\
&amp;amp;\sum_{j=1}^n \alpha_{ij}x_j + x_{n+i}= b_i,\ &amp;amp;i=1,\dotsb,m\\
&amp;amp; x_{n+1},x_{n+2},\dotsb,x_{n+m}\geq 0
\end{aligned}\tag{1.3}
\]&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>用单纯性方法计算线性规划时，必须要用标准形。&lt;/p>
&lt;h2 id="基本性质">基本性质&lt;/h2>
&lt;h3 id="可行域">可行域&lt;/h3>
&lt;p>线性规划中，所有约束条件均为线性等式及不等式，满足这些条件的点的集合时凸集。即&lt;strong>可行域是凸集&lt;/strong>。&lt;/p>
&lt;h3 id="最优极点">最优极点&lt;/h3>
&lt;p>线性规划如果存在最优解，那么最优解一定能够在某个&lt;strong>极点&lt;/strong>上达到。下面简要证明：根据表示定理（见附录），任何可行点&lt;span class="math">\(\boldsymbol{x}\)&lt;/span>可以表示为极点和极方向的组合： &lt;span class="math">\[
\begin{aligned}
&amp;amp;\boldsymbol x=\sum_{j=1}^k \lambda_j\boldsymbol x^j + \sum_{j=1}^l \mu_j \boldsymbol{d}^j.\\
&amp;amp;\sum_{j=1}^k \lambda_j = 1.\\
&amp;amp;\lambda_j \geq 0,\ j=1,2,\dotsb,k.\\
&amp;amp;\mu_j \geq 0,\ j=1,2,\dotsb,l.
\end{aligned}\tag{2.1}
\]&lt;/span> 把&lt;span class="math">\(\boldsymbol x\)&lt;/span>的表达式代入&lt;span class="math">\((1.2)\)&lt;/span>式，得到以&lt;span class="math">\(\lambda_j, \mu_j\)&lt;/span>为变量的等价的线性规划 &lt;span class="math">\[
\begin{aligned}
\min\ &amp;amp;\boldsymbol x=\sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^j + \sum_{j=1}^l \mu_j \boldsymbol c^T\boldsymbol{d}^j.\\
\mathop{s.t.}\ &amp;amp;\sum_{j=1}^k \lambda_j = 1.\\
&amp;amp;\lambda_j \geq 0,\ j=1,2,\dotsb,k.\\
&amp;amp;\mu_j \geq 0,\ j=1,2,\dotsb,l.
\end{aligned}\tag{2.2}
\]&lt;/span> 注意，我们之前设过&lt;span class="math">\(\boldsymbol c^T\)&lt;/span>是一个行向量。由于&lt;span class="math">\(\mu_j\)&lt;/span>没有上限，因此若对于某个&lt;span class="math">\(j\)&lt;/span>，有&lt;span class="math">\(\boldsymbol c^T \boldsymbol d^j&amp;lt;0\)&lt;/span>，则&lt;span class="math">\(\mu_j \boldsymbol c^T\boldsymbol{d}^j\)&lt;/span>可以随着&lt;span class="math">\(\mu_j\)&lt;/span>的无限增大而无限减小，从而使目标函数趋向&lt;span class="math">\(-\infty\)&lt;/span>。对于这种情形，我们称该问题无界，或不存在有限最优值。&lt;/p>
&lt;p>如果&lt;span class="math">\(\forall j, 有\boldsymbol c^T\boldsymbol{d}^j\geq 0\)&lt;/span>，这时为了极小化目标函数，需令 &lt;span class="math">\[\forall j∈\{1,2,\dotsb,l\},\ \mu_j=0, \tag{2.3}\]&lt;/span> 则&lt;span class="math">\((2.2)\)&lt;/span>可以简写为极点的凸组合： &lt;span class="math">\[
\begin{aligned}
\min \ &amp;amp; \sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^j\\
\mathop{s.t.}\ &amp;amp;\sum_{j=1}^k \lambda_j=1,\\
&amp;amp;\lambda_j\geq 0,\ j=1,2,\dotsb,k
\end{aligned}\tag{2.4}
\]&lt;/span> 在所有&lt;span class="math">\(\boldsymbol c^T\boldsymbol x^j\)&lt;/span>中，必然有一个最小的值，令其为 &lt;span class="math">\[
\boldsymbol c^T\boldsymbol x^p = \min_{1\leq j \leq k} \boldsymbol c^T\boldsymbol x^j \tag{2.5}
\]&lt;/span> 显然当 &lt;span class="math">\[
\lambda_p=1\ 且 \lambda_j = 0,\ j\neq p\tag{2.6}
\]&lt;/span> 时，目标函数取最小值，即&lt;span class="math">\((2.3)\)&lt;/span>和&lt;span class="math">\((2.6)\)&lt;/span>式组合到一起时线性规划&lt;span class="math">\((2.2)\)&lt;/span>的最优解。此时必有 &lt;span class="math">\[
\begin{aligned}
\boldsymbol c^T\boldsymbol x &amp;amp;= \sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^j + \sum_{j=1}^l \mu_j \boldsymbol c^T\boldsymbol{d}^j\\
&amp;amp;\geq \sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^j\\
&amp;amp;\geq \sum_{j=1}^k \lambda_j\boldsymbol c^T\boldsymbol x^p=\boldsymbol c^T\boldsymbol x^p
\end{aligned}
\]&lt;/span> 因此，极点&lt;span class="math">\(\boldsymbol x^p\)&lt;/span>是线性规划的解。&lt;/p>
&lt;blockquote>
&lt;p>定理1：设线性规划&lt;span class="math">\((1.2)\)&lt;/span>的可行域非空，则有下列结论：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>线性规划&lt;span class="math">\((1.2)\)&lt;/span>存在有限最优解的充要条件是所有&lt;span class="math">\(\boldsymbol c^T\boldsymbol d^j\)&lt;/span>为非负数。其中，&lt;span class="math">\(\boldsymbol d^j\)&lt;/span>是可行域的极方向。&lt;/li>
&lt;li>若线性规划&lt;span class="math">\((1.2)\)&lt;/span>存在优先最优解，则目标函数的最优值可在某个极点上达到。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>一般情况下，把无界问题也归为不存在最优解，&lt;strong>只讨论存在有限最优解的情形&lt;/strong>。&lt;/p>
&lt;h2 id="最优基本可行解">最优基本可行解&lt;/h2>
&lt;p>基矩阵、基（本）解（基矩阵的解）（基解vs可行解）--》&lt;/p>
&lt;p>基解由&lt;span class="math">\(\boldsymbol{x_b}\)&lt;/span>的基变量与非基变量&lt;span class="math">\(\boldsymbol{x_n}\)&lt;/span>组成--》&lt;/p>
&lt;p>不是所有基解都是满足非负条件。满足&lt;strong>非负条件&lt;/strong>的基解为基本可行解（=基解 &amp;amp;&amp;amp; 可行解）（解的所有分量大于等于0）--》&lt;/p>
&lt;p>退化（某个基变量取值为0）与非退化（基变量取值全部大于0）--》&lt;/p>
&lt;p>基本可行解与极点有着对应关系--》&lt;/p>
&lt;p>线性规化最优解为最优基本可行解&lt;/p>
&lt;blockquote>
&lt;p>定理2：令&lt;span class="math">\(K=\{\boldsymbol x|\boldsymbol A \boldsymbol x=\boldsymbol b,\boldsymbol x\geq 0 \}, \boldsymbol A 为 m\times n\)&lt;/span>矩阵。&lt;span class="math">\(\boldsymbol A\)&lt;/span>的秩为&lt;span class="math">\(m\)&lt;/span>，则&lt;span class="math">\(K\)&lt;/span>的极点集和&lt;span class="math">\(\boldsymbol A \boldsymbol x = \boldsymbol b, \boldsymbol x\geq 0\)&lt;/span>的基本可行解集&lt;strong>等价&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>证明：可见最优化理论与算法（第2版）定理2.2.3的证明。&lt;/p>
&lt;p>根据定理1和定理2，我们可知&lt;strong>线性规划的求解问题归结为求最优基本可行解&lt;/strong>。&lt;/p>
&lt;h2 id="基本可行解的存在问题">基本可行解的存在问题&lt;/h2>
&lt;p>在什么条件下存在最优解呢？其实&lt;strong>表示定理的第一点&lt;/strong>已经明示了答案。&lt;/p>
&lt;blockquote>
&lt;p>若&lt;span class="math">\(S=\{\boldsymbol x \vert \boldsymbol A \boldsymbol x= \boldsymbol b, \boldsymbol x \geq 0\}\)&lt;/span>为非空多面集，则有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>极点集非空，且存在有限个极点&lt;span class="math">\(\boldsymbol x^1,\boldsymbol x^2,\dotsb,\boldsymbol x^k\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>而有限的极点集必存在一个使目标函数最小的极点。如果从方程求解的角度来看，可以等价为以下定理：&lt;/p>
&lt;blockquote>
&lt;p>定理3： 如果&lt;span class="math">\(\boldsymbol A \boldsymbol x= \boldsymbol b, \boldsymbol x \geq 0\)&lt;/span>有可行解，则一定存在基本可行解。其中&lt;span class="math">\(\boldsymbol A\)&lt;/span>是&lt;span class="math">\(m \times n\)&lt;/span>维矩阵，且&lt;span class="math">\(rank(\boldsymbol A)=m\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h2 id="附录表示定理">附录：表示定理&lt;/h2>
&lt;blockquote>
&lt;p>表示定理：设&lt;span class="math">\(S=\{\boldsymbol x \vert \boldsymbol A \boldsymbol x= \boldsymbol b, \boldsymbol x \geq 0\}\)&lt;/span>为非空多面集，则有：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>极点集非空，且存在有限个极点&lt;span class="math">\(\boldsymbol x^1,\boldsymbol x^2,\dotsb,\boldsymbol x^k\)&lt;/span>&lt;/li>
&lt;li>极方向集合为空集的充要条件是&lt;span class="math">\(S\)&lt;/span>有界。若&lt;span class="math">\(S\)&lt;/span>无界，则存在有限个极方向&lt;span class="math">\(\boldsymbol d^1,\boldsymbol d^2,\dotsb,\boldsymbol d^l\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(\boldsymbol x\in S\)&lt;/span>的充要条件是式&lt;span class="math">\((2.1)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>关于上述定理的证明可见文章&lt;/p>
&lt;p>[1] Bazaraa M S, Jarvis J J. Linear programming and Network flows， New York： Wiley 1977&lt;/p></description></item><item><title>优化理论之线性规划的初始基本可行解.md</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%88%9D%E5%A7%8B%E5%9F%BA%E6%9C%AC%E5%8F%AF%E8%A1%8C%E8%A7%A3.md/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E5%88%9D%E5%A7%8B%E5%9F%BA%E6%9C%AC%E5%8F%AF%E8%A1%8C%E8%A7%A3.md/</guid><description>
&lt;h2 id="线性规划的初始基本可行解">线性规划的初始基本可行解&lt;!-- omit in toc -->&lt;/h2>
&lt;p>我们使用单纯形法的时候，需要一个初始基本可行解来开始迭代优化，求出其他改进的基本可行解。下面我们介绍两个求初始基本可行解的方法。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#两阶段法">两阶段法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#两阶段法核心思想">两阶段法核心思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大m法">大M法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大m法核心思想">大M法核心思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单个人工变量技巧">单个人工变量技巧&lt;/a>&lt;/li>
&lt;li>&lt;a href="#注意人工变量vs松弛变量">注意：人工变量vs松弛变量&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="两阶段法">两阶段法&lt;/h2>
&lt;h3 id="两阶段法核心思想">两阶段法核心思想&lt;/h3>
&lt;p>第一阶段：每个方程增加一个系数为1的&lt;strong>非负人工变量&lt;/strong>，这些人工变量组成单位矩阵，可求出一组拓展的基本可行解。接下来用消主元法消去所有添加的人工变量（使人工变量都是非基变量取0，不必遵守单纯形法的主元选择规则），剩下的就是一个初始基本可行解。&lt;/p>
&lt;p>第二阶段：根据第一阶段得到的初始基本可行解，用一般单纯形法求解问题。&lt;/p>
&lt;p>总的来说，第一阶段问题要让人工变量为0，第二阶段正常解原问题。&lt;/p>
&lt;h2 id="大m法">大M法&lt;/h2>
&lt;h3 id="大m法核心思想">大M法核心思想&lt;/h3>
&lt;p>类似于惩罚函数法。在约束中增加&lt;strong>非负人工变量&lt;/strong>,&lt;span class="math">\(\boldsymbol{x_a}\)&lt;/span>，同时修改目标函数，增加惩罚项&lt;span class="math">\(M\boldsymbol{e^Tx_a}\)&lt;/span>，其中&lt;span class="math">\(M\)&lt;/span>是一个很大的正数，这样由于极小化目标函数的过程中，必然要优化&lt;span class="math">\(M\)&lt;/span>，将迫使人工变量离基。&lt;/p>
&lt;p>具体方式：目标函数加上带&lt;span class="math">\(M\)&lt;/span>的人工变量，限制条件中增加非负人工变量，再用一般单纯形法求解。&lt;/p>
&lt;p>由于添加的非负人工变量的系数为1，能够组成单位阵，因此很容易获得初始基本可行解。&lt;/p>
&lt;h2 id="单个人工变量技巧">单个人工变量技巧&lt;/h2>
&lt;p>处理&lt;span class="math">\(\boldsymbol{\bar b=B^{-1}b}\)&lt;/span>存在负数项情形，引入一个标量&lt;span class="math">\(x_a\)&lt;/span>，转换为 &lt;span class="math">\[
\boldsymbol{x_B}+\boldsymbol{B^{-1}N x_N}-x_a \boldsymbol{e}=\boldsymbol{\bar b}\\
\boldsymbol{x}=\begin{bmatrix}\boldsymbol{x_B} \\ \boldsymbol{x_N}\end{bmatrix}≥\boldsymbol{0},x_a≥0
\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{e}=\{1,1,\dotsb,1\}^T\)&lt;/span>是m维列向量。为了消除&lt;span class="math">\(\boldsymbol{]\bar b}\)&lt;/span>中的所有负数，可以借以将&lt;span class="math">\(x_a\)&lt;/span>引入基变量来实现。令 &lt;span class="math">\[
b_r = \min\{b_i\}&amp;lt;0
\]&lt;/span> 以&lt;span class="math">\(x_a\)&lt;/span>所在列为主列，第r行为主行进行主元消去（&lt;span class="math">\(x_a\rightarrow x_r\)&lt;/span>）。此时，约束方程右端变为 &lt;span class="math">\[
\begin{cases}
\bar b_r&amp;#39; = -\bar b_r &amp;gt;0 \\
\bar b_i&amp;#39; = \bar b_i - \bar b_r ≥0, i\neq r
\end{cases}
\]&lt;/span> 于是，我们得到了一个关于添加单个人工变量的一个基本可行解（&lt;span class="math">\(\boldsymbol{\bar b}≥0\)&lt;/span>）。再由此基本可行解出发，用两阶段法或大M法求解。&lt;/p>
&lt;h2 id="注意人工变量vs松弛变量">注意：人工变量vs松弛变量&lt;/h2>
&lt;p>人工变量与前面介绍过的松弛变量是两个不同的概念。松弛变量的作用是把不等式约束改写成等式约束，改写前后的两个问题是等价的。松弛变量的取值能够表达现行的可行点是在可行域的内部还是在其边界，也就是说，在此可行解处，原来的约束是成立严格不等式还是等式。因此，松弛变量是“合法”的变量。而人工变量的引入，改变了原来的约束条件，从这个意义上讲，它们是“不合法”的变量。这两种变量不可混为一谈。&lt;/p></description></item><item><title>优化理论-框架</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-%E6%A1%86%E6%9E%B6/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-%E6%A1%86%E6%9E%B6/</guid><description>
&lt;h2 id="优化理论框架">优化理论框架&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#优化理论解的存在性">优化理论解的存在性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束数值优化条件">无约束数值优化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束优化一阶条件">无约束优化一阶条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束优化二阶条件">无约束优化二阶条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#无约束数值优化方法">无约束数值优化方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方向确定">方向确定&lt;/a>&lt;/li>
&lt;li>&lt;a href="#步长确定-精确搜索与不精确搜索">步长确定-精确搜索与不精确搜索&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#线性搜索法">线性搜索法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#信任法信赖域levenberg-marquardt方法">信任法（信赖域）——Levenberg-Marquardt方法&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化条件">有约束数值优化条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化一阶条件">有约束数值优化一阶条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kkt点">KKT点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化二阶条件">有约束数值优化二阶条件&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有约束数值优化方法">有约束数值优化方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#转换为无约束优化拉格朗日法">转换为无约束优化——拉格朗日法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#barrier-methods">Barrier methods&lt;/a>&lt;/li>
&lt;li>&lt;a href="#惩罚函数">惩罚函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#收敛速度">收敛速度&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="优化理论解的存在性">优化理论解的存在性&lt;/h2>
&lt;p>解的存在性：&lt;/p>
&lt;ul>
&lt;li>维尔斯特拉斯极值定理：若函数&lt;span class="math">\(f\)&lt;/span>在n维有界闭区域&lt;span class="math">\(S\)&lt;/span>上连续，那么&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(S\)&lt;/span>上一定有全局最小值；&lt;/li>
&lt;li>若函数&lt;span class="math">\(f\)&lt;/span>在n维闭区域&lt;span class="math">\(S\)&lt;/span>上连续并且向正无穷发散(coercive，即&lt;span class="math">\(lim||x||⇒∞,f(x)=+∞\)&lt;/span>)，那么&lt;span class="math">\(f\)&lt;/span>在&lt;span class="math">\(S\)&lt;/span>上一定有全局最小值。&lt;/li>
&lt;/ul>
&lt;p>以上定理只能保证最小值存在，没有建立最小值和极小值之间的关系。但是，对于一类特殊的函数，它在一定区域内的极小值一定是最小值，这类函数即&lt;strong>凸函数&lt;/strong>。凸函数为定义在凸区间上的一种函数，它满足任意两点的连线位于抽象的函数曲面之下；而凸区间则满足任意两点连线仍然在区间中。定义在凸区间内的严格凸函数有唯一的极小值，该极小值为该函数在该区间上的最小值。&lt;/p>
&lt;h2 id="无约束数值优化条件">无约束数值优化条件&lt;/h2>
&lt;p>参见笔记——《最优化条件：无约束问题最优化条件》&lt;/p>
&lt;h3 id="无约束优化一阶条件">无约束优化一阶条件&lt;/h3>
&lt;h3 id="无约束优化二阶条件">无约束优化二阶条件&lt;/h3>
&lt;h2 id="无约束数值优化方法">无约束数值优化方法&lt;/h2>
&lt;p>线搜索和信任域（信赖域），两者主要区别是计算步长和搜索方向的步骤相反。线搜索的思路是先选择搜索方向,然后才是长度。信任域的思路是先选择长度（信任域半径），然后选择方向及其长度。&lt;/p>
&lt;p>对于搜索方向的选择，我们又有最速下降、牛顿、拟牛顿、非线性共轭梯度等方法。主要基础理论就是泰勒定理和矩阵相关知识。&lt;/p>
&lt;h3 id="方向确定">方向确定&lt;/h3>
&lt;ul>
&lt;li>梯度法（最速下降法）&lt;/li>
&lt;li>牛顿法&lt;/li>
&lt;li>拟牛顿法&lt;/li>
&lt;li>DFP算法&lt;/li>
&lt;li>BFGS算法&lt;/li>
&lt;li>L-BFGS算法&lt;/li>
&lt;/ul>
&lt;h3 id="步长确定-精确搜索与不精确搜索">步长确定-精确搜索与不精确搜索&lt;/h3>
&lt;p>两个常用的步长控制算法是&lt;strong>线搜索&lt;/strong>和&lt;strong>信赖域&lt;/strong>方法。在一个线搜索方法中，模型函数给出一个步骤方向，然后沿着这个方向搜索以找到一个可以达到收敛的适当的点。在信赖域方法中，每一步要更新一个距离，而在这个距离内模型函数是被信任的。如果模型步骤位于该距离之内，则可以被使用；否则，模型函数在信赖域边界上的极小值将被使用。一般来说，信赖域方法更稳健，但是它们需要更多的数值线性代数运算。&lt;/p>
&lt;h4 id="线性搜索法">线性搜索法&lt;/h4>
&lt;p>线搜索技术是多变量函数优化的基础，它包括&lt;strong>精确线搜索技术&lt;/strong>和&lt;strong>非精确线搜索技术&lt;/strong>。常见的精确线搜索技术可分为&lt;strong>分割方法和插值方法&lt;/strong>两大类：分割方法有二分法、黄金分割法、斐波那契法等；插值方法有一点二次插值法（牛顿法）、二点二次插值法（包括割线法）、三点二次插值法、二点三次插值法等。非精确线搜索技术基于&lt;strong>非精确线搜索准则&lt;/strong>，常用的准则有 Armijo-Goldstein 准则、Wolfe-Powell 准则、强 Wolfe-Powell 准则和简单准则。这里特别指出精确与非精确是指对&lt;strong>步长搜索的精确性&lt;/strong>。&lt;/p>
&lt;p>什么是线搜索呢？我们知道最优化算法的基本框架就是依据&lt;strong>迭代公式&lt;span class="math">\(\vec x_{k+1}=\vec x_k+\alpha_k\vec d_k\)&lt;/span>不断产生下一个迭代点&lt;/strong>，直到满足某些终止条件。迭代公式的确定涉及搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>和搜索步长&lt;span class="math">\(\alpha_k\)&lt;/span>的确定，其中&lt;span class="math">\(\alpha_k\)&lt;/span>的确定过程就是所谓线搜索的过程。&lt;/p>
&lt;p>精确线搜索的精确有两种理解。第一，对于分割法，精确搜索每次使得搜索空间有固定比例的缩小。例如&lt;span class="math">\(l_k\)&lt;/span>是第&lt;span class="math">\(k\)&lt;/span>次搜索空间大小，则第&lt;span class="math">\(k+1\)&lt;/span>次搜索空间就是&lt;span class="math">\(\alpha l_k(\alpha ∈ (0,1))\)&lt;/span>，第二，对于插值法，求使得近似（多项式）函数&lt;span class="math">\(p(\vec x_k+\alpha_k\vec d_k)\)&lt;/span>取到最小值时对应的&lt;span class="math">\(\alpha\)&lt;/span>，换句话说：精确线搜索求使得目标函数沿搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>下降最多的搜索步长。其他精确线搜索还有共轭梯度法conjugate gradient method。&lt;/p>
&lt;p>非精确线搜索没有找出导数为零的点,而是使f(x)有一个充分的下降(sufficient descent)，即只求使得目标函数沿搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>有一定下降的搜索步长，这个步长选择不好甚至可能会导致偏离搜索目标（比如梯度下降法中学习率过大的情况）。非精确搜索方法的基础是backtracking，如果能够收敛，则需要考虑以下几种条件。&lt;/p>
&lt;ul>
&lt;li>wolfe conditions&lt;/li>
&lt;li>Curvature condition&lt;/li>
&lt;li>Armijo conditions&lt;/li>
&lt;li>goldstein conditions&lt;/li>
&lt;/ul>
&lt;p>需要指出的是，非精确搜索和精确搜索不是完全互斥的，一些精确搜索方法也满足非精确线搜索准则。&lt;/p>
&lt;p>实际计算中，&lt;strong>通常采用非精确线搜索技术&lt;/strong>，这是因为精确线搜索技术耗费大量的计算资源，而且对于多变量函数的优化，许多算法的收敛速度并不取决于是否采用精确线搜索技术。&lt;/p>
&lt;h4 id="信任法信赖域levenberg-marquardt方法">信任法（信赖域）——Levenberg-Marquardt方法&lt;/h4>
&lt;h2 id="有约束数值优化条件">有约束数值优化条件&lt;/h2>
&lt;p>参见笔记——《最优化条件：有约束问题最优化条件》&lt;/p>
&lt;h3 id="有约束数值优化一阶条件">有约束数值优化一阶条件&lt;/h3>
&lt;h3 id="kkt点">KKT点&lt;/h3>
&lt;h3 id="有约束数值优化二阶条件">有约束数值优化二阶条件&lt;/h3>
&lt;h2 id="有约束数值优化方法">有约束数值优化方法&lt;/h2>
&lt;h3 id="转换为无约束优化拉格朗日法">转换为无约束优化——拉格朗日法&lt;/h3>
&lt;h3 id="barrier-methods">Barrier methods&lt;/h3>
&lt;h3 id="惩罚函数">惩罚函数&lt;/h3>
&lt;h2 id="收敛速度">收敛速度&lt;/h2>
&lt;p>如果我们假设了算法产生的迭代点列&lt;span class="math">\(\{ {x_k}\}\)&lt;/span>是在某种范数下收敛的, 即&lt;span class="math">\(\lim\limits_{k→∞}||x_k–x^\ast||=0\)&lt;/span>, 那么考察收敛速度的话, &lt;strong>Q-收敛速度&lt;/strong>较为常用是这么定义的:&lt;/p>
&lt;p>若存在实数&lt;span class="math">\(\alpha &amp;gt; 0\)&lt;/span>及一个与迭代次数&lt;span class="math">\(k\)&lt;/span>无关的常数&lt;span class="math">\(q &amp;gt; 0\)&lt;/span>, 使得 &lt;span class="math">\[\lim_{k→∞}\frac{||x_{k+1}–x^\ast||}{||x_k–x^\ast||^\alpha} = q\]&lt;/span>&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>线性收敛速度: &lt;span class="math">\(\alpha = 1,q &amp;gt; 0\)&lt;/span>&lt;/li>
&lt;li>超线性收敛速度: &lt;span class="math">\(1 &amp;lt; \alpha &amp;lt; 2,q &amp;gt; 0\)&lt;/span>或者&lt;span class="math">\(\alpha = 1,q = 0\)&lt;/span>&lt;/li>
&lt;li>二阶收敛速度: &lt;span class="math">\(\alpha = 2\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>注: 还有其他的一些收敛速度的定义, 但并不常用。&lt;/p></description></item><item><title>优化理论-基础</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-%E5%9F%BA%E7%A1%80/</link><pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-%E5%9F%BA%E7%A1%80/</guid><description>
&lt;h2 id="优化理论基础">优化理论基础&lt;!-- omit in toc -->&lt;/h2>
&lt;p>证明链条：凸集与超平面-&amp;gt;凸集分离定理-&amp;gt;Farkas引理&amp;gt;Gordan定理和择一性定理-&amp;gt;Kuhn-Tucker条件&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#凸集的定义和性质">凸集的定义和性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#性质">性质&lt;/a>&lt;/li>
&lt;li>&lt;a href="#凸集的极点">凸集的极点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#凸集的方向">凸集的方向&lt;/a>&lt;/li>
&lt;li>&lt;a href="#射线">射线&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方向的定义">方向的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#极方向">极方向&lt;/a>&lt;/li>
&lt;li>&lt;a href="#凸集的极射线">凸集的极射线&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多面集的表示定理">多面集的表示定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多面集表示定理">多面集表示定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#锥与凸锥">锥与凸锥&lt;/a>&lt;/li>
&lt;li>&lt;a href="#凸集分离定理">凸集分离定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#farkas引理">Farkas引理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gordan定理">Gordan定理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#附录">附录&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="凸集的定义和性质">凸集的定义和性质&lt;/h2>
&lt;p>设&lt;span class="math">\(S⊆R^n\)&lt;/span>，若对&lt;span class="math">\(∀x_1,x_2∈S\)&lt;/span>及&lt;span class="math">\(∀λ∈[0,1]\)&lt;/span>，都有&lt;span class="math">\(λx_1+(1−λ)x_2∈S\)&lt;/span>，则称&lt;span class="math">\(S\)&lt;/span>为凸集。&lt;/p>
&lt;h3 id="性质">性质&lt;/h3>
&lt;p>设&lt;span class="math">\(S_1和S_2\)&lt;/span>是两个凸集，&lt;span class="math">\(β\)&lt;/span>实数，则&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(\beta S_1=\{\beta x|x \in S_1\}\)&lt;/span>是凸集&lt;/li>
&lt;li>&lt;span class="math">\(S_1+S_2= \{x_1+x_2|x_1 \in S_1, x_2 \in S_2\}\)&lt;/span>是凸集&lt;/li>
&lt;li>&lt;span class="math">\(S_1-S_2=\{x_1-x_2|x_1 \in S_1, x_2 \in S_2\}\)&lt;/span>是凸集&lt;/li>
&lt;li>&lt;span class="math">\(S_1 \cap S_2\)&lt;/span>是凸集&lt;/li>
&lt;/ul>
&lt;h2 id="凸集的极点">凸集的极点&lt;/h2>
&lt;p>设&lt;span class="math">\(S\)&lt;/span>是非空集合，&lt;span class="math">\(x∈S\)&lt;/span>，若&lt;span class="math">\(x\)&lt;/span>不能表示成&lt;span class="math">\(S\)&lt;/span>中两个不同点的凸组合，即若假设&lt;span class="math">\(x=λx_1+(1−λ)x_2\)&lt;/span>，必推出&lt;span class="math">\(x=x_1=x_2\)&lt;/span>，则称&lt;span class="math">\(x\)&lt;/span>是凸集&lt;span class="math">\(S\)&lt;/span>的极点。从图形的角度来看，&lt;span class="math">\(x\)&lt;/span>是多边形的顶点或曲边的点。&lt;/p>
&lt;h2 id="凸集的方向">凸集的方向&lt;/h2>
&lt;h3 id="射线">射线&lt;/h3>
&lt;p>&lt;span class="math">\(∀α_0,\vec{d}\in R^n,\vec{d}≠0\)&lt;/span>称集合 &lt;span class="math">\(S=\{α_0+k\vec{d}|k∈R,k≥0\}\)&lt;/span>为一条射线。称&lt;span class="math">\(α_0\)&lt;/span>为射线的顶点，&lt;span class="math">\(\vec{d}\)&lt;/span>为射线的方向。&lt;/p>
&lt;p>射线&lt;span class="math">\(S=\{\alpha_0+k\vec{d}|k \in R,k\geq 0\}\)&lt;/span>是凸集。&lt;/p>
&lt;p>&lt;strong>证明：&lt;/strong>&lt;span class="math">\(\forall \alpha,\beta \in S,\exist k_1 \in R,k_1\geq 0,\)&lt;/span>使得&lt;span class="math">\(\alpha = \alpha_0+k_1\vec{d};\exist k_2 \in R,k_2\geq 0,\)&lt;/span>使得&lt;span class="math">\(\beta = c\)&lt;/span>。对于&lt;span class="math">\(\forall \lambda \in [0,1],\lambda k_1+(1-\lambda)k_2 \geq 0\)&lt;/span>，于是&lt;span class="math">\(\lambda\alpha+(1-\lambda)\beta=\lambda(\alpha_0+k_1\vec{d})+(1-\lambda)(\alpha_0+k_2\vec{d})=\alpha_0+[\lambda k_1+(1-\lambda)k_2]\vec{d}\in S\)&lt;/span>。即射线&lt;span class="math">\(S\)&lt;/span>为凸集。&lt;/p>
&lt;h3 id="方向的定义">方向的定义&lt;/h3>
&lt;p>对于任意一个凸集&lt;span class="math">\(S\)&lt;/span>，&lt;span class="math">\(\forall \vec{d}\in R^n,d \neq 0,若\forall \alpha_0 \in S,\forall k \in R,k \geq 0,都有\alpha_0+k\vec{d} \in S\)&lt;/span>，则称&lt;span class="math">\(\vec{d}\)&lt;/span>为&lt;span class="math">\(S\)&lt;/span>的一个方向。&lt;/p>
&lt;p>&lt;strong>comment：&lt;/strong> 方向能够无限延伸，说明在此方向上所有的点都在凸集内。方向是射线，起点是任意的，在方向上无界。&lt;/p>
&lt;p>设&lt;span class="math">\(S=\{x∣Ax=b,x≥0\}≠∅\)&lt;/span>，&lt;span class="math">\(\vec{d}\)&lt;/span>是非零向量，则&lt;span class="math">\(\vec{d}\)&lt;/span>是&lt;span class="math">\(S\)&lt;/span>的方向 ⟺ &lt;span class="math">\(\vec{d}≥0且A\vec{d}=0\)&lt;/span>。&lt;/p>
&lt;h3 id="极方向">极方向&lt;/h3>
&lt;p>对于任意一个凸集&lt;span class="math">\(S\)&lt;/span>,对于 &lt;span class="math">\(S\)&lt;/span>中的任意两个方向&lt;span class="math">\(\vec{d}_1,\vec{d}_2\)&lt;/span>, 若&lt;span class="math">\(\forall k&amp;gt;0,\vec{d}_1\neq k \vec{d}_2\)&lt;/span>,则称这两个方向是不同的。&lt;/p>
&lt;p>若&lt;span class="math">\(\vec{d}\)&lt;/span>是S的一个方向，且不是S的任意两个&lt;strong>不同&lt;/strong>方向的&lt;strong>正线性组合&lt;/strong>。即：对于S的任意两个方向&lt;span class="math">\(\vec{d}_1,\vec{d}_2\)&lt;/span>，若存在&lt;span class="math">\(k_1&amp;gt;0,k_2&amp;gt;0\)&lt;/span>，使得&lt;span class="math">\(k_1\vec{d}_1+k_2\vec{d}_2=\vec{d}\)&lt;/span>，则&lt;span class="math">\(\vec{d}_1,\vec{d}_2\)&lt;/span>必是相同的方向，则称&lt;span class="math">\(\vec{d}\)&lt;/span>是S的一个极方向。&lt;/p>
&lt;h3 id="凸集的极射线">凸集的极射线&lt;/h3>
&lt;p>凸集S中的任意一条射线，若它的方向是极方向，则称这条射线为极射线。&lt;/p>
&lt;h2 id="多面集的表示定理">多面集的表示定理&lt;/h2>
&lt;p>&lt;strong>多面集&lt;/strong>：在空间&lt;span class="math">\(\mathbb{R}^n\)&lt;/span>中，由有限个闭半空间相交组成的集合： &lt;span class="math">\[S=\{x \in \mathbb{R}^n:P_i^T x \leq \alpha_i,i=1,2,\dotsb,m\}\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>多面锥&lt;/strong>：在空间&lt;span class="math">\(\mathbb{R}^n\)&lt;/span>中，由有限个&lt;strong>包含原点&lt;/strong>的半空间相交而成： &lt;span class="math">\[S=\{x \in \mathbb{R}^n:P_i^T x \leq 0,i=1,2,\dotsb,m\}\]&lt;/span>&lt;/p>
&lt;p>&lt;strong>多面体&lt;/strong>：有界的多面集。&lt;/p>
&lt;ul>
&lt;li>多面体是有限个点集的凸包。&lt;/li>
&lt;li>凸锥可以有有限个向量生成（凸锥组合）&lt;/li>
&lt;li>多面集是闭集&lt;/li>
&lt;li>多面集是凸集&lt;/li>
&lt;/ul>
&lt;h3 id="多面集表示定理">多面集表示定理&lt;/h3>
&lt;p>设&lt;span class="math">\(S=\{X∈\mathbb{R}^n:AX≤\vec{b},X≥\vec{0}\}\)&lt;/span>为非空多面集，其中 &lt;span class="math">\[A=\begin{pmatrix}
a_1 \\
a_2 \\
\vdots\\
a_m
\end{pmatrix} \in \mathbb{R}^{m\times n},
b=\begin{pmatrix}
b_1 \\
b_2 \\
\vdots\\
b_m
\end{pmatrix} \in \mathbb{R}^{m}\]&lt;/span> 则有：&lt;/p>
&lt;ul>
&lt;li>极点集非空，且存在有限个极点&lt;span class="math">\(\{X(1),⋯,X(k)\}, k \in N,k \geq 1\)&lt;/span>&lt;/li>
&lt;li>极方向集合为空集 ⟺ &lt;span class="math">\(S\)&lt;/span>有界。若&lt;span class="math">\(S\)&lt;/span>无界，则存在有限个极方向&lt;span class="math">\(\{d(1),d(2),⋯,d(l)\},l \in N ,l \geq 0\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(∀X∈R_n,X∈S\)&lt;/span>当且仅当&lt;span class="math">\(X\)&lt;/span> 可以被表示成&lt;span class="math">\(X_1,⋯,X_k\)&lt;/span>的凸组合加上&lt;span class="math">\(d_1,⋯,d_l\)&lt;/span>的非负线性组合，即存在集合 &lt;span class="math">\[\{λ_i∈R:\sum_{i=1}^k λ_i=1,λ_i≥0,i∈N,1≤i≤k\}与 \\
\{ \mu_i \in \mathbb R: \mu_i \ge 0, i \in \mathbb N, 1 \le i \le l \} \\
使得：X = \sum\limits_{i = 1}^{k} \lambda_i X_i + \sum\limits_{i = 1}^{l} \mu_i d_i\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;h2 id="锥与凸锥">锥与凸锥&lt;/h2>
&lt;p>&lt;strong>锥(cone)&lt;/strong> 定义：对于集合&lt;span class="math">\(C⊆R_n,∀x∈C,θ≥0\)&lt;/span>，有&lt;span class="math">\(θx⊆C\)&lt;/span>则&lt;span class="math">\(x\)&lt;/span>构成的集合称为锥。说明一下，锥不一定是连续的（可以是数条过原点的射线的集合）。&lt;/p>
&lt;p>&lt;strong>凸锥&lt;/strong>（convex cone）定义：凸锥包含了集合内点的所有凸锥组合。若锥&lt;span class="math">\(C⊆R_n,x_1,x_2...x_n∈C,θ_i≥0\)&lt;/span>，则凸锥组合&lt;span class="math">\(θ_1 x_1+θ_2 x_2+...+θ_n x_n\)&lt;/span>也属于凸锥集合&lt;span class="math">\(C\)&lt;/span>。这里说明一下，就是说一个集合&lt;strong>既是凸集又是锥&lt;/strong>，那么就是凸锥。&lt;/p>
&lt;p>&lt;strong>凸锥包&lt;/strong>（convex cone hull）定义：凸锥包是包含集合&lt;span class="math">\(C\)&lt;/span>的最小的凸锥，假设&lt;span class="math">\(x_1,x2_...x_n∈C\)&lt;/span>，凸锥包表示为： &lt;span class="math">\[\{θ_1x_1+θ_2x_2+...+θ_nx_n|x_1,x_2...x_n∈C，θ_i≥0\}\]&lt;/span>&lt;/p>
&lt;p>另见常见的凸集，凸锥，仿射集:&lt;a href="https://www.cnblogs.com/saysei/p/10124314.html">https://www.cnblogs.com/saysei/p/10124314.html&lt;/a>&lt;/p>
&lt;h2 id="凸集分离定理">凸集分离定理&lt;/h2>
&lt;p>&lt;strong>凸集分离定理&lt;/strong>（超平面分离定理）是应用凸集到最优化理论中的重要结果，这个结果在最优化理论中有重要的位置。所谓&lt;strong>两个凸集分离&lt;/strong>，直观地看是指两个凸集合没有交叉和重合的部分，因此可以用&lt;strong>一张超平面&lt;/strong>将两者隔在两边。&lt;strong>需要注意的是，凸集分离定理研究n维欧式空间，所以“紧”的概念等同于闭合+有界&lt;/strong>。&lt;/p>
&lt;p>设&lt;span class="math">\(S_1,S_2\subseteq R^n\)&lt;/span>为两个非空集合,且&lt;span class="math">\(S_1\cap S_2=\emptyset\)&lt;/span>，如果存在非零向量&lt;span class="math">\(\vec{p}\in R^n\)&lt;/span>及&lt;span class="math">\(\alpha \in R\)&lt;/span>使得 &lt;span class="math">\[S_1 \subseteq H^-=\{x\in R^n|\vec{p}^Tx\leq \alpha\}\]&lt;/span> &lt;span class="math">\[S_2 \subseteq H^+=\{x\in R^n|\vec{p}^Tx\geq \alpha\}\]&lt;/span> 则称超平面&lt;span class="math">\(H=\{x \in R^n |\vec{p}^T x=\alpha\}\)&lt;/span>分离了集合&lt;span class="math">\(S_1与S_2\)&lt;/span>。&lt;/p>
&lt;p>&lt;strong>证明：&lt;/strong> 首先我们需要证明一个引理。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>引理&lt;/strong> &lt;span class="math">\(K\subset R^n\)&lt;/span>，且K是非空闭凸集。那么在K种&lt;strong>存在唯一一个向量&lt;/strong>具有最小范数（长度）。&lt;/p>
&lt;p>&lt;strong>引理证明&lt;/strong>：令&lt;span class="math">\(\delta=\inf\{|x|\mid x\in K\}\)&lt;/span>。令&lt;span class="math">\(x_j\)&lt;/span>为K中的一个序列，且&lt;span class="math">\(|x_j|\to\delta\)&lt;/span>。注意到K是一个凸集，且&lt;span class="math">\((x_i+x_j)/2\)&lt;/span>也会在K中。所以&lt;span class="math">\((x_i+x_j)/2\)&lt;/span>的范数大于&lt;span class="math">\(\delta\)&lt;/span>。 &lt;span class="math">\[|(x_i+x_j)/2|\geq|\delta|\Rightarrow |x_i+x_j|^2 \geq 4\delta^2 \\
|x_i-x_j|^2=2|x_i|^2+2|x_j|^2-|x_i+x_j|^2\\
\leq 2|x_i|^2+2|x_j|^2-4\delta^2 \to 0\]&lt;/span> 由于&lt;span class="math">\(i,j\to\infty\)&lt;/span>，并且&lt;span class="math">\(x_i\)&lt;/span>是柯西序列，因此x的极限在K中。唯一性是由于y也在K中有范数，而&lt;span class="math">\(|x-y|^2 \leq 2|x_i|^2+2|x_j|^2-4\delta^2=0\)&lt;/span>，根据范数定义&lt;span class="math">\(x=y\)&lt;/span>。&lt;/p>
&lt;/blockquote>
&lt;p>接下来继续超平面分离定理证明。考虑两个不相交的分控凸集&lt;span class="math">\(A，B\)&lt;/span>，令&lt;span class="math">\(K=A+(-B)=\{x-y|x\in A,y\in B\}\)&lt;/span>。由于&lt;span class="math">\(-B\)&lt;/span>也是凸集，所以&lt;span class="math">\(K\)&lt;/span>也是凸集。由于K可能是开集，我们考虑K的闭包&lt;span class="math">\(\overline{K}\)&lt;/span>（闭集），有一个最小范数的向量&lt;span class="math">\(v\)&lt;/span>。由于&lt;span class="math">\(\overline{K}\)&lt;/span>是凸的，所以对于&lt;span class="math">\(\forall n \in K\)&lt;/span>，线段 &lt;span class="math">\[v+t(n-v),0\leq t \leq 1\]&lt;/span> 也在&lt;span class="math">\(\overline{K}\)&lt;/span>中，所以 &lt;span class="math">\[|v|^2\leq |v+t(n-v)^2|=|v|^2+2t&amp;lt;v,n-v&amp;gt;+t^2|n-v|^2\]&lt;/span> 上式是因为&lt;span class="math">\(v\)&lt;/span>是唯一最小范数。对于&lt;span class="math">\(0&amp;lt;t \leq 1\)&lt;/span>，我们有 &lt;span class="math">\[0\leq 2&amp;lt;v,n&amp;gt;-2|v|^2+t|n-v|^2\\
|v|^2-0.5t|n-v|^2\leq &amp;lt;v,n&amp;gt;\]&lt;/span> 当&lt;span class="math">\(t\to 0\)&lt;/span>时，&lt;span class="math">\(&amp;lt;n,v&amp;gt;\geq |v|^2\)&lt;/span>。因此对于任意&lt;span class="math">\(x \in A\)&lt;/span>和&lt;span class="math">\(y \in B\)&lt;/span>,有&lt;span class="math">\(&amp;lt;x-y,v&amp;gt; \geq |v|^2\)&lt;/span>。所以，如果&lt;span class="math">\(v\)&lt;/span>时非零的。那么： &lt;span class="math">\[\inf_{x \in A}\langle x,v \rangle \geq |v|^2+\sup_{y \in B}\langle y,v \rangle\]&lt;/span> 即存在一个平面分隔了凸集&lt;span class="math">\(A,B\)&lt;/span>。更广泛的&lt;span class="math">\(v=0\)&lt;/span>说明可见维基百科。&lt;/p>
&lt;p>另一个证明：&lt;a href="https://www.cnblogs.com/szqfreiburger/p/11573936.html">https://www.cnblogs.com/szqfreiburger/p/11573936.html&lt;/a>&lt;/p>
&lt;h2 id="farkas引理">Farkas引理&lt;/h2>
&lt;blockquote>
&lt;p>令&lt;span class="math">\(A\in \mathbb{R}^{m\times n},\mathbf{b}\in \mathbb{R}^m\)&lt;/span>，那么下面两个陈述只有一个成立：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>存在&lt;span class="math">\(\mathbf{x}\in \mathbb{R}^n，使得Ax=b,且x \geq 0\)&lt;/span>&lt;/li>
&lt;li>存在&lt;span class="math">\(y \in \mathbb{R}^m,使得A^Ty\geq 0，且b^Ty&amp;lt;0\)&lt;/span>.&lt;/li>
&lt;/ol>
&lt;p>需要注意的时，&lt;span class="math">\(\mathbf{x}\geq 0\)&lt;/span>是指向量&lt;span class="math">\(\mathbf{x}\)&lt;/span>的所有组成元素都非负。&lt;/p>
&lt;/blockquote>
&lt;p>先从几何角度了解一下这个引理的含义。我们认为矩阵&lt;span class="math">\(A\)&lt;/span>是由&lt;span class="math">\(n\)&lt;/span>个&lt;span class="math">\(m\)&lt;/span>维的向量组成的向量组，由于在(1)中&lt;span class="math">\(x\)&lt;/span>的每一个元素大于0，所以&lt;span class="math">\(Ax\)&lt;/span>为&lt;span class="math">\(n\)&lt;/span>个向量的非负线性组合，即凸锥组合。它们张成的是凸锥（或者说包含这&lt;span class="math">\(n\)&lt;/span>个向量的凸锥包），长成下图所示。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/凸锥包.jpg" alt="凸锥包" />&lt;p class="caption">凸锥包&lt;/p>
&lt;/div>
&lt;p>每一条棱表示&lt;span class="math">\(A\)&lt;/span>中的一个向量。而（1）结果&lt;span class="math">\(Ax=b\)&lt;/span>，表示&lt;span class="math">\(b\)&lt;/span>在凸锥中，如下左图。&lt;/p>
&lt;img src="../images/Farkas引理.jpg" alt="Farkas引理.jpg" />
&lt;center>
左为b在凸锥内的情况，右图为b在凸锥外的情况，总能找到过原点的超平面（二维情况下为直线，法向量为y），把b和凸锥分开。
&lt;/center>
&lt;p>对于情形（2），则是凸集分离定理的推广。单点也是凸集。特别的，如果&lt;span class="math">\(C\)&lt;/span>不仅是凸集，还是事实一中所提到的&lt;strong>凸锥&lt;/strong>，我们可以找到一个&lt;strong>过原点&lt;/strong>的平面，分开凸锥和它外面的一个点，也就是说如果&lt;span class="math">\(C\)&lt;/span>是一个凸锥，&lt;span class="math">\(x \notin C\)&lt;/span>，存在非零 &lt;span class="math">\(d\in R^n\)&lt;/span>满足对于所有的 &lt;span class="math">\(y in C\)&lt;/span>，有&lt;span class="math">\(d^Tx&amp;lt;0\)&lt;/span>，且&lt;span class="math">\(d^Ty&amp;gt;0\)&lt;/span>。上面右图所示。&lt;/p>
&lt;p>严格证明可以参考:&lt;a href="https://lijiancheng0614.github.io/2015/09/17/2015_09_17_Optimization%20Methods/">https://lijiancheng0614.github.io/2015/09/17/2015_09_17_Optimization%20Methods/&lt;/a>&lt;/p>
&lt;h2 id="gordan定理">Gordan定理&lt;/h2>
&lt;p>设&lt;span class="math">\(A\)&lt;/span>为&lt;span class="math">\(m×n\)&lt;/span>矩阵，则&lt;span class="math">\(Ax&amp;lt;0\)&lt;/span>有解，⟺ &lt;span class="math">\(A^Ty=0,y≥0(y≠0)\)&lt;/span>无解。&lt;/p>
&lt;h2 id="附录">附录&lt;/h2>
&lt;p>在&lt;span class="math">\(R^n\)&lt;/span>中，下面三个条件等价：&lt;/p>
&lt;ul>
&lt;li>“有界闭”（bounded and closed）；&lt;/li>
&lt;li>“紧”（compact）；&lt;/li>
&lt;li>“列紧”（sequentially compact）。&lt;/li>
&lt;/ul>
&lt;div class="figure">
&lt;embed src="../images/凸集凸函数凸优化.webp" />&lt;p class="caption">凸集凸函数凸优化.webp&lt;/p>
&lt;/div></description></item><item><title>无线通信之信道模型</title><link>https://surprisedcat.github.io/studynotes/%E6%97%A0%E7%BA%BF%E9%80%9A%E4%BF%A1%E4%B9%8B%E4%BF%A1%E9%81%93%E6%A8%A1%E5%9E%8B/</link><pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%97%A0%E7%BA%BF%E9%80%9A%E4%BF%A1%E4%B9%8B%E4%BF%A1%E9%81%93%E6%A8%A1%E5%9E%8B/</guid><description/></item><item><title>优化理论之一维优化-步长控制-线搜索与置信域</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96-%E6%AD%A5%E9%95%BF%E6%8E%A7%E5%88%B6-%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E7%BD%AE%E4%BF%A1%E5%9F%9F/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96-%E6%AD%A5%E9%95%BF%E6%8E%A7%E5%88%B6-%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E7%BD%AE%E4%BF%A1%E5%9F%9F/</guid><description>
&lt;h2 id="步长控制-线搜索与置信域">步长控制-线搜索与置信域&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;p>在最优化(optimization)问题中，线搜索(line search)和置信域(trust region)方法是寻找局部最小值(local minimum)基本迭代方法(iterative approach)。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#线搜索line-search">线搜索(Line search)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#精确线搜索步长">精确线搜索步长&lt;/a>&lt;/li>
&lt;li>&lt;a href="#非精确线搜索步长">非精确线搜索步长&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#wolfe-conditions">Wolfe conditions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#armijo-conditions">Armijo conditions&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;h3 id="线搜索line-search">线搜索(Line search)&lt;/h3>
&lt;p>以f(x)为例，线搜索会先找一个使f(x)下降的方向，接着计算一个步长，步长决定了x改变的大小。&lt;/p>
&lt;p>下降方向:可以通过梯度下降，牛顿法，拟牛顿法等计算。&lt;/p>
&lt;p>步长:有精确(exact)和非精确(inexact)两种，精确方法就是找出导数为零的极值点，例如共轭梯度法conjugate gradient method;非精确方法没有找出导数为零的点，而是使f(x)有一个充分的下降(sufficient descent)，例如backtracking，wolfe conditions，goldstein conditions&lt;/p>
&lt;p>线搜索流程:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>计算线搜索方向&lt;span class="math">\(\vec{p}_k\)&lt;/span>，使得该方向满足&lt;span class="math">\(\nabla f^T_k\vec{p}_k&amp;lt;0，\nabla f_k\ne\vec{0}\)&lt;/span>。（满足上述不等式则该方向为下降法向，因为&lt;span class="math">\(\nabla f_k\)&lt;/span>是方向上升方向，其夹角只有大于180度才会为负）&lt;/li>
&lt;li>计算步长&lt;span class="math">\(\alpha_k&amp;gt;0\)&lt;/span>，使得&lt;span class="math">\(f(x_k+\alpha_k*p_k)&amp;lt;f(x_k)\)&lt;/span>&lt;/li>
&lt;li>更新&lt;span class="math">\(x:x_{k+1}=x_k+\alpha_k*p_k\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>(完全囊括了梯度算法和牛顿类算法的框架，如图1所示。)&lt;/p>
&lt;img src="../images/Line_search_alogorithm_chart.png" alt="Line_search_alogorithm_chart.png" />
&lt;center>
图1 线搜索算法流程图
&lt;/center>
&lt;h4 id="精确线搜索步长">精确线搜索步长&lt;/h4>
&lt;p>如何确定合适的步长呢？如果我们从表达式上来看，令步长&lt;span class="math">\(\alpha\)&lt;/span>为自变量： &lt;span class="math">\[\phi(\alpha)=f(x_k+\alpha*p_k),\alpha&amp;gt;0\]&lt;/span> 在方向&lt;span class="math">\(\vec{p}_k\)&lt;/span>已经确定的情况下，&lt;span class="math">\(\alpha\)&lt;/span>的最佳选择无疑是使&lt;span class="math">\(\phi(\alpha)\)&lt;/span>的值最小，即使得在给定方向上下降最大的距离。但是，这计算起来有些不切实际，因为计算&lt;span class="math">\(\phi\)&lt;/span>函数的最小值增加了大量计算量，尤其是&lt;span class="math">\(\phi\)&lt;/span>函数有不少局部最小值和稳定点的时候，如图2所示：&lt;/p>
&lt;img src="../images/Step_length_image.png" alt="发现理想步长的复杂度" />
&lt;center>
图2 发现理想步长的复杂度 (Nocedal &amp;amp; Wright)
&lt;/center>
&lt;p>常见精确搜索方法（在导数不可求得情况下用的）：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>二分搜索&lt;/li>
&lt;li>斐波那契搜索&lt;/li>
&lt;li>黄金分割搜索&lt;/li>
&lt;li>二次插值法&lt;/li>
&lt;li>三次插值法&lt;/li>
&lt;li>D.S.C.法&lt;/li>
&lt;/ol>
&lt;p>精确搜索法的核心是&lt;strong>找到到达函数最小值的步长&lt;/strong>。但是一个实用的正常寻找合适步长的法子比找到&lt;span class="math">\(\phi\)&lt;/span>函数最小值要容易的多，只要使目标函数的值下降即可（不会震荡到函数值更高的点），用表达式来说就是： &lt;span class="math">\[f(x_k+\alpha*p_k)&amp;lt;f(x_k)\]&lt;/span> 该方法不能保证收敛到函数的最小值，因此我们可以添加两个&lt;strong>条件&lt;/strong>，来要求在每次迭代过程中都要显著地减小目标函数。&lt;/p>
&lt;h4 id="非精确线搜索步长">非精确线搜索步长&lt;/h4>
&lt;h5 id="wolfe-conditions">Wolfe conditions&lt;/h5>
&lt;p>Wolfe conditions 由 Armijo conditions和Curvature conditions构成,先分别介绍Armijo conditions和Curvature conditions&lt;/p>
&lt;h5 id="armijo-conditions">Armijo conditions&lt;/h5></description></item><item><title>优化理论之一维精确优化（下）</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96%E4%B8%8B/</link><pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96%E4%B8%8B/</guid><description>
&lt;h2 id="一维精确优化">一维精确优化&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#一维优化方法">一维优化方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#搜索法">搜索法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二分搜索">二分搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#黄金分割搜索">黄金分割搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#斐波那契搜索">斐波那契搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#近似法">近似法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二次插值法">二次插值法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#一点二次插值即牛顿法">一点二次插值（即牛顿法）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二点二次插值">二点二次插值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三点二次插值">三点二次插值&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#三次插值法">三次插值法&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#二点三次插值法cubic-interpolation-method-with-two-points">二点三次插值法（Cubic Interpolation Method with Two-Points）&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#综合方法">综合方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dsc法">D.S.C.法&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="一维优化方法">一维优化方法&lt;/h2>
&lt;h2 id="搜索法">搜索法&lt;/h2>
&lt;h3 id="二分搜索">二分搜索&lt;/h3>
&lt;h3 id="黄金分割搜索">黄金分割搜索&lt;/h3>
&lt;h3 id="斐波那契搜索">斐波那契搜索&lt;/h3>
&lt;h2 id="近似法">近似法&lt;/h2>
&lt;p>插值法是一类重要的一维搜索方法。其基本思想是在搜索区间中不断用低次多项式来近似目标函数，并逐步用插值多项式的极小点来逼近一维搜索问题的极小点。当函数具有较好的&lt;strong>解析性质时&lt;/strong>，插值法比搜索法效果更好。伟大的泰勒！泰勒展开，多项式！&lt;/p>
&lt;p>常见的插值方法有一点二次插值法、二点二次插值法、三点二次插值法和二点三次插值法。其中一点二次插值法就是牛顿法，二点二次插值法中的一种就是割线法。&lt;/p>
&lt;p>几次插值法就是插值多项式的最高阶数，几点就是选用几个点。比如二次插值法，插值多项式为&lt;span class="math">\(q(x)=ax^2+bx+c\)&lt;/span>有3个待定系数，可以从两个方面来确定系数，一是用点的值，二是用点的导数。&lt;/p>
&lt;h3 id="二次插值法">二次插值法&lt;/h3>
&lt;p>二次插值法就是不断构造二次插值多项式&lt;span class="math">\(q(x)=ax^2+bx+c\)&lt;/span>来近似目标函数&lt;span class="math">\(f(x)\)&lt;/span>并将最后一步构造的二次插值多项式的极小值点作为目标函数极小值点的方法，不同二次插值法的区别在于构造二次插值多项式时使用的信息不同。（注：以下介绍各方法基本原理时，&lt;span class="math">\(x_k\)&lt;/span>均指第&lt;span class="math">\(k\)&lt;/span>步迭代中构造的插值多项式的极小值点）&lt;/p>
&lt;h4 id="一点二次插值即牛顿法">一点二次插值（即牛顿法）&lt;/h4>
&lt;p>一点二次插值在第&lt;span class="math">\((k+1)\)&lt;/span>步构造二次插值多项式时使用 1 个函数、1 个一阶导数和 1 个二阶导数信息。一点，最高阶为2次，名字由来。 &lt;span class="math">\[\begin{aligned}
q(x_k)=&amp;amp;ax_k^2+bx_k+c&amp;amp;=f(x_k)\\
q&amp;#39;(x_k)=&amp;amp;2ax_k+b&amp;amp;=f&amp;#39;(x_k)\\
q&amp;#39;&amp;#39;(x_k)=&amp;amp;2a&amp;amp;=f&amp;#39;&amp;#39;(x_k)
\end{aligned}\]&lt;/span> 求关于&lt;span class="math">\(a,b,c\)&lt;/span>的解方程组： &lt;span class="math">\[a=\frac{\begin{vmatrix}f(x_k)&amp;amp;x_k&amp;amp;1\\f&amp;#39;(x_k)&amp;amp;1&amp;amp;0\\f&amp;#39;&amp;#39;(x_k)&amp;amp;0&amp;amp;0\end{vmatrix}}
{\begin{vmatrix}x_k^2&amp;amp;x_k&amp;amp;1\\2x_k&amp;amp;1&amp;amp;0\\2&amp;amp;0&amp;amp;0\end{vmatrix}}=\frac{f&amp;#39;&amp;#39;(x_k)}{2}\]&lt;/span> &lt;span class="math">\[b=\frac{\begin{vmatrix}x_k^2&amp;amp;f(x_k)&amp;amp;1\\2x_k&amp;amp;f&amp;#39;(x_k)&amp;amp;0\\2&amp;amp;f&amp;#39;&amp;#39;(x_k)&amp;amp;0\end{vmatrix}}
{\begin{vmatrix}x_k^2&amp;amp;x_k&amp;amp;1\\2x_k&amp;amp;1&amp;amp;0\\2&amp;amp;0&amp;amp;0\end{vmatrix}}=f&amp;#39;(x_k)-x_kf&amp;#39;&amp;#39;(x_k)\]&lt;/span> 要求&lt;span class="math">\(q&amp;#39;(x)=f&amp;#39;(x)=0\)&lt;/span>，所以此时&lt;span class="math">\(x_{k+1}=x_k-\frac{f&amp;#39;(x)}{f&amp;#39;&amp;#39;(x)}\)&lt;/span>。从原理上来看，牛顿法只是求导数为0的点，这个点可能是极大值也可能是极小值。&lt;/p>
&lt;h4 id="二点二次插值">二点二次插值&lt;/h4>
&lt;p>二点二次插值法（割线法, Secant Method）在第&lt;span class="math">\(\left(k+1 \right )\)&lt;/span>步构造二次插值多项式时使用 1 个函数和 2 个一阶导数信息 &lt;span class="math">\[q\left(x_{k-1} \right ) = f \left(x_{k-1} \right ),\ q&amp;#39;\left(x_{k-1} \right ) = f&amp;#39; \left(x_{k-1} \right ),\ q&amp;#39;\left(x_k \right ) = f&amp;#39; \left(x_k \right )\]&lt;/span> 由此导出二点二次插值法（割线法）的迭代公式为： &lt;span class="math">\[x_{k+1} = x_k - \frac{f&amp;#39; \left(x_k \right )}{\frac{f&amp;#39; \left(x_k \right ) - f&amp;#39; \left(x_{k-1} \right )}{x_k - x_{k-1}}} = x_k - \frac{x_k - x_{k-1}}{f&amp;#39;\left(x_k \right )-f&amp;#39;\left(x_{k-1} \right )}f&amp;#39;\left(x_k \right )\]&lt;/span> 而二点二次插值法（非割线法, Quadratic Interpolation Method with Two-Points）则使用 2 个函数和 1 个一阶导数信息 &lt;span class="math">\[q\left(x_{k-1} \right ) = f \left(x_{k-1} \right ),\ q&amp;#39;\left(x_{k-1} \right ) = f&amp;#39; \left(x_{k-1} \right ),\ q\left(x_k \right ) = f \left(x_k \right )\]&lt;/span> 由此导出二点二次插值法（非割线法）的迭代公式为： &lt;span class="math">\[x_{k+1} = x_k + \frac{1}{2}\frac{x_k - x_{k-1}}{\frac{f \left(x_k \right ) - f \left(x_{k-1} \right )}{f&amp;#39;\left(x_k \right )\left(x_k - x_{k-1} \right )} - 1} = x_k - \frac{1}{2}\frac{x_k-x_{k-1}}{f&amp;#39;\left(x_k \right )-\frac{f\left(x_k \right )-f\left(x_{k-1} \right )}{x_k - x_{k-1}}}f&amp;#39;\left(x_k \right )\]&lt;/span>&lt;/p>
&lt;p>二点二次插值法具有1.618斐波那契的超线性收敛速度。&lt;/p>
&lt;h4 id="三点二次插值">三点二次插值&lt;/h4>
&lt;h3 id="三次插值法">三次插值法&lt;/h3>
&lt;p>类似地，三次插值法就是不断构造三次插值多项式&lt;span class="math">\(p \left(x \right ) = ax^3 + bx^2 + cx + d\)&lt;/span>来近似目标函数 &lt;span class="math">\(f \left(x \right )\)&lt;/span> 并将最后构造的三次插值多项式的极小值点作为目标函数极小值点的方法。&lt;/p>
&lt;h4 id="二点三次插值法cubic-interpolation-method-with-two-points">二点三次插值法（Cubic Interpolation Method with Two-Points）&lt;/h4>
&lt;p>在第 &lt;span class="math">\(k\)&lt;/span> 步构造三次插值多项式时使用 2 个函数和 2 个一阶导数信息 &lt;span class="math">\[p \left(a_k \right ) = f \left(a_k \right ), p&amp;#39;\left(a_k \right ) = f&amp;#39;\left(a_k \right ), p \left(b_k \right ) = f \left(b_k \right ), p&amp;#39;\left(b_k \right ) = f&amp;#39;\left(b_k \right )\]&lt;/span> &lt;span class="math">\(a_k &amp;lt; b_k\)&lt;/span> 的值根据 &lt;span class="math">\(f&amp;#39; \left(x_k \right )\)&lt;/span> 与 &lt;span class="math">\(0\)&lt;/span> 的大小关系，从 &lt;span class="math">\(a_{k-1},\ b_{k-1},\ x_k\)&lt;/span> 中选取。由上述信息得到的第 &lt;span class="math">\(\left(k+1 \right )\)&lt;/span> 步迭代中构造的三次插值多项式的极小值点公式为： &lt;span class="math">\[\begin{aligned} &amp;amp;x_{k+1} = a_k + \frac{\eta_k-f&amp;#39;\left(a_k \right )-\omega_k}{2\eta_k-f&amp;#39;\left(a_k \right )+f&amp;#39;\left(b_k \right )}\left(b_k-a_k \right ) \\ &amp;amp;\eta_k = \sqrt{\omega_k^2-f&amp;#39;\left(a_k \right )f&amp;#39;\left(b_k \right )} \\ &amp;amp;\omega_k = \frac{3\left[f\left(b_k \right )-f\left(a_k \right ) \right ]}{b_k-a_k}-f&amp;#39;\left(a_k \right )f&amp;#39;\left(b_k \right ) \end{aligned}\]&lt;/span>&lt;/p>
&lt;p>二点三次插值法的收敛速度为2阶。&lt;/p>
&lt;h2 id="综合方法">综合方法&lt;/h2>
&lt;h3 id="d.s.c.法">D.S.C.法&lt;/h3></description></item><item><title>优化理论之一维精确优化（上）</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96%E4%B8%8A/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96%E4%B8%8A/</guid><description>
&lt;h2 id="一维精确优化">一维精确优化&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#一维优化方法">一维优化方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#搜索法">搜索法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二分搜索">二分搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#黄金分割搜索">黄金分割搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#斐波那契搜索">斐波那契搜索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#近似法">近似法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#二次插值法">二次插值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#三次插值法">三次插值法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#综合方法">综合方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dsc法">D.S.C.法&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="一维优化方法">一维优化方法&lt;/h2>
&lt;p>三种常见的非线性优化问题为：&lt;/p>
&lt;ul>
&lt;li>一维无约束问题&lt;/li>
&lt;li>多维无约束问题&lt;/li>
&lt;li>多维约束问题&lt;/li>
&lt;/ul>
&lt;p>第一种问题是最容易求解的，而第三种是最困难的。在实际应用中，我们通常将多维约束问题简化为多维无约束问题，进而简化为一维无约束问题。事实上，大部分非线性规划问题是基于单变量函数的最小化，并且是无约束的，因此如果我们想构造出有效的多维无约束或约束算法，就需要有效的一维优化算法。&lt;/p>
&lt;p>如果不考虑导数，一维优化方法是&lt;strong>搜索法与近似法&lt;/strong>。&lt;/p>
&lt;p>在&lt;strong>搜索法&lt;/strong>中，首先构建包含&lt;span class="math">\(x^\ast\)&lt;/span>的区间&lt;span class="math">\([x_L,x_U]\)&lt;/span>，然后根据函数不断的减小区间，直到&lt;span class="math">\([x_{L,k},x_{U,k}]\)&lt;/span>充分小，区间&lt;span class="math">\([x_{L,k},x_{U,k}]\)&lt;/span>的中点做为最小值。这种方法可以用于任何函数，函数不一定要可导。&lt;/p>
&lt;p>在&lt;strong>近似法&lt;/strong>中，函数用低阶的多项式来近似（泰勒展开），通常选择二阶或三阶，然后用初等微积分分析，推断出&lt;span class="math">\(x^∗\)&lt;/span>的近似值，这样就减小了区间，然后重复这个过程直到&lt;span class="math">\(x^∗\)&lt;/span>值充分精确。这种方法要求函数&lt;span class="math">\(f(x)\)&lt;/span>是连续可导的。&lt;/p>
&lt;p>接下来会介绍一些一维优化方法，如下所示：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>二分搜索&lt;/li>
&lt;li>斐波那契搜索&lt;/li>
&lt;li>黄金分割搜索&lt;/li>
&lt;li>二次插值法&lt;/li>
&lt;li>三次插值法&lt;/li>
&lt;li>D.S.C. 法&lt;/li>
&lt;/ol>
&lt;p>前三种是搜索法，第四与第五种是近似法，第六种是一种实用的方法，它结合了搜索法与近似法。&lt;/p>
&lt;h2 id="搜索法">搜索法&lt;/h2>
&lt;p>考虑一个&lt;strong>单峰&lt;/strong>函数，在区间&lt;span class="math">\([x_L,x_U]\)&lt;/span>内有最小值，这个区间称为不确定范围，通过不断缩小这个不确定范围可以得出&lt;span class="math">\(f(x)\)&lt;/span>的最小值&lt;span class="math">\(x^∗\)&lt;/span>。在搜索方法中，使用&lt;span class="math">\(f(x)\)&lt;/span>在合适点处的值就能确定出来。讨论的函数形如图1.&lt;/p>
&lt;img src="../images/convex_func.gif" alt="函数图像" />
&lt;center>
图1 函数图像
&lt;/center>
&lt;h3 id="二分搜索">二分搜索&lt;/h3>
&lt;p>如果&lt;span class="math">\(f(x)\)&lt;/span>在点&lt;span class="math">\(x_a\)&lt;/span>处的值是已知的，其中&lt;span class="math">\(x_L&amp;lt;x_a&amp;lt;x_U\)&lt;/span>，那么点&lt;span class="math">\(x^∗\)&lt;/span>可能在&lt;span class="math">\(x_L\)&lt;/span>与&lt;span class="math">\(x_a\)&lt;/span>之间，或者&lt;span class="math">\(x_a\)&lt;/span>与&lt;span class="math">\(x_U\)&lt;/span>之间，如图2所示，因此获得信息不足以进一步缩小不确定范围。然而，如果我们知道&lt;span class="math">\(f(x)\)&lt;/span>在两个点&lt;span class="math">\(x_a,x_b\)&lt;/span> 处的值，那就可以缩小了，这时候会有三种情况：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(f(x_a)&amp;lt;f(x_b)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x_a)&amp;gt;f(x_b)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x_a)=f(x_ b)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>对于第一种情况，&lt;span class="math">\(x^∗\)&lt;/span>的范围可能是&lt;span class="math">\(x_L&amp;lt;x^∗&amp;lt;x_a\)&lt;/span>或者&lt;span class="math">\(x_a&amp;lt;x^∗&amp;lt;x_b\)&lt;/span>，即&lt;span class="math">\(x_L&amp;lt;x^∗&amp;lt;x_b\)&lt;/span>，如图2所示。&lt;span class="math">\(x_b&amp;lt;x^∗&amp;lt;x_U\)&lt;/span>的情况被排除了，否则的话&lt;span class="math">\(f(x)\)&lt;/span>会有两个极小值：一个在&lt;span class="math">\(x_b\)&lt;/span>的左边，一个在&lt;span class="math">\(x_b\)&lt;/span>的右边。同样的，对于第二种情况，我们肯定有&lt;span class="math">\(x_a&amp;lt;x^∗&amp;lt;x_U\)&lt;/span>，如图3所示。对于第三种情况，我们有&lt;span class="math">\(x_a&amp;lt;x^∗&amp;lt;x_b\)&lt;/span>，即不等式&lt;span class="math">\(x_L&amp;lt;x^∗&amp;lt;x_b\)&lt;/span>与&lt;span class="math">\(x_a&amp;lt;x^∗&amp;lt;x_U\)&lt;/span>都满足，如图4所示。&lt;/p>
&lt;img src="../images/binary_search1.png" alt="binary_search_1" />
&lt;center>
图2 二分搜索情况1
&lt;/center>
&lt;img src="../images/binary_search2.png" alt="binary_search_2" />
&lt;center>
图3 二分搜索情况2
&lt;/center>
&lt;img src="../images/binary_search3.png" alt="binary_search_3" />
&lt;center>
图4 二分搜索情况3
&lt;/center>
&lt;p>知道了这三种关系之后，我们需要逐步缩小&lt;span class="math">\(x_a,x_b\)&lt;/span>的范围。如果缩小的尺度没有掌握好，会使得最优值&lt;span class="math">\(x^\ast\)&lt;/span>被排除到&lt;span class="math">\(x_a,x_b\)&lt;/span>之外。一种缩小不确定范围的基本策略是二分搜索。对于这个方法，我们取初始值&lt;span class="math">\(x_1\)&lt;/span>为&lt;span class="math">\(x_L,x_U\)&lt;/span>的中点，即&lt;span class="math">\(x_1=\frac{x_L+x_U}{2}\)&lt;/span>，并计算&lt;span class="math">\(f(x)\)&lt;/span> 在两点&lt;span class="math">\(x_a=x_1−ε/2\)&lt;/span>与&lt;span class="math">\(x_b=x1+ε/2\)&lt;/span>的值，其中ε是很小的正数，然后根据&lt;span class="math">\(f(x_a)&amp;lt;f(x_b)\)&lt;/span>还是&lt;span class="math">\(f(x_a)&amp;gt;f(x_b)\)&lt;/span>，判断范围是&lt;span class="math">\(x_L\)&lt;/span>到&lt;span class="math">\(x_1+ε/2\)&lt;/span>还是&lt;span class="math">\(x_1−ε/2\)&lt;/span>到&lt;span class="math">\(x_U\)&lt;/span>，如果&lt;span class="math">\(f(x_a)=f(x_b)\)&lt;/span>，那么两者都可以。这样范围立刻可以缩小到&lt;span class="math">\(x_1\pm ε/2\)&lt;/span>附近，即&lt;span class="math">\(x_L,x_U\)&lt;/span>的中点附近，探索范围缩小一半，不断重复这个过程直到满足精度要求。例如，如果二分查找应用到图5所示的函数上，那么不确定范围在四次迭代后从&lt;span class="math">\(0&amp;lt;x^∗&amp;lt;1\)&lt;/span>减小到&lt;span class="math">\(9/16+ε/2&amp;lt;x^∗&amp;lt;5/8−ε/2\)&lt;/span>(误差范围&lt;span class="math">\(ε\)&lt;/span>)。&lt;/p>
&lt;img src="../images/binary_search_steps.png" alt="binary_search_steps" />
&lt;center>
图4 二分搜索逐步缩小范围
&lt;/center>
&lt;h3 id="黄金分割搜索">黄金分割搜索&lt;/h3>
&lt;p>我们在使用二分法是，每次切割区间需要计算两次函数值（&lt;span class="math">\(f(x-ε/2),f(x+ε/2)\)&lt;/span>）。其中一个值是用来舍弃的。在单峰函数求极值的过程中，始终需要两个试探值，这都是拜这个原则所赐，再来复习下：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\(x^\ast,x_a,x_b \in D=[x_L,x_U]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x_a) \le f(x_b),x^\ast\in[x_L,x_b]\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(f(x_a) \ge f(x_b),x^\ast\in[x_a,x_U]\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>如果我们能够合理利用试探点信息，就可以每个迭代周期只用计算一次函数值。我们做出如下两个简单要求：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>两个试探点&lt;span class="math">\(\lambda_k,\mu_k\)&lt;/span>距离边界&lt;span class="math">\([a_k,b_k]\)&lt;/span>距离相等（&lt;span class="math">\(a_0=x_L,b_0=x_U\)&lt;/span>）。&lt;span class="math">\(\lambda_k-a_k=b_k-\mu_k\)&lt;/span>&lt;/li>
&lt;li>每次迭代，搜索空间缩小的比例为常数&lt;span class="math">\(\tau\)&lt;/span>。&lt;span class="math">\(b_{k+1}-a_{k+1}=\tau*(b_k-a_k)\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>我们假设试探点&lt;span class="math">\(f(\lambda_k) \le f(\mu_k)\)&lt;/span>（在另一种场景&lt;span class="math">\(f(\lambda_k) \ge f(\mu_k)\)&lt;/span>可以得到一样的结果），则&lt;span class="math">\(a_{k+1}=a_k,b_{k+1}=\mu_k\)&lt;/span>。&lt;/p>
&lt;p>根据要求2，我们带入&lt;span class="math">\(a_{k+1}=a_k,b_{k+1}=\mu_k\)&lt;/span>，可得： &lt;span class="math">\[b_{k+1}-a_{k+1}=\mu_k-a_k=\tau(b_k-a_k)\\
\Rightarrow \mu_k=a_k+\tau(b_k-a_k) \tag{2.1}\]&lt;/span> 将&lt;span class="math">\((2.1)\)&lt;/span>代入要求1可得： &lt;span class="math">\[\lambda_k-a_k=b_k-\mu_k=b_k-(a_k+\tau(b_k-a_k))\\
\Rightarrow \lambda_k=a_k+(1-\tau)(b_k-a_k)\tag{2.2}\]&lt;/span> 根据&lt;span class="math">\((2.1)\)&lt;/span>和&lt;span class="math">\(a_{k+1}=a_k,b_{k+1}=\mu_k\)&lt;/span>，进一步推导&lt;span class="math">\(u_{k+1}\)&lt;/span>： &lt;span class="math">\[\begin{aligned}
u_{k+1}&amp;amp;=a_{k+1}+\tau(b_{k+1}-a_{k+1 })\\
&amp;amp;=a_k+\tau(u_k-a_k)\\
&amp;amp;=a_k+\tau(a_k+\tau(b_k-a_k)-a_k)\\
&amp;amp;=a_k+\tau^2(b_k-a_k) \tag{2.3}
\end{aligned}\]&lt;/span> 由于搜索模式限制，我们必须通过计算两个函数点，才能判断出区间。如果能够合理设置数值，我们可以交替的使用上次没用到的试探点作为下一次迭代的试探点。&lt;/p>
&lt;p>在情形&lt;span class="math">\(f(\lambda_k) \le f(\mu_k)\)&lt;/span>中，&lt;span class="math">\(b_{k+1}=\mu_k，\lambda_k\)&lt;/span>没有用到，因此我们依旧把&lt;span class="math">\(\lambda_k\)&lt;/span>作为k+1次迭代的试探点。令： &lt;span class="math">\[\begin{aligned}
\mu_{k+1}&amp;amp;=\lambda_k\\
a_k+\tau^2(b_k-a_k)&amp;amp;=a_k+(1-\tau)(b_k-a_k)\\
\tau^2&amp;amp;=1-\tau(\tau&amp;gt;0)\\
\tau&amp;amp;=\frac{\sqrt{5}-1}{2}\approx0.618
\end{aligned}\]&lt;/span> 也就是说，当&lt;span class="math">\(\tau=\frac{\sqrt{5}-1}{2}\approx0.618\)&lt;/span>的时候，&lt;span class="math">\(\mu_{k+1}=\lambda_k,f(\mu_{k+1})=f(\lambda_k)\)&lt;/span>，每次迭代只需要引入一个新值，计算一次函数值。&lt;/p>
&lt;p>同理，在情形&lt;span class="math">\(f(\lambda_k) \ge f(\mu_k)\)&lt;/span>中，&lt;span class="math">\(a_{k+1}=\lambda_k,b_{k+1}=b_k,\lambda_{k+1}=\mu_k,f(\lambda_{k+1})=f(\mu_k)\)&lt;/span>。&lt;/p>
&lt;p>黄金分割法的搜索效率不如二分法，缩短率&lt;span class="math">\(\tau=0.618\)&lt;/span>，属于线性收敛。但是除了第一次迭代，每次只需要在计算一次函数值，这里效率高一些。&lt;/p>
&lt;h3 id="斐波那契搜索">斐波那契搜索&lt;/h3>
&lt;p>对于斐波那契搜索，我们把黄金分割搜索的条件放宽，只要求1个：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>考虑第k次迭代的搜索空间： 两个试探点&lt;span class="math">\(\lambda_k,\mu_k\)&lt;/span>距离边界&lt;span class="math">\([a_k,b_k]\)&lt;/span>距离相等（&lt;span class="math">\(a_0=x_L,b_0=x_U\)&lt;/span>）。&lt;span class="math">\(\lambda_k-a_k=b_k-\mu_k\)&lt;/span>。&lt;/li>
&lt;/ol>
&lt;p>令第k次迭代的搜索空间为&lt;span class="math">\(I_k=[a_k,b_k]\)&lt;/span>。同样根据试探节点的函数值大小，我们有，如果&lt;span class="math">\(f(\lambda_k)&amp;lt;f(\mu_k)\)&lt;/span>，则选择左区间： &lt;span class="math">\[I_{k+1}^L=[a_k,\mu_k]\]&lt;/span> 如果&lt;span class="math">\(f(\lambda_k)&amp;gt;f(\mu_k)\)&lt;/span>，则选择右区间： &lt;span class="math">\[I_{k+1}^R=[\lambda_k,b_k]\]&lt;/span>&lt;/p>
如果右区间&lt;span class="math">\(I_{k+1}^R\)&lt;/span>被选中，那么它将包含最小值，另外还知道点&lt;span class="math">\(\mu_k \rightarrow \lambda_{k+1}\)&lt;/span>处的函数值。如果我们知道点&lt;span class="math">\(\mu_{k+1}\)&lt;/span>处的值，那么我们就有充分的信息来进一步减小不确定区域，然后不断重复此过程。这种方法每次迭代&lt;strong>只需要估计一个函数值&lt;/strong>，计算量相对于二分搜索要小。如图5所示。 &lt;img src="../images/斐波那契搜索区间.png" alt="斐波那契搜索空间" />
&lt;center>
斐波那契搜索空间
&lt;/center>
&lt;p>由图5可知 &lt;span class="math">\[I_k=I_{k+1}^L+I_{k+2}^R\]&lt;/span> 根据要求1，我们知道左右两个区间大小相等。那么&lt;span class="math">\(I_{k+1}^L=I_{k+1}^R=I_{k+1},I_{k+2}^L=I_{k+2}^R=I_{k+2}\)&lt;/span>。由此可得递推公式： &lt;span class="math">\[I_k=I_{k+1}+I_{k+2}\]&lt;/span> 如果上面的过程重复多次，那么我们会得到如下的区间序列&lt;span class="math">\({I_1,I_2,…,I_n}\)&lt;/span>: &lt;span class="math">\[I_1=I_2+I_3\\
I_2=I_3+I_4\\
\dotsb\\
I_n=I_{n+1}+I_{n+2}\\
\]&lt;/span> 假设&lt;span class="math">\(n+2\)&lt;/span>次迭代后区间消失，即&lt;span class="math">\(I_{n+2}=0\)&lt;/span>，那么我们就得到斐波那契序列，如果我们令&lt;span class="math">\(k=n\)&lt;/span>，可以得到: &lt;span class="math">\[\begin{aligned}
I_{n+1}&amp;amp;=I_n-I_{n+2}=I_n=F_0 I_n\\
I_{n}&amp;amp;=I_{n+1}+I_{n+2}=I_n=F_1 I_n\\
I_{n-1}&amp;amp;=I_{n}+I_{n+1}=2I_n=F_2 I_n\\
I_{n-2}&amp;amp;=I_{n-1}+I_{n}=3I_n=F_3 I_n\\
I_{n-3}&amp;amp;=I_{n-2}+I_{n-1}=5I_n=F_4 I_n\\
\dotsb\\
I_{1}&amp;amp;=I_2+I_3=F_n I_n\\
\end{aligned}\]&lt;/span> 所生成的序列&lt;span class="math">\(\{1,1,2,3,5,8,13,…\}=\{F_0,F_1,F_2,F_3,F_4,F_5,F_6,…\}\)&lt;/span>&lt;/p>
&lt;p>如果迭代的总数为&lt;span class="math">\(n\)&lt;/span>，那么斐波那契搜索将不确定区间缩小到 &lt;span class="math">\[I_n=\frac{I_1}{F_n}\]&lt;/span> 例如如果&lt;span class="math">\(n=11\)&lt;/span>，那么&lt;span class="math">\(F_n=144\)&lt;/span>，这样的话&lt;span class="math">\(I_n\)&lt;/span>将缩小到不足&lt;span class="math">\(I_1\)&lt;/span>的1%，其中会有11次迭代。因为每次迭代只需要一个函数估计值，所以一共需要11个函数估计值，如果二分搜索要达到同样的精度需要14个函数估计值，故斐波那契搜索比二分搜索效率更高。事实上，相对于其他几个搜索方法，从计算效率上看它是最高效的。&lt;/p>
&lt;p>斐波那契算法中试探点为： &lt;span class="math">\[\lambda_k=a_k+(1-\frac{F_{n-k}}{F_{n-k+1}})(b_k-a_k)\\
\mu_k=a_k+\frac{F_{n-k}}{F_{n-k+1}}(b_k-a_k)\\\]&lt;/span> 不难看出&lt;span class="math">\(\frac{F_{n-k}}{F_{n-k+1}}\)&lt;/span>相当于黄金分割算法中&lt;span class="math">\(\tau\)&lt;/span>，为缩短率。缩短率一开始为0.5，后来逐渐趋近于0.618。需要注意的时，斐波那契搜索需要提前知道迭代的次数。如果最后要求的区间不大于&lt;span class="math">\(\sigma\)&lt;/span>，则 &lt;span class="math">\[F_n\ge \frac{b_1-a_1}{\sigma}\\
F_n=\frac{1}{\sqrt{5}}[(\frac{1+\sqrt{5}}{2})^{n+1}-(\frac{1-\sqrt{5}}{2})^{n+1}]\]&lt;/span> 可推得n。可以证明斐波那契搜索时分割方法求一维最优化问题的&lt;strong>最优策略&lt;/strong>。&lt;/p>
&lt;h2 id="近似法">近似法&lt;/h2>
&lt;h3 id="二次插值法">二次插值法&lt;/h3>
&lt;h3 id="三次插值法">三次插值法&lt;/h3>
&lt;h2 id="综合方法">综合方法&lt;/h2>
&lt;h3 id="d.s.c.法">D.S.C.法&lt;/h3></description></item><item><title>优化理论之一维不精确优化</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%B8%8D%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96/</link><pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%B8%8D%E7%B2%BE%E7%A1%AE%E4%BC%98%E5%8C%96/</guid><description>
&lt;h2 id="一维不精确优化">一维不精确优化&lt;!-- omit in toc -->&lt;/h2>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;p>精确一维搜索方法往往需要花费很大的时间，特别当迭代点远离问题的解时，精确求解通常不十分有效。另外，实际上很多最优化方法，如牛顿法和拟牛顿法，其收敛速度并不依赖于精确一维搜索过程。因此，只要保证目标函数&lt;span class="math">\(f(x)\)&lt;/span>在每一次迭代有满意的下降，可大大节省计算量。&lt;/p>
&lt;p>我们在这里再次说明，不做特殊说明下，我们的目标是求函数的极小值。&lt;/p>
&lt;p>&lt;span class="math">\[\min_{x ∈ D} f(x)\\
D为n维约束集\]&lt;/span>&lt;/p>
&lt;p>不精确搜索一般而言主要有两种准则&lt;strong>Armijo-Goldstein 准则、Wolfe-Powell 准则&lt;/strong>，还有基于两大准则的推广，例如强 Wolfe-Powell 准则和简单准则。它们的出发点都是保证搜索步长不太大或不太小。不精确一维搜索方法总体希望收敛快，每一步不要求达到精确最小，速度快，虽然步数增加，则整个收敛过程更快速。&lt;/p>
&lt;p>为什么要遵循这些准则？&lt;/p>
&lt;p>由于采用了不精确的一维搜索，所以，为了能让算法收敛（即：求得极小值），人们逐渐发现、证明了一些规律，当你遵循这些规律的时候，算法就&lt;strong>很有可能&lt;/strong>收敛。因此，为了达到&lt;strong>让算法收敛&lt;/strong>的目的，我们就要遵循这些准则。如果你不愿意遵循这些已经公认有效的准则，而是要按自己的准则来设计算法，那么恭喜你，如果你能证明你的做法是有效的，未来若干年后，书本里可能也会出现你的名字。&lt;/p>
&lt;h2 id="不精确步长搜索基础算法">不精确步长搜索基础算法&lt;/h2>
&lt;h3 id="试探法backtracking-approach">试探法（Backtracking Approach）&lt;/h3>
&lt;p>如果备选的步长合适，那么通过试探法，我们可以只用充分下降条件来做终止条件，同时选出一个不错的步长。（因为这时候 α 是从大往小变化，所以不可能变得太小）。其算法的具体步骤如下：&lt;/p>
&lt;blockquote>
&lt;p>初始化：一般选择&lt;span class="math">\(\gamma∈[0.5,0.8] ,c∈(0,1)\)&lt;/span>&lt;br />依据以下步骤从&lt;span class="math">\(\vec x_{old}\)&lt;/span>更新到&lt;span class="math">\(\vec x_{new}\)&lt;/span>:&lt;br />Step1：计算后向跟踪步长 &lt;span class="math">\[\begin{aligned}\alpha^\ast&amp;amp;:= \max \gamma^v\\
s.t. \quad &amp;amp; v ∈\{0,1,2,\dots\}\\
&amp;amp;f(\vec x_{old}+\gamma^v\vec d)≤f(x_{old})+c\gamma^v f&amp;#39;(\vec x_{old},\vec d)\end{aligned}\]&lt;/span> 其中&lt;span class="math">\(\gamma^v\)&lt;/span>是几何序列（等比序列）。&lt;br />Step2： &lt;span class="math">\(\vec x_{new}=\vec x_{old}+\alpha^\ast\vec d\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>试探法是大多数线性搜索方法的基础。基于这个算法，我们来详细谈谈它的拓展。&lt;/p>
&lt;h2 id="充分下降准则简单准则">充分下降准则(简单准则)&lt;/h2>
&lt;p>我们要求目标函数的最小值，最重要的就是要让函数有&lt;strong>足够&lt;/strong>的下降。足够这个词，是希望一种发散的下降，而不是会收敛到某一点的下降，举个反例：&lt;/p>
&lt;p>&lt;span class="math">\[\min f(x)=(x+1)^2\\
x(k)=5/k,k=1,2,3,\dotsb\]&lt;/span> 这个&lt;span class="math">\(x\)&lt;/span>序列确实一直让函数值在下降，但是不够充分，每次下降的越来越小，直到收敛到点&lt;span class="math">\((0,1)\)&lt;/span>。无穷的收敛性有时候很好，也有时候令人恼怒。因此，我们需要指定一个充分下降条件。当迭代到第&lt;span class="math">\(k\)&lt;/span>次时：&lt;/p>
&lt;p>&lt;span class="math">\[f(\vec x_k+\alpha\vec d_k)≤f(\vec x_k)+\alpha c\nabla f(\vec x_k)\vec d_k\quad PS:f&amp;#39;(\vec x_k;\vec d_k)=\nabla f(\vec x_k)\vec d_k\]&lt;/span>&lt;/p>
&lt;p>这个不等式如何保证呢？首先我们需要找到一个严格下降方向&lt;span class="math">\(\vec d\)&lt;/span>，其方向导数是负数，即 &lt;span class="math">\[\lim_{\alpha→0}\frac{f(\vec x_{old}+\alpha\vec d)-f({x_{old})}}{\alpha}=f&amp;#39;(x_{old};\vec d)&amp;lt;0\quad \alpha\in(0,\bar \alpha)\]&lt;/span> 取一个&lt;span class="math">\(c ∈(0,1)\)&lt;/span>，那么 &lt;span class="math">\[\lim_{\alpha→0}\frac{f(\vec x_{old}+\alpha\vec d)-f({x_{old})}}{\alpha}=f&amp;#39;(x_{old};\vec d)&amp;lt;cf&amp;#39;(x_{old};\vec d)&amp;lt;0\\
\Rightarrow f(\vec x_{old}+\alpha\vec d)≤f(x_{old})+\alpha cf&amp;#39;(x_{old};\vec d)\]&lt;/span> 一般而言，&lt;span class="math">\(c\)&lt;/span>是一个小量&lt;span class="math">\([0.0001,0.1]\)&lt;/span>，其作用是放缓曲线，下图举了个例子&lt;/p>
&lt;div class="figure">
&lt;img src="../images/充分下降准则.png" alt="充分下降准则.png" />&lt;p class="caption">充分下降准则.png&lt;/p>
&lt;/div>
&lt;p>如果直接使用&lt;span class="math">\(\nabla f(x)^Td\)&lt;/span>则有可能导致没有可行的步长，因此使用一个平缓参数&lt;span class="math">\(c\)&lt;/span>。&lt;span class="math">\(c\)&lt;/span>一般是一个小数，把直线图像抬起来，有的时候&lt;span class="math">\(c=0.0001\)&lt;/span>，那么&lt;span class="math">\(f(x_{old})+\alpha cf&amp;#39;(x_{old};\vec d)\)&lt;/span>基本差不多是一条水平线了。但是&lt;span class="math">\(c\)&lt;/span>是一个定值，保证了下降过程不会收敛。&lt;/p>
&lt;p>此外，如果只满足充分下降准则，也可以称之满足于简单准则。&lt;/p>
&lt;h2 id="wolfe-powell准则">Wolfe-Powell准则&lt;/h2>
&lt;p>仅靠充分下降条件并不能保证算法每一步都能有很好的进步（make reasonable progress）。事实上，我们可以发现，当&lt;span class="math">\(α\)&lt;/span>充分小时，充分下降条件总是满足的。所以，算法可能会常常产生令人无法接受的小步长。为了避免这种情况，我们引入另一个条件，称为曲率条件，即需要&lt;span class="math">\(α_k\)&lt;/span>满足 &lt;span class="math">\[\nabla f(x_k+\alpha_k d_k)^T p_k\ge c_2\nabla f_k^T d_k\]&lt;/span> 其中&lt;span class="math">\(c_2∈(0,1)\)&lt;/span>为一常数。&lt;/p>
&lt;p>因此， Wolfe-Powell准则归结为&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>充分下降准则&lt;/li>
&lt;li>曲率条件&lt;/li>
&lt;/ol>
&lt;div class="figure">
&lt;img src="../images/Wolfe条件.png" alt="Wolfe条件" />&lt;p class="caption">Wolfe条件&lt;/p>
&lt;/div>
&lt;h2 id="armijo-goldstein准则">Armijo-Goldstein准则&lt;/h2>
&lt;p>Armijo-Goldstein准则的核心内容有两条：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>充分下降准则&lt;/li>
&lt;li>搜索的步长不应该太小&lt;/li>
&lt;/ol>
&lt;p>&lt;span class="math">\[1:f(\vec x_k+\alpha\vec d_k)≤f(\vec x_k)+\alpha c\nabla f(\vec x_k)\vec d_k\]&lt;/span> &lt;span class="math">\[2:f(\vec x_k+\alpha\vec d_k)≥f(\vec x_k)+\alpha (1-c)\nabla f(\vec x_k)\vec d_k\]&lt;/span> 其中，&lt;span class="math">\(c ∈ (0,0.5)\)&lt;/span>&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Armijo-Goldstein.png" alt="Armijo-Goldstein" />&lt;p class="caption">Armijo-Goldstein&lt;/p>
&lt;/div>
&lt;h2 id="推广强wolfe-powell准则">推广：强Wolfe-Powell准则&lt;/h2>
&lt;p>强Wolfe条件对&lt;span class="math">\(α_k\)&lt;/span>的限制为 &lt;span class="math">\[\begin{array}{c}f(x_k+\alpha_k p_k)\le f(x_k)+c_1\alpha_k\nabla f_k^T p_k \\ |\nabla f(x_k+\alpha_k p_k)^T p_k|\le c_2|\nabla f_k^T p_k| \end{array}\]&lt;/span> 注意到，这个条件比Wolfe条件只多了绝对值符号。强Wolfe条件实际上限制了&lt;span class="math">\(ϕ′(α_k)\)&lt;/span>不能为一个很大的正数，即可行的&lt;span class="math">\(α\)&lt;/span>不能离最低点太远。&lt;/p>
&lt;p>这个笔记并不完整，一些内容可见&lt;http://keson96.github.io/2016/12/08/2016-12-08-Line-Search/>，我在这上面花了不少时间，但是不知道是否值得，现在先暂停下。&lt;/p></description></item><item><title>优化理论之一维优化综述</title><link>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96%E7%BB%BC%E8%BF%B0/</link><pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B9%8B%E4%B8%80%E7%BB%B4%E4%BC%98%E5%8C%96%E7%BB%BC%E8%BF%B0/</guid><description>
&lt;h2 id="一维优化综述">一维优化综述&lt;!-- omit in toc -->&lt;/h2>
&lt;p>摘自：&lt;a href="https://www.longzf.com/optimization/2/line_search/">https://www.longzf.com/optimization/2/line_search/&lt;/a>&lt;/p>
&lt;p>注1：本文讨论需要优化的函数都是单峰函数（或单谷函数）。&lt;/p>
&lt;p>一维优化技术又称为&lt;strong>线性搜索技术&lt;/strong>。因为一维空间直观的表现就是一条直线，而在这条线上找最优值，就是线性搜索。&lt;/p>
&lt;p>线搜索技术是多变量函数优化的基础，它包括&lt;strong>精确线搜索技术&lt;/strong>和&lt;strong>非精确线搜索技术&lt;/strong>。常见的精确线搜索技术可分为&lt;strong>分割方法和插值方法&lt;/strong>两大类：分割方法有二分法、黄金分割法、斐波那契法等；插值方法有一点二次插值法（牛顿法）、二点二次插值法（包括割线法）、三点二次插值法、二点三次插值法等。非精确线搜索技术基于&lt;strong>非精确线搜索准则&lt;/strong>，常用的准则有 Armijo-Goldstein 准则、Wolfe-Powell 准则、强 Wolfe-Powell 准则和简单准则。这里特别指出精确与非精确是指对&lt;strong>步长搜索的精确性&lt;/strong>。&lt;/p>
&lt;p>什么是线搜索呢？我们知道最优化算法的基本框架就是依据&lt;strong>迭代公式&lt;span class="math">\(\vec x_{k+1}=\vec x_k+\alpha_k\vec d_k\)&lt;/span>不断产生下一个迭代点&lt;/strong>，直到满足某些终止条件。迭代公式的确定涉及搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>和搜索步长&lt;span class="math">\(\alpha_k\)&lt;/span>的确定，其中&lt;span class="math">\(\alpha_k\)&lt;/span>的确定过程就是所谓线搜索的过程。&lt;/p>
&lt;p>精确线搜索的精确有两种理解。第一，对于分割法，精确搜索每次使得搜索空间有固定比例的缩小。例如&lt;span class="math">\(l_k\)&lt;/span>是第&lt;span class="math">\(k\)&lt;/span>次搜索空间大小，则第&lt;span class="math">\(k+1\)&lt;/span>次搜索空间就是&lt;span class="math">\(\alpha l_k(\alpha ∈ (0,1))\)&lt;/span>，第二，对于插值法，求使得近似（多项式）函数&lt;span class="math">\(p(\vec x_k+\alpha_k\vec d_k)\)&lt;/span>取到最小值时对应的&lt;span class="math">\(\alpha\)&lt;/span>，换句话说：精确线搜索求使得目标函数沿搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>下降最多的搜索步长。&lt;/p>
&lt;p>非精确线搜索则只求使得目标函数沿搜索方向&lt;span class="math">\(\vec d_k\)&lt;/span>有一定下降的搜索步长，这个步长甚至可能会导致偏离搜索目标（比如梯度下降法中学习率过大的情况）。&lt;/p>
&lt;p>需要指出的是，非精确搜索和精确搜索不是完全互斥的，一些精确搜索方法也满足非精确线搜索准则。&lt;/p>
&lt;p>实际计算中，&lt;strong>通常采用非精确线搜索技术&lt;/strong>，这是因为精确线搜索技术耗费大量的计算资源，而且对于多变量函数的优化，许多算法的收敛速度并不取决于是否采用精确线搜索技术。&lt;/p></description></item><item><title>强化学习-基础</title><link>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80/</link><pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80/</guid><description>
&lt;h2 id="强化基础">强化基础&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#分类">分类&lt;/a>&lt;/li>
&lt;li>&lt;a href="#强化学习基础模型马尔可夫决策">强化学习基础模型马尔可夫决策&lt;/a>&lt;/li>
&lt;li>&lt;a href="#状态价值函数与行为价值函数">状态价值函数与行为价值函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bellman方程递推公式">Bellman方程递推公式&lt;/a>&lt;/li>
&lt;li>&lt;a href="#不基于模型的预测vs">不基于模型的预测(&lt;span class="math">\(V(s)\)&lt;/span>)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#蒙特卡洛价值更新">蒙特卡洛价值更新&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时间差分价值更新">时间差分价值更新&lt;/a>&lt;/li>
&lt;li>&lt;a href="#不基于模型的控制qsa">不基于模型的控制(&lt;span class="math">\(Q(s,a)\)&lt;/span>)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#on-policy-vs-off-policy">On-policy VS Off-policy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#ε-greedy策略">ε-greedy策略&lt;/a>&lt;/li>
&lt;li>&lt;a href="#on-policy-蒙特卡洛mc控制">On-policy 蒙特卡洛（MC）控制&lt;/a>&lt;/li>
&lt;li>&lt;a href="#on-policy-td-control--sarsasarsalambda">On-policy TD control —— SARSA,SARSA(&lt;span class="math">\(\lambda\)&lt;/span>)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#td0更新的sarsa">TD(0)更新的SARSA&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tdlambda更新的sarsalambda">TD(&lt;span class="math">\(\lambda\)&lt;/span>)更新的SARSA(&lt;span class="math">\(\lambda\)&lt;/span>)&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#为什么off-policy-mc-control不好">为什么Off-policy MC control不好&lt;/a>&lt;/li>
&lt;li>&lt;a href="#off-policy-td-control--q-learning">Off-policy TD control —— Q-learning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#td与dp的关系">TD与DP的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#特点">特点&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="分类">分类&lt;/h2>
&lt;ul>
&lt;li>Valued-based / policy-based / A3C=A+C&lt;/li>
&lt;li>Model based / Model free&lt;/li>
&lt;li>回合更新（蒙特卡洛） / 时间差分（步更新）&lt;/li>
&lt;li>On-policy / off-policy&lt;/li>
&lt;li>稳态和非稳态&lt;/li>
&lt;li>全部可观测MDP / 部分可观测&lt;/li>
&lt;/ul>
&lt;p>DRL：本质把DNN作为一个actor。&lt;/p>
&lt;h2 id="强化学习基础模型马尔可夫决策">强化学习基础模型马尔可夫决策&lt;/h2>
&lt;p>MDP--&amp;gt;动态规划（策略迭代，值迭代）&lt;/p>
&lt;h3 id="状态价值函数与行为价值函数">状态价值函数与行为价值函数&lt;/h3>
&lt;p>在给定策略&lt;span class="math">\(\pi\)&lt;/span>下，状态转移概率与即时收获期望（这两项其实都是概率平均即期望）： &lt;span class="math">\[
P_{s,s&amp;#39;}^{\pi}=\sum_{a\in A}\pi(a|s)P_{s,s&amp;#39;}^a \tag{1}
\]&lt;/span> &lt;span class="math">\[
R_{s}^{\pi} = \sum_{a\in A}\pi(a|s)R_s^a\tag{2}
\]&lt;/span> 其中，&lt;span class="math">\(\pi(a|s)\)&lt;/span>表示给定策略&lt;span class="math">\(\pi\)&lt;/span>下，在状态&lt;span class="math">\(s\)&lt;/span>时，选用动作&lt;span class="math">\(a\)&lt;/span>的概率。&lt;span class="math">\(P_{s,s&amp;#39;}^a\)&lt;/span>表示在状态&lt;span class="math">\(s\)&lt;/span>时，选用动作&lt;span class="math">\(a\)&lt;/span>后，转移到目标状态&lt;span class="math">\(s&amp;#39;\)&lt;/span>的概率。&lt;/p>
&lt;blockquote>
&lt;p>状态价值函数：表示从状态&lt;span class="math">\(s\)&lt;/span>开始，遵循当前策略&lt;span class="math">\(pi\)&lt;/span>时所获得的收获的期望，即&lt;span class="math">\(v_\pi (s) = E[G_t | S_t = s]\)&lt;/span>&lt;/p>
&lt;p>行为价值函数(状态行为对价值函数)：遵循当前策略&lt;span class="math">\(pi\)&lt;/span>时，对当前状态&lt;span class="math">\(s\)&lt;/span>执行某一具体行为&lt;span class="math">\(a\)&lt;/span>，所能获得的收获的期望，即&lt;span class="math">\(q_\pi(s,a) = E[G_t | S_t = s, A_t = a]\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;h3 id="bellman方程递推公式">Bellman方程递推公式&lt;/h3>
&lt;p>我们根据定义可以推到出现在具体状态&lt;span class="math">\(s\)&lt;/span>与下一可能状态&lt;span class="math">\(S_{t+1}\)&lt;/span>的关系。注意&lt;span class="math">\(S_{t+1}\)&lt;/span>是一个随机变量，是&lt;span class="math">\(t+1\)&lt;/span>时刻状态的随机变量。 &lt;span class="math">\[
\begin{aligned}
v(s) &amp;amp;= E[G_t | S_t = s]\\
&amp;amp;=E[R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + \dotsb | S_t = s]\\
&amp;amp;=E[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \dotsb)| S_t = s]\\
&amp;amp;=E[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
&amp;amp;=E[R_{t+1} + \gamma v(S_{t+1}) | S_t =s]
\end{aligned}\tag{3}
\]&lt;/span> 其中，&lt;span class="math">\(E[v(S_{t+1})|S_t = s]\)&lt;/span>表示此时状态为s时，&lt;span class="math">\(t+1\)&lt;/span>时刻状态价值函数的期望。简而言之，我们有 &lt;span class="math">\[
v_\pi(s) = E[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]\tag{4}
\]&lt;/span> 同理可得： &lt;span class="math">\[
q_\pi(s,a) = E[R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1}) | S_t = s, A_t = a ]\tag{5}
\]&lt;/span> 由于行为是连接状态转换的桥梁，一个行为的价值与状态的价值关系紧密，我们可以用下式来表达 &lt;span class="math">\[
v_\pi(s)=\sum_{a\in A}\pi(a|s)q_\pi(s,a)\tag{6}
\]&lt;/span> 状态的价值是相关行为价值的概率加权平均。类似的，一个行为价值可以用该行为所能到达的后续状态的价值概率加权平均来表达： &lt;span class="math">\[
q_\pi(s,a) = \underbrace{R_s^a}_{即时回报}+\underbrace{\gamma\sum_{s&amp;#39;\in S}P_{s,s&amp;#39;}^a v_\pi(s&amp;#39;)}_{下一步状态价值的期望}\tag{7}
\]&lt;/span> 联立&lt;span class="math">\((6)(7)\)&lt;/span>两式，可以得到如下两递推公式： &lt;span class="math">\[
v_\pi(s)=\sum_{a\in A}\pi(a|s)\big(R_s^a+\gamma\sum_{s&amp;#39;\in S}P_{s,s&amp;#39;}^a v_\pi(s&amp;#39;)\big)\tag{8}
\]&lt;/span> &lt;span class="math">\[
q_\pi(s,a) = R_s^a+\gamma\sum_{s&amp;#39;\in S}P_{s,s&amp;#39;}^a \big(\sum_{a\in A}\pi(a&amp;#39;|s&amp;#39;)q_\pi(s&amp;#39;,a&amp;#39;)\big)\tag{9}
\]&lt;/span> 这两个地推公式的迭代（策略迭代，价值迭代）就可以得出收敛的价值函数。&lt;/p>
&lt;p>策略迭代：根据公式&lt;span class="math">\((8)\)&lt;/span>迭代更新状态价值，通过贪婪法迭代获得策略。最终能够收敛（压缩映射定理证明）&lt;/p>
&lt;p>值迭代：根据公式&lt;span class="math">\(v_*(s)=\max\limits_{a\in A}(R_s^a + \gamma \sum\limits_{s&amp;#39;\in S}P_{s,s&amp;#39;}^a v_*(s&amp;#39;))\)&lt;/span>，逐步迭代更新。&lt;/p>
&lt;h2 id="不基于模型的预测vs">不基于模型的预测(&lt;span class="math">\(V(s)\)&lt;/span>)&lt;/h2>
&lt;p>蒙特卡洛方法 MC；时间差分 TD；动态规划DP&lt;/p>
&lt;h3 id="蒙特卡洛价值更新">蒙特卡洛价值更新&lt;/h3>
&lt;p>&lt;strong>MC法更新&lt;/strong>：&lt;strong>基础是累进更新平均值&lt;/strong>。在这种方法中，我们只需要直到上次状态值和此次新加入的值。 &lt;span class="math">\[
\begin{aligned}
\mu_k &amp;amp;= \frac{1}{k}\sum_{j=1}^k x_j\\
&amp;amp;=\frac{1}{k}(x_k+\sum_{j=1}^{k-1} x_j)\\
&amp;amp;=\frac{1}{k}(x_k+(k-1)\mu_{k-1})\\
&amp;amp;=u_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})
\end{aligned}
\]&lt;/span> 具体到蒙特卡洛法更新状态价值就是： &lt;span class="math">\[
N(S_t)\leftarrow N(S_t)+1\\
V(S_t)\leftarrow \underbrace{V(S_t)}_{\mu_{k-1}}+\frac{1}{\underbrace{N(S_t)}_{k}}(\underbrace{G_t}_{x_k}-\underbrace{V(S_t)}_{\mu_{k-1}})\tag{10}
\]&lt;/span> 在一些&lt;strong>实时或者无法准确统计状态被访问次数时&lt;/strong>，可以用一个系数&lt;span class="math">\(\alpha\)&lt;/span>来代替状态计数的倒数&lt;span class="math">\(\frac{1}{N(S_t)}\)&lt;/span>，此时公式&lt;span class="math">\((10)\)&lt;/span>可以简化为： &lt;span class="math">\[
\color{red}{\star V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))}\tag{11}
\]&lt;/span> 公式&lt;span class="math">\((11)\)&lt;/span>&lt;strong>是以后所有更新公式的一个基础&lt;/strong>。&lt;/p>
&lt;h3 id="时间差分价值更新">时间差分价值更新&lt;/h3>
&lt;p>&lt;strong>TD法更新&lt;/strong>：MC法更新中，&lt;span class="math">\(G_t=R_{t+1}+\gamma R_{t+2} + \dotsb + \gamma^{T-1} R_{t+T}\)&lt;/span>是一个完整的episode。而TD方法是用不完整的轨迹序列完成更新，是使用bootstrapping的方法。 &lt;span class="math">\[
V(S_t) \leftarrow V(S_t) + \alpha(\underbrace{\underbrace{R_{t+1}+\gamma V(S_{t+1}}_{TD目标值\rightarrow G_t})-V(S_t)}_{TD误差})\tag{12}
\]&lt;/span> 其中&lt;span class="math">\(G_t\approx R_{t+1}+\gamma V(S_{t+1})\)&lt;/span>，如果&lt;span class="math">\(V(S_{t+1})\)&lt;/span>是真实值，那么&lt;span class="math">\(G_t\)&lt;/span>就是无偏估计，但实际上&lt;span class="math">\(V(S_{t+1})\)&lt;/span>是采样累进更新来的，所以TD法的更新可能会有偏差。&lt;/p>
&lt;p>&lt;strong>TD-n更新&lt;/strong>：TD法本质上用了下一步的即时收获和状态价值。我们可以使用n步来预测。 &lt;span class="math">\[
G_t^{(n)}= R_{t+1}+\gamma R_{t+2} + \dotsb + \gamma^{n-1} R_{t+n}+\gamma^n V(S_{t+n})\\
V(S_t) \leftarrow V(S_t) + \alpha(G_t^{(n)}-V(S_t))\tag{13}
\]&lt;/span> 当&lt;span class="math">\(n=1\)&lt;/span>时，即为TD法更新，当&lt;span class="math">\(n=T or\infty\)&lt;/span>时，即为MC法。&lt;/p>
&lt;p>&lt;strong>TD(λ)更新&lt;/strong>：在TD-n中，n是一个离散的超参数，为了能够简化计算并&lt;strong>综合考虑所有步数n的影响&lt;/strong>，我们引入了一个带&lt;strong>λ&lt;/strong>的加权求和方法。 &lt;span class="math">\[
G_t^{(\lambda)}= (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_t^{(n)} + \lambda^{T-t-1}G_t^{T-t}\\
\approx (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_t^{(n)}(T\rightarrow\infty) \\
V(S_t) \leftarrow V(S_t) + \alpha(G_t^{(\lambda)}-V(S_t))\tag{14}
\]&lt;/span>&lt;/p>
&lt;p>他们之间的关系可以参考笔记《强化学习之 DP、MC、TD》&lt;/p>
&lt;h2 id="不基于模型的控制qsa">不基于模型的控制(&lt;span class="math">\(Q(s,a)\)&lt;/span>)&lt;/h2>
&lt;p>从已知模型的、基于全宽度采样的动态规划学习转至模型未知、基于采样的蒙特卡洛或时序差分学习进行控制是朝着高效解决中等规模实际问题的一个突破。&lt;/p>
&lt;p>不基于模型的控制无法得知整体的状态价值和状态转移概率，因此我们需要脚踏实地，从当前状态的&lt;strong>行为状态价值对&lt;/strong>出发。因此，&lt;strong>在不基于模型的控制中，行为价值函数很重要&lt;/strong>。&lt;/p>
&lt;p>&lt;strong>即我们现在更关注&lt;span class="math">\(Q(s,a)\)&lt;/span>而非&lt;span class="math">\(V(s)\)&lt;/span>&lt;/strong>。&lt;/p>
&lt;h3 id="on-policy-vs-off-policy">On-policy VS Off-policy&lt;/h3>
&lt;ul>
&lt;li>行为策略：指导个体和环境进行实际交互的策略&lt;/li>
&lt;li>目标策略：评价状态价值、行为价值或者待优化的策略&lt;/li>
&lt;li>如果&lt;span class="math">\(行为策略 == 目标策略\rightarrow on-policy\)&lt;/span>（现时策略、同策略）&lt;/li>
&lt;li>如果&lt;span class="math">\(行为策略 \neq 目标策略\rightarrow off-policy\)&lt;/span>（借鉴策略、异策略）&lt;/li>
&lt;/ul>
&lt;p>在off-policy下，产生行为序列的策略和评价状态价值的策略时两个策略。&lt;/p>
&lt;h3 id="ε-greedy策略">ε-greedy策略&lt;/h3>
&lt;p>假设我们在状态&lt;span class="math">\(s\)&lt;/span>有&lt;span class="math">\(m\)&lt;/span>个可以选择的动作。 &lt;span class="math">\[
\pi(a|s) = \begin{cases}
\varepsilon/m + 1- \varepsilon, a^\ast = \argmax_{a\in A} Q(s,a)\\
\varepsilon/m, Others
\end{cases}\tag{15}
\]&lt;/span> 可以证明，ε-greedy方法可以提升原有的策略&lt;span class="math">\(\pi\)&lt;/span>。&lt;/p>
&lt;h3 id="on-policy-蒙特卡洛mc控制">On-policy 蒙特卡洛（MC）控制&lt;/h3>
&lt;p>on-policy MC 控制 = MC+ε-greedy方法&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>初始化一系列状态价值&lt;span class="math">\(Q(S_0,A_0)\)&lt;/span>，ε-greedy策略选择动作&lt;/li>
&lt;li>依照策略和环境交互，得到一组（或几组）轨迹，根据MC更新方法（公式&lt;span class="math">\((10),(11)\)&lt;/span>），更新状态价值。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>需要注意，区别于公式(10)现在更新的不是&lt;span class="math">\(V(S_t)\)&lt;/span>,而是 &lt;span class="math">\[\color{red}{Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t))}\tag{16}\]&lt;/span>&lt;/li>
&lt;/ul>
&lt;ol start="3" style="list-style-type: decimal">
&lt;li>回到步骤1&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>公式(16)也是接下来工作的基础&lt;/strong>。注意，并不需要每次交互都要更新状态价值，可以多跑几组数据来更新状态价值。&lt;strong>但是实际中，我们通常仅得到一个完整状态序列就进行一次策略迭代以加速更新&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>GLIE定理(Greedy in the Limit with Infinite Exploration)：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>所有状态行为对会被无数次探索：&lt;span class="math">\(\lim\limits_{k\rightarrow \infty} N_k(s,a)=\infty\)&lt;/span>&lt;/li>
&lt;li>随着采样次数趋近于无穷，策略最终收敛至一个贪婪策略:&lt;span class="math">\(\lim\limits_{k\rightarrow \infty}\pi_k(a|s) = 1(a = \argmax\limits_{a&amp;#39;\in A}Q_k(s,a&amp;#39;))\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>那么MC控制能够&lt;strong>收敛到最优的状态行为价值函数&lt;/strong>，即&lt;span class="math">\(Q_k(s,a)\xrightarrow{k\rightarrow\infty} q^\ast(s,a)\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>显然，&lt;strong>当我们无限采样且使ε-greedy策略中，ε=1/k，其中k是完整的采样次数&lt;/strong>。这个策略就满足GLIE定理。&lt;/p>
&lt;h3 id="on-policy-td-control-sarsasarsalambda">On-policy TD control —— SARSA,SARSA(&lt;span class="math">\(\lambda\)&lt;/span>)&lt;/h3>
&lt;h4 id="td0更新的sarsa">TD(0)更新的SARSA&lt;/h4>
&lt;p>在每一个采样序列中，&lt;strong>每一个时间步&lt;/strong>，在状态&lt;span class="math">\(S\)&lt;/span>下采取一个行为&lt;span class="math">\(A\)&lt;/span>到达状态&lt;span class="math">\(S&amp;#39;\)&lt;/span>后，都要更新状态行为对&lt;span class="math">\((S,A)\)&lt;/span>的价值&lt;span class="math">\(Q(S,A)\)&lt;/span>，交互过程和评价过程都使用ε-greedy策略（on-policy）。&lt;/p>
&lt;p>&lt;strong>关键——行为价值函数更新过程&lt;/strong>： &lt;span class="math">\[
Q(S,A)\leftarrow Q(S,A)+\alpha(R+\gamma Q(S&amp;#39;,A&amp;#39;)-Q(S,A))\tag{17}
\]&lt;/span> &lt;img src="../images/sarsa.png" alt="sarsa" />&lt;/p>
&lt;blockquote>
&lt;p>算法的收敛性。当行为策略满足GLIE定理，且学习率&lt;span class="math">\(\alpha\)&lt;/span>满足 &lt;span class="math">\[\sum_{t=1}^\infty \alpha_t=\infty, \text{and} \sum_{t=1}^\infty \alpha_t^2&amp;lt;\infty\]&lt;/span> SARSA算法将收敛至最优策略和最优价值函数。&lt;/p>
&lt;/blockquote>
&lt;h4 id="tdlambda更新的sarsalambda">TD(&lt;span class="math">\(\lambda\)&lt;/span>)更新的SARSA(&lt;span class="math">\(\lambda\)&lt;/span>)&lt;/h4>
&lt;p>和TD-n法更新价值函数一样，我们可以定义n步Q收获 &lt;span class="math">\[
q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\dotsb+\gamma^{n-1} R_{t+n}+\gamma^nQ(S_t,A_t)
\]&lt;/span> 类似的，我们根据n步Q收获可以定义&lt;strong>SARSA(n)&lt;/strong>的行为价值更新函数： &lt;span class="math">\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha(q_t^{(n)}-Q(S_t,A_t))\tag{18-1}
\]&lt;/span> 和TD(n)一样，&lt;span class="math">\(n\)&lt;/span>是一个需要条件的超参数，为了计算的方便，我们模仿&lt;span class="math">\(TD(\lambda)\)&lt;/span>的加权方式给&lt;span class="math">\(q^{(n)}_t\)&lt;/span>进行加权求和,结合所有n步Q收获： &lt;span class="math">\[
q_t^\lambda = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1} q_t^{(n)}
\]&lt;/span> 我们可以用&lt;span class="math">\(q_t^\lambda\)&lt;/span>代替公式&lt;span class="math">\((18)\)&lt;/span>中的&lt;span class="math">\(q_t^{(n)}\)&lt;/span>。这就是SARSA&lt;span class="math">\((\lambda)\)&lt;/span>的夹着更新公式。 &lt;span class="math">\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha(q_t^{\lambda}-Q(S_t,A_t))\tag{18-2}
\]&lt;/span>&lt;/p>
&lt;p>但是，&lt;strong>迭代一次Q值，需要完整的一次episode&lt;/strong>。为了解决这个问题，引入&lt;strong>效用迹&lt;/strong>的概念，实现增量更新。效用迹针对的状态行为对。 &lt;span class="math">\[
E_0(s,a)=0(初始化为0)\\
E_t(s,a)=\gamma\lambda E_{t-1}(s,a)+\mathbf{1}(S_t=s,A_t=a),\gamma,\lambda\in[0,1]\tag{19}
\]&lt;/span> 其中，&lt;span class="math">\(\mathbf{1}\)&lt;/span>是判别函数，成立为1，不成立为0。它体现的是一个结果与某一个状态行为对的因果关系，与得到该结果&lt;strong>最近的&lt;/strong>状态行为对，以及那些在此之前&lt;strong>频繁发生的&lt;/strong>状态行为对得到这个结果的影响最大。引入&lt;strong>效用迹&lt;/strong>后，Q值的更新公式为： &lt;span class="math">\[
\delta_t = R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_{t},A_{t})\\
Q(s,a)\leftarrow Q(s,a)+\alpha\delta_t E_t(s,a)\tag{20}
\]&lt;/span> 可以证明在&lt;span class="math">\(\lambda\in(0,1)\)&lt;/span>时，公式(20)与公式(18-2)是等效的（一个是前向认识，一个是反向认识），但是公式(20)中的数据学习完即可丢弃，&lt;strong>在线学习更有效&lt;/strong>。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/sarsa_lambda.png" alt="sarsa_lambda.png" />&lt;p class="caption">sarsa_lambda.png&lt;/p>
&lt;/div>
&lt;p>这里要提及一下的是&lt;span class="math">\(E(s,a)\)&lt;/span>在&lt;strong>每浏览完一个状态序列后需要重新置0&lt;/strong>，这体现了效用迹仅在一个状态序列中发挥作用；其次要提及的是算法更新&lt;span class="math">\(Q\)&lt;/span>和&lt;span class="math">\(E\)&lt;/span>的时候针对的不是某个状态序列里的&lt;span class="math">\(Q\)&lt;/span>或&lt;span class="math">\(E\)&lt;/span>，而是针对&lt;strong>个体掌握的&lt;/strong>整个状态空间和行为空间产生的&lt;span class="math">\(Q\)&lt;/span>和&lt;span class="math">\(E\)&lt;/span>。&lt;/p>
&lt;h3 id="为什么off-policy-mc-control不好">为什么Off-policy MC control不好&lt;/h3>
&lt;p>根据重要性采样，在off-policy MC更新下，收获函数应为 &lt;span class="math">\[
G_t^{\pi/\mu}=\underbrace{\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}\dotsb\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}}_{方差被放的很大}G_t\\
V(S_t) = V(S_t)+\alpha(G_t^{\pi/\mu}-V(S_t))\tag{21}
\]&lt;/span> 其中，&lt;span class="math">\(\mu(a|s)\)&lt;/span>是行为策略，&lt;span class="math">\(\pi(a|s)\)&lt;/span>是目标策略。但是&lt;span class="math">\(G_t^{\pi/\mu}\)&lt;/span>的方差会特别大，使得估计偏差大到无法使用，且&lt;span class="math">\(\mu(a|s)=0\)&lt;/span>时无法使用。因此基于蒙特卡洛的借鉴策略学习目前认为仅有理论上的研究价值，在实际中用处不大。&lt;/p>
&lt;h3 id="off-policy-td-control-q-learning">Off-policy TD control —— Q-learning&lt;/h3>
&lt;p>相较于off-policy MC更新中多个步骤的概率商相乘，&lt;strong>TD更新只用修正一次概率&lt;/strong>，方差还是可以接受的。 &lt;span class="math">\[
G_t^{\pi/\mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}G_t
\]&lt;/span>&lt;/p>
&lt;p>Q-learning是一个典型的off-policy TD控制方案。行为策略&lt;span class="math">\(µ\)&lt;/span>是基于行为价值函数&lt;span class="math">\(Q(s,a)\)&lt;/span>&lt;strong>ϵ-贪婪策略&lt;/strong>，借鉴策略&lt;span class="math">\(π\)&lt;/span>则是基于&lt;span class="math">\(Q(s，a)\)&lt;/span>的&lt;strong>完全贪婪策略&lt;/strong>。需要指出，Q-learning没有用到重要性采样。Q-learning的价值更新公式如下： &lt;span class="math">\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha({\color{red}R_{t+1}+\gamma Q(S_{t+1},A&amp;#39;)}-Q(S_t,A_t))\tag{22}
\]&lt;/span> 注意：红色部分的TD目标是基于借鉴策略&lt;span class="math">\(π\)&lt;/span>（贪婪策略）产生的行为&lt;span class="math">\(A’\)&lt;/span>得到的&lt;span class="math">\(Q\)&lt;/span>值。而每次的交互使用的是策略&lt;span class="math">\(\mu\)&lt;/span>（ϵ-贪婪策略），状态&lt;span class="math">\(S_t\)&lt;/span>依据&lt;span class="math">\(\mu\)&lt;/span>得到行为&lt;span class="math">\(A_t\)&lt;/span>并朝着&lt;span class="math">\(S_{t+1}\)&lt;/span>前进。&lt;/p>
&lt;p>由于&lt;span class="math">\(A&amp;#39;\)&lt;/span>是根据贪婪策略选出来的，所以公式（22）可以简化为： &lt;span class="math">\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha({\color{red}R_{t+1}+\gamma \max_{a&amp;#39;}Q(S_{t+1},a&amp;#39;)}-Q(S_t,A_t))\tag{23}
\]&lt;/span>&lt;/p>
&lt;div class="figure">
&lt;img src="../images/Q-learning.png" alt="Q-learning" />&lt;p class="caption">Q-learning&lt;/p>
&lt;/div>
&lt;h3 id="td与dp的关系">TD与DP的关系&lt;/h3>
&lt;p>&lt;img src="../images/DP与TD关系1.jpg" alt="DP与TD关系1" /> &lt;img src="../images/DP与TD关系2.jpg" alt="DP与TD关系2" />&lt;/p>
&lt;h2 id="特点">特点&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>explore and exploit&lt;/li>
&lt;li>Delayed reward&lt;/li>
&lt;li>Time critical(时间处理实现)&lt;/li>
&lt;li>Agent actor稳定提升&lt;/li>
&lt;/ol></description></item><item><title>强化学习-多臂老虎机问题</title><link>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98/</link><pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98/</guid><description>
&lt;h2 id="多臂老虎机问题">多臂老虎机问题&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#名词解释">名词解释&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bandit算法">Bandit算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#基本的bandit对应">基本的Bandit对应&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="名词解释">名词解释&lt;/h2>
&lt;ul>
&lt;li>stochastic MAB(stationary random rewards MAB)&lt;/li>
&lt;li>non-stochastic MAB(Adversarial Bandits) 对抗性质，有对手会改变每个臂的收益，根据对手是否与玩家独立可分为oblivious和non-oblivious两种&lt;/li>
&lt;li>restless 马尔可夫MAB的一种，每个臂都会独立地进行状态转移，无论臂是否被选中&lt;/li>
&lt;li>rested MAB 马尔可夫MAB的一种，只有被选中的臂进行状态转移，其他臂状态不变，处于冻结状态&lt;/li>
&lt;li>non-stationary MAB&lt;/li>
&lt;li>contextual MAB 会带有一定的附属信息，可以根据附属信息来帮助判断选择哪个或哪一类臂&lt;/li>
&lt;li>variants&lt;/li>
&lt;li>dualing MAB&lt;/li>
&lt;li>etc&lt;/li>
&lt;/ul>
&lt;h2 id="bandit算法">Bandit算法&lt;/h2>
&lt;ul>
&lt;li>汤普森采样&lt;/li>
&lt;li>e-greedy&lt;/li>
&lt;li>UCB及其变种&lt;/li>
&lt;li>COFIBA&lt;/li>
&lt;li>exp3&lt;/li>
&lt;li>hedge&lt;/li>
&lt;li>softmax&lt;/li>
&lt;/ul>
&lt;h2 id="基本的bandit对应">基本的Bandit对应&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>stochastic -- UCB&lt;/li>
&lt;li>adversarial -- Exp3&lt;/li>
&lt;li>Oblivious: 每个杆变换的分布独立于拉栏者，基本上这种就是撞大运，研究的少&lt;/li>
&lt;li>Non-oblivious： 每个杆的分布会一句拉杆者的策略而变化，有点博弈的意思。&lt;/li>
&lt;li>Markovian -- Gittins indices&lt;/li>
&lt;/ol></description></item><item><title>强化学习-概要</title><link>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%A6%82%E8%A6%81/</link><pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%A6%82%E8%A6%81/</guid><description>
&lt;h2 id="强化学习">强化学习&lt;!-- omit in toc -->&lt;/h2>
&lt;h2 id="分类">分类&lt;/h2>
&lt;ul>
&lt;li>Valued-based / policy-based / A3C=A+C&lt;/li>
&lt;li>Model based / Model free&lt;/li>
&lt;li>回合更新（蒙特卡洛） / 时间差分（步更新）&lt;/li>
&lt;li>On-policy / off-policy&lt;/li>
&lt;li>稳态和非稳态&lt;/li>
&lt;li>全部可观测MDP / 部分可观测&lt;/li>
&lt;/ul>
&lt;p>DRL：本质把DNN作为一个actor。&lt;/p>
&lt;h2 id="特点">特点&lt;/h2>
&lt;ol style="list-style-type: decimal">
&lt;li>explore and exploit&lt;/li>
&lt;li>Delayed reward&lt;/li>
&lt;li>Time critical(时间处理实现)&lt;/li>
&lt;li>Agent actor稳定提升&lt;/li>
&lt;/ol></description></item><item><title>机器学习-梯度下降算法汇总（1）</title><link>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB1/</link><pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB1/</guid><description>
&lt;h2 id="梯度下降算法1">梯度下降算法（1）&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#梯度下降法与梯度计算">梯度下降法与梯度计算&lt;/a>&lt;/li>
&lt;li>&lt;a href="#计算梯度通过有限差值的数值计算梯度近似梯度可以适用于有些不能微分的函数">计算梯度①通过有限差值的数值计算梯度（近似梯度，可以适用于有些不能微分的函数）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#计算梯度通过微积分计算解析梯度">计算梯度②通过微积分计算解析梯度&lt;/a>&lt;/li>
&lt;li>&lt;a href="#各种梯度下降法">各种梯度下降法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#batch_gradient_descent">Batch_gradient_descent&lt;/a>&lt;/li>
&lt;li>&lt;a href="#stochastic_gradient_descent">Stochastic_gradient_descent&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mini-batch_gradient_descent">Mini-batch_gradient_descent&lt;/a>&lt;/li>
&lt;li>&lt;a href="#挑战">挑战&lt;/a>&lt;/li>
&lt;li>&lt;a href="#梯度优化算法">梯度优化算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#momentum">Momentum&lt;/a>&lt;/li>
&lt;li>&lt;a href="#nesterov加速梯度法">Nesterov加速梯度法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#未完待续">未完待续&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>本文首先查看了多种梯度下降算法，并总结了数据训练过程中遇到的挑战。接下来，我们介绍了最常用的梯度优化算法，并指出它们做出这些算法优化的缘由。我们还会简要的说明如何在并行与分布式场景下优化梯度下降算法。最后，我们会追加一些优化梯度下降算法的有用策略。&lt;/p>
&lt;h3 id="梯度下降法与梯度计算">梯度下降法与梯度计算&lt;/h3>
&lt;p>从最基本的来说，梯度下降算法是一个通过向负梯度方向(-∇θJ(θ))，更新参数θ来最小化目标函数J(θ)的方法。还有超参数学习率η决定我们向（本地）最优移动的步长。计算梯度，可以有两种办法，①缓慢、近似但是最简单的办法（数值梯度）②快速、精确但是更易于出错的微积分方法（解析梯度）。我们分别介绍：&lt;/p>
&lt;h4 id="计算梯度通过有限差值的数值计算梯度近似梯度可以适用于有些不能微分的函数">计算梯度①通过有限差值的数值计算梯度（近似梯度，可以适用于有些不能微分的函数）&lt;/h4>
&lt;pre class="sourceCode python">&lt;code class="sourceCode python">&lt;span class="kw">def&lt;/span> eval_numerical_gradient(f, x):
&lt;span class="co">&amp;quot;&amp;quot;&amp;quot;&lt;/span>
&lt;span class="co"> a naive implementation of numerical gradient of f at x&lt;/span>
&lt;span class="co"> - f should be a function that takes a single argument&lt;/span>
&lt;span class="co"> - x is the point (numpy array) to evaluate the gradient at&lt;/span>
&lt;span class="co"> &amp;quot;&amp;quot;&amp;quot;&lt;/span>
fx = f(x) &lt;span class="co"># evaluate function value at original point&lt;/span>
grad = np.zeros(x.shape)
h = &lt;span class="fl">0.00001&lt;/span>
&lt;span class="co"># iterate over all indexes in x&lt;/span>
it = np.nditer(x, flags=[&lt;span class="st">&amp;#39;multi_index&amp;#39;&lt;/span>], op_flags=[&lt;span class="st">&amp;#39;readwrite&amp;#39;&lt;/span>])
&lt;span class="kw">while&lt;/span> not it.finished:
&lt;span class="co"># evaluate function at x+h&lt;/span>
ix = it.multi_index
old_value = x[ix]
x[ix] = old_value + h &lt;span class="co"># increment by h&lt;/span>
fxh = f(x) &lt;span class="co"># evalute f(x + h)&lt;/span>
x[ix] = old_value &lt;span class="co"># restore to previous value (very important!)&lt;/span>
&lt;span class="co"># compute the partial derivative&lt;/span>
grad[ix] = (fxh - fx) / h &lt;span class="co"># the slope&lt;/span>
it.iternext() &lt;span class="co"># step to next dimension&lt;/span>
&lt;span class="kw">return&lt;/span>&lt;/code>&lt;/pre>
&lt;p>grad这个函数在每一个维度上前进了一小步（h=0.00001）,并近似计算出每一个维度的偏导数，最后组成梯度。实际上，更好的方式是使用&lt;strong>中心差分公式&lt;/strong>作为偏导数计算：&lt;span class="math">\([f(x+h)−f(x−h)]/2h\)&lt;/span>。解释在:&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Numerical_differentiation">https://en.wikipedia.org/wiki/Numerical_differentiation&lt;/a>&lt;/p>
&lt;p>需要指出的是超参数步长（学习率）的设置是什么重要（也是十分令人头疼的步骤），步长小收敛的速度慢，步长大容易直接跳到极值点的另一侧。&lt;/p>
&lt;p>基于数值的梯度计算方法需要计算很多次目标函数，参数有多少个维度，就要计算多少次偏导，就要计算多少次目标函数，十分低效。对于现代神经网络动辄有成千上万的维度，计算一次梯度就要花费那么大计算量，十分不值得。而且每次计算梯度，只能获得h（步长）的收益，更使得计算量难以估计。&lt;/p>
&lt;h4 id="计算梯度通过微积分计算解析梯度">计算梯度②通过微积分计算解析梯度&lt;/h4>
&lt;p>我们可以通过解析方式（求导）计算梯度的解析解，这无疑又快有精确，但是对于复杂函数求导，实际计算很容易出错，所以我们常常使用数值梯度计算来验证解析梯度的正确性，又被称为&lt;strong>梯度检查（gradient check）&lt;/strong>。通过解析解求梯度只需要计算一次，即将当前点带入梯度式子即可。&lt;/p>
&lt;h3 id="各种梯度下降法">各种梯度下降法&lt;/h3>
&lt;p>有三种梯度下降法的变种，差别在于我们使用&lt;strong>多少数据&lt;/strong>来计算目标函数的梯度。根据数据量，我们在参数的精度和所花的时间之间做权衡。&lt;/p>
&lt;h4 id="batch_gradient_descent">Batch_gradient_descent&lt;/h4>
&lt;p>Batch gradient descent（Vanilla gradient descent，批量梯度下降，BGD），这就是最基本的梯度下降法，每次运行梯度运算会把所有的训练数据都带入计算，所以它的运行速度十分缓慢，并且在大数据寄情况下，能够导致内存溢出。同时，BGD不允许我们在线更新模型，因为它要把所有数据都跑一遍。总的来说，代码如下：&lt;/p>
&lt;pre class="sourceCode python">&lt;code class="sourceCode python">&lt;span class="kw">for&lt;/span> i in &lt;span class="dt">range&lt;/span>(nb_epochs):
params_grad = evaluate_gradient(loss_function, data, params)
params = params - learning_rate * params_grad&lt;/code>&lt;/pre>
&lt;p>在给定的循环次数内，我们首先要计算损失函数在整个数据集(data)上的梯度向量，来更新我们的参数向量params。现在很多深度学习库都提供自动差分计算能力，来求解梯度。如果你自己求解梯度，需要注意进行梯度检查。 我们接下来会根据整体数据集的负梯度和学习率来更新参数。BGD对于凸的目标函数有收敛的全局最优解，对于非凸函数有局部最优解。&lt;/p>
&lt;img src="../images/bgd.png" alt="图1 BGD" />
&lt;center>
图1 BGD
&lt;/center>
&lt;h4 id="stochastic_gradient_descent">Stochastic_gradient_descent&lt;/h4>
&lt;p>随机梯度下降，SGD。与此相反，随机梯度下降只根据每一个（？？）训练数据进行一次参数更新。 &lt;span class="math">\[θ=θ−η⋅∇θJ(θ;x(i);y(i)).\]&lt;/span> BGD对于大数据集做了许多冗余的运算（我觉得不能叫冗余，而是信价比很低），因为它在更新参数之前，重复计算了很多相似数据的梯度。SGD通过每次只做一次更新来原理这种冗余。所以，SGD会快很多，且常常被用于在线学习。 SGD会以较高的方差频繁的更新，导致目标函数的抖动十分剧烈，如图2所示。&lt;/p>
&lt;img src="../images/sgd_fluc.png" alt="图2: SGD抖动" />
&lt;center>
图2: SGD抖动
&lt;/center>
&lt;p>不同于BGD稳定地向目标函数最小值前进，SGD的抖动从另一个角度来说使它有机会跳出局部最优找到更好的局部解（甚至最优解，参考模拟退火）。但是，这给收敛带来了很大问题。解决的方案就是&lt;strong>逐步降低学习率&lt;/strong>，这使得SGD有着和BGD相似的收敛能力。代码如下所示：&lt;/p>
&lt;pre class="sourceCode python">&lt;code class="sourceCode python">&lt;span class="kw">for&lt;/span> i in &lt;span class="dt">range&lt;/span>(nb_epochs):
np.random.shuffle(data)
&lt;span class="kw">for&lt;/span> example in data:
params_grad = evaluate_gradient(loss_function, example,params)
params = params - learning_rate * params_grad&lt;/code>&lt;/pre>
&lt;img src="../images/sgd.png" alt="图3 SGD" />
&lt;center>
图3 SGD
&lt;/center>
&lt;h4 id="mini-batch_gradient_descent">Mini-batch_gradient_descent&lt;/h4>
&lt;p>最小batch梯度下降，MBGD综合了两种（BGD）的策略，每次选取一组最小batch（n个数据）进行训练。 &lt;span class="math">\[θ=θ−η⋅∇θJ(θ;x(i:i+n);y(i:i+n)).（[i，i+n)共个数据）\]&lt;/span> 这个办法a)降低了每次更新的抖动，更加容易收敛；b）计算效率高。通常最小batch的范围在50-256之间。&lt;/p>
&lt;pre class="sourceCode python">&lt;code class="sourceCode python">&lt;span class="kw">for&lt;/span> i in &lt;span class="dt">range&lt;/span>(nb_epochs):
np.random.shuffle(data)
&lt;span class="kw">for&lt;/span> batch in get_batches(data, batch_size=&lt;span class="dv">50&lt;/span>):
params_grad = evaluate_gradient(loss_function, batch, params)
params = params - learning_rate * params_grad&lt;/code>&lt;/pre>
&lt;img src="../images/mbgd.png" alt="图4 MBGD" />
&lt;center>
图4 MBGD
&lt;/center>
&lt;p>总结表格，各种算法实现：&lt;/p>
&lt;p>&lt;a href="https://github.com/tsycnh/mlbasic">https://github.com/tsycnh/mlbasic&lt;/a> |梯度下降算法|优点|缺点| |:-:|:-:|:-:| |BGD|凸优化全局最优|计算量大，迭代速度慢，训练速度慢 |SGD|训练速度快，支持在线学习|准确度下降，有噪声，非全局最优 |MBGD|1. 训练速度较快, 取决于小批量的数目 2. 支持在线学习|准确度不如 BGD, 仍然有噪声, 非全局最优解&lt;/p>
&lt;h3 id="挑战">挑战&lt;/h3>
&lt;p>从上面可以看出，在优化中MBGD无疑是折衷下最佳方案。但是单纯的MBGD算法没有办法确保收敛，并且提供了一系列需要解决的问题：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>选择一个&lt;strong>合适的学习&lt;/strong>率非常困难。小的收敛慢，大的可能不收敛，或者在最小值周边浮动。&lt;/li>
&lt;li>学习率规划尝试在训练中调节学习率，例如退火：根据一个预定的机制或目标变动之低于一个门限，就降低学习率。但是，这个调节机制或门限需要预先设定，并且不能够根据数据集特点自适应。&lt;/li>
&lt;li>此外，相同的学习率被应用于所有的参数更新。但是如果我们的数据是稀疏的，同时特点有非常不同的频率，我们可能希望更新的程度是相当不同的，比如对于一个很少发生的事情一次性做一个大幅度的更新。&lt;/li>
&lt;li>另一个关键挑战是避免陷入神经网络中常见的最小化非凸误差函数的局部最小值。Dauphin et al指出&lt;strong>鞍点&lt;/strong>的存在使得问题更加难解，如果一个维度使得误差函数值上升而另一个维度使得误差值下降。这些鞍点通常被一个相同误差的平台所包围，这使得SGD难以逃脱，因为梯度在所有维度上都接近于零。&lt;/li>
&lt;/ol>
&lt;h3 id="梯度优化算法">梯度优化算法&lt;/h3>
&lt;p>在接下来的文章中，我们将概述一些被深度学习社区广泛使用的算法来处理上述挑战。我们不会讨论在高维数据集的实际计算中不可行的算法，例如二阶方法，如牛顿法。&lt;/p>
&lt;h4 id="momentum">Momentum&lt;/h4>
&lt;p>Momentum，动量法。SGD对避开“沟壑”有问题，例如，通常在局部最优附近，某一维度会远比其他维度陡峭。在这些情况下，SGD在沟谷的斜坡上振荡，而只在底部向局部最优方向缓慢前进，如图5所示：&lt;/p>
&lt;img src="../images/sgd_without_momentum.png" alt="图5：不加动量的SGD" />
&lt;center>
图5：不加动量的SGD
&lt;/center>
&lt;p>动量是一种在相关方向上加速SGD并抑制振荡的方法，如图6所示。它通过将过去的一部分分量，加到当前更新矢量来实现。 &lt;span class="math">\[v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta)\]&lt;/span> &lt;span class="math">\[\theta = \theta - v_t\]&lt;/span>&lt;/p>
&lt;img src="../images/momentum_vector.png" alt="图6：动量法矢量示意图" />
&lt;center>
图6：动量法矢量示意图
&lt;/center>
&lt;img src="../images/sgd_with_momentum.png" alt="图7： 加动量的SGD" />
&lt;center>
图7： 加动量的SGD
&lt;/center>
&lt;p>γ通常是0.9，或者相似的值。&lt;/p>
&lt;p>本质上，当使用动量时，我们把球推下山。当球滚下山时，它积累了动量，在途中变得越来越快。同样的事情也发生在我们的参数更新上：动量项对于梯度指向相同方向的维度增加，对于梯度改变方向的维度减少更新。因此，我们获得更快的收敛速度和减少振荡。（&lt;strong>通过仿真，确实降低振动，收敛更快&lt;/strong>）&lt;/p>
&lt;img src="../images/momentum.png" alt="图8 Momentum动量法" />
&lt;center>
图8 Momentum动量法
&lt;/center>
&lt;h4 id="nesterov加速梯度法">Nesterov加速梯度法&lt;/h4>
&lt;p>Nesterov accelerated gradient修正动量，不让动量冲的太猛。&lt;/p>
&lt;p>然而，当一个球滚下山坡从而带有一个初始动量之后，这个动量在其他位置就是盲目的，表现并不能令人满意。现在我们假设有一个智能球，知道这个初始动量将会把它带到何处，这样就可以提前做出一些修正，而不是仅仅用当前点的梯度来作为接下来动量的增量。&lt;/p>
&lt;p>Nesterov加速梯度法，是一种能够提升动量精确性的方法。我们知道，在参数θ处，会带有一个初始动量γvt-1 。如果我们直接计算θ-γvt-1 处的梯度（这是估计的梯度），然后将这个梯度作为修正值来叠加在原始动量上，可以获得更加优良的效果。公式如下所示:（γ一般取0.9） &lt;span class="math">\[v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} )\]&lt;/span> &lt;span class="math">\[\theta = \theta - v_t \]&lt;/span>&lt;/p>
&lt;p>根据动量法和NAG法，我们可以通过图9进行对比。动量法（Momentum）首先会计算当前位置的梯度（第一段蓝线），然后加上之前累计的初始动量（第二段蓝线），总的矢量和即为走的最终路线。而NAG方法，首先根据之前累计的初始动量前进（棕色的线），然后估计在经过这个动量之后位置的梯度（红线）作为修正，实际走的是二者之和（绿线）。NAG波动也小了很多。&lt;strong>实际上NAG方法用到了二阶信息，所以才会有这么好的结果。&lt;/strong>&lt;/p>
&lt;img src="../images/momentum_vs_nag.png" alt="图9 动量法（蓝线） vs NAG法（棕，红，绿）" />
&lt;center>
图9 动量法（蓝线） vs NAG法（棕，红，绿）
&lt;/center>
&lt;img src="../images/momentum_nag.png" alt="图10 Momentum_NAG" />
&lt;center>
图10 Nesterov加速梯度法
&lt;/center>
&lt;h3 id="未完待续">未完待续&lt;/h3>
&lt;p>既然我们能够使得我们的更新适应误差函数的斜率以相应地加速SGD，我们同样也想要使得我们的更新能够适应每一个单独参数，以根据每个参数的重要性决定大的或者小的更新。下一章将说明如何自适应更新参数。（Adagrad、RMSprop、Adadelta、Adam、MaxAdam等等）&lt;/p></description></item><item><title>机器学习-梯度下降算法汇总（2）</title><link>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB2/</link><pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB2/</guid><description>
&lt;h2 id="梯度下降算法2">梯度下降算法（2）&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#梯度优化算法">梯度优化算法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#adagrad">Adagrad&lt;/a>&lt;/li>
&lt;li>&lt;a href="#rmsprop均方根传播">RMSprop(均方根传播)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#adadelta">Adadelta&lt;/a>&lt;/li>
&lt;li>&lt;a href="#adam">Adam&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#我自己的实验到这里速度很慢了根本到不了一个理想值">我自己的实验到这里速度很慢了，根本到不了一个理想值&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#adamax">AdaMax&lt;/a>&lt;/li>
&lt;li>&lt;a href="#nadam">Nadam&lt;/a>&lt;/li>
&lt;li>&lt;a href="#amsgrad">AMSGrad&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lookahead新优化算法">Lookahead(新优化算法)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#算法可视化图">算法可视化图&lt;/a>&lt;/li>
&lt;li>&lt;a href="#优化器选择">优化器选择&lt;/a>&lt;/li>
&lt;li>&lt;a href="#并行与分布式sgd">并行与分布式SGD&lt;/a>&lt;/li>
&lt;li>&lt;a href="#hogwild">Hogwild&lt;/a>&lt;/li>
&lt;li>&lt;a href="#downpour_sgd">Downpour_SGD&lt;/a>&lt;/li>
&lt;li>&lt;a href="#delay-tolerant_algorithms_for_sgd">Delay-tolerant_Algorithms_for_SGD&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tensorflow">TensorFlow&lt;/a>&lt;/li>
&lt;li>&lt;a href="#elastic_averaging_sgd">Elastic_Averaging_SGD&lt;/a>&lt;/li>
&lt;li>&lt;a href="#优化sgd的其他策略">优化SGD的其他策略&lt;/a>&lt;/li>
&lt;li>&lt;a href="#数据集的洗牌和课程学习">数据集的洗牌和课程学习&lt;/a>&lt;/li>
&lt;li>&lt;a href="#批量归一化">批量归一化&lt;/a>&lt;/li>
&lt;li>&lt;a href="#early_stopping">Early_stopping&lt;/a>&lt;/li>
&lt;li>&lt;a href="#梯度噪音">梯度噪音&lt;/a>&lt;/li>
&lt;li>&lt;a href="#总结">总结&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>上一篇文章我们理解到了Adagrad算法，即一个逐渐降低学习率的算法。它带来一个弊端，学习率可能过早的太小，以致于离最优点还很远就收敛了。因此有人提出了RMSprop算法来缓解这个问题，我们这篇文章从这里开始讲起。原文中先将Adadelta，但是我觉得原文这里有些难以理解，所以先从RMSprop说起。&lt;/p>
&lt;h3 id="梯度优化算法">梯度优化算法&lt;/h3>
&lt;h4 id="adagrad">Adagrad&lt;/h4>
&lt;p>Adagrad在原来基础上，做了这样一个优化：让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。&lt;/p>
&lt;p>之前的方法中，我们对于所有的参数&lt;span class="math">\(\boldsymbol{\theta}\)&lt;/span>，都是用相同的学习速率η。而在Adagrade算法中，对于每一个维度的参数&lt;span class="math">\({\theta}_i\)&lt;/span>，采用不同的学习速率。我们先看看&lt;span class="math">\({\theta}_i\)&lt;/span>每一个维度的更新过程，然后再把它向量化。简单来看，我们使用&lt;span class="math">\(\boldsymbol{g_t}\)&lt;/span>来表示t时刻的梯度向量，&lt;span class="math">\(g_{t,i}\)&lt;/span>表示某一维度参数&lt;span class="math">\({\theta}_i\)&lt;/span>在t时刻的&lt;strong>偏导数&lt;/strong>。 &lt;span class="math">\[g_{t,i}=\frac{{\partial}J(\boldsymbol{\theta_t)}}{d\theta_{t,i}}\]&lt;/span> 对于一般SGD算法，每一个周期t内，&lt;span class="math">\(\theta_{i}\)&lt;/span>在更新是采用相同的学习率：&lt;span class="math">\(\theta_{t+1,i}=\theta_{t,i}-\eta\cdot{g_{t,i}}\)&lt;/span>. 但是对于Adagrad算法，学习率的更新的规则如下所示： &lt;span class="math">\[\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{G_{t,ii}}+\epsilon}\cdot g_{t,i}\]&lt;/span> 其中，&lt;span class="math">\(\boldsymbol{G_t}\)&lt;/span>是d*d的对角矩阵，其中元素&lt;span class="math">\(G_{t,ii}\)&lt;/span>表示表示之前此维度偏导数的平方和。而&lt;span class="math">\(\epsilon\)&lt;/span>的存在是为了防止分母为0，一般取值为1e-8.有意思的是，如果没有平方根这个算法的效果会很差。 由于参数&lt;span class="math">\(G_t\)&lt;/span>包含过去梯度的平方和，我们可以向量化这个表达式： &lt;span class="math">\[\boldsymbol{\theta_{t+1}}=\boldsymbol{\theta_{t}}-\frac{\eta}{\sqrt{\boldsymbol{G_{t}}+\epsilon\cdot\boldsymbol{I}}}\odot\boldsymbol{g_{t}}\]&lt;/span> 其中，&lt;span class="math">\(\odot\)&lt;/span>表示矩阵和向量的积。这样随着矩阵&lt;span class="math">\(\boldsymbol{G_t}\)&lt;/span>元素逐渐变大，学习率η会逐渐减少。Adagrad算法最大的问题在于：由于每增加一个正项，在整个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，在学习率无限小时，Adagrad算法将无法取得额外的信息，导致算法停止。尤其是初始值的梯度很大，导致算法提前收敛。接下来的算法旨在解决这个不足。&lt;/p>
&lt;img src="../images/adagrad.png" alt="Adagrad based MBGD" />
&lt;center>
图1 Adagrad效果(batch=5)Adagrad算法提前停止
&lt;/center>
&lt;h4 id="rmsprop均方根传播">RMSprop(均方根传播)&lt;/h4>
&lt;p>RMSprop算法对学习率的自适应做出了修改。之前Adagrad算法中，学习率是一只下降的，为了避免过早收敛，Geoff Hinton 提出了一种新的学习率自动更新方式，类似TCP协议中平滑计算RTT的方式： &lt;span class="math">\[TCP的平均RTT : SRTT = \alpha \times SRTT + (1-\alpha)\times RTT\]&lt;/span> &lt;span class="math">\[RMSprop 学习率自适应参数: E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g^2_t\]&lt;/span> 通常，&lt;span class="math">\(\gamma\)&lt;/span>取0.9，基础学习率&lt;span class="math">\(\eta\)&lt;/span>取0.001。我在实际使用中发现用0.001收敛的速度很慢，可以尝试使用0.2。而且损失函数的曲线明显平滑了许多。&lt;span class="math">\(E[g^2]_{t}\)&lt;/span>是0-&amp;gt;t时刻梯度的平滑值。综上所述，RMSprop的梯度更新算法为： &lt;span class="math">\[{\Delta}{\theta}_t=-\frac{\eta}{\sqrt{E[g^2]_t+{\epsilon}}}g_t\]&lt;/span> 另一个重要的建议是&lt;strong>mini-batch的值选区的大一些，否在防止震荡不收敛&lt;/strong>。&lt;/p>
&lt;img src="../images/rmsprop.png" alt="RMSprop based MBGD" />
&lt;center>
图2 RMSprop效果(batch=5)
&lt;/center>
&lt;h4 id="adadelta">Adadelta&lt;/h4>
&lt;p>Adadelta算法是Adagrad算法的拓展,它能够降低Adagrad算法在学习率上单项的过度下降。相对于一直累加过去所有的梯度的平方，Adadelta算法管理了一个窗口,将过去梯度相加的个数固定为&lt;span class="math">\(\omega\)&lt;/span>。&lt;/p>
&lt;p>但是相较于低效率地存储前&lt;span class="math">\(\omega\)&lt;/span>个梯度的平方，“梯度的和”被递归定义为逐渐衰减的之前所有梯度平方的和。这个定义有点像TCP协议中计算RTT的方式： &lt;span class="math">\[E[g^2]_t=\gamma E[g^2]_{t-1}+(1-\gamma)g^2_t\]&lt;/span> 在t时刻的实时平均&lt;span class="math">\(E[g^2]_{t}\)&lt;/span>只取决于之前的均值和当前的梯度。&lt;span class="math">\(\gamma\)&lt;/span>的作用和RMSprop中的类似，一般可以取0.9。为了清楚起见，我们重新写一般SGD算法的参数更新过程： &lt;span class="math">\[{\Delta}{\theta}_t = -{\eta}{\cdot}g_{t,i}\]&lt;/span> &lt;span class="math">\[{\theta}_{t+1} = {\theta}_t + {\Delta}{\theta}_t\]&lt;/span> 然后，我们再重写下Adagrad更新参数向量的公式： &lt;span class="math">\[\boldsymbol{\theta_{t+1}}=\boldsymbol{\theta_{t}}-\frac{\eta}{\sqrt{\boldsymbol{G_{t}}+\epsilon\cdot\boldsymbol{I}}}\odot\boldsymbol{g_{t}}\]&lt;/span> 现在，我们只要把对角矩阵&lt;span class="math">\(G_t\)&lt;/span>换成逐渐衰减的梯度平方的均值&lt;span class="math">\({E[g^2]_t}\)&lt;/span>: &lt;span class="math">\[{\Delta}{\theta}_t=-\frac{\eta}{\sqrt{E[g^2]_t+{\epsilon}}}g_t\]&lt;/span> 如果我们认为&lt;span class="math">\({\sqrt{E[g^2]_t+{\epsilon}}}\)&lt;/span>近似等于均方根值RMS，也可以用下式表示： &lt;span class="math">\[\Delta\theta_t=-\frac{\eta}{RMS[g]_t}g_t\]&lt;/span> 到这里读者一定会很奇怪，这不是RMSprop算法吗？到这里如果把&lt;span class="math">\(\gamma\)&lt;/span>设置为0.9，确实和RMSprop算法一样，所以下面才说的Adadelta的独创。&lt;/p>
我们看到目前的部分还存在两个明显问题，第一超参数学习率&lt;span class="math">\(\eta\)&lt;/span>还得手动设置；第二表达式的量纲不统一。我们假设&lt;span class="math">\(\theta\)&lt;/span>的单位是&lt;span class="math">\(x\)&lt;/span>。那么BGD、SGD、Momentum、NAG的量纲就是&lt;span class="math">\(1/x\)&lt;/span>。 &lt;span class="math">\[\Delta\theta单位\propto \boldsymbol{g}某一维度单位\propto\frac{\partial{f}}{\partial{\theta_i}}\propto\frac{1}{x}\]&lt;/span> 而Adagrad的&lt;span class="math">\(\Delta\theta\)&lt;/span>是无量纲数。如果我们想要获得匹配的量纲，必须使用二阶信息例如使用Hessian信息（或其他近似）的牛顿法： &lt;span class="math">\[\Delta\theta单位\propto \frac{\boldsymbol{g}}{\boldsymbol{H}}\propto\frac{\frac{\partial{f}}{\partial{\theta_i}}}{\frac{\partial^2f}{\partial{\theta^2}}}\propto{x}\]&lt;/span> 我们知道二阶的牛顿是正确的（这点先不去证明，反正是对的），但是二阶信息的计算量大大增加了，所以我们考虑如下变化： &lt;span class="math">\[\Delta\theta\propto\frac{\frac{\partial{f}}{\partial{\theta_i}}}{\frac{\partial^2f}{\partial{\theta^2}}}\Rightarrow\frac{\Delta\theta}{{\frac{\partial{f}}{\partial{\theta_i}}}}\propto\frac{1}{\frac{\partial^2f}{\partial{\theta^2}}}\propto\frac{1}{\boldsymbol{{H}}}\propto{x^2}\]&lt;/span> 再根据： &lt;span class="math">\[\frac{1}{\boldsymbol{H}}\cdot\boldsymbol{g}\propto{x}\]&lt;/span> &lt;span class="math">\(\boldsymbol{g}\)&lt;/span>即为当前梯度，分母中的&lt;span class="math">\(\frac{\partial{f}}{\partial{\theta_i}}\)&lt;/span>就是&lt;span class="math">\(RMS[g]_t\)&lt;/span>。关键分子还缺少一个&lt;span class="math">\(\Delta\theta\)&lt;/span>。Adadelta算法使用了平滑的均方根值&lt;span class="math">\(RMS[\Delta\theta]_{t-1}\)&lt;/span>来作为分子代替&lt;span class="math">\(\eta\)&lt;/span>，即： &lt;span class="math">\[\Delta\theta_t=-\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}g_t\]&lt;/span> &lt;span class="math">\[其中：RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon}\]&lt;/span> &lt;span class="math">\[其中：E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t\]&lt;/span> 这样，Adadelta算法不用再设置超参数学习率，量纲也得到了统一。 &lt;img src="../images/Adadelta.png" alt="Adadelta based MBGD" />
&lt;center>
图3 RMSprop效果(batch=5)
&lt;/center>
&lt;h4 id="adam">Adam&lt;/h4>
&lt;p>自适应矩算法（Adaptive monent），是一种对随机目标函数执行一阶梯度优化的算法。其主要利用了&lt;strong>自适应性低阶矩估计&lt;/strong>。Adam 算法很容易实现，并且有很高的计算效率和较低的内存需求。Adam 算法梯度的对角缩放（diagonal rescaling）具有不变性，因此很适合求解带有大规模数据或参数的问题。该算法同样适用于解决大噪声和稀疏梯度的非稳态（non-stationary）问题。超参数可以很直观地解释，并只需要少量调整。&lt;/p>
&lt;p>Adam算法的核心更新规律是： &lt;span class="math">\[\theta_{t+1} = \theta_t-\frac{\eta}{\sqrt{\hat v_t}+\epsilon}\hat m_t\]&lt;/span> 和过去一样，&lt;span class="math">\(\eta\)&lt;/span>是学习率，&lt;span class="math">\(\hat m_t\)&lt;/span>是梯度的一阶矩，&lt;span class="math">\(\hat v_t\)&lt;/span>是梯度的二阶矩。我们用&lt;span class="math">\(g_t\)&lt;/span>表示t时刻的梯度。那么一阶矩&lt;span class="math">\(\hat m_t\)&lt;/span>和二阶矩&lt;span class="math">\(\hat v_t\)&lt;/span>分别可以指数移动平均表示： &lt;span class="math">\[m_t \leftarrow \beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g_t\]&lt;/span> &lt;span class="math">\[v_t \leftarrow \beta_2\cdot v_{t-1}+(1-\beta_2)\cdot g_t^2\]&lt;/span> 但是，再初始的时候，一阶矩&lt;span class="math">\(m_0\)&lt;/span>和&lt;span class="math">\(v_0\)&lt;/span>都是从0开始，因此矩估计会偏向0（因为&lt;span class="math">\(1-\beta\)&lt;/span>接近0，所以初始阶段矩大约都接近0），因此我们在初始阶段需要用它除以一个&lt;strong>小数&lt;/strong>： &lt;span class="math">\[\hat m_t = \frac{m_t}{1-\beta_1^t}\]&lt;/span> &lt;span class="math">\[\hat v_t = \frac{v_t}{1-\beta_2^t}\]&lt;/span> t表示参数&lt;span class="math">\(\beta_1\)&lt;/span>、&lt;span class="math">\(\beta_2\)&lt;/span>的t次方，在编程中表示t次迭代，这样在最开始的时候&lt;span class="math">\(1-\beta\)&lt;/span>是一个小数，可以抵消最开始移动平均矩很小的缺点，随着迭代次数t增加，&lt;span class="math">\(1-\beta^t\)&lt;/span>逐渐趋近于1，实际的移动平均矩会逐渐占据更新过程的主导权。 论文作者建议超参数取值：&lt;span class="math">\(\eta=0.001,\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}\)&lt;/span>。&lt;/p>
&lt;h5 id="我自己的实验到这里速度很慢了根本到不了一个理想值">我自己的实验到这里速度很慢了，根本到不了一个理想值&lt;/h5>
&lt;h4 id="adamax">AdaMax&lt;/h4>
&lt;p>Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。从Adam算法可以算是二范数的比较，如果我们把二范数变成无穷范数，那么： &lt;span class="math">\[v_t \leftarrow \beta_2^p\cdot v_{t-1}+(1-\beta_2^p)\cdot g_t^p \\ where\quad p\rightarrow\infty\]&lt;/span> 为了防止混淆，我们用&lt;span class="math">\(u_t\)&lt;/span>代替&lt;span class="math">\(v_t\)&lt;/span>,通过无穷范数是最大值性质，我们可以推得： &lt;span class="math">\[u_t = max(\beta_2\cdot u_{t-1},|g_t|)\]&lt;/span> 相应的，更新公式为： &lt;span class="math">\[\theta_{t+1} = \theta_{t} - \dfrac{\eta}{u_t} \hat{m}_t\]&lt;/span> 其中，&lt;span class="math">\(\eta = 0.002,\beta_1=0.9,\beta_2=0.999\)&lt;/span>。&lt;/p>
&lt;h4 id="nadam">Nadam&lt;/h4>
&lt;p>Nadam可以看作是Nesterov和Adam方法的结合。Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。&lt;/p>
&lt;h4 id="amsgrad">AMSGrad&lt;/h4>
&lt;h4 id="lookahead新优化算法">Lookahead(新优化算法)&lt;/h4>
&lt;h4 id="算法可视化图">算法可视化图&lt;/h4>
&lt;h4 id="优化器选择">优化器选择&lt;/h4>
&lt;p>那么，我们应该选择使用哪种优化算法呢？如果输入数据是稀疏的，选择任一自适应学习率算法可能会得到最好的结果。选用这类算法的另一个好处是无需调整学习率，选用默认值就可能达到最好的结果。&lt;/p>
&lt;p>总的来说，RMSprop是Adagrad的扩展形式，用于处理在Adagrad中急速递减的学习率。RMSprop与Adadelta相同，所不同的是Adadelta在更新规则中使用参数的均方根进行更新。最后，Adam是将偏差校正和动量加入到RMSprop中。在这样的情况下，RMSprop、Adadelta和Adam是很相似的算法并且在相似的环境中性能都不错。Kingma等人[9]指出在优化后期由于梯度变得越来越稀疏，偏差校正能够帮助Adam微弱地胜过RMSprop。综合看来，Adam可能是最佳的选择。&lt;/p>
&lt;p>有趣的是，最近许多论文中采用不带动量的SGD和一种简单的学习率的退火策略。已表明，通常SGD能够找到最小值点，但是比其他优化的SGD花费更多的时间，与其他算法相比，SGD更加依赖鲁棒的初始化和退火策略，同时，SGD可能会陷入鞍点，而不是局部极小值点。因此，如果你关心的是快速收敛和训练一个深层的或者复杂的神经网络，你应该选择一个自适应学习率的方法。&lt;/p>
&lt;h3 id="并行与分布式sgd">并行与分布式SGD&lt;/h3>
&lt;p>当存在大量的大规模数据和廉价的集群时，利用分布式SGD来加速是一个显然的选择。SGD本身有固有的顺序：一步一步，我们进一步进展到最小。SGD提供了良好的收敛性，但SGD的运行缓慢，特别是对于大型数据集。相反，SGD异步运行速度更快，但客户端之间非最理想的通信会导致差的收敛。此外，我们也可以在一台机器上并行SGD，这样就无需大的计算集群。以下是已经提出的优化的并行和分布式的SGD的算法和框架。&lt;/p>
&lt;h4 id="hogwild">Hogwild&lt;/h4>
&lt;p>Niu等人[14]提出称为Hogwild!的更新机制，Hogwild!允许在多个CPU上并行执行SGD更新。在无需对参数加锁的情况下，处理器可以访问共享的内存。这种方法只适用于稀疏的输入数据，因为每一次更新只会修改一部分参数。在这种情况下，该更新策略几乎可以达到一个最优的收敛速率，因为CPU之间不可能重写有用的信息。&lt;/p>
&lt;h4 id="downpour_sgd">Downpour_SGD&lt;/h4>
&lt;p>Downpour SGD是SGD的一种异步的变形形式，在Google，Dean等人[6]在他们的DistBelief框架（TensorFlow的前身）中使用了该方法。Downpour SGD在训练集的子集上并行运行多个模型的副本。这些模型将各自的更新发送给一个参数服务器，参数服务器跨越了多台机器。每一台机器负责存储和更新模型的一部分参数。然而，因为副本之间是彼此不互相通信的，即通过共享权重或者更新，因此可能会导致参数发散而不利于收敛。&lt;/p>
&lt;h4 id="delay-tolerant_algorithms_for_sgd">Delay-tolerant_Algorithms_for_SGD&lt;/h4>
&lt;p>通过容忍延迟算法的开发，McMahan和Streeter[11]将AdaGraad扩展成并行的模式，该方法不仅适应于历史梯度，同时适应于更新延迟。该方法已经在实践中被证实是有效的。&lt;/p>
&lt;h4 id="tensorflow">TensorFlow&lt;/h4>
&lt;p>TensorFlow[1]是Google近期开源的框架，该框架用于实现和部署大规模机器学习模型。TensorFlow是基于DistBelief开发，同时TensorFlow已经在内部用来在大量移动设备和大规模分布式系统的执行计算。在2016年4月发布的分布式版本依赖于图计算，图计算即是对每一个设备将图划分成多个子图，同时，通过发送、接收节点对完成节点之间的通信。&lt;/p>
&lt;h4 id="elastic_averaging_sgd">Elastic_Averaging_SGD&lt;/h4>
&lt;p>Zhang等人[22]提出的弹性平均SGD（Elastic Averaging SGD，EASGD）连接了异步SGD的参数客户端和一个弹性力，即参数服务器存储的一个中心变量。EASGD使得局部变量能够从中心变量震荡得更远，这在理论上使得在参数空间中能够得到更多的探索。经验表明这种增强的探索能力通过发现新的局部最优点，能够提高整体的性能&lt;/p>
&lt;h3 id="优化sgd的其他策略">优化SGD的其他策略&lt;/h3>
&lt;p>最后，我们介绍可以与前面提及到的任一算法配合使用的其他的一些策略，以进一步提高SGD的性能。对于其他的一些常用技巧的概述可以参见[10]。&lt;/p>
&lt;h4 id="数据集的洗牌和课程学习">数据集的洗牌和课程学习&lt;/h4>
&lt;p>总的来说，我们希望避免向我们的模型中以一定意义的顺序提供训练数据，因为这样会使得优化算法产生偏差。因此，在每一轮迭代后对训练数据洗牌是一个不错的主意。&lt;/p>
&lt;p>另一方面，在很多情况下，我们是逐步解决问题的，而将训练集按照某个有意义的顺序排列会提高模型的性能和SGD的收敛性，如何将训练集建立一个有意义的排列被称为课程学习[3]。&lt;/p>
&lt;p>Zaremba and Sutskever[20]只能使用课程学习训练LSTM来评估简单程序，并表明组合或混合策略比单一的策略更好，通过增加难度来排列示例。&lt;/p>
&lt;h4 id="批量归一化">批量归一化&lt;/h4>
&lt;p>为了便于学习，我们通常用0均值和单位方差初始化我们的参数的初始值来归一化。 随着不断训练，参数得到不同的程度的更新，我们失去了这种归一化，随着网络变得越来越深，这种现象会降低训练速度，且放大参数变化。&lt;/p>
&lt;p>批量归一化[8]在每次小批量数据反向传播之后重新对参数进行0均值单位方差标准化。通过将模型架构的一部分归一化，我们能够使用更高的学习率，更少关注初始化参数。批量归一化还充当正则化的作用，减少（有时甚至消除）Dropout的必要性。&lt;/p>
&lt;h4 id="early_stopping">Early_stopping&lt;/h4>
&lt;p>如Geoff Hinton所说：“Early Stopping是美丽好免费午餐”（NIPS 2015 Tutorial slides）。你因此必须在训练的过程中时常在验证集上监测误差，在验证集上如果损失函数不再显著地降低，那么应该提前结束训练。&lt;/p>
&lt;h4 id="梯度噪音">梯度噪音&lt;/h4>
&lt;p>Neelakantan等人[12]在每个梯度更新中增加满足高斯分布&lt;span class="math">\(N(0,σ^2_t)\)&lt;/span>的噪音： &lt;span class="math">\[g_{t,i}=g_{t,i}+N(0,σ_t^2)\]&lt;/span> 高斯分布的方差需要根据如下的策略退火： &lt;span class="math">\[σ_t^2=\frac{η}{(1+t)^γ}\]&lt;/span> 他们指出增加了噪音，使得网络对不好的初始化更加鲁棒，同时对深层的和复杂的网络的训练特别有益。他们猜测增加的噪音使得模型更优机会逃离当前的局部最优点，以发现新的局部最优点，这在更深层的模型中更加常见。&lt;/p>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>在这篇博客文章中，我们初步研究了梯度下降的三个变形形式，其中，小批量梯度下降是最受欢迎的。 然后我们研究了最常用于优化SGD的算法：动量法，Nesterov加速梯度，Adagrad，Adadelta，RMSprop，Adam以及不同的优化异步SGD的算法。 最后，我们已经考虑其他一些改善SGD的策略，如洗牌和课程学习，批量归一化和early stopping。&lt;/p></description></item><item><title>机器学习-梯度和方向导数</title><link>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E5%92%8C%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E5%92%8C%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0/</guid><description>
&lt;h2 id="梯度和方向导数">梯度和方向导数&lt;!-- omit in toc -->&lt;/h2>
&lt;p>梯度是向量，是输入空间的向量。其方向指向函数值上升最快的方向，模是值函数的陡峭程度。模越大，越陡峭。&lt;/p>
&lt;p>方向导数是标量，指函数值沿着某一方向&lt;span class="math">\(\vec{v}\)&lt;/span>的变换率。 &lt;span class="math">\[\nabla_v f(\vec{x})=\lim_{t→∞}\frac{f(\vec x+t\vec v)-f(\vec x)}{t}\]&lt;/span>&lt;/p>
&lt;p>单位长度内，&lt;strong>上升最多&lt;/strong>的方向是梯度所指的方向，梯度方向的方向导数指是梯度的模。&lt;/p>
&lt;p>沿&lt;span class="math">\(\vec v\)&lt;/span>的方向导数和梯度的关系： &lt;span class="math">\[\nabla_v f(\vec{x})=\vec{v}\cdot\nabla f(\vec{x}),\\
\nabla f(\vec{x})是点\vec x的梯度，\cdot是内积\]&lt;/span> 即使某点的梯度不存在，方向导数也可能存在。这时候可以用定义去求。&lt;/p>
&lt;p>&lt;span class="math">\(\nabla_v f，f&amp;#39;_v，f&amp;#39;(x;v)，Df_x(v)，\frac{\partial f(x)}{\partial v}\)&lt;/span>都是指方向导数，为了省事向量的符号都没有打。&lt;/p>
&lt;p>如果在方向&lt;span class="math">\(\vec v\)&lt;/span>上，方向导数小于0，那么在&lt;span class="math">\(\vec v\)&lt;/span>上总可以找到一小步&lt;span class="math">\(\bar t\)&lt;/span>，使得&lt;span class="math">\(f(\vec x+t\vec v)&amp;lt;f(\vec x)，t ∈(0,\bar t)\)&lt;/span>。当方向导数大于0，也有类似的结论。&lt;/p></description></item><item><title>数学分析之拉格朗日余项与误差</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%BD%99%E9%A1%B9%E4%B8%8E%E8%AF%AF%E5%B7%AE/</link><pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E4%B9%8B%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%BD%99%E9%A1%B9%E4%B8%8E%E8%AF%AF%E5%B7%AE/</guid><description>
&lt;h2 id="拉格朗日余项与误差">拉格朗日余项与误差&lt;!-- omit in toc -->&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#拉格朗日余项">拉格朗日余项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉格朗日误差界">拉格朗日误差界&lt;/a>&lt;/li>
&lt;li>&lt;a href="#例题">例题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#交错级数">交错级数&lt;/a>&lt;/li>
&lt;li>&lt;a href="#拉格朗日插值余项">拉格朗日插值余项&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="拉格朗日余项">拉格朗日余项&lt;/h2>
&lt;p>拉格朗日余项和泰勒公式密不可分。泰勒级数的全形如下： &lt;span class="math">\[
\begin{aligned}
f(x)&amp;amp;=f(a)+f&amp;#39;(a)(x-a)+\frac{f&amp;#39;&amp;#39;(a)}{2!}(x-a)^2+\dotsb+\frac{f^{(n)}(a)}{n!}(x-a)^n+\dotsb \\
&amp;amp;=\sum^{\infty}_{n=0}\frac{f^{(n)}(a)}{n!}(x-a)^n
\end{aligned}\]&lt;/span> 其中，前n项可以称为函数在点&lt;span class="math">\(x=a\)&lt;/span>处的&lt;span class="math">\(n\)&lt;/span>阶泰勒展开式。&lt;strong>所谓的阶，是指&lt;span class="math">\(x\)&lt;/span>的幂次；&lt;span class="math">\(n\)&lt;/span>阶就是&lt;span class="math">\(x\)&lt;/span>的最高次幂为&lt;span class="math">\(n\)&lt;/span>，而不是展开到第&lt;span class="math">\(n\)&lt;/span>项。&lt;/strong>&lt;/p>
&lt;p>但是，泰勒展开能够无限地逼近原函数，是以n趋向于无穷为前提条件的。如果我们只是将函数展开到n阶，则后面的部分（我们称为&lt;strong>余项&lt;/strong>, remainder）就被省略了，这必然会产生误差。记余项的表达式为&lt;span class="math">\(R_n(x)\)&lt;/span>，用它表示n项之后的余项（&lt;strong>注意下标是 n 而不是 n+1&lt;/strong>），我们可以将泰勒展开写成如下形式: &lt;span class="math">\[
f(x)=f(a)+f&amp;#39;(a)(x-a)+\frac{f&amp;#39;&amp;#39;(a)}{2!}(x-a)^2+\dotsb+\frac{f^{(n)}(a)}{n!}(x-a)^n+R_n(x)
\]&lt;/span> 或者说 &lt;span class="math">\[f(x)=P_n(x)+R_n(x)\]&lt;/span> 这式子的意思是原函数可以分解为两个部分，一部分是 n 阶泰勒展开，另一部分就是拉格朗日余项。那么，做一个移项，就可以有：&lt;span class="math">\(R_n(x)=f(x)-P_n(x)\)&lt;/span>。&lt;/p>
&lt;p>这就是说，用n阶幂级数来逼近原函数，其误差就是这一余项。而拉格朗日余项就是表达这些被省略部分的一个公式，我们可以用它来估计误差。为此，我们先写出函数的第 n+1 阶展开式： &lt;span class="math">\[\frac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1}\]&lt;/span> 而&lt;strong>拉格朗日余项&lt;/strong>（在具体题目中，我们也称为&lt;strong>拉格朗日误差界&lt;/strong> Lagrange Error Bound） 在形式上就类似于函数的第 n+1 阶，其表达式如下： &lt;span class="math">\[R_n(x)=\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1},c在x与a之间。\]&lt;/span> 需要注意的是，c只是在x与a之间，可能是&lt;span class="math">\(x\le c\le a\)&lt;/span>，也可能是&lt;span class="math">\(a\le c\le x\)&lt;/span>。同时，拉格朗日余项定理像微分中值定理（Mean Value Theorem）、介值定理（Intermediate Value Theorem）一样，都是存在性定理，即只能告知值的存在（我们知道有这样一个值），但不能告知其具体位置（但不知道它到底是多少）。如果我们使用麦克劳林式（Maclaurin Series，即&lt;span class="math">\(a = 0\)&lt;/span>时的特殊形式），就可以将拉格朗日余项变成较简单的形式: &lt;span class="math">\[R_n(x)=\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1},c在x与0之间。\]&lt;/span>&lt;/p>
&lt;h2 id="拉格朗日误差界">拉格朗日误差界&lt;/h2>
&lt;p>观察拉格朗日余项定理的形式，对于给定&lt;span class="math">\(a、n和x\)&lt;/span>的情况下，如果要确定这个余项的界限，唯一要确定的就是前面的这个第 n+1 阶导数的取值范围。若令有&lt;strong>正数&lt;/strong>&lt;span class="math">\(M=|f^{(n+1)}(c)|\)&lt;/span>，则只需要确定M的范围，那么，整个余项的范围也就唯一给定了。由于误差值可能为正，也可能为负，为便利讨论，我们一般都取其绝对值进行考查，即 &lt;span class="math">\[\begin{aligned}
|R_n(x)|&amp;amp;=|\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}| \\
&amp;amp;=\frac{M}{(n+1)!}|(x-a)^{n+1}|
\end{aligned}\]&lt;/span> 而对于麦克劳林式，我们就变成考查： &lt;span class="math">\[\begin{aligned}
|R_n(x)|&amp;amp;=|\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}| \\
&amp;amp;=\frac{M}{(n+1)!}|x^{n+1}|
\end{aligned}\]&lt;/span> 这时我们就要介绍一个基本定理，即拉格朗日余项定理：&lt;/p>
&lt;blockquote>
&lt;p>若存在正数Max，使得对任意&lt;span class="math">\(\xi\)&lt;/span>，如果&lt;span class="math">\(a \le \xi \le x\)&lt;/span>或&lt;span class="math">\(x \le \xi \le a\)&lt;/span>，均有&lt;span class="math">\(|f^{(n+1)}(\xi)|\le M\)&lt;/span>，则: &lt;span class="math">\[|R_n(x)|=|\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}| \le \frac{M}{(n+1)!}|(x-a)^{n+1}|\]&lt;/span> 也就是说&lt;span class="math">\(M\)&lt;/span>的取值范围其实就由&lt;span class="math">\(|f^{(n+1)}(\xi)|,(a \le \xi \le x\)&lt;/span>或&lt;span class="math">\(x \le \xi \le a)\)&lt;/span>唯一给定。这一不等式也叫&lt;strong>泰勒不等式&lt;/strong>。这是我们进行余项范围估计的主要理论基础。&lt;/p>
&lt;/blockquote>
&lt;h2 id="例题">例题&lt;/h2>
&lt;p>&lt;strong>例1&lt;/strong> 将&lt;span class="math">\(y=sin(x)\)&lt;/span>展开成麦克劳林式，并估计其误差：&lt;/p>
&lt;p>根据基本公式：&lt;span class="math">\(sin(x)=x-\frac{1}{3!}x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\dotsb+(-1)^n\frac{x^{2n+1}}{(2n+1)!}+\dotsb\)&lt;/span>，很容易得出5阶麦克劳林式：&lt;span class="math">\(sin(x)\approx x-\frac{1}{3!}x^3+\frac{1}{5!}x^5\)&lt;/span>&lt;/p>
&lt;p>其余项按照拉格朗日余项定理，可以写成（注意：&lt;strong>5阶的余项是6次方&lt;/strong>）： &lt;span class="math">\[R_5(x)=\frac{f^6(c)}{6!}x^6,c在0和x之间\]&lt;/span> 而&lt;span class="math">\(|f^6(x)|=|-sin(x)| \le 1\)&lt;/span>，所以&lt;span class="math">\(R_5(x) \le \frac{x^6}{6!}\)&lt;/span>。&lt;/p>
&lt;p>&lt;strong>例2&lt;/strong> 将&lt;span class="math">\(y=sin(x)\)&lt;/span>展开成麦克劳林式，据此估计sin 0.2的值，并计算其误差。&lt;/p>
&lt;p>使用例1结论：&lt;span class="math">\(sin(0.2)\approx 0.2-\frac{1}{3!}(0.2)^3+\frac{1}{5!}(0.2)^5 \approx 0.1986693333\)&lt;/span>&lt;/p>
&lt;p>同时，&lt;span class="math">\(R_5(0.2) \le \frac{0.2^6}{6!}=8.888889*10^{-8}\)&lt;/span>&lt;/p>
&lt;h2 id="交错级数">交错级数&lt;/h2>
&lt;p>在一些情况下，使用交错级数计算误差相对简单，其基本定理如下：&lt;/p>
&lt;blockquote>
&lt;p>若交错级数&lt;span class="math">\(\sum^{\infty}\limits_{n=0}u_n=\sum^{\infty}\limits_{n=0}(-1)^{n+1}(v_{n})\)&lt;/span>满足以下条件：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>正项递减，即&lt;span class="math">\(0\le v_{n+1} \le v_n;\)&lt;/span>&lt;/li>
&lt;li>正项趋零，即&lt;span class="math">\(\lim\limits_{x \to +\infty}v_n=0\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>则必有其余项的范围&lt;span class="math">\(|R_n(x)|=|S-S_n|&amp;lt;|u_{n+1}|=v_{n+1}\)&lt;/span>&lt;/p>
&lt;/blockquote>
&lt;p>其中，S表示全部和，&lt;span class="math">\(S_n\)&lt;/span>表示部分和，也就是我们高中数学里习惯说的前n项和。简单的讲，&lt;strong>交错级数前n项和的余项，不会超过接下来这一项的绝对值，这就是交错级数的余项公式。&lt;/strong>&lt;/p>
&lt;h2 id="拉格朗日插值余项">拉格朗日插值余项&lt;/h2>
&lt;p>在插值过程中，有一个拓展概念叫拉格朗日插值余项。取插值结点：&lt;span class="math">\(a≤x0&amp;lt;x1&amp;lt;······&amp;lt;xn≤b\)&lt;/span> 满足&lt;span class="math">\(L_n(x_k)=f(x_k)\)&lt;/span>的 n 次多项式插值余项： &lt;span class="math">\[R_n(x)=f(x)-L_n(x) \\
=\frac{f^{(n+1)}(\xi_n)}{(n+1)!}\omega_{n+1}(x)\]&lt;/span> 其中，&lt;span class="math">\(\omega_{n+1}(x)=(x-x_0)(x-x_1)\dotsb (x-x_n)\)&lt;/span>。选取：&lt;span class="math">\(x_0,x_1,\dotsb,x_n\)&lt;/span>，使&lt;span class="math">\(\underset{a\le x \le b}{max}|\omega_{n+1}(x)|=min\)&lt;/span>&lt;/p>
&lt;p>结论:选取切比雪夫多项式&lt;span class="math">\(T_{n+1}(x)\)&lt;/span>的全部零点。&lt;/p>
&lt;p>使用切比雪夫节点插值和等距插值对函数&lt;span class="math">\(f(x)=\frac{1}{1+x^2}\)&lt;/span>模拟：&lt;/p>
&lt;img src="../images/chebyshev_point.bmp" alt="切比雪夫结点插值" />
&lt;center>
插值函数L10(x)取切比雪夫结点插值
&lt;/center>
&lt;img src="../images/equal_distance_point.png" alt="等距结点插值" />
&lt;center>
插值函数L10(x)取等距结点插值
&lt;/center></description></item><item><title>离散数学-代数群环域</title><link>https://surprisedcat.github.io/studynotes/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6-%E4%BB%A3%E6%95%B0%E7%BE%A4%E7%8E%AF%E5%9F%9F/</link><pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6-%E4%BB%A3%E6%95%B0%E7%BE%A4%E7%8E%AF%E5%9F%9F/</guid><description>
&lt;h2 id="代数中的群环域">代数中的群、环、域&lt;!-- omit in toc -->&lt;/h2>
&lt;p>转载自&lt;a href="https://yunhao.space/2018/11/07/group-ring-field-in-mathematics/">https://yunhao.space/2018/11/07/group-ring-field-in-mathematics/&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#引例整数加法群">引例：整数加法群&lt;/a>&lt;/li>
&lt;li>&lt;a href="#群">群&lt;/a>&lt;/li>
&lt;li>&lt;a href="#群的定义">群的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#群的四个公理">群的四个公理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#只满足前两个条件封闭性结合律rightarrow半群">只满足前两个条件，封闭性，结合律&lt;span class="math">\(\Rightarrow\)&lt;/span>半群&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可交换群rightarrow阿贝尔群">可交换群&lt;span class="math">\(\Rightarrow\)&lt;/span>阿贝尔群&lt;/a>&lt;/li>
&lt;li>&lt;a href="#环">环&lt;/a>&lt;/li>
&lt;li>&lt;a href="#环的定义">环的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#可交换环">可交换环&lt;/a>&lt;/li>
&lt;li>&lt;a href="#除环阿贝尔群with群">除环(阿贝尔群with群)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#域">域&lt;/a>&lt;/li>
&lt;li>&lt;a href="#域的定义">域的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="#有限域或伽罗瓦域">有限域或伽罗瓦域&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="figure">
&lt;img src="../images/群环域.jpg" alt="群环域的管旭" />&lt;p class="caption">群环域的管旭&lt;/p>
&lt;/div>
&lt;h2 id="引例整数加法群">引例：整数加法群&lt;/h2>
&lt;p>最常见的群之一是整数集&lt;span class="math">\(\mathbb{Z}\)&lt;/span>和整数的加法所构成的群。它由以下数列组成： &lt;span class="math">\[…,−4,−3,−2,−1,0,1,2,3,4,…\]&lt;/span> 群有四个公理。以上面的加法群为例。如下：&lt;/p>
&lt;ul>
&lt;li>封闭性： 对于任何两个整数a和b，它们的和a+b也是整数。换句话说，在任何时候，把两个整数相加都能得出整数的结果。这个性质叫做在加法下封闭。&lt;/li>
&lt;li>结合律： 对于任何整数a, b和c，(a+b)+c=a+（b+c）。用话语来表达，先把a加到b，然后把它们的和加到c，所得到的结果与把a加到b与c的和是相等的。这个性质叫做结合律。&lt;/li>
&lt;li>单位元： 如果a是任何整数，那么0+a=a+0=a。零叫做加法的单位元，因为把它加到任何整数都得到相同的整数。&lt;/li>
&lt;li>逆元： 对于任何整数a，存在另一个整数b使得a+b=b+a=0。整数b叫做整数a的逆元，记为−a。&lt;/li>
&lt;/ul>
&lt;h2 id="群">群&lt;/h2>
&lt;h3 id="群的定义">群的定义&lt;/h3>
&lt;p>群&lt;span class="math">\((G,⋅)\)&lt;/span>是由集合&lt;span class="math">\(G\)&lt;/span>和二元运算&lt;span class="math">\(&amp;quot;⋅&amp;quot;\)&lt;/span>构成的，符合以下四个性质（称“群公理”）的数学结构。其中，二元运算结合任何两个元素&lt;span class="math">\(a和b\)&lt;/span>而形成另一个元素，记为&lt;span class="math">\(a⋅b\)&lt;/span>，符号&lt;span class="math">\(&amp;quot;⋅&amp;quot;\)&lt;/span>是具体的运算，比如整数加法。&lt;/p>
&lt;h3 id="群的四个公理">群的四个公理&lt;/h3>
&lt;ul>
&lt;li>封闭性： 对于所有G中a, b，运算a⋅b的结果也在G中。&lt;/li>
&lt;li>结合律： 对于所有G中的a, b和c，等式(a⋅b)⋅c=a⋅(b⋅c)成立。&lt;/li>
&lt;li>单位元： 存在G中的一个元素e，使得对于所有G中的元素a，总有等式e⋅a=a⋅e=a成立。&lt;/li>
&lt;li>逆元： 对于每个G中的a，存在G中的一个元素b使得总有a⋅b=b⋅a=e，此处e为单位元。&lt;/li>
&lt;/ul>
&lt;h3 id="只满足前两个条件封闭性结合律rightarrow半群">只满足前两个条件，封闭性，结合律&lt;span class="math">\(\Rightarrow\)&lt;/span>半群&lt;/h3>
&lt;p>半群的运算经常指示为乘号。&lt;/p>
&lt;p>集合&lt;span class="math">\(S\)&lt;/span>和其上的二元运算&lt;span class="math">\(⋅: S×S→S\)&lt;/span>。若&lt;span class="math">\(⋅\)&lt;/span>满足结合律，即：&lt;span class="math">\(∀x,y,z∈S，有(x⋅y)⋅z=x⋅(y⋅z)\)&lt;/span>，则称&lt;strong>有序对&lt;/strong>&lt;span class="math">\((S,⋅)\)&lt;/span>为半群，运算&lt;span class="math">\(⋅\)&lt;/span>称为该半群的乘法。 即半群只满足群的四个公理中的&lt;strong>封闭性和结合律&lt;/strong>。&lt;/p>
&lt;h3 id="可交换群rightarrow阿贝尔群">可交换群&lt;span class="math">\(\Rightarrow\)&lt;/span>阿贝尔群&lt;/h3>
&lt;p>群运算的次序很重要，把元素&lt;span class="math">\(a\)&lt;/span>与元素&lt;span class="math">\(b\)&lt;/span>结合，所得到的结果不一定与把元素&lt;span class="math">\(b\)&lt;/span>与元素&lt;span class="math">\(a\)&lt;/span>结合相同；&lt;span class="math">\(a⋅b=b⋅a\)&lt;/span>(交换律)不一定恒成立。&lt;/p>
&lt;p>阿贝尔群的群运算&lt;strong>符合交换律&lt;/strong>，因此阿贝尔群也被称为&lt;strong>交换群&lt;/strong>。它由自身的集合&lt;span class="math">\(G\)&lt;/span>和二元运算&lt;span class="math">\(∗\)&lt;/span>构成。它除了满足一般的群公理，封闭性、结合律、单位元、逆元之外，还满足 &lt;span class="math">\[a∗b=b∗a\]&lt;/span> 因为阿贝尔群的群运算满足交换律和结合律，群元素乘积的值与乘法运算时的次序无关。&lt;/p>
&lt;p>群运算不满足交换律的群被称为“非阿贝尔群”，或“非交换群”。&lt;/p>
&lt;p>阿贝尔群有两种主要运算符号,加法和乘法。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">约定&lt;/th>
&lt;th align="left">运算&lt;/th>
&lt;th align="left">单位元&lt;/th>
&lt;th align="left">幂&lt;/th>
&lt;th align="left">逆元&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">加法运算&lt;/td>
&lt;td align="left">&lt;span class="math">\(x+y\)&lt;/span>&lt;/td>
&lt;td align="left">0&lt;/td>
&lt;td align="left">&lt;span class="math">\(nx\)&lt;/span>&lt;/td>
&lt;td align="left">&lt;span class="math">\(−x\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">乘法运算&lt;/td>
&lt;td align="left">&lt;span class="math">\(x∗y或xy\)&lt;/span>&lt;/td>
&lt;td align="left">e或1&lt;/td>
&lt;td align="left">&lt;span class="math">\(x^n\)&lt;/span>&lt;/td>
&lt;td align="left">&lt;span class="math">\(x^{−1}\)&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="环">环&lt;/h2>
&lt;h3 id="环的定义">环的定义&lt;/h3>
&lt;p>集合R和定义于其上的二元运算&lt;span class="math">\(+和⋅，(R,+,⋅)\)&lt;/span>构成一个环，若它们满足：&lt;/p>
&lt;ul>
&lt;li>&lt;span class="math">\((R,+)\)&lt;/span>形成一个交换群(阿贝尔群)，其单位元称为零元，记作0。即： 封闭性：&lt;span class="math">\((R,+)\)&lt;/span>是封闭的 结合律：&lt;span class="math">\((a+b)+c=a+(b+c)\)&lt;/span> 单位元：&lt;span class="math">\(0+a=a+0=a\)&lt;/span> 逆元：&lt;span class="math">\(∀a,∃−a, 满足a+−a=−a+a=0\)&lt;/span> 交换律： (a+b)=(b+a)&lt;/li>
&lt;li>&lt;span class="math">\((R,⋅)\)&lt;/span>形成一个半群。即： 封闭性：&lt;span class="math">\((R,⋅)\)&lt;/span>是封闭的 结合律：&lt;span class="math">\((a⋅b)⋅c=a⋅(b⋅c)\)&lt;/span>&lt;/li>
&lt;li>乘法关于加法满足分配律。即：&lt;span class="math">\(a⋅(b+c)=(a⋅b)+(a⋅c)\qquad(a+b)⋅c=(a⋅c)+(b⋅c)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>就是一个交换群和一个半群的结合。环在群的基础上限制更加严格了一些。&lt;/p>
&lt;p>以&lt;strong>矩阵加法或矩阵乘法&lt;/strong>为运算，所有于一环内n×n矩阵所组成的集合，为环。&lt;/p>
&lt;h3 id="可交换环">可交换环&lt;/h3>
&lt;p>&lt;span class="math">\((R,+)\)&lt;/span>形成一个交换群(阿贝尔群)， &lt;span class="math">\((R,⋅)\)&lt;/span>形成一个半群，且满足交换率。多了一个&lt;span class="math">\(&amp;quot;⋅&amp;quot;\)&lt;/span>满足交换律的条件。&lt;/p>
&lt;h3 id="除环阿贝尔群with群">除环(阿贝尔群with群)&lt;/h3>
&lt;p>除环（division ring），又译反对称体（skew field），是一类特殊的环，在环内&lt;strong>除法运算有效&lt;/strong>。除环内&lt;strong>必有非0元素&lt;/strong>，且环内所有的非0量都有对应的倒数（比如说，对于&lt;span class="math">\(x\)&lt;/span>来说，存在数&lt;span class="math">\(a\)&lt;/span>，使得&lt;span class="math">\(a⋅x=x⋅a=1\)&lt;/span>）。除环不一定是交换环。&lt;/p>
&lt;p>换种说法，一个环是除环当且仅当其可逆元群包含了环中所有的非零元素。&lt;/p>
&lt;p>也就是说，&lt;span class="math">\((R,⋅)\)&lt;/span>有&lt;strong>乘法单位元&lt;/strong>，并且每个非零元素都有对应的&lt;strong>乘法逆元&lt;/strong>。即，在环的基础上要求&lt;span class="math">\((R,⋅)\)&lt;/span>是群。&lt;/p>
&lt;h2 id="域">域&lt;/h2>
&lt;h3 id="域的定义">域的定义&lt;/h3>
&lt;p>域是个集合&lt;span class="math">\(F\)&lt;/span>且带有&lt;strong>加法和乘法&lt;/strong>两种运算，这里“运算”可以想成是种映射，对任意两元素&lt;span class="math">\(a,b∈F\)&lt;/span>，这映射将此两元素对应到某元素，且这些运算满足如下性质：&lt;/p>
&lt;ul>
&lt;li>封闭性：&lt;/li>
&lt;li>在加法运算上封闭，对所有属于&lt;span class="math">\(F的 a,b， a+b属于F\)&lt;/span>&lt;/li>
&lt;li>在乘法运算上封闭，对所有属于&lt;span class="math">\(F的 a,b， a∗b属于F\)&lt;/span>&lt;/li>
&lt;li>结合律：&lt;/li>
&lt;li>加法有结合律，对所有属于F的&lt;span class="math">\(a,b,c， (a+b)+c=a+(b+c)\)&lt;/span>&lt;/li>
&lt;li>乘法有结合律，对所有属于F的&lt;span class="math">\(a,b,c， (a∗b)∗c=a∗(b∗c)\)&lt;/span>&lt;/li>
&lt;li>单位元：&lt;/li>
&lt;li>加法单位元，在F中有元素0，使得所有&lt;span class="math">\(a∈F，a+0=a\)&lt;/span>&lt;/li>
&lt;li>乘法单位元，在F中有元素1，使得所有&lt;span class="math">\(a∈F，a∗1=a\)&lt;/span>&lt;/li>
&lt;li>加法单位元0&lt;strong>不等于&lt;/strong>乘法单位元1&lt;/li>
&lt;li>逆元：&lt;/li>
&lt;li>加法逆元，对所有属于&lt;span class="math">\(F的 a，存在 −a 使得 a+(−a)=0\)&lt;/span>&lt;/li>
&lt;li>乘法逆元，对所有属于&lt;span class="math">\(F的 a，且a≠0，存在 a^{−1} 使得 a∗a^{−1}=1\)&lt;/span>&lt;/li>
&lt;li>交换律：&lt;/li>
&lt;li>加法交换律，对所有属于F的&lt;span class="math">\(a,b， a+b=b+a\)&lt;/span>&lt;/li>
&lt;li>乘法交换律，对所有属于F的&lt;span class="math">\(a,b， a∗b=b∗a\)&lt;/span>&lt;/li>
&lt;li>分配律：&lt;/li>
&lt;li>对所有属于F的&lt;span class="math">\(a,b,c，有a⋅(b+c)=(a⋅b)+(a⋅c)\)&lt;/span>&lt;/li>
&lt;li>对所有属于F的&lt;span class="math">\(a,b,c，有(a+b)⋅c=(a⋅c)+(b⋅c)\)&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>其中&lt;span class="math">\(0≠1\)&lt;/span>的要求排除了平凡的只由一个元素组成的域。&lt;/p>
&lt;p>通过群和环的概念我们可以得到简化定义：&lt;/p>
&lt;ul>
&lt;li>域是交换性除环。&lt;/li>
&lt;li>域是一种交换环&lt;span class="math">\((F,+,∗)\)&lt;/span>，当中加法单位元0不等于乘法单位元1，且所有&lt;strong>非0元素&lt;/strong>有乘法逆元。&lt;/li>
&lt;li>域是两个阿贝尔群，乘法对加法满足分配律。&lt;/li>
&lt;/ul>
&lt;p>由以上性质可以得出一些最基本的推论：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;span class="math">\(−(a∗b)=(−a)∗b=a∗(−b)\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(a∗0=0\)&lt;/span>&lt;/li>
&lt;li>&lt;span class="math">\(如果a∗b=0，则要么a=0，要么b=0\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;p>域在环的基础上限制更加严格了一些。&lt;/p>
&lt;h3 id="有限域或伽罗瓦域">有限域或伽罗瓦域&lt;/h3>
&lt;p>包含有限个元素的域称为有限域或伽罗瓦域。有限域最常见的例子是当 p 为素数时，整数对 p 取模。有限域的元素个数称为它的阶。&lt;/p></description></item><item><title>数学-方差标准差均方根均方</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6-%E6%96%B9%E5%B7%AE%E6%A0%87%E5%87%86%E5%B7%AE%E5%9D%87%E6%96%B9%E6%A0%B9%E5%9D%87%E6%96%B9/</link><pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6-%E6%96%B9%E5%B7%AE%E6%A0%87%E5%87%86%E5%B7%AE%E5%9D%87%E6%96%B9%E6%A0%B9%E5%9D%87%E6%96%B9/</guid><description>
&lt;h2 id="统计估计概念方差标准差均方误差均方根误差平均绝对误差协方差等等">统计/估计概念（方差、标准差、均方误差、均方根误差、平均绝对误差、协方差等等）&lt;!-- omit in toc -->&lt;/h2>
&lt;p>说明：&lt;/p>
&lt;ul>
&lt;li>均值&lt;span class="math">\(\mu = E(X)\)&lt;/span>&lt;/li>
&lt;li>样本均值&lt;span class="math">\(\bar{\mu} = \frac{1}{N}\sum^N_{n=1}{x_n}(N个样本的均值)\)&lt;/span>&lt;/li>
&lt;li>有偏估计：由样本值求得的估计值与待估参数的真值之间有系统误差，其期望值不是待估参数的真值。&lt;/li>
&lt;li>无偏估计：估计量的数学期望等于被估计参数的真实值。&lt;/li>
&lt;li>补充：范数是一种定义在向量或矩阵上的“距离”，表格中所列的是向量范式定义。&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">名称&lt;/th>
&lt;th align="center">英文名&lt;/th>
&lt;th align="center">公式&lt;/th>
&lt;th align="center">意义&lt;/th>
&lt;th align="center">注释&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">方差&lt;/td>
&lt;td align="center">VAR&lt;/td>
&lt;td align="center">&lt;span class="math">\(D(x)=E\{\sum[X-E(X)]^2\}\)&lt;/span>&lt;/td>
&lt;td align="center">随机变量或统计数据与均值的偏离程度&lt;/td>
&lt;td align="center">描述数据集本身性质&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">样本方差&lt;/td>
&lt;td align="center">Sample VAR&lt;/td>
&lt;td align="center">&lt;span class="math">\(s^2=\frac{1}{N-1}\sum^N_{n=1}{(x_n-\bar{x})^2}\)&lt;/span>，其中&lt;span class="math">\(\bar{x}\)&lt;/span>为样本均值&lt;/td>
&lt;td align="center">依据所给样本对随机变量的方差做出的一个估计&lt;/td>
&lt;td align="center">底数&lt;span class="math">\(N-1\)&lt;/span>，无偏估计，统计性质，大于总体方差&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">总体方差&lt;/td>
&lt;td align="center">Population VAR&lt;/td>
&lt;td align="center">&lt;span class="math">\(s^2_p=\frac{1}{N}\sum^N_{n=1}{(x_n-\mu)^2}\)&lt;/span>，其中&lt;span class="math">\(\mu\)&lt;/span>为总体均值&lt;/td>
&lt;td align="center">依据所给样本对随机变量的方差做出的一个估计&lt;/td>
&lt;td align="center">底数&lt;span class="math">\(N\)&lt;/span>，&lt;strong>有偏估计&lt;/strong>，统计性质，小于样本方差&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">标准差&lt;/td>
&lt;td align="center">SD&lt;/td>
&lt;td align="center">&lt;span class="math">\(\sigma=\sqrt{D(X)}\)&lt;/span>&lt;/td>
&lt;td align="center">反映组内个体间的离散程度,量纲与统计对象相同&lt;/td>
&lt;td align="center">数据集本身性质&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">均方差&lt;/td>
&lt;td align="center">SD&lt;/td>
&lt;td align="center">&lt;span class="math">\(\sigma=\sqrt{D(X)}\)&lt;/span>&lt;/td>
&lt;td align="center">反映组内个体间的离散程度,量纲与统计对象相同&lt;/td>
&lt;td align="center">&lt;strong>就是标准差&lt;/strong>，数据集本身性质&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">均方误差&lt;/td>
&lt;td align="center">MSE&lt;/td>
&lt;td align="center">&lt;span class="math">\(MSE(\hat{x})=\frac{1}{N}\sum^N_{n=1}{(x_n-\mu)^2}\)&lt;/span>&lt;/td>
&lt;td align="center">它是“误差”的平方的期望值。误差就是估计值与被估计量的差。&lt;/td>
&lt;td align="center">估计（预测）的性质&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">均方根误差&lt;/td>
&lt;td align="center">RMSE&lt;/td>
&lt;td align="center">&lt;span class="math">\(RMSE(\hat{x})=\sqrt{MSE(\hat{x}_n})\)&lt;/span>&lt;/td>
&lt;td align="center">代表预测的值和观察到的值之差的样本标准差&lt;/td>
&lt;td align="center">估计（预测）的性质&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">均方根值&lt;/td>
&lt;td align="center">RMS&lt;/td>
&lt;td align="center">&lt;span class="math">\(RMS=\sqrt{\frac{\sum^N_{n=1}x^2_n}{N}}\)&lt;/span>&lt;/td>
&lt;td align="center">在数据统计分析中，将所有值平方求和，求其均值，再开平方&lt;/td>
&lt;td align="center">又称有效值，防止正负数平均后减小，例如计算交流电功率，统计性质&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">平均绝对误差&lt;/td>
&lt;td align="center">MAE&lt;/td>
&lt;td align="center">&lt;span class="math">\(MAE(\hat{x})=\frac{1}{N}\sum^N_{n=1}{\vert x_n-\mu \vert}\)&lt;/span>&lt;/td>
&lt;td align="center">所有单个观测值与算术平均值的偏差的绝对值的平均&lt;/td>
&lt;td align="center">防止正负值抵消，估计(预测)性质&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">平均绝对百分比误差&lt;/td>
&lt;td align="center">MAPE&lt;/td>
&lt;td align="center">&lt;span class="math">\(MAPE(\hat{x})=\frac{100\%}{N}\sum^N_{n=1}{\vert \frac{x_n-\mu}{x_n}\vert}\)&lt;/span>&lt;/td>
&lt;td align="center">MAE指标的百分比化。MAPE为0%表示完美模型，MAPE大于100 %则表示劣质模型。当真实值有数据等于0时，存在分母0除问题，该公式不可用！&lt;/td>
&lt;td align="center">平均绝对值误差的相对化，剔除数值范围影响，估计(预测)性质&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">协方差&lt;/td>
&lt;td align="center">Covariance&lt;/td>
&lt;td align="center">&lt;span class="math">\(cov(X,Y)=E\{{[X-E(X)][Y-E(Y)]\}}\)&lt;/span>&lt;/td>
&lt;td align="center">协方差表示的是两个变量的总体的误差。如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。如果X 与Y 是统计独立的，那么二者之间的协方差就是0（反之不成立）&lt;/td>
&lt;td align="center">衡量两个变量之间的&lt;strong>线性关系&lt;/strong>，同增同减正相关，一增一减负相关，0是不相关&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(L1\)&lt;/span>范数&lt;/td>
&lt;td align="center">L1 Norm&lt;/td>
&lt;td align="center">&lt;span class="math">\(\|x\|_1=\sum^N_{n=1}\vert x_n \vert\)&lt;/span>&lt;/td>
&lt;td align="center">向量或矩阵元素的绝对值之和&lt;/td>
&lt;td align="center">用于向量或矩阵，曼哈顿距离、最小绝对误差等，属于L-P范数&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">&lt;span class="math">\(L2\)&lt;/span>范数&lt;/td>
&lt;td align="center">L2 Norm&lt;/td>
&lt;td align="center">&lt;span class="math">\(\|x\|_2=\sqrt{\sum^N_{n=1}x^2_n}\)&lt;/span>&lt;/td>
&lt;td align="center">向量或矩阵元素的平方和再开平方&lt;/td>
&lt;td align="center">用于向量或矩阵，欧氏距离，属于L-P范数&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">&lt;span class="math">\(L\infty\)&lt;/span>范数&lt;/td>
&lt;td align="center">&lt;span class="math">\(L\infty\)&lt;/span> Norm&lt;/td>
&lt;td align="center">&lt;span class="math">\(\|x\|_\infty=\sqrt[\infty]{\sum^N_{n=1}x^\infty_n}=max(X)\)&lt;/span>&lt;/td>
&lt;td align="center">主要被用来度量所有元素的最大值&lt;/td>
&lt;td align="center">用于向量或矩阵，属于L-P范数&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>数学分析-四次阶乘</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E5%9B%9B%E6%AC%A1%E9%98%B6%E4%B9%98/</link><pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E5%9B%9B%E6%AC%A1%E9%98%B6%E4%B9%98/</guid><description>
&lt;h2 id="四次阶乘">四次阶乘&lt;!-- omit in toc -->&lt;/h2>
&lt;p>所谓的四次阶乘（又称四重阶乘） 不是&lt;span class="math">\(n!^{(4)}\)&lt;/span>，而是&lt;span class="math">\((2n)!/n!\)&lt;/span>,前几个四次阶乘为 &lt;span class="math">\[1, 2, 12, 120, 1680, 30240, 665280, ....\]&lt;/span> 它也等于 &lt;span class="math">\[{\begin{aligned}
2^{n}{\frac {(2n)!}{n!2^{n}}}&amp;amp;=2^{n}{\frac {[2\cdot 4\cdots 2n](1\cdot 3\cdots (2n-1))}{2\cdot 4\cdots 2n}}\\
&amp;amp;=(1\cdot 2)\cdot (3\cdot 2)\cdots [(2n-1)\cdot 2]=(4n-2)!^{{(4)}}.\end{aligned}}\]&lt;/span>&lt;/p></description></item><item><title>数学分析-数学中几个点（驻点，极值点，鞍点，拐点）</title><link>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E6%95%B0%E5%AD%A6%E4%B8%AD%E5%87%A0%E4%B8%AA%E7%82%B9%E9%A9%BB%E7%82%B9%E6%9E%81%E5%80%BC%E7%82%B9%E9%9E%8D%E7%82%B9%E6%8B%90%E7%82%B9/</link><pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90-%E6%95%B0%E5%AD%A6%E4%B8%AD%E5%87%A0%E4%B8%AA%E7%82%B9%E9%A9%BB%E7%82%B9%E6%9E%81%E5%80%BC%E7%82%B9%E9%9E%8D%E7%82%B9%E6%8B%90%E7%82%B9/</guid><description>
&lt;h2 id="数学中几个点驻点极值点鞍点拐点">数学中几个点（驻点，极值点，鞍点，拐点）&lt;!-- omit in toc -->&lt;/h2>
&lt;p>函数的导数导致了图像中出现了几种点，现在详细理一下。&lt;/p>
&lt;h2 id="驻点">驻点&lt;/h2>
&lt;p>在数学，特别在微积分，函数在一点处的&lt;strong>一阶导数为零&lt;/strong>，该点即函数的&lt;strong>驻点&lt;/strong>（Stationary Point）或&lt;strong>稳定点&lt;/strong>，也就是说若p为驻点则 &lt;span class="math">\[\frac{dy}{dx}\biggm\vert_p=0\]&lt;/span>&lt;/p>
&lt;p>在这一点，函数的输出值停止增加或减少。驻点有一个&lt;strong>先决条件&lt;/strong>，就是函数在这个点可微（可导）。在此之上，只有一个条件，即一阶导数为零。&lt;/p>
&lt;h2 id="极值点">极值点&lt;/h2>
&lt;p>在数学中，极大值与极小值（又被称为极值）是指在一个域（邻域或定义域）上函数取得最大值（或最小值）的点的函数值。而使函数取得极值的点（的横坐标）被称作极值点。&lt;/p>
&lt;p>值得注意的是，一个函数的&lt;strong>驻点不一定是这个函数的极值点&lt;/strong>（考虑到这一点左右一阶导数符号不改变的情况）；反过来，在某设定区域内，一个函数的&lt;strong>极值点也不一定是这个函数的驻点&lt;/strong>（考虑到边界条件）或者不可微点。对于可微函数，&lt;strong>极值点一定是驻点。&lt;/strong>&lt;/p>
&lt;h2 id="鞍点">鞍点&lt;/h2>
&lt;p>一个&lt;strong>不是局部极值点的驻点&lt;/strong>称为鞍点。驻点--》鞍点 思考一个只有一个变数的函数。这函数在鞍点的一次导数等于零，二次导数换正负符号·例如，函数 &lt;span class="math">\[y=x^3\]&lt;/span> 就有一个鞍点在原点。&lt;/p>
&lt;p>思考一个拥有两个以上变数的函数。它的曲面在鞍点好像一个马鞍，在某些方向往上曲，在其他方向往下曲。在一幅等高线图里，一般来说，当两个等高线圈圈相交叉的地点，就是鞍点。例如，两座山中间的山口就是一个鞍点&lt;/p>
&lt;h2 id="拐点">拐点&lt;/h2>
&lt;p>设&lt;span class="math">\(f(x)\)&lt;/span>在（a，b）内，二阶可导，在&lt;span class="math">\(x_0\)&lt;/span>处的二阶导数为零 &lt;span class="math">\[f&amp;#39;&amp;#39;(x_0)=0\]&lt;/span> 若在&lt;span class="math">\(x_0\)&lt;/span>两侧附近，&lt;span class="math">\(f&amp;#39;&amp;#39;(x)\)&lt;/span>异号，则点&lt;span class="math">\((x_0，f(x_0))\)&lt;/span>为曲线的拐点。否则（保持同号），不是拐点。&lt;/p>
&lt;img src="../images/points_in_math.png" alt="数学中的各种点" />
&lt;center>
图1 数学中的各种点
&lt;/center></description></item><item><title>读书笔记之小白统计学</title><link>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%B0%8F%E7%99%BD%E7%BB%9F%E8%AE%A1%E5%AD%A6/</link><pubDate>Sat, 23 Nov 1991 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%B0%8F%E7%99%BD%E7%BB%9F%E8%AE%A1%E5%AD%A6/</guid><description>
&lt;h2 id="读书笔记之小白统计学">读书笔记之小白统计学&lt;!-- omit in toc -->&lt;/h2>
&lt;p>本篇为读书笔记，内容为微信公众号：小白统计学：以通俗易懂的语言介绍并推广统计学，让即使完全不懂统计的小白也能够看懂。公众号简介：&lt;/p>
&lt;p>公众号：stats_for_dummy。文章汇总&lt;a href="https://mp.weixin.qq.com/s/fvyKnVZ1G6sBVFNBl2abAA">https://mp.weixin.qq.com/s/fvyKnVZ1G6sBVFNBl2abAA&lt;/a>&lt;/p>
&lt;p>该公众平台不是以盈利为主，旨在推广医学统计学，让众多的统计小白能够真正了解统计学。下面是关于本平台的简单介绍：&lt;/p>
&lt;p>（1）所有文章均为作者原创，可能有的文章的部分内容作者在其它地方也曾发表过，但都是作者自己的原创内容。如果摘录、引用等请注明出处，尊重作者版权。&lt;/p>
&lt;p>（2）由于文章主要走的是通俗、浅显的路子，因此有的文章中的一些概念可能通俗有余，严谨不足。如果想了解对某些概念的严谨定义，请参考相应的统计学教材。&lt;/p>
&lt;p>（3）本平台只是抛砖引玉，将作者多年对统计的理解以通俗的形式表达出来，面向对象主要是对统计感兴趣的各位同道，希望达到相互交流的目的。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#概念和观点理解">概念和观点理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#小白学统计系列之三p值到底是个什么东西">小白学统计系列之三：p值到底是个什么东西&lt;/a>&lt;/li>
&lt;li>&lt;a href="#从女士品茶到假设检验">从“女士品茶”到假设检验&lt;/a>&lt;/li>
&lt;li>&lt;a href="#小白学统计系列之五法官的艰难抉择统计学中的两类错误">小白学统计系列之五：法官的艰难抉择——统计学中的两类错误&lt;/a>&lt;/li>
&lt;li>&lt;a href="#貌合神离的标准差与标准误">貌合神离的标准差与标准误&lt;/a>&lt;/li>
&lt;li>&lt;a href="#p005真的值得庆贺吗兼谈置信区间">P&amp;lt;0.05真的值得庆贺吗？——兼谈置信区间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#正态分布的理解">正态分布的理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分类资料与计数资料">分类资料与计数资料&lt;/a>&lt;/li>
&lt;li>&lt;a href="#计数资料离散资料和连续资料">计数资料、离散资料和连续资料&lt;/a>&lt;/li>
&lt;li>&lt;a href="#什么是虚拟变量应用中应注意什么问题">什么是虚拟变量，应用中应注意什么问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于抽样误差">关于抽样误差&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量大于30就算正态了吗">样本量大于30就算正态了吗？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#判断正态性的一些简易方法">判断正态性的一些简易方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多变量与多因素的区别">多变量与多因素的区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#信度与效度评价">信度与效度评价&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#问卷的信度和效度评价">问卷的信度和效度评价&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;a href="#问卷用不用做信度和效度评价">问卷用不用做信度和效度评价&lt;/a>&lt;/li>
&lt;li>&lt;a href="#传染病模型中的拐点">传染病模型中的拐点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一些基础统计方法">一些基础统计方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#定量资料的组间比较">定量资料的组间比较&lt;/a>&lt;/li>
&lt;li>&lt;a href="#分类资料的组间比较">分类资料的组间比较&lt;/a>&lt;/li>
&lt;li>&lt;a href="#t检验应用的注意事项">t检验应用的注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析的变异分解思想">方差分析的变异分解思想&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析的一些特殊情况">方差分析的一些特殊情况&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析后的两两比较方法选择">方差分析后的两两比较方法选择&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析的应用注意事项">方差分析的应用注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#方差分析中的随机效应与固定效应">方差分析中的随机效应与固定效应&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多重校正的fdr方法">多重校正的FDR方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#卡方检验的注意事项">卡方检验的注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#卡方检验等于单因素logistic回归吗">卡方检验等于单因素logistic回归吗&lt;/a>&lt;/li>
&lt;li>&lt;a href="#协方差与相关系数的关系">协方差与相关系数的关系&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相关分析的注意事项">相关分析的注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#临床常见的疑惑解答之一">临床常见的疑惑解答之一&lt;/a>&lt;/li>
&lt;li>&lt;a href="#临床常见的疑惑解答之二">临床常见的疑惑解答之二&lt;/a>&lt;/li>
&lt;li>&lt;a href="#统计描述与绘图">统计描述与绘图&lt;/a>&lt;/li>
&lt;li>&lt;a href="#相关性的可视化">相关性的可视化&lt;/a>&lt;/li>
&lt;li>&lt;a href="#组间差异比较的另类绘图">组间差异比较的另类绘图&lt;/a>&lt;/li>
&lt;li>&lt;a href="#文章中的统计描述">文章中的统计描述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#比例和率的区别">比例和率的区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#率和比例的介绍">率和比例的介绍&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计与调查的相关内容">实验设计与调查的相关内容&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计之随机">实验设计之随机&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计之均衡">实验设计之均衡&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计之重复">实验设计之重复&lt;/a>&lt;/li>
&lt;li>&lt;a href="#实验设计之对照">实验设计之对照&lt;/a>&lt;/li>
&lt;li>&lt;a href="#常见的实验设计方法">常见的实验设计方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#利用excel进行随机分组">利用excel进行随机分组&lt;/a>&lt;/li>
&lt;li>&lt;a href="#利用excel进行随机抽样">利用excel进行随机抽样&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于复杂抽样的介绍">关于复杂抽样的介绍&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算相关内容">样本量估算相关内容&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量计算需要考虑的因素">样本量计算需要考虑的因素&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之一">样本量估算需要考虑的因素之一&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之二">样本量估算需要考虑的因素之二&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之三">样本量估算需要考虑的因素之三&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之四">样本量估算需要考虑的因素之四&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之五">样本量估算需要考虑的因素之五&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之六">样本量估算需要考虑的因素之六&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之七">样本量估算需要考虑的因素之七&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样本量估算需要考虑的因素之八">样本量估算需要考虑的因素之八&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归方法大家族">回归方法大家族&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归方法杂谈之一">回归方法杂谈之一&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归方法杂谈之二">回归方法杂谈之二&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归方法杂谈之三">回归方法杂谈之三&lt;/a>&lt;/li>
&lt;li>&lt;a href="#苹果案例之logistic回归">苹果案例之logistic回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#苹果案例之poisson回归">苹果案例之Poisson回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#苹果案例之负二项回归">苹果案例之负二项回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deming回归与passing-bablok回归">Deming回归与Passing-Bablok回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样条回归之一">样条回归之一&lt;/a>&lt;/li>
&lt;li>&lt;a href="#样条回归之二">样条回归之二&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性回归分析思路">线性回归分析思路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#混杂因素与统计学悖论">混杂因素与统计学悖论&lt;/a>&lt;/li>
&lt;li>&lt;a href="#回归系数的理解">回归系数的理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单因素与多因素的不同">单因素与多因素的不同&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于线性的理解">关于“线性”的理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#box-cox变换">Box-Cox变换&lt;/a>&lt;/li>
&lt;li>&lt;a href="#box-tidwell变换">Box-Tidwell变换&lt;/a>&lt;/li>
&lt;li>&lt;a href="#要不要用逐步回归">要不要用逐步回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#单因素分析有没有必要做">单因素分析有没有必要做？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#先做单因素再做多因素是否正确">先做单因素、再做多因素，是否正确？&lt;/a>&lt;/li>
&lt;li>&lt;a href="#先做单因素再做多因素的理解">先做单因素再做多因素的理解&lt;/a>&lt;/li>
&lt;li>&lt;a href="#多因素筛选策略">多因素筛选策略&lt;/a>&lt;/li>
&lt;li>&lt;a href="#置信区间与预测区间">置信区间与预测区间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#线性回归中的方差齐性">线性回归中的方差齐性&lt;/a>&lt;/li>
&lt;li>&lt;a href="#异常值的问题">异常值的问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#缺失值的类型">缺失值的类型&lt;/a>&lt;/li>
&lt;li>&lt;a href="#缺失值的简单处理">缺失值的简单处理&lt;/a>&lt;/li>
&lt;li>&lt;a href="#用多重插补法处理缺失值">用多重插补法处理缺失值&lt;/a>&lt;/li>
&lt;li>&lt;a href="#自变量对数变换如何解释">自变量对数变换如何解释&lt;/a>&lt;/li>
&lt;li>&lt;a href="#因变量对数变换如何解释">因变量对数变换如何解释&lt;/a>&lt;/li>
&lt;li>&lt;a href="#生存分析的相关内容">生存分析的相关内容&lt;/a>&lt;/li>
&lt;li>&lt;a href="#生存分析方法简介">生存分析方法简介&lt;/a>&lt;/li>
&lt;li>&lt;a href="#中位生存时间与中位随访时间">中位生存时间与中位随访时间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#log-rank检验应用注意事项">log-rank检验应用注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#指数回归">指数回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#weibull回归">Weibull回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#等比例cox回归">等比例cox回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#时依协变量与分层cox回归">时依协变量与分层cox回归&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于重复测量数据的分析">关于重复测量数据的分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据介绍">重复测量数据介绍&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量方差分析">重复测量方差分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据的趋势探索">重复测量数据的趋势探索&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据的趋势比较">重复测量数据的趋势比较&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据之重复测量方差分析">重复测量数据之重复测量方差分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据之广义估计方程">重复测量数据之广义估计方程&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量数据之多水平模型">重复测量数据之多水平模型&lt;/a>&lt;/li>
&lt;li>&lt;a href="#重复测量方法的比较">重复测量方法的比较&lt;/a>&lt;/li>
&lt;li>&lt;a href="#本人对统计学方法的观点">本人对统计学方法的观点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#专家说我的方法过时了怎么办">专家说我的方法过时了怎么办&lt;/a>&lt;/li>
&lt;li>&lt;a href="#预测建模需要注意点什么问题">预测建模需要注意点什么问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#大数据和医工交叉时代下对医学统计的思考">大数据和医工交叉时代下对医学统计的思考&lt;/a>&lt;/li>
&lt;li>&lt;a href="#危险因素探索中的几个要点">危险因素探索中的几个要点&lt;/a>&lt;/li>
&lt;li>&lt;a href="#横断面与病例对照研究的区别">横断面与病例对照研究的区别&lt;/a>&lt;/li>
&lt;li>&lt;a href="#如何学习统计学的体会">如何学习统计学的体会&lt;/a>&lt;/li>
&lt;li>&lt;a href="#统计方法的算法与软件操作哪个重要">统计方法的算法与软件操作哪个重要&lt;/a>&lt;/li>
&lt;li>&lt;a href="#预测建模的基本思路">预测建模的基本思路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一些数据分析思路">一些数据分析思路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#我学统计学的经历">我学统计学的经历&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于课题申请或论文撰写的方法">关于课题申请或论文撰写的方法&lt;/a>&lt;/li>
&lt;li>&lt;a href="#课题申请中的方法学描述">课题申请中的方法学描述&lt;/a>&lt;/li>
&lt;li>&lt;a href="#课题申请中方法学的注意事项">课题申请中方法学的注意事项&lt;/a>&lt;/li>
&lt;li>&lt;a href="#关于课题设计的一些问题">关于课题设计的一些问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#论文撰写中的统计方法">论文撰写中的统计方法&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="概念和观点理解">概念和观点理解&lt;/h2>
&lt;p>这部分主要是一些统计学概念和一些观点的通俗介绍，如标准误到底是什么意思，P值如何理解，中心极限定理是在说什么，等等。&lt;/p>
&lt;h3 id="小白学统计系列之三p值到底是个什么东西">小白学统计系列之三：p值到底是个什么东西&lt;/h3>
&lt;p>p值（p value）就是当原假设为真时&lt;strong>所得到的样本观察结果或更极端结果出现&lt;/strong>的概率。p值表示一个事件发生的可能性，在假设检验中是对原假设的设定，我们希望这个p值比较小，说明原假设是个小概率事件。当小概率事件发生时，我们就有理由认为我们的原假设是有问题的，从而选择备择假设（原假设与备择假设为互斥事件）。p值只能用来拒绝原假设，而不能肯定原假设，即当p值比较大时，不能说明原假设是对的，只能说统计学上不能证明原假设是错的，这时同样没有证明原假设是对的，其依旧处于不能说明对，也不能证明错的状态。只有当拒绝原假设，即统计学上认为原假设是错的是，我们才能认为原假设的互斥事件——备择假设是对的。&lt;/p>
&lt;h3 id="从女士品茶到假设检验">从“女士品茶”到假设检验&lt;/h3>
&lt;p>假设检验的理解，来源自《女士品茶：20 世纪统计怎样变革了科学》，这本书还是不错的。&lt;/p>
&lt;h3 id="小白学统计系列之五法官的艰难抉择统计学中的两类错误">小白学统计系列之五：法官的艰难抉择——统计学中的两类错误&lt;/h3>
&lt;p>统计学中两类错误：第一类：去真、第二类：取伪。&lt;strong>给定采样样品后&lt;/strong>，采用假设检验，设定拒绝域&lt;span class="math">\(\alpha\)&lt;/span>的做法让两类错误无法同时降低，此消彼长。只有通过提供样品、试验次数或者提高实验准确性才能同时降低二者。&lt;/p>
&lt;h3 id="貌合神离的标准差与标准误">貌合神离的标准差与标准误&lt;/h3>
&lt;p>标准差是“样本原始数据的标准差”。标准误是“样本统计量的标准差”。标准差和标准误的区别，最实质的地方至少是两点：一是针对计算的对象不同，标准差是根据一次抽样的原始数据计算的，而标准误是根据多次抽样的样本统计量（可以是均值，也可以是率等）计算的。二是标准差只是一个描述性指标，只是描述原始数据的波动情况，而标准误是跟统计推断有关的指标，大多数的统计量计算都需要用到标准误。&lt;/p>
&lt;p>举例：对于抽取的10个样本，每个样本容量为100，每个样本都有各自的标准差，每个样本也都可以计算一个均值，这样10个样本就可以计算出10个均值。将这10个均值作为原始数据，仍然可以计算出均值和标准差，这里作为统计量的均值的标准差就是标准误，它是用10个均值计算的，而标准差是用一次样本中的原始数据计算的。&lt;/p>
&lt;p>如果真要严格定义，应该说标准误是“样本统计量的标准差”更加合适，而不是非要局限于均数或率等。而标准差是“样本原始数据的标准差”。&lt;/p>
&lt;h3 id="p0.05真的值得庆贺吗兼谈置信区间">P&amp;lt;0.05真的值得庆贺吗？——兼谈置信区间&lt;/h3>
&lt;p>p值提供的信息不够丰富，只能说明得到该抽样统计量发生（以及更极端情况）的概率，例如，抽样20个样品，计算得到方差是5.5，p值为0.01，这意味着我随机抽样20个样品，从总体抽样1000次，那么发生样本方差大于等于5.5的概率为0.01，即大约10次。p值算是点估计在假设检验中体现，为了获得更丰富的信息，我们将区间估计的思想也用到假设检验中，就能得到&lt;strong>置信区间&lt;/strong>。&lt;/p>
&lt;p>置信区间，就是用样本数据计算两个值，用这两个数确定一个区间，这个区间以一定的可信程度包含被估计的参数。根据上面的定义，可以将置信区间这个词拆成两个部分理解：置信和区间。先说区间，比较容易理解，就是一段数值范围，如果确认这个范围呢？它是根据样本数据计算的点估计和标准误来计算的，表现为（参数估计值±边际误差），所谓边际误差，就是考虑到了样本与总体是有一定差异的。由于现实中几乎所有抽样分布都会近似呈正态分布，因此边际误差通常都是用正态分位数的一个“z值&lt;em>标准误”来表示，也就是我们通常见到的“1.96&lt;/em>标准误”（1.96是双边Z检验，单侧累积概率为0.975时的值）。再说置信，他表示“一定的可信程度”，例如95%的置信区间就是计算一个区间，我们有95%的信心认为这个区间包含了被估计的参数。&lt;/p>
&lt;p>置信区间的前缀数字95%称为置信系数，我们习惯用95%，但不一定非要用这个。也可以用90%、99%等。一般来说，置信系数越大，所得的区间越宽，也就是越可信；置信系数越小，所得区间越窄，越不可信。&lt;/p>
&lt;div class="figure">
&lt;embed src="../images/置信区间大小.webp" />&lt;p class="caption">置信区间大小.webp&lt;/p>
&lt;/div>
&lt;p>如果要说置信区间的理论意义，也是只存在于理论中。比如95%的置信区间，意思是：如果从一个总体中重复多次抽取不同的样本，对每一个样本都可以计算一个置信区间，那么理论上有95%的置信区间包含了总体参数。一个总体参数总是固定的，对于每次抽样计算的置信区间，要么包含这个参数，要么不包含这个参数，但总的来说，100次抽样样本中，大概会有95次包含了这个参数。&lt;/p>
&lt;p>置信区间的宽窄反映了对参数估计的精确度，置信区间越窄，说明越精确，置信区间越宽，说明越不精确。一般来说，样本量越大，计算的置信区间越窄。因为样本量越大，标准误越小。当样本量跟总体一样多时，计算的置信区间就窄成了只有一个值了。这时就是最精确了。&lt;/p>
&lt;h3 id="正态分布的理解">正态分布的理解&lt;/h3>
&lt;p>我自己的笔记更好。&lt;a href="../学习笔记/概率统计随机过程之如何推导得到正态分布—正态分布的理解角度.md">概率统计随机过程之如何推导得到正态分布—正态分布的理解角度&lt;/a>&lt;/p>
&lt;h3 id="分类资料与计数资料">分类资料与计数资料&lt;/h3>
&lt;ul>
&lt;li>分类资料（数据）：没有单位，可以是数字，也可以是其他类型，比如 是否、男女、实验组/对照组等&lt;/li>
&lt;li>计数数据：有单位，一般是有单位的频数，如天数、次数&lt;/li>
&lt;li>有些使用数值作为数据时，分类数据和计数数据是差不多的，不太需要严格区分&lt;/li>
&lt;/ul>
&lt;h3 id="计数资料离散资料和连续资料">计数资料、离散资料和连续资料&lt;/h3>
&lt;ul>
&lt;li>计数资料一般是频数，通过数数得来的，而离散资料只是数值是离散的，并不一定通过计数得来的。&lt;/li>
&lt;li>离散数据并不一定都是整数，例如取值结果为&lt;span class="math">\({0,0.5,1,1.5,\dotsb}\)&lt;/span>的数据也是离散数据。&lt;/li>
&lt;li>在一定区间内可以任意取值的数据叫连续数据，其数值是连续不断的，相邻两个数值可作无限分割，即可取无限个数值。&lt;/li>
&lt;li>计数资料一般都是非负整数，当数据值比较大，且近似取值不会影响最终结果时，可以近似当成连续数据来处理。&lt;/li>
&lt;li>连续数据：没有正负、离散或整数的限制。&lt;/li>
&lt;/ul>
&lt;h3 id="什么是虚拟变量应用中应注意什么问题">什么是虚拟变量，应用中应注意什么问题&lt;/h3>
&lt;p>在线性回归中，我们的自变量一般都是数值型数据，如果分类数据也会对回归结果产生影响，那么如何将其引入回归方程中呢？就需要使用虚拟变量。说白了，&lt;strong>虚拟变量就是将分类数据引入回归方程的一种方法&lt;/strong>。&lt;/p>
&lt;p>比如，人每天的基础代谢（BMR）和身高、体重、年龄以及性别有关，前面三项都好说，一般的多元线性回归都实用，现在需要将性别因素也考虑进去，性别是个分类变量，无法用具体数值表示，因此我们就是用虚拟变量表示。对于性别女，我们使用0表示，男用1表示，这样带入回归方程有： &lt;span class="math">\[
BMR=\alpha_0+\alpha_1×\text{身高}+\alpha_2×\text{体重}+\alpha_3×\text{年龄}+\alpha_4×\text{性别}\\
性别=\begin{cases}
1,男\\
0,女
\end{cases}
\]&lt;/span> 实际上，虚拟变量就是用0，1表示分类变量，得到一个包含离散变量的回归方程。一般用0表示属性或特征不存在，1表示属性或特征存在。&lt;/p>
&lt;p>上面使用的虚拟变量指标是了两种可能，所以能用一个0，1虚拟变量表示，当分类变量的水平数大于两个，我们要用0，1，2……表示吗？答案是否定的。理由是：若我们用连续的数字表达分类变量，那么就隐藏地给这个变量添加了一个很强的假设：每个分类之间依顺序有完美的线性关系，因此，对于有&lt;span class="math">\(m\)&lt;/span>个水平的分类变量，我们添加&lt;span class="math">\(m-1\)&lt;/span>个0-1虚拟变量来替代（一个例外是，如果回归方程无截距项，可引入&lt;span class="math">\(m\)&lt;/span>个0-1虚拟变量）。之所以要少引入一个虚拟变量，是为了防止多重共线性，又叫虚拟变量陷阱。&lt;/p>
&lt;p>此外，上面的例子，虚拟变量通过与其他变量&lt;strong>相加&lt;/strong>融入回归方程中，还有通过&lt;strong>相乘&lt;/strong>融入进去的例子。例如，查看一个家庭的总体教育支出&lt;span class="math">\(Y_e\)&lt;/span>，除了受家庭收入水平&lt;span class="math">\(X_1\)&lt;/span>、孩子年龄&lt;span class="math">\(X_2\)&lt;/span>影响外，一个非常重要的因素就是有没有孩子，而有没有孩子是一个分类变量&lt;span class="math">\(\beta\)&lt;/span>。我们需要通过乘法将其引入回归方程： &lt;span class="math">\[
Y_3=\alpha_0+\alpha_1×X_1+\alpha_2×\beta×X_2\\
\beta=\begin{cases}
1,有孩子\\
0,无孩子
\end{cases}
\]&lt;/span>&lt;/p>
&lt;p>虚拟变量应用场景：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>分段回归&lt;/li>
&lt;li>稳定性检验。稳定行：利用不同样本的得到的回归方程系数没有显著性差异。&lt;/li>
&lt;li>季节性波动调整&lt;/li>
&lt;/ol>
&lt;h3 id="关于抽样误差">关于抽样误差&lt;/h3>
&lt;p>抽样误差的来源：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>没有做到真正的随机&lt;/li>
&lt;li>抽取的样本没有代表性&lt;/li>
&lt;/ol>
&lt;p>当存在系统性抽样错误，即使样本数量再大，可能得出结论也是错的。&lt;/p>
&lt;h3 id="样本量大于30就算正态了吗">样本量大于30就算正态了吗？&lt;/h3>
&lt;p>这个说法其实是将中心极限定理和抽样的原始数据搞混了，而且没注意到样本均值也是个随机变量。中心极限定理说的是：不管原始数据的分布是什么样的（可能是正态，也可能偏态，还可能超级变态），如果从这个原始数据中多次抽样的话，对于每个样本计算出&lt;strong>均值&lt;/strong>，如果每个样本中的例数大于30，这些计算出的&lt;strong>样本均值的分布&lt;/strong>接近正态。而不是说：一个样本中的原始数据的个数大于30，这个原始数据的分布接近正态。&lt;/p>
&lt;p>&lt;strong>如果针对原始数据，无论如何都是要进行正态性检验的&lt;/strong>。&lt;/p>
&lt;h3 id="判断正态性的一些简易方法">判断正态性的一些简易方法&lt;/h3>
&lt;p>几种简易的判断正态性的方法（统计专业人士请绕行）：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>根据均值和标准差。首先，分别计算均值和标准差，然后看一下数据中有百分之多少的人在均值±1个标准差、均值±2个标准差、均值±3个标准差之内。如果分别大概是68%、95%、99%左右，说明差不多是正态的。&lt;/li>
&lt;li>计算四分位数间距和标准差，如果四分位数间距/标准差的值大约在1.35左右，可以认为满足正态分布。比如上面的10个数中，四分位数间距是1.9，标准差是1.3，1.9/1.3大约为1.4左右，比较接近1.35，可以认为是正态的。&lt;/li>
&lt;li>通过几幅图来判断，最常用的图有箱式图、直方图、茎叶图、QQ图等。（再在有计算机辅助情形下最直观）&lt;/li>
&lt;/ol>
&lt;h3 id="多变量与多因素的区别">多变量与多因素的区别&lt;/h3>
&lt;p>回归中的多变量、多因素、多重、多元有什么区别？&lt;/p>
&lt;p>多变量线性回归或多重线性回归（multivariable or multiple linear regression）是一回事，多因素线性回归或多重线性回归则是有多个自变量。但它们都是只有&lt;strong>1个因变量&lt;/strong>。 &lt;span class="math">\[
y=\alpha_0+\alpha_1 x_1+\dotsb+\alpha_n x_n+\varepsilon
\]&lt;/span> 多元或多变量线性回归模型（multivariate linear regression model）是指&lt;strong>多个因变量&lt;/strong>的回归模型。&lt;/p>
&lt;p>读者注：数理统计有时候确实不那么区分。甚至线性回归默认自变量或因变量可以是一维乃至多维矢量。&lt;/p>
&lt;h3 id="信度与效度评价">信度与效度评价&lt;/h3>
&lt;ul>
&lt;li>信度代表的是数据的可靠性程度和一致性程度，它能够反映数据的&lt;strong>稳定性和集中程度&lt;/strong>。所谓“信”，意思是不会偏离太多、行为在可预想范围内。&lt;/li>
&lt;li>效度是指测量工具能够准确测量出事物真实情况的能力，它能够反映数据的&lt;strong>准确性&lt;/strong>。所谓“效”，意味着效果、准确性，达到目的的程度。&lt;/li>
&lt;/ul>
&lt;div class="figure">
&lt;img src="../images/信度效度.png" alt="信度效度.png" />&lt;p class="caption">信度效度.png&lt;/p>
&lt;/div>
&lt;p>如果用射击来类比：&lt;/p>
&lt;p>右下图的弹孔是散布在整个靶图上的，有两个特点：1、点与点之间的距离很大，说明运动员的稳定性差；2、几乎没有弹孔落在靶心，说明运动员的准确性也差。说明该射击运动员既没有稳定性，也没有准确性。如果将每个弹孔看作一个数据信息（个案），那么该数据集合是既没有信度（稳定性）也没有效度（准确性）。&lt;/p>
&lt;p>左下图的弹孔密集的落在一个狭小的区域内，但是偏离了靶心，说明该运动员的射击稳定性很好，但是准确性则不足。同样的，如果弹孔看作数据，那么该数据集合的特点是具有高信度，效度却很低。&lt;/p>
&lt;p>右上图的弹孔是分散的，但是大部分的弹孔落在了靶心，说明运动员的稳定性不足，但是准确性还是不错的。形容数据集合的话，那么该数据集合是高效度和低信度的。&lt;/p>
&lt;p>左上图的弹孔密集的落在了靶心，说明该运动员的稳定性和准确性都很好。用来形容数据集合则说明该数据集合是高信度和高效度的。&lt;/p>
&lt;h4 id="问卷的信度和效度评价">问卷的信度和效度评价&lt;/h4>
&lt;p>问卷的信度在于评价收集上来的数据是否真实可靠，也就是检查填写问卷的这些人是不是认真的填写了问卷，还是乱填的。大家可以想象一下，如果一个人胡乱的填写数据，那么有很大的可能他的答案与其它人的答案是南辕北辙的，差异很大，那么就会影响到整份问卷在的信度。因此，大家在收集问卷数据时，应该想各种办法让大家能够认证回答。&lt;/p>
&lt;p>问卷的效度是用来研究题目的设置是否能够有效的测量问卷设计者当初设计的初衷，也就是说检验问卷题目的设计是否合理。如果题目是合理的，那么它就能够有效地测量出问卷设计者的设计目的和初衷。&lt;/p>
&lt;p>信度的分析类型：信度分析的目的是检验受访者是否真实的回答了问卷的问题，收集上来的数据是否真实可靠。根据测量工具的不同，信度指标可以分成四类，如下图所示：&lt;/p>
&lt;div class="figure">
&lt;embed src="../images/信度类型.jfif" />&lt;p class="caption">信度类型.jfif&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>重测信度：是指用同样的测量工具，对同一组被测者隔一定时间重复测量，考察两次测量结果的相关程度，可以直接采用相关分析，得到的相关系数即为重测信度系数。重复信度能够检验时间差异带来的数据误差，该误差不是测量工具不直接有测量工具造成，而且测量的是同一组被测者，因此称为外在信度。&lt;/li>
&lt;li>复本信度是指让同一组被测者一次填写两份平行问卷，计算两份数据的相关系数，复本信度要求两份问卷除了在问题表述不同之外，其余方面要完全一致，实际操作比较困难。&lt;/li>
&lt;li>折半信度是指将一份问卷分成两部分，计算这两部分的相关系数，即折半信度系数，以此来衡量整份问卷的信度。&lt;/li>
&lt;li>α系数又称克朗巴哈系数，是最常用的测量内部一致性信度的方法，计算出的克朗巴哈α系数是所有可能折半信度的均值，取值在0-1之间，系数越高一致性越好，说明数据的真实性越高。α系数是问卷数据真实性检验的最重要指标。&lt;/li>
&lt;/ul>
&lt;p>效度分析类型：效度是指测量工具能够准确测量出事物所要测量特性的程度，效度越高表示测量真实性越高，由于真实值往往未知，所以我们对于效度的评价也不可能永恒不变的，随着设定和对比的“真实值”变化，效度指标也会发生变化。也就是说，效度的指标的种类会有很多。效度分析可以分成以下三种内容：&lt;/p>
&lt;div class="figure">
&lt;embed src="../images/效度类型.jfif" />&lt;p class="caption">效度类型.jfif&lt;/p>
&lt;/div>
&lt;ul>
&lt;li>内容效度：是一种定性的评价标准，主要通过经验判断进行，主要是通过专家和有经验的业内人士进行评价的方法。还可以对问卷在正式使用前进行小范围的使用，结合结果进行题项的修正以说明问卷的有效性。内容效度一般不需要使用SPSS进行数据分析，但是需要专家和权威老师的指导，前测修改过程，最后对问卷的内容效度进行说明。&lt;/li>
&lt;li>结构效度：是指测量题项与测量方向之间的对应关系，其测量方法是因子分析。因子分析的数据计算理论比较复杂。&lt;/li>
&lt;li>校标效度：人为指定确定的结果作为“准确标准”，考察其他待测结果与其是否一致。例如，考试卷中的选择题都会有准确答案，那么每题的正确率就是这个班级在该题上的效度。&lt;/li>
&lt;/ul>
&lt;h3 id="问卷用不用做信度和效度评价">问卷用不用做信度和效度评价&lt;/h3>
&lt;p>作者认为：问卷的客观特征和行为问题不用做信效度评价。量表需要信度和效度分析。&lt;/p>
&lt;p>作者所说的问卷的强调目的是为了获取（客观的、直接的信息），而量表是为了测量（主观的、隐藏的信息）。但是调查问卷的范围有些泛化，将一些量表例如人格分析、心理量表也当成了调查问卷，因此在遇到后需要仔细甄别。&lt;/p>
&lt;h3 id="传染病模型中的拐点">传染病模型中的拐点&lt;/h3>
&lt;p>拐点：从微积分的概念来讲，函数f的图像上改变凹性的点叫做函数f的拐点。也就是说，从上凹到下凹或者从下凹到上凹，这一个点就是拐点。&lt;/p>
&lt;p>因此，在医学上拐点说的不是病例数的正负变化，而是病例的增长速度的正负变化（二阶导）。&lt;/p>
&lt;p>传染病模型，一般病例数的增长都是呈S型曲线（logistic生长曲线），前面增长越来越快，过了中间拐点，后面增长越来越慢。&lt;/p>
&lt;h2 id="一些基础统计方法">一些基础统计方法&lt;/h2>
&lt;p>这部分主要是一些基础统计学方法的介绍，如t检验、方差分析、卡方检验、相关分析等方法的正确应用。&lt;/p>
&lt;h3 id="定量资料的组间比较">定量资料的组间比较&lt;/h3>
&lt;div class="figure">
&lt;img src="../images/分类数据统计分析方法.png" alt="分类数据统计分析方法" />&lt;p class="caption">分类数据统计分析方法&lt;/p>
&lt;/div>
&lt;h3 id="分类资料的组间比较">分类资料的组间比较&lt;/h3>
&lt;div class="figure">
&lt;img src="../images/数值数据统计分析方法.png" alt="数值数据统计分析方法" />&lt;p class="caption">数值数据统计分析方法&lt;/p>
&lt;/div>
&lt;h3 id="t检验应用的注意事项">t检验应用的注意事项&lt;/h3>
&lt;p>t检验，亦称student t检验（Student's t test），主要用于样本含量较小（例如&lt;span class="math">\(n &amp;lt; 30\)&lt;/span>），总体标准差&lt;span class="math">\(σ\)&lt;/span>&lt;strong>未知&lt;/strong>的正态分布。&lt;/p>
&lt;p>t检验前置要求：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>已知一个总体均数；&lt;/li>
&lt;li>可得到一个样本均数及该样本标准差；&lt;/li>
&lt;li>样本来自正态或近似正态总体，t检验鲁棒性较强。&lt;/li>
&lt;/ol>
&lt;p>t分布的概率密度函数： &lt;span class="math">\[
p(t)=\frac{\Gamma
(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{t^2}{\nu})^{-(\nu+1)2}
\]&lt;/span> 其中，&lt;span class="math">\(\nu\)&lt;/span>表示t分布的自由度。对于从（近似）正态总体&lt;span class="math">\(N(\mu,\sigma^2)\)&lt;/span>随机抽样的i.i.d样本&lt;span class="math">\(X=\{X_1,X_2,\dotsb,X_n\}\)&lt;/span>，其样本均值为&lt;span class="math">\(\bar{X}\)&lt;/span>，样本方差为&lt;span class="math">\(S^2\)&lt;/span>，统计量： &lt;span class="math">\[
\frac{\bar{X}-\mu}{S/\sqrt{n}} \sim t(n-1)
\]&lt;/span> 服从自由度为&lt;span class="math">\(n-1\)&lt;/span>的t分布。&lt;/p>
&lt;p>t检验主要用法：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;strong>单总体t检验&lt;/strong>是检验一个样本&lt;strong>平均数&lt;/strong>与一个已知的总体平均数的差异是否显著。&lt;/li>
&lt;li>&lt;strong>双总体t检验&lt;/strong>是检验两个样本&lt;strong>平均数&lt;/strong>与其各自所代表的总体的差异是否显著。注意此时要求两个总体有&lt;strong>方差齐性&lt;/strong>，方差不齐时则使用&lt;strong>Welch检验&lt;/strong>。而根据两个样本是&lt;strong>独立的还是配对的&lt;/strong>，又分为一是独立样本t检验（各实验处理组之间毫无相关存在，即为独立样本），该检验用于检验两组非相关样本被试所获得的数据的差异性；一是配对样本t检验，用于检验匹配而成的两组被试获得的数据或同组被试在不同条件下所获得的数据的差异性，这两种情况组成的样本即为相关样本。&lt;/li>
&lt;li>检验同一统计量的两次测量值之间的差异是否为零。&lt;/li>
&lt;li>回归系数的显著性检验。&lt;/li>
&lt;/ol>
&lt;p>作者提出的3重常见t检验使用错误：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>不考虑数据的正态性，只要是两组比较就直接用t检验。&lt;/li>
&lt;li>将t检验用于多组的两两比较，增加假阳性错误。&lt;/li>
&lt;li>不考虑资料是否独立，采用独立资料的t检验分析非独立数据。&lt;/li>
&lt;/ol>
&lt;h3 id="方差分析的变异分解思想">方差分析的变异分解思想&lt;/h3>
&lt;p>统计学中描述变异，一般是使用方差、标准差这类表述。而方差的分解来给我们带来很多有用信息。&lt;/p>
&lt;p>首先，方差分析就是基于变易分解，将总体方差分解为组间方差和组内方差，分别体现不同自变量和随机性对结果的影响。一般由于数据量越大，累积的方差越大，因此还需要除以数据量，就是自由度。方差一般服从卡方分布，因此方差的比值服从F分布。如果组间变异远远大于组内变异，那么组间均方除以组内均方的值肯定很大，反之，这一值就会很小。但是，到底大到什么程度才认为有统计学意义呢，那就得根据F分布了。&lt;/p>
&lt;p>此外，双总体t检验的公式实际上也是一种方差分析。以两组独立样本比较的公式为例，分子是什么？组间差异。分母又是什么？均数差值的标准误。两者的比值就是一种服从t分布的方差分析。&lt;/p>
&lt;p>对于线性回归，首先因变量y的值各不相同，这就是变异，线性回归就是为了弄明白，为什么这些y值不一样。所以才要有自变量x，看看哪个自变量对y的变异解释的更多。很明显，哪个解释的多，哪个自变量就对y的影响大。所以，为什么线性回归的结果中会出现方差分析的字眼，因为它也在方差分解啊，把总的y的变异分解为模型所能解释的部分，以及不能解释的部分。&lt;/p>
&lt;h3 id="方差分析的一些特殊情况">方差分析的一些特殊情况&lt;/h3>
&lt;p>&lt;strong>方差分析与实验设计是密切关联的&lt;/strong>。有一种实验设计方案，可能就有一种对应的方差分析。比如完全随机设计采用单因素方差分析，随机区组设计采用随机区组方差分析，析因设计采用析因设计的方差分析，交叉设计采用交叉设计的方差分析，嵌套设计采用嵌套设计的方差分析，裂区设计采用裂区设计的方差分析。&lt;/p>
&lt;h3 id="方差分析后的两两比较方法选择">方差分析后的两两比较方法选择&lt;/h3>
&lt;p>事后分析的两两分析一大作用就是&lt;strong>控制假阳性&lt;/strong>，因为两两比较次数多了，容易产生假阳性的结果。&lt;/p>
&lt;p>作者给出的各种两两比较方法总结：&lt;/p>
&lt;p>如果各组例数相等，建议首选Tukey法；如果例数不等，建议首选Scheffe法（如果比较组数不多，如3组，Bonferroni法也可以作为首选）；如果要分别比较每个试验组与对照组，建议采用Dunnett法；如果各组方差相差较大，建议采用Games-Hotwell法。&lt;/p>
&lt;div class="figure">
&lt;img src="../images/方差分析事后分析.jpg" alt="方差分析事后分析.jpg" />&lt;p class="caption">方差分析事后分析.jpg&lt;/p>
&lt;/div>
&lt;h3 id="方差分析的应用注意事项">方差分析的应用注意事项&lt;/h3>
&lt;p>（1）缺乏对数据的正态性检验，组间比较都采用方差分析，而不考虑秩和检验。&lt;/p>
&lt;p>（2）两两比较直接采用t检验，而不是专门的两两比较方法。&lt;/p>
&lt;p>（3）采用方差分析处理重复测量资料，增加假阳性错误。&lt;/p>
&lt;p>（4）实验设计考虑不周，误用其它设计的统计分析方法。&lt;/p>
&lt;h3 id="方差分析中的随机效应与固定效应">方差分析中的随机效应与固定效应&lt;/h3>
&lt;h3 id="多重校正的fdr方法">多重校正的FDR方法&lt;/h3>
&lt;h3 id="卡方检验的注意事项">卡方检验的注意事项&lt;/h3>
&lt;h3 id="卡方检验等于单因素logistic回归吗">卡方检验等于单因素logistic回归吗&lt;/h3>
&lt;h3 id="协方差与相关系数的关系">协方差与相关系数的关系&lt;/h3>
&lt;h3 id="相关分析的注意事项">相关分析的注意事项&lt;/h3>
&lt;h3 id="临床常见的疑惑解答之一">临床常见的疑惑解答之一&lt;/h3>
&lt;h3 id="临床常见的疑惑解答之二">临床常见的疑惑解答之二&lt;/h3>
&lt;h2 id="统计描述与绘图">统计描述与绘图&lt;/h2>
&lt;h3 id="相关性的可视化">相关性的可视化&lt;/h3>
&lt;h3 id="组间差异比较的另类绘图">组间差异比较的另类绘图&lt;/h3>
&lt;h3 id="文章中的统计描述">文章中的统计描述&lt;/h3>
&lt;h3 id="比例和率的区别">比例和率的区别&lt;/h3>
&lt;h3 id="率和比例的介绍">率和比例的介绍&lt;/h3>
&lt;h2 id="实验设计与调查的相关内容">实验设计与调查的相关内容&lt;/h2>
&lt;p>这部分主要是介绍实验设计、流行病学调查有关的一些内容，包括各种概念的介绍、如何实现随机分组、随机抽样等&lt;/p>
&lt;h3 id="实验设计之随机">实验设计之随机&lt;/h3>
&lt;h3 id="实验设计之均衡">实验设计之均衡&lt;/h3>
&lt;h3 id="实验设计之重复">实验设计之重复&lt;/h3>
&lt;h3 id="实验设计之对照">实验设计之对照&lt;/h3>
&lt;h3 id="常见的实验设计方法">常见的实验设计方法&lt;/h3>
&lt;h3 id="利用excel进行随机分组">利用excel进行随机分组&lt;/h3>
&lt;h3 id="利用excel进行随机抽样">利用excel进行随机抽样&lt;/h3>
&lt;h3 id="关于复杂抽样的介绍">关于复杂抽样的介绍&lt;/h3>
&lt;h2 id="样本量估算相关内容">样本量估算相关内容&lt;/h2>
&lt;p>这部分主要是关于样本量估算的一些介绍，重点是介绍样本量估算需要考虑哪些因素。&lt;/p>
&lt;h3 id="样本量计算需要考虑的因素">样本量计算需要考虑的因素&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之一">样本量估算需要考虑的因素之一&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之二">样本量估算需要考虑的因素之二&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之三">样本量估算需要考虑的因素之三&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之四">样本量估算需要考虑的因素之四&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之五">样本量估算需要考虑的因素之五&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之六">样本量估算需要考虑的因素之六&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之七">样本量估算需要考虑的因素之七&lt;/h3>
&lt;h3 id="样本量估算需要考虑的因素之八">样本量估算需要考虑的因素之八&lt;/h3>
&lt;h2 id="回归方法大家族">回归方法大家族&lt;/h2>
&lt;p>这部分主要是介绍了一些常见的各种回归方法，并不是很深入，侧重对各种方法的一些归类介绍，让大家明白有哪些回归方法。&lt;/p>
&lt;h3 id="回归方法杂谈之一">回归方法杂谈之一&lt;/h3>
&lt;h3 id="回归方法杂谈之二">回归方法杂谈之二&lt;/h3>
&lt;h3 id="回归方法杂谈之三">回归方法杂谈之三&lt;/h3>
&lt;h3 id="苹果案例之logistic回归">苹果案例之logistic回归&lt;/h3>
&lt;h3 id="苹果案例之poisson回归">苹果案例之Poisson回归&lt;/h3>
&lt;h3 id="苹果案例之负二项回归">苹果案例之负二项回归&lt;/h3>
&lt;h3 id="deming回归与passing-bablok回归">Deming回归与Passing-Bablok回归&lt;/h3>
&lt;h3 id="样条回归之一">样条回归之一&lt;/h3>
&lt;h3 id="样条回归之二">样条回归之二&lt;/h3>
&lt;h2 id="线性回归分析思路">线性回归分析思路&lt;/h2>
&lt;p>这部分主要是一些与线性回归分析有关的概念、分析思路、技巧等的介绍，如混杂因素的理解、要不要做单因素分析、单因素和多因素结果不同时应该怎么办，等等。&lt;/p>
&lt;h3 id="混杂因素与统计学悖论">混杂因素与统计学悖论&lt;/h3>
&lt;h3 id="回归系数的理解">回归系数的理解&lt;/h3>
&lt;h3 id="单因素与多因素的不同">单因素与多因素的不同&lt;/h3>
&lt;h3 id="关于线性的理解">关于“线性”的理解&lt;/h3>
&lt;h3 id="box-cox变换">Box-Cox变换&lt;/h3>
&lt;h3 id="box-tidwell变换">Box-Tidwell变换&lt;/h3>
&lt;h3 id="要不要用逐步回归">要不要用逐步回归&lt;/h3>
&lt;h3 id="单因素分析有没有必要做">单因素分析有没有必要做？&lt;/h3>
&lt;h3 id="先做单因素再做多因素是否正确">先做单因素、再做多因素，是否正确？&lt;/h3>
&lt;h3 id="先做单因素再做多因素的理解">先做单因素再做多因素的理解&lt;/h3>
&lt;h3 id="多因素筛选策略">多因素筛选策略&lt;/h3>
&lt;h3 id="置信区间与预测区间">置信区间与预测区间&lt;/h3>
&lt;h3 id="线性回归中的方差齐性">线性回归中的方差齐性&lt;/h3>
&lt;h3 id="异常值的问题">异常值的问题&lt;/h3>
&lt;h3 id="缺失值的类型">缺失值的类型&lt;/h3>
&lt;h3 id="缺失值的简单处理">缺失值的简单处理&lt;/h3>
&lt;h3 id="用多重插补法处理缺失值">用多重插补法处理缺失值&lt;/h3>
&lt;h3 id="自变量对数变换如何解释">自变量对数变换如何解释&lt;/h3>
&lt;h3 id="因变量对数变换如何解释">因变量对数变换如何解释&lt;/h3>
&lt;h2 id="生存分析的相关内容">生存分析的相关内容&lt;/h2>
&lt;p>这部分主要是一些与生存分析有关的内容，包括常见回归方法的介绍，一些常见问题的处理等。&lt;/p>
&lt;h3 id="生存分析方法简介">生存分析方法简介&lt;/h3>
&lt;h3 id="中位生存时间与中位随访时间">中位生存时间与中位随访时间&lt;/h3>
&lt;h3 id="log-rank检验应用注意事项">log-rank检验应用注意事项&lt;/h3>
&lt;h3 id="指数回归">指数回归&lt;/h3>
&lt;h3 id="weibull回归">Weibull回归&lt;/h3>
&lt;h3 id="等比例cox回归">等比例cox回归&lt;/h3>
&lt;h3 id="时依协变量与分层cox回归">时依协变量与分层cox回归&lt;/h3>
&lt;h2 id="关于重复测量数据的分析">关于重复测量数据的分析&lt;/h2>
&lt;p>这部分主要是一些与重复测量数据有关的方法和结果解读介绍。&lt;/p>
&lt;h3 id="重复测量数据介绍">重复测量数据介绍&lt;/h3>
&lt;h3 id="重复测量方差分析">重复测量方差分析&lt;/h3>
&lt;h3 id="重复测量数据的趋势探索">重复测量数据的趋势探索&lt;/h3>
&lt;h3 id="重复测量数据的趋势比较">重复测量数据的趋势比较&lt;/h3>
&lt;h3 id="重复测量数据之重复测量方差分析">重复测量数据之重复测量方差分析&lt;/h3>
&lt;h3 id="重复测量数据之广义估计方程">重复测量数据之广义估计方程&lt;/h3>
&lt;h3 id="重复测量数据之多水平模型">重复测量数据之多水平模型&lt;/h3>
&lt;h3 id="重复测量方法的比较">重复测量方法的比较&lt;/h3>
&lt;h2 id="本人对统计学方法的观点">本人对统计学方法的观点&lt;/h2>
&lt;p>这部分主要是本人对如何学习统计学、统计方法应用等的一些观点，未必正确，纯属个人观点。&lt;/p>
&lt;h3 id="专家说我的方法过时了怎么办">专家说我的方法过时了怎么办&lt;/h3>
&lt;h3 id="预测建模需要注意点什么问题">预测建模需要注意点什么问题&lt;/h3>
&lt;h3 id="大数据和医工交叉时代下对医学统计的思考">大数据和医工交叉时代下对医学统计的思考&lt;/h3>
&lt;p>作者认为统计学是大数据的基础之一。面对不断出现的新名词，打好统计学基础，再去尝试接触自己需要的方法，你会发现很多方法其实都是触类旁通的，就像你学好了易筋经，你会发现再练其它招式都是事半功倍。&lt;/p>
&lt;p>读者注：我很认同这一点，算然有些老派，但是符合规律。&lt;/p>
&lt;h3 id="危险因素探索中的几个要点">危险因素探索中的几个要点&lt;/h3>
&lt;h3 id="横断面与病例对照研究的区别">横断面与病例对照研究的区别&lt;/h3>
&lt;h3 id="如何学习统计学的体会">如何学习统计学的体会&lt;/h3>
&lt;h3 id="统计方法的算法与软件操作哪个重要">统计方法的算法与软件操作哪个重要&lt;/h3>
&lt;h3 id="预测建模的基本思路">预测建模的基本思路&lt;/h3>
&lt;h3 id="一些数据分析思路">一些数据分析思路&lt;/h3>
&lt;h3 id="我学统计学的经历">我学统计学的经历&lt;/h3>
&lt;p>总结：少玩手机多读书、多琢磨，加以应用，在实践中学习。&lt;/p>
&lt;h2 id="关于课题申请或论文撰写的方法">关于课题申请或论文撰写的方法&lt;/h2>
&lt;p>这部分主要是本人根据课题标书、论文等撰写中的一些问题，将其总结，提出一些建议。&lt;/p>
&lt;h3 id="课题申请中的方法学描述">课题申请中的方法学描述&lt;/h3>
&lt;h3 id="课题申请中方法学的注意事项">课题申请中方法学的注意事项&lt;/h3>
&lt;h3 id="关于课题设计的一些问题">关于课题设计的一些问题&lt;/h3>
&lt;h3 id="论文撰写中的统计方法">论文撰写中的统计方法&lt;/h3></description></item><item><title>读书笔记之计量、统计必读的好文章50篇.md</title><link>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E8%AE%A1%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%BF%85%E8%AF%BB%E7%9A%84%E5%A5%BD%E6%96%87%E7%AB%A050%E7%AF%87/</link><pubDate>Sat, 23 Nov 1991 00:00:00 +0000</pubDate><guid>https://surprisedcat.github.io/studynotes/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E8%AE%A1%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%BF%85%E8%AF%BB%E7%9A%84%E5%A5%BD%E6%96%87%E7%AB%A050%E7%AF%87/</guid><description>
&lt;h2 id="读书笔记计量统计必读的好文章50篇">读书笔记计量、统计必读的好文章50篇&lt;!-- omit in toc -->&lt;/h2>
&lt;p>经管之家于2018年2月发表在微信公账号上的列出了50篇计量、统计领域必读的50篇好文章，本篇为50篇读书笔记的综合。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#一个外行的计量经济学学习之路">一个外行的计量经济学学习之路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#一位美国top10统计专业本科生的经验别走我这样的弯路">一位美国TOP10统计专业本科生的经验：别走我这样的弯路&lt;/a>&lt;/li>
&lt;li>&lt;a href="#牛人整理的统计学教材全">牛人整理的统计学教材（全）&lt;/a>&lt;/li>
&lt;li>&lt;a href="#原创国外流行的计量经济学著作-知多少">原创｜国外流行的计量经济学著作 知多少&lt;/a>&lt;/li>
&lt;li>&lt;a href="#jg系列no006写给统计学初学者的心得体会胖胖小龟宝">【JG系列•NO.006】写给统计学初学者的心得体会｜胖胖小龟宝&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="一个外行的计量经济学学习之路">一个外行的计量经济学学习之路&lt;/h2>
&lt;p>第一阶段：计量就是统计——很难很高深；第二阶段：学习统计/计量还得从实践中来，上课学习SPSS，根据需求学东西；不求甚解；第三阶段：查缺补漏——计量似乎也不是想象中的那么难，复习回顾，系统性再次学习，但是很多新的东西还是不会；第四阶段：躲不过了，硬着头皮也要上，学习真正难的东西，看了比较深的书；第五阶段：计量也不是想象中的那么难，完成学习再看遇到的问题轻松了很多。&lt;/p>
&lt;h2 id="一位美国top10统计专业本科生的经验别走我这样的弯路">一位美国TOP10统计专业本科生的经验：别走我这样的弯路&lt;/h2>
&lt;p>略，参考意义对我不大&lt;/p>
&lt;h2 id="牛人整理的统计学教材全">牛人整理的统计学教材（全）&lt;/h2>
&lt;p>整理了119本国外统计学教材，原文如下&lt;/p>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;amp;mid=2650200437&amp;amp;idx=2&amp;amp;sn=e974480697d64c5f62cfe93524b0a542&amp;amp;chksm=bed6bdf289a134e4e057b82a969b977efbf99cec3a6e9710e94a6c0f4ec388279c5155c92a5e&amp;amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;amp;mid=2650200437&amp;amp;idx=2&amp;amp;sn=e974480697d64c5f62cfe93524b0a542&amp;amp;chksm=bed6bdf289a134e4e057b82a969b977efbf99cec3a6e9710e94a6c0f4ec388279c5155c92a5e&amp;amp;scene=21#wechat_redirect&lt;/a>&lt;/p>
&lt;h2 id="原创国外流行的计量经济学著作-知多少">原创｜国外流行的计量经济学著作 知多少&lt;/h2>
&lt;p>同上一个，但是偏重经济学&lt;/p>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;amp;mid=2650200086&amp;amp;idx=3&amp;amp;sn=48cfb7b0b6c2701748e625f3c2ddec31&amp;amp;chksm=bed6bc9189a13587ff12326f81eebf2af2bb51ac83b0f63a0ef6e4e7eb11299f6d435a34b096&amp;amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;amp;mid=2650200086&amp;amp;idx=3&amp;amp;sn=48cfb7b0b6c2701748e625f3c2ddec31&amp;amp;chksm=bed6bc9189a13587ff12326f81eebf2af2bb51ac83b0f63a0ef6e4e7eb11299f6d435a34b096&amp;amp;scene=21#wechat_redirect&lt;/a>&lt;/p>
&lt;h2 id="jg系列no.006写给统计学初学者的心得体会胖胖小龟宝">【JG系列•NO.006】写给统计学初学者的心得体会｜胖胖小龟宝&lt;/h2></description></item></channel></rss>